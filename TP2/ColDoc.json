[
  {
    "keywords": [
      "Energy efficiency",
      "Green software",
      "Web browsers",
      "WebAssembly",
      "Eficiência energética",
      "Navegadores web",
      "Software Verde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "On the performance of WebAssembly",
    "autor": "Macedo, João Gonçalves de",
    "data": "2022-04-01",
    "abstract": "The worldwide Web has dramatically evolved in recent years. Web pages are dynamic, expressed by pro grams written in common programming languages given rise to sophisticated Web applications. Thus,\nWeb browsers are almost operating systems, having to interpret/compile such programs and execute\nthem. Although JavaScript is widely used to express dynamic Web pages, it has several shortcomings and\nperformance inefficiencies. To overcome such limitations, major IT powerhouses are developing a new\nportable and size/load efficient language: WebAssembly.\nIn this dissertation, we conduct the first systematic study on the energy and run-time performance\nof WebAssembly and JavaScript on the Web. We used micro-benchmarks and real applications to have\nmore realistic results. The results show that WebAssembly, while still in its infancy, is starting to already\noutperform JavaScript, with much more room to grow. A statistical analysis indicates that WebAssembly\nproduces significant performance differences compared to JavaScript. However, these differences differ\nbetween micro-benchmarks and real-world benchmarks. Our results also show that WebAssembly improved\nenergy efficiency by 30%, on average, and show how different WebAssembly behaviour is among three\npopular Web Browsers: Google Chrome, Microsoft Edge, and Mozilla Firefox. Our findings indicate that\nWebAssembly is faster than JavaScript and even more energy-efficient. Our benchmarking framework is\nalso available to allow further research and replication."
  },
  {
    "keywords": [
      "Amplitude amplification",
      "Heuristic",
      "Model-free",
      "Maximum finding",
      "Query complexity",
      "Amplificação de amplitude",
      "Complexidade de query",
      "Heurística",
      "Modelo livre",
      "Procura pelo máximo",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Quantum reinforcement learning: a heuristic approach to solve deterministic MDPs",
    "autor": "Brito, Renato Alberto Soares de",
    "data": "2022-02-13",
    "abstract": "This thesis works on Reinforcement Learning tree search and attempts to find the best\npossible sequence of actions the agent needs to execute to get the most reward while using\nless computational effort than by just applying a quantum maximum finding algorithm.\nTo achieve this we will use the property that makes it possible to limit our search space to\nthe elements that were marked by the oracle in Grover’s Algorithm, by marking a fourth of\nthe search space and following it with a quantum maximum finding subroutine. From this,\none of the marked elements is obtained and the information encoded in it is used to update\na probabilistic distribution stored in a classical memory.\nThe goal is to encounter the minimum amount of iterations of this process and compare the\nresults, i.e., percentage of success which is measured as the number of times the algorithm\nproduces a solution (element with maximum reward) and the number of queries used - with\na traditional quantum maximum finding procedure. If this is observed, it is also hypothe sized that the algorithm could be used to observe a step further into the future compared to\nthe traditional procedure, i.e., use the same or fewer queries to evaluate a larger number\nof sequences fruit of increasing the horizon of the episodes. The last hypothesis tests the\ndepth of the circuits, more specifically the number of gates used. If the algorithm evaluates\nshallower circuits than the quantum maximum finding, the approach can be applied on the\ncurrent quantum machines (NISQ) because the shallower circuits produces more error-proof\nmeasurements.\nThe results show that the proposed algorithm has no advantages compared to a traditional\nquantum maximum finding procedure due to using more queries to achieve the same rate\nof success which, consequently, invalidates the first and second hypothesis. For the third\nhypothesis, the gate complexity was not directly measured. Instead, was opted to measure\nthe number of queries used by circuit which might not be sufficient to conclude that the\nalgorithm uses shallower circuits."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "O impacto da aplicação de modelos de maturidade nas áreas clínicas do Sistema Nacional de Saúde",
    "autor": "Silva, Lara Correia e",
    "data": "2019-11-08",
    "abstract": "The effects of the rapid technological revolution occurring in our society are undeniable.\nIn the health area, the quick growth of Information Technologies has had a particular\nand striking impact as it has led to an urgent need to improve the health care provided to\nthe population. It is imperative that care delivery becomes an increasingly computerized\nprocess in order to facilitate not only the work of all health professionals, but the lives\nof all users.\nHowever, it is necessary that this phenomenon of clinical informatization is evaluated,\nin order to make it possible to determine the current state of health institutions, as\nmonitored, so that a path of gradual progression can be defined and followed, and\ninternal flows, processes and systems can be improved.\nIn Portugal, many initiatives have been implemented, such as the National Strategy\nfor the Health Information Ecosystem 2020, in particular the SNS Sem Papel. The\nobjective is to improve access to the National Health System and to expedite the sharing\nof clinical information by eliminating paper in hospital institutions. Obstacles and\nresistance to change can naturally occur as these initiatives are implemented.\nThus, the emergence of entities such as HIMSS Analytics, capable of creating maturity\nmodels that provide a clear and concise method, capable of helping institutions to\nachieve their goals, becomes crucial. Within the scope of this dissertation, two maturity\nmodels created by this entity, EMRAM and AMAM, were studied in order to understand\ntheir dynamics and scrutinize how they possibilitate the gradual improvement of the\nanalytics of the institutions and the progressive dematerialization of their systems, flows\nand processes."
  },
  {
    "keywords": [
      "Adversarial attacks",
      "FGSM",
      "DeepFool",
      "JSMA",
      "PGD",
      "Carlini",
      "Adversarial training",
      "Defensive distillation",
      "SSIM",
      "CIFAR-10",
      "GTSRB",
      "Ataques adversariais",
      "Treino adversarial",
      "Defesa via destilação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Exploring adversarial attacks and defenses",
    "autor": "Branco, Tiago Manuel Sampaio",
    "data": "2024-05-20",
    "abstract": "Deep Learning classifiers are capable of an outstanding performance. Yet, they are vulnera ble to adversarial attacks, i.e. it is possible to craft a slightly modified version of a correctly\nclassified image that, although its contents are still clearly recognisable to a human being,\nthe classifier outputs an incorrect classification.\nIn this thesis we evaluate the effectiveness of adversarial attacks, namely their trans ferability to other models, and some proposed defenses. Transferability occurs when an\nadversarial sample is crafted with a model, and it succeeds in achieving a misclassification\nin another model.\nTo make this study as comprehensive as possible, we explore several attack methods,\nnamely: Fast Gradient Sign Method (FGSM), Deepfool, Jacobian Saliency Map Attack\n(JSMA), Carlini, Projected Gradient Descent (PGD) and Few Pixels.\nTo evaluate the impact of the model’s architecture in the transferability rate we use sev eral common architectures: VGG16, three ResNet with different depths, and a small Con volution Neural Network. Two common datasets were used for evaluation: CIFAR-10 and\nGerman Traffic Sign Recognition Benchmark (GTSRB).\nDifferent attack methods use different approaches and parameters to craft adversarial\nsamples. Hence, it is not trivial to control the degree of perturbation. To be able to achieve\nthe same level of perturbation with every method we resorted to an image comparison\nmetric: Structural Similarity Index Measure (SSIM).\nFor each method we performed a search within its parameter space to find the parameters\nthat on average attain a specific level of perturbation. To evaluate the impact of the level of\nperturbation on transferability rates, we evaluate two different values for the SSIM metric.\nOur results show that while it is possible to craft an adversarial sample in a particular\nmodel, the transferability rates vary considerably from method to method.\nRegarding defensive methods we explored Adversarial Training and Defensive Distilla tion. The results show that the ability to prevent an adversarial attack, or robustness, varies\nsignificantly depending on the conditions that the attack is performed and on the defensive\nmethods used. Furthermore, there is a trade-off between robustness and accuracy, with\ndefensive models having lower accuracy than non-defended models."
  },
  {
    "keywords": [
      "Middleware",
      "Robot operating system",
      "Data acquisition system",
      "Sistema de Aquisição de Dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "ROS-based data acquisition system",
    "autor": "Sousa, Bruno José Infante",
    "data": "2022-05-16",
    "abstract": "Nowadays, we are living in a time where sensors and applications that take advantage\nof them are increasingly taking part of our daily lives. Thus, it is increasingly common\nto be surrounded by sensors, like image, sound or even luminosity or motion sensors,\namong several other types. In this context arises the challenge of how to connect sensors\nand applications that use them. The basic approach is to have each sensor bound to an\napplication with its own private interface. The desirable approach is the oposite: a sensor\nshould serve any application that requires its data using a well known interface. For this\npurpose a middleware solution is needed.\nNowadays, two trends are emerging in automotive industry: electrification and au tonomous driving. Both cases means more sensors and software to deal with these sensors.\nAlso in this context the concept of a middleware makes sense to connect sensors and ap plications. More specifically, in this dissertation it is shown how Robot Operating System\n(ROS) can be used to bridge the gap between the in-vehicle sensors and the applications\nthat process the data from those sensors. Through a distributed architecture, remote inter action between different components is possible, thus facilitating resources allocation and\nmanagement.\nThe project on which the work developed during this dissertation focuses, is part of the\nEasyRide Program, the result of a partnership between the University of Minho and Bosch."
  },
  {
    "keywords": [
      "614:681.3.06",
      "681.3.06:614"
    ],
    "titulo": "Extracção de conhecimento a partir do software Open-Source de Business Intelligence Pentaho em Unidade de Cuidados Intensivos",
    "autor": "Viana, Marta Alexandra Rolo Neiva",
    "data": "2012",
    "abstract": "As organizações de saúde têm como principal objectivo a prestação de\nserviços de qualidade à população, e a tomada de decisões de forma rápida\ne e caz é essencial para que tais objectivos sejam atingidos. Deste modo,\nneste sector, a adopção de ferramentas tecnológicas automatizadas que facilitam\neste processo tem vindo a aumentar ao longo dos anos. Neste contexto,\nsurge o conceito de Business Intelligence (BI) que auxilia a tomada de decisão\npor parte dos pro ssionais de saúde, uma vez que estes sistemas se baseiam\nna Extracção de Conhecimento (EC) gerado pelos sistemas de informação\ntransaccionais, sendo capazes de integrar uma enorme quantidade de dados\nprovenientes de diversas fontes, normalmente de bases de dados que se encontram\nem diferentes tecnologias, plataformas e totalmente desintegradas.\nAssim, ultrapassando-se a heterogeneidade das bases de dados, através da\nestruturação dos dados, extrai-se informação que permitirá atingir conhecimento\nimportante para as decisões clínicas.\nEspeci camente, a Unidade de Cuidados Intensivos (UCI) de um hospital\né a unidade mais cara e que mais recursos exige, de tal forma que os sistemas\nde BI podem desempenhar um papel preponderante não só na racionalização\ndos custos, mas também na melhoria da qualidade dos cuidados prestados,\natravés da monitorização dos dados clínicos dos pacientes. Deste modo, este\nprojecto pioneiro incidiu na análise da aplicação do Pentaho, um software\nOpen-Source (OS) de BI, nos processos de EC a estas unidades hospitalares,\ntendo como fonte os dados dos pacientes de um hospital localizado no Norte\nde Portugal, avaliando o conhecimento obtido e o seu impacto na tomada de\ndecisão.\nEste software disponibiliza ferramentas que analisam, sintetizam, assimilam\ne dão sentido às enormes quantidades de informação, sendo capaz\nde estabelecer ligações so sticadas e discernir padrões, dando oportunidade\npara tirar conclusões e agir de forma preventiva."
  },
  {
    "keywords": [
      "FFT",
      "GPGPU",
      "GLSL",
      "cuFFT",
      "Performance",
      "Compute",
      "Cooley-Tukey",
      "Stockham",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "High performance fourier transforms on GPUs with GLSL",
    "autor": "Mota, Jorge Francisco Teixeira Bastos da",
    "data": "2023-02-24",
    "abstract": "The Fast Fourier Transform is a family of algorithms indispensable for the computation of the Discrete Fourier\nTransform. As a result, these transforms are the core of many applications in several areas and are required to be\ncomputed efficiently in many scenarios.\nThe continuous evolution of GPUs has increased the popularity of parallelizable algorithm implementations on\nthis type of hardware. Traditionally GPUs were associated to graphics background, however, with the popularization\nof the compute functionality of this hardware, most modern GPUs now have this capability, hence, algorithms\nnow are more likely to be implemented in the general-purpose compute pipeline of GPUs. As a result, many\napplications take advantage of compute programming in GPGPU-capable frameworks such as GLSL, a high-level\nshading language frequently used in the context of computer graphics.\nIn this dissertation we provide, refine and compare GPU-driven implementations of the family of FFT algorithms\nin GLSL, with the goal to provide programmers with efficient and simplified compute kernels for this transform,\nfrom the classic Cooley-Tukey algorithm to more suitable algorithms for the GPU such as the Stockham algorithm\nwith higher radix.\nAccordingly, we also use the cuFFT NVIDIA framework for reference in the comparisons of the GLSL algorithms\nimplementations with the goal to analyse their significance on the tradeoff of using specialized implementations of\nthe FFT algorithms or integrating dedicated software tools for any case of application.\nFinally, we demonstrate how all improvements discussed in this dissertation culminate in performance improvement in a real-time rendering technique that heavily depends on multiple of these transforms in the Nau3D engine\nas a case of study."
  },
  {
    "keywords": [
      "Processamento de Linguagem Natural (PNL)",
      "Linguagem natural (LN)",
      "Aprendizagem Automática (AA)",
      "Aprendizagem Profunda (AP)",
      "Inteligência Artificial (IA)",
      "Processamento em Lote (PL)",
      "Natural Language Processing (NLP)",
      "Natural Language (NL)",
      "Machine Learning (ML)",
      "Deep Learning (DL)",
      "Artificial Intelligence (AI)",
      "Batch Processing (BP)",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Multilanguage chatbot with artificial intelligence for client support",
    "autor": "Sá, João Miguel Santos",
    "data": "2024-06-28",
    "abstract": "A inteligência artificial surgiu nos anos 50, com a criação de um espaço de estudo com o objetivo principal\nde desenvolver máquinas inteligentes. A sua evolução foi abrupta e hoje existem mesmo programas que,\nao escreverem um conjunto de palavras, conseguem gerar imagens surpreendentes com base no que\nfoi escrito em segundos, algo que um pintor famoso faria, mas que demoraria horas ou mesmo dias a\nconcluir.\nEm vários setores empresariais, desde a educação aos cuidados de saúde, entre outros, tem havido\num aumento notável do interesse e da utilização de chatbots. Esta tendência crescente abriu caminho\npara a implementação de chatbots em diversas áreas. Especificamente, estes chatbots servem como\nintermediários informativos em empresas como a Retail Consult, uma empresa de tecnologia especializada\nem desenvolvimento de software na área do retalho e na utilização de processamento em lote, que envolve\no tratamento de grandes volumes de dados.\nAlém disso, o sistema de chatbot foi concebido para reconhecer e compreender uma multiplicidade de\nlínguas, incluindo o inglês, que será a língua principal do utilizador, bem como o português e o espanhol,\nassegurando uma comunicação na língua preferida do utilizador. Esta capacidade multilingue é particu larmente crucial para a Retail Consult, dada a sua presença global com escritórios espalhados por vários\npaíses. Sem dúvida, esta versatilidade linguística acrescenta um valor imenso ao produto. Para conseguir\neste reconhecimento linguístico, foram utilizadas técnicas de inteligência artificial e de processamento da\nlinguagem natural, que foram implementadas através de um quadro bem estruturado concebido para a\ncriação de chatbots e de software similar, incluindo assistentes virtuais.\nO desenvolvimento de um serviço com estas características está preparado para aumentar significa tivamente a produtividade interna da organização. Por exemplo, um analista de sistemas pode perguntar\ndiretamente ao chatbot o que pretende, poupando tempo que, de outra forma, teria sido gasto na procura\ne consulta de informações à base de dados. Quer se trate de determinar o número de trabalhos em\nexecução ou de identificar os que apresentam falhas, este sistema procura simplificar as operações e\nmelhorar a eficiência dentro da empresa. Com base no trabalho apresentado na introdução, onde discutimos os papéis cruciais do processa mento em lote e da integração do chatbot no ambiente em específico, agora aprofundamos a dinâmica do\nsetor de retalho nesta análise minuciosa. O objetivo está centrado na criação de um chatbot versátil capaz\nde comunicar com os utilizadores em várias línguas, fornecer respostas precisas e, acima de tudo, auxiliar\nna tradução de idiomas. Além disso, os resultados confirmam a competência do chatbot na identificação\ne tradução precisa de idiomas, com base em nossa análise avançada, onde investigamos a importância\ndo processamento em lote e introduzimos a estrutura de IA conversacional RASA.\nEsses elementos, contribuem para uma experiência mais envolvente e satisfatória para o utilizador.\nOu seja, este trabalho permitiu e ampliou o nosso conhecimento sobre a interação entre processamento\nem lote, tecnologia de chatbot e comunicação multilíngue no contexto do retalho."
  },
  {
    "keywords": [
      "Redes oportunistas",
      "Dispositivos móveis",
      "Oportunidades de contacto",
      "Big data",
      "Opportunistic networks",
      "Mobile devices",
      "Contact opportunities",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Avaliação de desempenho de redes oportunistas",
    "autor": "Ramalho, Luís Paulo de Sousa",
    "data": "2016",
    "abstract": "Nos últimos anos, a grande utilização de dispositivos móveis tem levado a um considerável aumento de interesse da comunidade de investigação e da indústria na área das redes oportunistas. Nestas redes, os dispositivos móveis aproveitam a sua mobilidade para trocarem dados entre si. Uma vez que estes dispositivos são principalmente transportados por humanos, surgem oportunidades de contacto aquando da proximidade física entre eles, permitindo essa troca de dados. É por esta razão que os investigadores têm vindo a explorar a mobilidade humana, aplicando-a ao estudo destas redes.\nNeste contexto surge o objetivo principal deste trabalho que visa explorar a disponibilidade de dados reais sobre o movimento de pessoas para avaliar o desempenho de redes oportunistas, considerando que a análise de traces Wi-Fi pode fornecer informações importantes sobre o assunto, em particular, para estimar o atraso e a largura de banda. Entre outros, um dos grandes desafios a abordar é a grande quantidade de dados a analisar (big data), onde serão consideradas técnicas para analisar esses grandes volumes de dados de forma eficiente.\nNesse sentido foram desenvolvidos algoritmos que permitissem avaliar o desempenho destas redes. Um primeiro capaz de gerar logs RADIUS sintéticos num ambiente controlado, onde se pode configurar a simulação com os valores que se pretendam. A sua validação possibilitou constatar a eficiência do mesmo, permitindo gerar corretamente uma grande quantidade de registos sintéticos e num curto espaço de tempo. Um segundo algoritmo que possibilita extrair encontros de uma lista de registos preexistente. A validação deste algoritmo também permitiu verificar que é possível extrair uma grande quantidade de encontros de forma simples e rápida. Por fim, um terceiro algoritmo que visa o encaminhamento epidémico de mensagens a partir de um conjunto de encontros preexistente e através da utilização de duas diferentes estratégias de transmissão de mensagens.\nA partir dos resultados alcançados nas experiências realizadas com o terceiro algoritmo, foi possível constatar-se que de facto as DTNs caracterizam-se pelas suas baixas taxas de entrega. Apesar disso, face ao protocolo aplicado nas experiências realizadas, esperavam-se taxas mais elevadas. Foi possível igualmente constatar-se que a taxa de entrega a alcançar é influenciada pela estratégia de transmissão de mensagens que se adote, uma vez que foi sempre superior quando se utilizou a segunda estratégia. Por outro lado, através dos resultados obtidos foi também possível concluir que a variação do tamanho da mensagem não se reflete na taxa de entrega alcançada. Os resultados também confirmaram o facto do protocolo epidémico requerer uma enorme quantidade de recursos, dada à excessiva quantidade de transmissões que efetua e ao considerável espaço de armazenamento que as stations necessitam ter. Assim, dado ao facto das stations serem nós móveis e possuírem baterias relativamente limitadas, é de prever que tendam a ficar sem energia passado algum tempo.\nAo nível do atraso, os resultados mostraram que os valores médios e máximos dependeram sempre da estratégia de transmissão de mensagens que se utilizou. Por outro lado, não se verificou uma dependência em relação ao tamanho das mensagens geradas na rede, uma vez que não existiu uma tendência visível nos resultados ao variar o tamanho das mesmas."
  },
  {
    "keywords": [
      "Sampling",
      "Quality of service",
      "Long-range dependence",
      "Análise seletiva",
      "Qualidade de serviço",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Evaluating the impact of traffic sampling in network analysis",
    "autor": "Mendes, João Emanuel da Silva",
    "data": "2022-04-08",
    "abstract": "The sampling of network traffic is a very effective method in order to comprehend the\nbehaviour and flow of a network, essential to build network management tools to control\nService Level Agreements (SLAs), Quality of Service (QoS), traffic engineering, and the\nplanning of both the capacity and the safety of the network.\nWith the exponential rise of the amount traffic caused by the number of devices connected\nto the Internet growing, it gets increasingly harder and more expensive to understand the\nbehaviour of a network through the analysis of the total volume of traffic. The use of\nsampling techniques, or selective analysis, which consists in the election of small number of\npackets in order to estimate the expected behaviour of a network, then becomes essential.\nEven though these techniques drastically reduce the amount of data to be analyzed, the fact\nthat the sampling analysis tasks have to be performed in the network equipment can cause a\nsignificant impact in the performance of these equipment devices, and a reduction in the\naccuracy of the estimation of network state.\nIn this dissertation project, an evaluation of the impact of selective analysis of network\ntraffic will be explored, at a level of performance in estimating network state, and statistical\nproperties such as self-similarity and Long-Range Dependence (LRD) that exist in original\nnetwork traffic, allowing a better understanding of the behaviour of sampled network traffic."
  },
  {
    "keywords": [
      "Sistemas de data warehousing",
      "Sistemas de povoamento de data warehouses  (ETL)",
      "Modelação de sistemas ETL",
      "Geração de esqueletos ETL",
      "YAWL",
      "Kettle",
      "Informática",
      "Talend",
      "Data warehousing systems",
      "Systems to populate data warehouses (ETL)",
      "ETL modeling  systems",
      "ETL skeleton generation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Transformação de especificações ETL em YAWL para processos  Kettle",
    "autor": "Freitas, Janine Marlene Duarte Silva",
    "data": "2019-12-23",
    "abstract": "YAWL é uma linguagem gráfica para a especificação de processos, com uma semântica bem definida, \nque permite o desenho, especificação, simulação e validação de sistemas, cujos processos a modelar \nexijam características específicas de comunicação, concorrência e sincronização entre si. A YAWL é \nbaseada, por um lado, em padrões bem definidos de fluxo de trabalho e, por outro, nas conhecidas \nRedes de Petri Coloridas. Devido às suas características, vários estudos utilizaram esta linguagem na \nmodelação de sistemas de ETL (Extract-Transformation-Load), tentando facilitar e agilizar todo este \nprocesso. Este trabalho de dissertação teve como objetivo desenvolver e implementar um sistema \nde geração de “esqueletos” para sistemas de ETL a partir de um conjunto de especificações YAWL. \nNesse sentido, foi idealizado e desenvolvido um sistema capaz de representar e produzir de forma \nsemi-automática a configuração de processos ETL para várias ferramentas de implementação de \nprocessos ETL, nomeadamente Kettle, Informatica e Talend, todas elas ferramentas conceituadas \nno mercado dos sistemas de povoamento de data warehouses. Nesta dissertação apresentamos e \ndescrevemos tal sistema, desde as suas fases de fundamentação e conceptualização até às suas \nfases de teste e exploração."
  },
  {
    "keywords": [
      "Autonomous driving",
      "Artificial intelligence",
      "Machine learning",
      "Computer vision",
      "Edge computing",
      "Condução autónoma",
      "Inteligência artificial",
      "Aprendizagem de máquina",
      "Visão por computador",
      "Computação de borda",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automatic driving: 2D detection and tracking using artificial intelligence techniques",
    "autor": "Pinto, José Miguel Fernandes Madeira",
    "data": "2022",
    "abstract": "Road accidents are estimated to be the cause of millions of deaths and tens of millions of injuries every year. For this reason, any measure that reduces accidents' probability or severity will save lives.\nSpeeding, driving under the influence of psychotropic substances and distraction are leading causes of road accidents. Causes that can be classified as human since they all come from driver errors. Autonomous driving is a potential solution to this problem as it can reduce road accidents by removing human error from the task of driving.\nThis dissertation aims to study Artificial Intelligence techniques and Edge Computing networks to explore solutions for autonomous driving. To this end, Artificial Intelligence models for detecting and tracking objects based on Machine Learning and Computer Vision, and Edge Computing networks for vehicles were explored.\nThe YOLOv5 model was studied for object detection, in which different training parameters and data pre-processing techniques were applied. For object tracking, the StrongSORT model was chosen, for which its performance was evaluated for different combinations of its components. Finally, the Simu5G simulation tool was studied in order to simulate an edge computing network, and the viability of this type of network to aid autonomous driving was analysed."
  },
  {
    "keywords": [
      "Base de dados",
      "Distribuído",
      "Concorrência",
      "Consistência",
      "Transação",
      "Database",
      "Distributed",
      "Concurrency",
      "Consistency",
      "Transaction",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "High performance data processing",
    "autor": "Faria, Nuno Filipe Pinto",
    "data": "2020-12-22",
    "abstract": "À medida que as aplicações atingem uma maior quantidade de utilizadores, precisam de processar uma crescente quantidade de pedidos. Para além disso, precisam de muitas vezes satisfazer pedidos de utilizadores de diferentes partes do globo, onde\nas latências de rede têm um impacto significativo no desempenho em instalações\nmonolíticas. Portanto, distribuição é uma solução muito procurada para melhorar a\nperformance das camadas aplicacional e de dados. Contudo, distribuir dados não é\numa tarefa simples se pretendemos assegurar uma forte consistência. Isto leva a que\nmuitos sistemas de base de dados dependam de protocolos de sincronização pesados,\ncomo two-phase commit, consenso distribuído, bloqueamento distribuído, entre outros,\nenquanto que outros sistemas dependem em consistência fraca, não viável para alguns\ncasos de uso.\nEsta tese apresenta o design, implementação e avaliação de duas soluções que\ntêm como objetivo reduzir o impacto de assegurar garantias de forte consistência\nem sistemas de base de dados, especialmente aqueles distribuídos pelo globo. A\nprimeira é o Primary Semi-Primary, uma arquitetura de base de dados distribuída\ncom total replicação que permite que as réplicas evoluam independentemente, para\nevitar que os clientes precisem de esperar que escritas precedentes que não geram\nconflitos sejam propagadas. Apesar das réplicas poderem processar tanto leituras\ncomo escritas, melhorando a escalabilidade, o sistema continua a oferecer garantias de\nconsistência forte, através do envio da certificação de transações para um nó central.\nO seu design é independente de modelos de dados, mas a sua implementação pode\ntirar partido do controlo de concorrência nativo oferecido por algumas base de dados,\ncomo é mostrado na implementação usando PostgreSQL e o seu Snapshot Isolation.\nOs resultados apresentam várias vantagens tanto em ambientes locais como globais. A\nsegunda solução são os Multi-Record Values, uma técnica que particiona dinâmicamente\nvalores numéricos em múltiplos registros, permitindo que escritas concorrentes possam\nexecutar com uma baixa probabilidade de colisão, reduzindo a taxa de abortos e/ou\ncontenção na adquirição de locks. Garantias de limites inferiores, exigido por objetos\ncomo saldos bancários ou inventários, são assegurados por esta estratégia, ao contrário\nde muitas outras alternativas. O seu design é também indiferente do modelo de dados,\nsendo que as suas vantagens podem ser encontradas em sistemas SQL e NoSQL, bem\ncomo distribuídos ou centralizados, tal como apresentado na secção de avaliação."
  },
  {
    "keywords": [
      "Biomedical text mining",
      "Patents",
      "Information retrieval task",
      "Optical character recognition",
      "@note2",
      "Mineração de textos biomédicos",
      "Patentes",
      "Obtenção de informação",
      "Reconhecimento ótico de caracteres",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of text mining tools for information retrieval and extraction from patents",
    "autor": "Alves, Tiago Alexandre Pinto",
    "data": "2016",
    "abstract": "Biomedical literature is composed of a large and ever increasing number of publications, written in natural language. Patents are a relevant fraction of these publications, considered important sources of information due to all the curated information available in the documents, from the granting process. Although being real technological libraries, their unstructured data turns the search of information within these documents a challenging task. Biomedical text mining is a scientific field that explores this task, creating methodologies to search and structure the information in the biomedical literature.\nInformation retrieval is one of the biomedical text mining tasks, in which the relevant information is obtained from an extensive collection of documents using several text retrieval methodologies. Getting all the information available on a patent document requires the download of the respective PDF document, that is then converted into a machine-readable text by technologies as Optical Character Recognition (OCR).\nIn this project, an information retrieval, and a PDF to text conversion system were developed building a “patent pipeline” which was integrated into @note2, an open-source computational framework for biomedical text mining. The patent pipeline can be disintegrated into four different tasks: the patent search, the retrieval of patent metadata, the retrieval of their PDF files, and the extraction of all the information from these documents.\nA set of patents from the BioCreative V CHEMDNER task was used to test the developed pipeline, evaluating the framework performance and the real capacity to retrieve the requested patents and extract their unstructured information. The results were promising, bringing to the scientific community the published patent information and allowing the posterior implementation of other biomedical text mining processes over these documents."
  },
  {
    "keywords": [
      "681.3.06"
    ],
    "titulo": "Application of Formal Methods in the ITASAT Project",
    "autor": "Quinta, Daniel Ribeiro",
    "data": "2013-07-30",
    "abstract": "Critical software can be potentially dangerous if not well verified, leading to serious failures. Accordingly, there is a need for improved validation and verification methods in order to have guarantees about the software final product. The aim of this project is to define a more linear and organized verification and validation plan to, formally, verify the most critical parts of the OBDH (On-Board Data Handling) subsystem of ITASAT, supported by the Alloy formal language.\n\nAlloy supports the description of systems whose state involves complex relational structure. The application of Alloy and Alloy Analyzer was motivated by the need for a formal specification that is more closely tailored to state-machines, and more amenable to automatic analysis. Structural and behavioural properties are described declaratively, by conjoining relations and constrains, making it possible to develop and analyze a model incrementally. Due to the high cost of using these methods, they are mainly used in the development of high-critical software where safety and security are crucial. \n\nThis dissertation presents a set of guidelines for analysis and modelling of software systems which support the creation of a formal model and allow some extra behaviours such as synchronization, interruptions and flags. A new tool, ModelMaker, was developed in order to create models using these guidelines in a more interactive way."
  },
  {
    "keywords": [
      "“Sticky policies”",
      "Sistema criptográfico",
      "PKE",
      "IBE",
      "ABE",
      "PRE",
      "Controlo de acessos",
      "DAC",
      "MAC",
      "ABAC",
      "RBAC",
      "Linguagem de políticas",
      "XACML",
      "XrML",
      "EPAL",
      "Cryptographic system",
      "Access control",
      "Policy language",
      "681.3:658.0",
      "658.0:681.3",
      "681.188",
      "681.3-7"
    ],
    "titulo": "Reforço da privacidade através do controlo da pegada digital",
    "autor": "Macedo, Ricardo Joaquim Pereira de",
    "data": "2013",
    "abstract": "Atualmente existe ainda uma relação assimétrica entre os utilizadores e os fornecedores\nde serviços disponibilizados pela internet. É prática comum, aquando da apresentação de um\nserviço, que o utilizador seja questionado sobre a aceitação, ou não, de um conjunto de políticas\nreferentes ao uso de informação privada facultada ao fornecedor (por exemplo, a morada,\no número de telefone, preferências, etc...). Geralmente os utilizadores aceitam a política com\nbase na confiança que têm no fornecedor e/ou no contrato formal que lhes é apresentado. Os\ncasos de violação de privacidade por parte de alguns fornecedores de serviços, vendendo ou\nfacultando informação privada sobre os seus clientes a outros, são amplamente conhecidos e resultam\nem grande medida da falta de controlo que os utilizadores finais têm sobre a informação\nque entregam aos fornecedores.\nEste problema também tem grande impacto no ambiente empresarial. Quase toda a informação\nde uma organização é guardada em claro. Mesmo que esta seja guardada num local seguro,\naqueles que conhecerem bem o sistema poderão ter indevidamente acesso a informação privada\nda organização. Além disto, se a organização for alvo de um ataque informático e o atacante\nconseguir aceder aos dados poderá consulta-los livremente.\nNeste trabalho propomos a implementação de um mecanismo que possibilite o envio de informações\nsem que o utilizador tenha necessidade de confiar no local onde as mesmas serão\narmazenadas, através da utilização do conceito de “sticky policies”. Através da utilização de\ntécnicas criptográficas, é estabelecido um vínculo entre a informação cifrada e as políticas de\nacesso à informação. O sistema desenvolvido garante que, para um terceiro aceder às informações\npessoais de um utilizador, terá que cumprir o conjunto de regras definidas pelo dono da\ninformação.\nVisto que um utilizador autorizado a aceder às informações pode ter um comportamento\nincorreto, partilhando indevidamente as informações, propomos também adicionar mecanismos\nde auditoria dos acessos à informação gerida pelo sistema."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Análise de segurança para soluções de software para a cloud",
    "autor": "Rocha, João Diogo Pereira da",
    "data": "2017",
    "abstract": "A gestão do ciclo de vida de produtos informáticos é um processo contínuo que, nos\ncasos de maior sucesso, se prolonga por períodos que ultrapassam duas décadas e se transforma\nnum aspeto crítico à atividade das empresas que desenvolvem e comercializam esses\nprodutos. A evolução natural de um produto deste tipo resulta de estímulos previsíveis\nassociados à inovação tecnológica nas plataformas computacionais, como a cada vez maior\nutilização de plataformas móveis, à melhoria de funcionalidades existentes e à introdução\nde características novas. Incorpora também estímulos menos previsíveis, tais como alterações\nnos modelos de negócio resultantes de mudanças nos paradigmas de prestação de\nserviços, como a migração para o modelo Cloud.\nNeste contexto, as implicações para a gestão da segurança da informação das alterações\ngradualmente introduzidas nem sempre são claras, e é comum as equipas de desenvolvimento\nnão estarem preparadas para lidar com a diversidade de novos riscos que surgem\ncom estas mudanças.\nDeste modo, e na perspetiva de combater as possíveis falhas supra referidas, pretende-se\ncom este projeto efetuar uma análise de segurança de produtos de software para a Cloud\nda Primavera bss , identificando potenciais riscos e ameaças de segurança e apresentando\nsoluções viáveis para os mitigar."
  },
  {
    "keywords": [
      "Artificial intelligence in medicine",
      "Clinical decision support system",
      "Clinical pratice guidelines",
      "Combining clinical pratice guidelines",
      "Computer-interpreter guidelines",
      "Multi criteria decision analysis",
      "Analise de decisão com múltiplos critérios",
      "Combinação de protocolos clínicos",
      "Computer-interpreter guidelines",
      "Inteligência artificial em medicina",
      "Protocolos clínicos",
      "Sistemas de apoio à decisão clínica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Conflict resolution in clinical treatments",
    "autor": "Oliveira, Pedro José Costa de",
    "data": "2019-12-23",
    "abstract": "Currently, in the health area, there is a need for systems that provide support for the decision of health professionals through specific recommendations for each patient based\non Clinical Practice Guidelines (CPGs) for automatic interpretation. CPGs are documents that have enormous importance in the daily life of health professionals, playing a\nkey role in reducing variations in medical practice, improving the quality of health care,\nand reducing health care costs. These documents reflect knowledge about how best to\ndiagnose and treat diseases in the form of a list of clinical recommendations.\nHowever, there may be conflicts and interactions in the application of these clinical\nrecommendations, that which in their maximum exponent may impair the patient’s\nclinical condition. These conflicts are transported to decision support systems, creating\nthe need to develop computational methods to solve these same conflicts. In the case of\nmultimorbid patients, this resolution of conflicts can be very problematic because these\npatients suffer from several pathologies at the same time, and that the use of a drug for\none particular pathology may have a detrimental effect on the application of another\ndrug in another pathology.\nTherefore, the objective of this dissertation topic is the determination of conflicts and\ninteractions between drugs and the determination of these same alternatives."
  },
  {
    "keywords": [
      "Program comprehension",
      "Software visualization",
      "Python-tutor",
      "Graphs",
      "Software animation",
      "Compreensão de programas",
      "Visualização de programas",
      "Grafos",
      "Animação de programas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Python-tutor on program comprehension",
    "autor": "Soares, Diogo Filipe Lopes",
    "data": "2020-12-22",
    "abstract": "The time spent analysing a software with the goal of comprehending it is huge and\nexpensive. Reduce the time necessary to a professional understand a program is essential\nfor the advance of technology. Therefore, the program comprehension has always been\nan area of interest as realizing how a programmer thinks can help facilitate many of their\ndaily activities, making the developer a more productive worker. As the world begins to\nreshape itself thanks to the advances of technology, this area of research gains more and\nmore relevance. This project aim to study the tools developed within the comprehension of\nprograms that usually are associated to software maintenance and analysing the animation\nweb tool Python-Tutor. After this study, it’s required to explore Python-Tutor to understand\nhow it can be improved with the addition of important features to program comprehension\nas Control Flow Graph (CFG), Data Flow Graph (DFG), Function Call Graph (FCG) and\nSystem Control Graph (SCG). The idea behind this is to allow new programmers to view\ntheir programs and create a visual image of them in order to understand them and improving\ntheir skills to understand someone else’s programs."
  },
  {
    "keywords": [
      "Interfaces com o utilizador",
      "Prototipagem",
      "Modelação e análise formais",
      "User interfaces",
      "Prototyping",
      "Formal analysis and modeling",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Gerador de protótipos de interfaces gráficas para o IVY Workbench",
    "autor": "Araújo, João Miguel Matela Aidos Manso de",
    "data": "2020-01-09",
    "abstract": "A interface de um programa é um elemento importante na experiência que o utilizador tem\ncom o software, pois constitui o principal método de interação com a lógica do programa.\nA existência de métodos fiáveis de verificação de sistemas de software permite o a conceção\ndestes de acordo com a especificação e, em casos mais críticos, evitar erros com consequências graves. Estes métodos rigorosos, no entanto, contrastam com a prática mais comum no\ndesenho de interfaces. Um dos métodos mais utilizados para o desenho e avaliação de interfaces é a prototipagem. Os protótipos permitem transmitir aspetos do design da interface\ne até avaliar a sua usabilidade, mas não oferecem as garantias sobre o seu funcionamento\nque os métodos de verificação oferecem.\nO IVY Workbench é uma ferramenta que suporta a modelação do comportamento de sistemas interativos e a verificação formal dos mesmos. A ferramenta contém um conjunto de\nplugins que suportam o processo de modelação e análise, incluindo um editor de modelos,\num verificador de propriedades e um animador. Este último permite visualizar e interagir\ncom os modelos, mas não suporta associá-los a mockups representativos das interfaces.\nA interação com os modelos facilita a sua validação por parte de quem os está a desenvolver. Não facilita, no entanto, a comunicação com os potenciais clientes do sistema modelado,\npara quem um protótipo será um meio mais eficaz de comunicação.\nNeste documento propõe-se uma solução para o problema acima, assente no desenvolvimento de um novo plugin capaz de suportar a construção e animação de protótipos de\nsistemas interativos modelados no IVY. É descrito todo o processo de desenvolvimento,\ndesde o levantamento de requisitos, até exemplos de aplicação que permitem demonstrar\nas novas funcionalidades existentes."
  },
  {
    "keywords": [
      "Agendamento",
      "Desenvolvimento Full Stack",
      "Interoperabilidade",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Plataforma de agendamento em ambiente hospitalar",
    "autor": "Chaves, António Jorge Monteiro",
    "data": "2021-08-10",
    "abstract": "Desde a sua génese, os Sistemas de Informação Hospitalar (SIH) tem proporcionado um conjunto de métodos e ferramentas inovadoras que tem contribuído significativamente para o aumento da produtividade e eficiência dos processos hospitalares e, bem assim, para o incremento da qualidade dos serviços. Com efeito, nos dias que correm, a sua utilização na área da saúde é mais do que uma simples funcionalidade, é uma necessidade. Em particular, no centro da organização de qualquer unidade hospitalar, o agendamento representa um dos processos que maiores benefícios pode tirar da implementação e evolução tecnológica dos sistemas de informação, sobretudo quando estes possuem como desígnio principal a melhoria das condições dos serviços prestados aos utentes. É neste contexto que surge a presente dissertação, a qual possui como objetivo primordial o estudo e enquadramento da utilização de ontologias de última geração aplicadas no desenvolvimento de SIH, com especial enfoque na criação de novos conceitos de agendamento de pedidos, suportados pelo desenvolvimento de uma plataforma assente em mecanismos e ferramentas inovadoras. Para além disso, o desenvolvimento e implementação da plataforma em apreço pretendeu também contribuir para a otimização de fluxos de agendamento, através da simplificação de operações convencionais e da introdução de novas funcionalidades."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "GSD: a web application for teacher timetable management - dissertation report",
    "autor": "Reis, João",
    "data": "2020-06-16",
    "abstract": "This document presents a Masters Thesis in Software Engineering, in the area of Academic\nManagement, Support Tools and Web Software.\nIn a given higher education institution, the teaching service is assigned by it’s department\ntwice a year, following a model of their own self-re-creation.\nInitially, it is intended to standardize the information in a transversal way to all the\ndepartments and create a model that will serve as the basis for the implementation of a web\napplication.\nThis application will allow it’s users to enter and verify information, generating database\nentries in a DSL of their choice (such as SQL), feeding a timetable construction platform."
  },
  {
    "keywords": [
      "Recommender system",
      "Data analysis",
      "Software architecture",
      "e-commerce",
      "Business intelligence",
      "Sistema de recomendação",
      "Análise de dados",
      "Arquitetura de software",
      "Comércio eletrônico",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Data analysis and recommender system architecture for e-commerce platforms",
    "autor": "Cunha, Gil Fernando Ferreira da",
    "data": "2021-02-22",
    "abstract": "E-commerce is constantly expanding, leading to greater market competitiveness. The number of\nonline platforms offering products or services is increasing; so there is a growing need for companies\nto stand out from the competition, which leads to the application of various marketing strategies.\nHowever, not all are adequate and mismanagement, as well as a bad investment of these strategies,\nmay prejudice companies.\nHence the implementation of recommendation systems in e-commerce platforms, as a safe and\neconomical strategy. By investing in a good recommendation mechanism, one can provide better\nuser experience, taking his interests into account. As a result, more traffic on the platforms is\nensured, which may result in a higher sales rate and, consequently, a higher number of revenues.\nHowever, to develop a recommendation system, the first step must consist in obtaining information\nabout the sales platform, where data about its users and products/services form the basis of recom mendations. But not all information is useful, which can influence the accuracy of the forecasting\nmodels used by the system to produce results.\nFollowing this perspective, a data analysis methodology is proposed, as well as an architecture of\na recommendation system, which allows to extract and treat relevant data, in order to integrate a\nrecommendation engine for most e-commerce platforms."
  },
  {
    "keywords": [
      "Brain-computer interface",
      "Electroencephalogram",
      "Falls recognition",
      "Pertubation-evoked potential",
      "Deep learning",
      "Interface cérebro-computador",
      "Eletroencefalograma",
      "Reconhecimento de queda",
      "Potencial evocado de perturbação",
      "Aprendizagem profunda",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Decoding human movement intentions and postural reactions through brain signals",
    "autor": "Neto, Raimundo Nonato Barros",
    "data": "2023-04-12",
    "abstract": "Falls are one of the most common causes of injuries in the elderly population. As a result, treatment costs have\nalso increased. Recent efforts to restore lower limb function in these populations have seen an increase in the\nuse of wearable robotic systems, however, fall prevention measures in these systems require early detection of\nloss of balance to be effective. In short, the development of technologies, such as a brain-computer interface, that\nis capable of recognizing situations at risk of falling based on the loss of balance caused by several factors, is\nessential. Previous studies have investigated whether kinematic variables contain information about an impending\nfall, but few have examined the potential of using electroencephalography (EEG) as a predictor of falling and how\nthe brain responds to prevent a fall. Perceived disturbances of balance are always accompanied by a specific\ncortical activation, called disturbance-evoked potential (PEP).\nIn this study, the recognition of daily activities (walking, lifting, crouching, going up and down stairs) was also\npart of the initial objective, however, due to the challenges encountered, the object of study of the present work\nwas focused on the recognition and binary classification of the presence of loss of balance (PEPs) in brain signals.\nThus, this dissertation intends to take the first steps toward the decoding of brain activity in response to imbalanced\nevents. Initially, to acquire the data, an experimental protocol was designed, so that the participants, using EEG,\nwere submitted to gliding-like perturbations while walking on the treadmill. Two healthy subjects were exposed to\na glide-like perturbation, and these perturbations occurred interspersed over a period lasting 30 to 60 seconds.\nEach subject performed 2 experiments, that is, perturbations provoked while the individual walked on the treadmill:\ni) at a speed of 1.6 km/h and ii) at a speed of 2.5 km/h.\nBased on the approached methods, the perturbation evoked potential (PEP) components were found between\n70-155 ms after the onset of the external perturbation. To decode pre-processed EGG data, four (4) artificial neural\nnetworks were tested and different network architecture parameters and electrode layouts were compared. Overall,\nthe convolutional neural network trained to predict EEG balance disturbances had a far superior classification\nperformance than the other architectures, whose mean accuracy was 91.51 ˘ 2.91%, using a short window\nlength of 200 ms. The electrode layout composed of 5 channels (Fz, C3, Cz, C4, and Pz) presented the shortest\nexecution time to train the model, whose average value was 196 ˘ 44.24ms. In addition, it was possible to verify\nthat the use of a single electrode (Cz) obtained satisfactory precision results (86.47 +/- 0.03%). These discoveries\nmay contribute to the development of a system capable of detecting equilibrium disturbances in real-time."
  },
  {
    "keywords": [
      "681.3:61",
      "61:681.3"
    ],
    "titulo": "Uma solução para navegação indoor",
    "autor": "Alves, Nair Isabel Braga Simões",
    "data": "2012",
    "abstract": "Os sistemas de localização e navegação indoor permitem determinar a posição\nde pessoas ou certos objetos em grandes edifícios, assim como ajudam na navegação\ne orientação dentro destes ambientes fechados. A sua aplicabilidade pode-se\nestender a diversas áreas, tais como: hospitalar, o turismo, e até infantaria militar\nou um guia para invisuais.\nDe acordo com as necessidades específicas de localização indoor, a abordagem\nclássica do GPS é inadequada, uma vez que este é completamente inoperacional\nem espaços fechados. A navegação indoor levanta assim desafios adicionais quando\ncomparada com o outdoor. O interesse no indoor tem-se expandido significativamente,\ne como tal, diversos métodos têm vindo a ser estudados e apresentados,\ncom resultados, custos e contextos bastante diferentes entre si.\nDentro deste contexto, a presente dissertação tem como objetivo criar uma\nsolução para a navegação no interior de edifícios, que resolva os problemas caraterísticos\ndo indoor. Para tal, foi necessário fazer um levantamento bibliográfico,\nprincipalmente na web devido a ser uma área muito recente. Dessa pesquisa resultou\numa avaliação dos métodos e soluções de indoor existentes.\nPerante a dificuldade de encontrar uma forma consensual de representar espaços\nindoor, foi apresentado um possível modelo para a captação e representação destes\nespaços. Assim, tendo como base os dados provenientes do Open Street Map\n(OSM) e um software open source de routing multi-modal - o Open Trip Planner\n(OTP), foi criada uma plataforma de routing adequada à perceção da informação\nindoor. Esta plataforma - NavIOS, encontra-se disponível na Web.\nOs objetivos deste projeto foram alcançados com sucesso, conseguindo-se uma\nrepresentação simples dos espaços indoor, uma solução de routing para estes espaços,\ne que esta representação fosse percetível à solução criada. Apenas não se conseguiram disponibilizar na plataforma os dados 3D, para visualização e navegação,\napesar de estes dados terem sido previamente preparados. Este será um\npasso futuro, na continuação deste projeto."
  },
  {
    "keywords": [
      "Dispositivos móveis",
      "Smartphones",
      "Tablets",
      "Aplicações móveis",
      "HTML5",
      "Plataformas móveis",
      "Desenvolvimento multiplataforma",
      "Mobile devices",
      "Mobile applications",
      "Mobile operating system",
      "Cross-platform development",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aplicação móvel android de apoio a gestão de empréstimos e reservas das bibliotecas da Universidade do Minho",
    "autor": "Barros, Júlio Dinis Lopes de",
    "data": "2015",
    "abstract": "Os dispositivos móveis, em particular os tablets e smartphones, alcançaram uma enorme\npopularidade ao longo dos últimos anos devido à sua grande versatilidade e multifuncionalidade,\nconquistando deste modo, meritoriamente um espaço de destaque no nosso dia-a-dia, tanto a\nnível pessoal como profissional. Neste contexto, os utentes das bibliotecas da Universidade do\nMinho não são uma exceção, e os SDUM (Serviços de Documentação da Universidade do Minho)\nno cumprimento da sua missão, definiu como uma das linhas gerais proporcionar aos utentes\numa melhor qualidade de assistência, com o desenvolvimento de uma aplicação móvel de gestão\nde empréstimos e reservas de publicações em posse.\nContudo, existe um ainda um grande entrave no mercado do desenvolvimento de aplicações para\ndispositivos móveis, devido à sua fragmentação em termos de plataformas móveis utilizadas (iOs,\nAndroid, Windows Phone, etc.). Esta diversificação exige um maior esforço no desenvolvimento\ndas aplicações, de modo que obriga o desenvolvimento das mesmas para cada plataforma móvel\nem particular. É neste sentido que as abordagens de desenvolvimento multiplataforma ganharam\nrelevância, permitindo o desenvolvimento de aplicações para várias plataformas a partir de um\núnico código fonte.\nO principal objetivo desta dissertação é desenvolver uma aplicação de apoio a gestão de\nempréstimos e reservas nas bibliotecas da Universidade do Minho. Os objetivos intercalados são:\nrealização de estudos sobre as abordagens e ferramentas de desenvolvimento multiplataforma,\nadotar métodos de engenharia de requisitos e conceção, implementação e teste da solução final\n(fundamental no processo de correção de falhas, de modo que a permitir uma solução final com\nmaior qualidade). A primeira fase do modelo de processo de engenharia de requisitos consiste no\nlevantamento/definição e priorização de requisitos, que tem como objetivo conhecer as técnicas\nde levantamento, assim como identificar e aplicar as que melhor se adequam a este projeto. Após\na execução da fase de Analise e negociação, efetuou-se a documentação dos requisitos a um nível\nde detalhe apropriado, como consta no Anexo B – Documento de Especificação de Requisitos."
  },
  {
    "keywords": [
      "Meta-omics",
      "MOSCA",
      "Flask",
      "MOSGUITO",
      "API",
      "Metaómica",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Development of MOSGUITO: a user-friendly graphical interface for meta-omics data analyses",
    "autor": "Pereira, José Henrique Lopes",
    "data": "2022-12-13",
    "abstract": "Complex microbial communities are essential to all ecosystems, and by linking microbial\nidentity to function, meta-omics technologies facilitate the interpretation of the processes cat alyzed by microorganisms. MOSCA is a command-line pipeline that performs bioinformatics\nanalyses of metagenomics, metatranscriptomics, and metaproteomics. MOSGUITO is a web based tool developed in React, which allows the configuration of MOSCA’s workflow and\nthe visualization of MOSCA outputs. Although the metadata and the configuration options\nof MOSCA could be easily customized and downloaded through MOSGUITO, MOSGUITO\nwas unable to interact with MOSCA automatically. In this thesis, a third-tier client-server\narchitecture was developed containing the Client MOSGUITO, the Server MOSCA, and a\nDatabase. MOSGUITO as a client-side can retrieve, store and delete data from the Database\nand start running analysis on MOSCA as a server. MOSCA as a server can receive files from\nthe client-side and start an analysis run. The database can store results from MOSCA, input\nfiles from users, and respective user information from their login session. A full guide to how\nto utilize this new version of MOSGUITO is provided. MOSGUITO client-side can interact\nwith MOSCA as a server using Flask APIs, end users don’t need to have knowledge on\ncommand-line pipelines to use MOSCA, nor the computer resources to download it. There fore users using MOSGUITO can optimize the usage and configuration of MOSCA, being\nable to analyze the data from omics experiments with a simple interaction with MOSGUITO."
  },
  {
    "keywords": [
      "Backup",
      "Importação",
      "CLAV",
      "Import"
    ],
    "titulo": "CLAV: gestão de backups e importação de dados",
    "autor": "Lindo, António Alexandre Carvalho",
    "data": "2022-12-19",
    "abstract": "O CLAV é um projeto nacional financiado pelo Simplex. O objetivo deste projeto é classificar\ne avaliar toda a documentação circulante na Administração Pública portuguesa. Desta\nforma, as entidades públicas disporão de uma ferramenta que possibilita a identificação\nda documentação que deve ser eliminada ou arquivada. No entanto, como em todas as\nplataformas, podem ocorrer imprevistos que resultem em perda de informação.\nNesta dissertação foi criada uma ferramenta web externa ao CLAV, que permita a execução\nde backups e importação de informação na plataforma, a fim de ser possível o armazena mento da informação em volumes externos ao CLAV de modo a esta informação poder voltar\na ser recolocada no CLAV mais tarde. Para a criação dos pacotes de backup, recorreu-se a\nformatos standard de armazenamento de informação."
  },
  {
    "keywords": [
      "Natural language processing (nlp)",
      "Emotion recognition",
      "Learning",
      "Neural network",
      "Human resources",
      "Employee retention",
      "Processamento de linguagem natural (pln)",
      "Extração de emoções",
      "Aprendizagem profunda",
      "Redes neuronais",
      "Recursos humanos",
      "Retenção de colaboradores",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Natural language processing applied to human resources",
    "autor": "Santos, António Manuel Almeida Pinto Bandeira",
    "data": "2024-01-19",
    "abstract": "As technology continues to advance, many companies are seeking ways to integrate Artificial Intel ligence (AI) into their operations in order to optimize their workflow and improve efficiency. One area\nthat could greatly benefit from AI solutions is the Human Resources (HR) department. This disserta tion explores the use of AI and Natural Language Processing (NLP) to extract emotions from text-based\ncommunications, such as emails and instant messages between HR representatives and employees. By\nproviding insight into the states of mind being expressed in text, this technology has the potential to im prove communication and understanding between the two parties, which in turn may lead to better conflict\nresolution, increased employee engagement, and improved productivity. The study will examine the use\nof various machine learning algorithms ranging from Convolutional Neural Network (CNN) and Recurrent\nNeural Network (RNN) to current state-of-the-art architectures such as transformers like BERT and XLNet\nto extract better insights from text and to identify the emotions contained in messages. The results of this\nthesis contribute to the development of an AI solution capable of identifying one out of seven emotions in\nboth general and specialized conversations with an accuracy of 75.38%, having the capacity of enhancing\nthe efficiency of HR departments by facilitating communication and understanding with employees."
  },
  {
    "keywords": [
      "681.3:57",
      "57:681.3"
    ],
    "titulo": "Network inference: extension of linear programming model for time-series data",
    "autor": "Matos, Marta R. A.",
    "data": "2013",
    "abstract": "With the widespread availability of high-throughput technologies, it is now possible\nto study the behavior of dozens or even hundreds of gene/proteins through a\nsingle experiment. Still, these experiments provide only the gene/protein expression\nvalues, telling nothing about their interactions with each other. To understand\nthese interactions, network inference methods need to be applied. By understanding\nsuch interactions, new light can be shed into biological processes and, in particular,\ninto disease’s mechanisms of action, providing new insights for drug design: which\ngenes/proteins should be targeted in order to cure/prevent a specific disease.\nIn this thesis, we developed and tested two alternative extensions for a previously\ndeveloped model based on linear programming. Such model infers signal transduction\nnetworks from perturbation steady-state data. The extensions now developed take\nadvantage of perturbation time-series data, which further improves the resolution of\ncausal relationships between genes/proteins.\nIn a first phase, we use artificial networks with simulated data to test the performance\nof both extensions in different conditions. Additionally, we compare their performance\nto the original model and to a state-of-the-art model for perturbation timeseries\ndata, DDEPN. Overall, our second extension exhibits a better performance,\nand significantly higher sensitivity. This extension assumes a given gene/protein can\nonly influence its targets if it is in an active form.\nIn a second phase, we use two experimental datasets related to ERBB signaling\nand evaluate the resulting networks: 1) by finding literature support for the inferred\nedges, and 2) by using a network assembled with Ingenuity IPA as true network to\ndo a quantitative assessment. Our results are further compared to DDEPN and the\noriginal model in a quantitative way. Quantitatively, our second model extension is\nshown to perform better than both the original model and DDEPN. Qualitatively,\nwe find literature support for most of the inferred edges in both datasets, while also\ninferring a few plausible edges for which no literature evidence was found."
  },
  {
    "keywords": [
      "I/O Optimization",
      "Multi-node deep learning",
      "Otimização de E/S",
      "Aprendizagem profunda multi-nodo",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Otimizações de armazenamento distribuído para aprendizagem profunda",
    "autor": "Moreira, Maria Beatriz Cardoso Gonçalves Barbosa e",
    "data": "2024-02-19",
    "abstract": "In today’s world the utilization of Deep Learning (DL) is intrinsically integrated in the activity of several\nenterprises and industries. It allows us to extract knowledge from data, detect patterns and make pre dictions, increasing the competitivity and quality of the services provided. However, the DL frameworks\n(e.g., TensorFlow, PyTorch, Apache MxNet) require not only considerable of computational power, but also\nefficient data storage, since they need to deal with large amounts of data. In particular, in each iteration of\nthe DL model train different batches of the training dataset are accessed to be processed and incorporated\nin the model. The retrieval of this data can be a bottleneck to the performance of the system, since the\ndatasets are getting increasingly bigger, reaching sizes in the order of TBs.\nIn the case of multi-node DL this becomes increasingly critical since there are many compute nodes\ntraining models, possibly with the same dataset, resulting in more requests directed to the shared file\nsystem competing with each other. If data could be stored nearer to the computational nodes and those\nnodes shared the data with one another, it would reduce the I/O pressure in the shared storage system\nand potentially reduce the time taken by these accesses and, consequently, the training time.\nThis thesis presents DistMonarch, a DL framework agnostic system that takes advantage of the storage\nsystem hierarchy by copying data to levels closer to each compute node and allows the nodes to share\ndata with each other, in a transparent manner. Results show that using this system reduces accesses to\nthe shared file system by up to 90% and training time of some models and configurations by up to 48%."
  },
  {
    "keywords": [
      "Deep learning",
      "Infrared",
      "Computer vision",
      "CNN",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Influence of near-infrared images in the object detection task",
    "autor": "Silva, Pedro Nuno Rodrigues da",
    "data": "2018",
    "abstract": "With the rise of data, the creation of algorithms capable of using that data is an evolution that appears\nnaturally. Taking advantage of those algorithms, impressive advances have been made in the ability\nfor a computer to recognize objects. Nevertheless, even after all those advances, further ones can still\nbe achieved.\nWith the reduction of infrared cameras prices and at the same time the increase in the picture quality\nof those same cameras, they are becoming reliable solutions for commercial applications. These\nimages provide an all new kind of information that is not available with the use of only the traditional\nvisible light images. As such, in this work, it is tested if the additional usage of infrared images, in\ncomplement with the visible image, has any kind of influence in the results for object detection for\ndifferent levels of illumination, in the interior of a vehicle.\nIn order to test this influence, several tests are done in equivalent conditions and the results between\nusing infrared images and visible light images compared. In addition to that, there were also experiments\ndone in the usage of both types of images at the same time as a way to improve detection.\nIt was also documented the influence of some more traditional modifications over the images of the\ntraining set, such as data augmentation and changes in the number of classes.\nTo keep the results of the experiments as comparable as possible, a training methodology was\nplanned and used in all of the training processes of the algorithms."
  },
  {
    "keywords": [
      "Processamento de imagens",
      "Atlas",
      "Imagens histológicas de cérebros de rato",
      "Registo de imagens",
      "Image processing",
      "Brain rat histological images",
      "Image registration",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Processamento de imagens histológicas de cérebros marcados com IHC para estudos neurofisiológicos",
    "autor": "Vieira, Rui Emanuel Gomes",
    "data": "2023-12-05",
    "abstract": "A curiosidade do ser humano é algo que estará sempre presente no caminho da \nhumanidade, sendo este aspeto que nos faz querer perceber como nós e o mundo que nos \nrodeia funciona. Com os diferentes estudos e ensaios realizados ao longo dos tempos, foi \npossível observar que os seres humanos não são tão diferentes de outros animais em diversos \naspetos, desta maneira, ao estudar os comportamentos e anatomia destes animais foi possível \ndescobrir e estudar de forma análoga novos aspetos dos seres humanos. \nNesta dissertação o foco principal é o cérebro de rato da estripe Wistar e tem como \nobjetivo auxiliar o estudo desta estrutura. Mais especificamente foram usadas dois tipos de \nimagens: uma de microscopia de secções do cérebro do rato e outra de um atlas que \nrepresentam esquematicamente as primeiras. De modo a facilitar o processo de análise das \nimagens de microscopia, esta dissertação explica o desenvolvimento de uma linha de \nprocessos capaz de auxiliar esta etapa essencial na realização de qualquer tipo de estudo com \nas estruturas. \nO resultado obtido ao executar o método, publicamente disponível num repositório,\nconsiste na sobreposição da representação esquemática de uma parte do cérebro do rato com \na imagem de microscopia do tecido que lhe corresponde, o que permite a visualização mais \nclara dos componentes presentes na imagem original da secção coronal do cérebro de rato, \nauxiliando no seu estudo."
  },
  {
    "keywords": [
      "Comunicação assíncrona",
      "Middleware",
      "Sistemas de queues",
      "Asynchronous communication",
      "Queue systems",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Middleware para integração com sistemas de queues",
    "autor": "Marques, João Paulo Oliveira de Andrade",
    "data": "2022-03-03",
    "abstract": "Com o crescente número de componentes para gestão de sistemas de queues e o crescente número de\naplicações cliente a fazer uso desses mesmos componentes é necessário a criação de um Middleware para de sacoplar as aplicações cliente dos sistemas de queues. Os sistemas de queues são também conhecidos com\nMessage-Oriented Middleware (MOM).\nO acoplamento das aplicações cliente a esses componentes torna-as muito dependentes destes, pelo que a\nintrodução de um Middleware faz com que a aplicação cliente fique isolada das particularidades e das tarefas de\nmanutenção. Posto isto, fica o Middleware dependente dessas particularidades e das tarefas de manutenção.\nO RabbitMQ, o ActiveMQ e o Kafka são exemplos de sistemas de queues onde existe um sistema intermediário\nexterno entre as aplicações que estão a comunicar, e o ZeroMQ que é um sistema de queues onde a própria aplicação\nfica como um nodo do sistema de queues, isto é, o ZeroMQ é um sistema intermediário interno. Todos estes são\nimplementados de diferentes formas, pelo que a troca de um sistema para outro leva a uma reestruturação das\naplicações que o estejam a usar, por isso estes serão estudados durante esta dissertação de forma a avaliar as\nsuas caraterísticas, vantagens e desvantagens para realizar a sua integração no Middleware a desenvolver.\nO Middleware desenvolvido desacopla as aplicações dos sistemas de queues, permitindo assim a troca de\num sistema para outro sem ser necessária uma reestruturação da aplicação. Este integrou o RabbitMQ, o ActiveMQ\ne o Kafka por forma a ser possível realizar as operações básicas de envio e leitura de mensagens. Além destas\noperações é também possível reler mensagens quando seja necessário.\nPor forma a demonstrar e testar o Middleware ir-se-á recorrer a um caso de estudo."
  },
  {
    "keywords": [
      "Consola de operador",
      "Gestão de chamadas",
      "Desenvolvimento front-end",
      "Desenvolvimento web",
      "Interface do utilizador",
      "Arquitetura do cliente",
      "Attendant console",
      "Call handling",
      "Ffront-end development",
      "Web development",
      "User interface",
      "Client architecture",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Consola de atendimento para gestão de chamadas telefónicas",
    "autor": "Mendes, André Filipe da Rocha",
    "data": "2023-12-15",
    "abstract": "Esta dissertação apresenta um estudo compreensivo e extensivo de todo o processo de planeamento, desenvolvimento e implementação de um protótipo de uma consola de operador com o objetivo de auxiliar\nos seus utilizadores a gerir um elevado número de chamadas, de forma a ir de encontro às necessidades de várias organizações, face aos dias de hoje. Devido ao ritmo acelerado associado à evolução do\nsoftware, é do interesse destas organizações melhorarem a eficiência dos locais de trabalho, algo que o\nprotótipo desenvolvido pretende alcançar, agilizando e otimizando as várias operações de rececionistas e\noperadores.\nEste documento inicia com um estudo aprofundado de várias soluções de consolas de operador\npresentes no mercado, com o intuito de analisar o que estas oferecem, incluindo a solução da Altice\nLabs, com o objetivo final de estabelecer uma comparação entre estas, e analisar possíveis aspetos a\nmelhorar desta última, assim como a sua posição e papel no mercado atual.\nA fase de planeamento inicia com um processo de tomada de decisão relativo às diferentes abordagens possíveis face ao trabalho a desenvolver. Após uma longa e rigorosa ponderação, que inclui uma\nfase de testes englobando outras soluções oferecidas por outras organizações, a abordagem relativa ao\ndesenvolvimento de um protótipo de raiz, focado num cliente web, foi selecionada, uma vez que foi a\núnica, na altura, viável.\nPor este motivo, a fase de planeamento foi bastante extensa, envolvendo uma seleção de funcionalidades e requisitos com uma abordagem centrada ao utilizador, assegurando uma compreensão das suas\nnecessidades e preferências, tendo sempre em mente as operações suportadas pelos serviços a integrar,\nque se revelaram como um quanto restritivos, passando também pelo desenho de uma interface modular\ne intuitiva, com foco nas várias funcionalidades planeadas.\nA fase de desenvolvimento focou-se principalmente na implementação daquilo planeado na fase anterior, esclarecendo os motivos que levaram à escolha das tecnologias escolhidas e tendo sido tomado o\ncuidado de abordar a arquitetura e o funcionamento da aplicação, explicando os motivos que levaram às\ndecisões tomadas. Para terminar esta fase, foi também exposto o resultado final e as várias alterações sofridas face ao planeamento, na tentativa de melhorar o produto final.\nPara terminar o documento, a eficácia da plataforma desenvolvida é avaliada através de um teste de\nusabilidade, com o objetivo de analisar o comportamento dos utilizadores na utilização da aplicação e\ncoletar a opinião dos participantes, contribuindo para o levantamento de aspetos a melhorar e para tirar\nconclusões relativas ao produto final.\nOs resultados obtidos neste teste de usabilidade foram positivos, indicando que a plataforma desenvolvida desempenhou um bom papel ao auxiliar os utilizadores a lidarem com um elevado fluxo de\nchamadas, aumentando a sua produtividade e eficiência, mas também servindo como fundação para\ntrabalhos futuros na área."
  },
  {
    "keywords": [
      "Diálogos",
      "Sistema de conversação",
      "Sistemas educacionais",
      "Sistema de ensino inteligente",
      "Educational system",
      "Intelligent tutoring system",
      "Conversational system",
      "Natural language processing",
      "Dialogue",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Sistemas de conversação para software educacional",
    "autor": "Costa, Luís Manuel Leite",
    "data": "2022",
    "abstract": "Tutores artificiais são agentes de software que auxiliam nos processos de ensino e formação. Nos dias de hoje, esses sistemas têm como objetivo fornecer instruções aos alunos sem a intervenção direta de um professor. Para que isso aconteça com sucesso, é necessário que estes sistemas possuam um sistema de conversação que seja capaz de interagir com os alunos de forma simples e cativante, mantendo um diálogo adequado em todos os momentos de ensino que concedem. Essencialmente, os sistemas de conversação são assistentes virtuais que utilizam interfaces de comunicação que interagem com os utilizadores através de frases escritas e orais, e, geralmente, possuem a capacidade de compreender o utilizador. Por norma, o computador assume o papel de professor. No entanto, o aluno também “ensina” o sistema fornecendo dados e a sua perspetiva em relação ao problema, que podem posteriormente ser utilizados para personalizar o seu ensino.\nÉ neste contexto que foram realizados os trabalhos desta dissertação, cujo principal objetivo foi a implementação de um componente de conversação para um sistema educacional, que fosse capaz de criar novas dinâmicas entre o sistema e o utilizador, incentivando-o, assim, a despender mais tempo na ferramenta."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Implementing an integrated syntax directed editor for LISS",
    "autor": "Vaz, Damien da Silva",
    "data": "2016-12-21",
    "abstract": "The aim of this master work is to implement LISS language in ANTLR compiler generator\nsystem using an attribute grammar which create an abstract syntax tree (AST) and generate\nMIPS assembly code for MARS (MIPS Assembler and Runtime Simulator) . Using that AST,\nit is possible to create a Syntax Directed Editor (SDE) in order to provide the typical help\nof a structured editor which controls the writing according to language syntax as defined\nby the underlying context free grammar."
  },
  {
    "keywords": [
      "API",
      "REST",
      "Node.js",
      "Strapi",
      "Vue.js",
      "Vuetify",
      "JavaScript",
      "Back-End",
      "Front-end",
      "Full-stack",
      "MySQL",
      "NoSQL",
      "Web Application",
      "Swagger",
      "Documentation",
      "Aplicação web",
      "Documentação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Hypatiamat - I want to solve questions about...",
    "autor": "Carvalho, Válter Ferreira Picas",
    "data": "2022-12-13",
    "abstract": "Hypatiamat is a Portuguese project comprised of several applications that aim to develop the\nMath skills of students from the 1st through 9th grades (Basic Education). The ingraining\nof mental calculation strategies, numbering systems, and logical operations lead to a better\nsuccess rate in this subject in later years.\nOne of the project’s components is the online platform (https://www.hypatiamat.com),\nwhich aims to foster autonomous learning through more interactive practices due to the\ncurrent ease of technological access in this age group, by trying to appropriate teaching\nto everyday life. Several tools are made available, such as videos, tutorials, explanations,\nquestions, etc. on various Math topics that students can easily access at any time.\nTeachers that aim to enhance their students’ learning process using this digital approach\ncan exercise it in multiple applications provided by the platform, where the interactions are\ncarried out and controlled through these means.\nThe monolithic architecture (written in PHP) has received contributions from multiple\ndevelopers over the years in order to address the scalability issues introduced with this\nplatform’s growing popularity, which thus far demanded manual efforts for maintenance\nand content insertion. As such, there has been an incremental process of modernization,\nturning the various constituent applications into distinct microservices.\n\"I Want to Solve Questions About...\" is one of these applications where students are provided\nwith a large selection of questions in the form of mini-games (multiple choice, true or false,\n...), regarding the themes mentioned above.\nThe first objective of the dissertation is to develop a back-office that allows the teachers in\ncharge of the project to manage existing questions as well as add new ones for the students,\nsince the current process requires updating the database manually.\nThe second one is the modernization of the application’s interface at the technological\nlevel, by making use of adequate frameworks and programming languages and at the user\nlevel, by making an effort to maintain the intuitive workflow that led to its popularity but\nwith a modernized design, in order to be consistent with other online tools."
  },
  {
    "keywords": [
      "Alloy",
      "Geração de dicas automáticas",
      "Métodos formais",
      "Mineração de dados",
      "Automated hint generation",
      "Data mining",
      "Formal methods",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Mining hints for fixing formal specifications",
    "autor": "Neto, Henrique Gabriel dos Santos",
    "data": "2024-01-08",
    "abstract": "O crescimento da complexidade de aplicações informáticas tornou falhas e erros de software uma inevitabilidade. Para ajudar a garantir que uma aplicação funciona como previsto, profissionais recorrem a modelos de software para detetar e corrigir problemas nas fases iniciais de desenvolvimento. Especificações formais são modelos de software que permitem a desenvolvedores especificar rigorosamente estruturas e comportamentos de software. Infelizmente, a sua complexidade inerente também pode impor problemas nos principiantes que as tentam aprender. Uma maneira possível de abordar este problema seria o emprego de práticas de reparação de especificações e geração automática de sugestões\npara ajudar os alunos a corrigir tentativas erradas. Alloy4Fun é uma plataforma online para a aprendizagem de Alloy, uma linguagem de especificação formal com capacidades de analise automática. Alloy4Fun permite a instrutores criar e partilhar desafios de especificação formal com avaliação automática. Recentemente, uma técnica de geração automática\nde sugestões for desenvolvida para esta plataforma, mas provou ser insatisfatória devido ao seu fraco desempenho. O objetivo desta tese foi explorar outras técnicas para geração de sugestões, nomeadamente técnicas de geração de sugestões baseadas em dados, que poderiam usar o conjunto de dados publico de submissões históricas de estudantes do Aloy4Fun para fornecer dicas de forma mais eficiente. O principal resultado desta tese, SpecAssistant, é um novo sistema de geração de dicas baseado em dados para Alloy. Este extrai informação do conjunto de dados do Alloy4Fun para construir grafos de submissões, dos quais são extraídas sugestões a partir de regras personalizadas pelos  desenvolvedores de cada desafio. Para avaliar o SpecAssistant, realizamos uma série de experiências quantitativas, com o objetivo de avaliar a disponibilidade e o desempenho do nosso sistema. As nossas descobertas demostram que o SpecAssistant consegue fornecer dicas para uma porção significativa de submissões, apresentado um desempenho que supera o sistema de sugestões precedente."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "AIoTA: an IoT Platform on MonetDB",
    "autor": "Ferreira, Pedro Emanuel Silva",
    "data": "2016",
    "abstract": "The growth of the Internet and embedded systems have allowed physical devices to collect\nand exchange data in the Internet-of-Things (IoT). IoT allows objects to be monitored and\ncontrolled remotely across an existing network infrastructure, while creating opportunities\nto assimilate computer systems with the real world. The expansion of IoT’s connectivity\nhas lead devices to exchange large amounts of data, due to constantly required monitoring.\nThe output of these devices can be seen as streams with data made available incrementally\nover time. This has created a new demand to collect, process and analyze IoT data in an\nefficient and scalable way.\nIn the meantime, databases have been organizing collections of data for several decades.\nAt a low level, database management systems (DBMSs) to organize data efficiently. In particular,\nData Stream Management Systems (DSMSs) have emerged to handle uninterrupted\nflows of streaming data and integrate them with relational databases [Aggarwal, 2007].\nWith this objective in mind, DSMSs have distinguished from traditional DBMSs with new\narchitectures, data models, algorithms and specific query languages to deal with streams.\nAs streams are uninterrupted, DSMSs aim to process them incrementally. This lead to the\ncontinuous queries concept, where streaming data is processed with small batches each\ntime.\nMeanwhile, other database management systems have explored alternate ways to organize\ndata. MonetDB is a pioneer column-oriented relational database management system\n(RDBMS), storing relations column-wise opposed to rows as the majority of RDBMSs.\nColumnar-wise storage allows several benefits such as per-column query parallelization,\ndata compression and late materialization. MonetDB is being developed at CentrumWiskunde\n& Informatica (CWI) in Amsterdam since 1993, having achieved faster benchmark results\nthan popular RDBMSs such as PostgreSQL [Muhleisen, 2014].\nThis master thesis has the objective to create a streaming engine over MonetDB while\nfocusing on IoT processing. Amsterdam Internet-of-Things App (AIoTA) is a full-stack\napplication aiming to be integrated easily with IoT devices to collect streaming data, while\ntaking advantage of MonetDB’s columnar-wise storage to process it and deliver results\nimmediately."
  },
  {
    "keywords": [
      "Placenta",
      "Curvas de crescimento",
      "Regressão não linear",
      "GAMLSS",
      "LMST",
      "Growth curves",
      "Nonlinear regression",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Regressão não linear em curvas de crescimento para parâmetros placentares em R",
    "autor": "Lemos, Daniela Filipa Machado",
    "data": "2023-12-14",
    "abstract": "Nos últimos anos, tem existido um crescente interesse na avaliação dos parâmetros biométricos da pla centa e a sua relação com resultados obstétricos. Evidências têm sido publicadas sugerindo que as\nmedidas da placenta e a sua evolução são capazes de refletir alterações no desenvolvimento do feto e\naté mesmo doenças do recém-nascido e do adulto. Tendo em conta, que os gráficos de crescimento\ndesempenham um papel crucial na avaliação e vigilância da população pediátrica, surgiu o tema desta\ndissertação. O principal objetivo passa por estudar a aplicabilidade de modelos de regressão não linear\nem curvas de crescimento de parâmetros como o Diâmetro 1 (D1) e 2 (D2), Espessura (EP) e Peso pla centar (PP) e Peso fetal (PF). Para isso, utilizou-se um conjunto de dados de parturientes portuguesas\nrecolhidos no CGC (Centro de Genética Clínica), Porto.\nNeste estudo foi utilizada uma abordagem de regressão semiparamétrica para a construção de cur vas de crescimento de referência. Esta metodologia utiliza os modelos aditivos generalizados para lo calização, escala e forma (GAMLSS), Lamda-Mu-Sigma (LMS), LMS com Box-Cox t (BCT) e LMS com\nBox-Cox-powe-exponential (BCPE), oferecendo vantagens distintas sobre os métodos tradicionais como\na regressão quantílica. Uma das principais vantagens do GAMLSS é a sua flexibilidade para acomodar\nqualquer distribuição estatística, permitindo a modelação de vários parâmetros biométricos.\nAtravés da aplicação da metodologia proposta, foi demonstrado que com a utilização do método\nGAMLSS com BCT e P-splines para os parâmetros D1 e D2 e do método LMST para os parâmetros EP, PP\ne PF, podemos alcançar curvas de crescimento representativas. Além disso, foi desenvolvida uma apli cação web, disponível em https://placentalgrowth.shinyapps.io/uminho_pt/, utilizando\no ambiente R. Permite que profissionais de saúde e investigadores analisem e interpretem as curvas de\ncrescimento desenvolvidas com facilidade. Os resultados deste estudo fornecem informações importantes\nsobre o desenvolvimento da placenta e têm implicações significativas para a prática clínica em obstetrícia,\npermitindo o seu avanço e acompanhamento da saúde materno-fetal."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Tecnicas de Deep Learning para a determinação da idade óssea",
    "autor": "Pinheiro, Gonçalo Manuel Barbosa Teles",
    "data": "2019-12-23",
    "abstract": "A avaliação da idade óssea (a maturação esquelética) é uma prática clínica comum para investigar doenças endocrinológicas, genéticas e de crescimento em crianças. Geralmente é realizada por exame radiológico da mão esquerda usando o método Greulich e Pyle (G & P) ou o Tanner Whitehouse (TW). \nNo entanto, ambos procedimentos clínicos demonstraram várias limitações, desde o esforço do exame que tem que ser feito pelos radiologistas até a significativa variabilidade intra e inter-operador. Para resolver este problema, várias abordagens com recurso a sistemas de apoio ao diagnóstico médico (especialmente tomando como base o método TW) foram propostas. Nenhum deles demonstrou capacidades de generalização para diferentes raças, faixas etárias e géneros. \nA avaliação de exames radiológicos requer a análise de um profissional com a máxima atenção. No caso do método de Greulich e Pyle a radiografia da mão do paciente é comparada com um atlas padrão sendo possível observar deficiências no crescimento dos pacientes. Este é um trabalho exaustivo e sujeito a erros devido ao nível de atenção que é necessário durante o diagnóstico.  \nOs métodos de deep learning têm sido aplicados a diversas tarefas de análise de imagem médica como, por exemplo, classificação de lesões e segmentação de tecidos. O principal objectivo deste trabalho e desenvolver um modelo capaz de automaticamente determinar a idade óssea. Neste trabalho foram primeiramente testadas várias arquitecturas de redes neuronais convolucionais na determinação da idade óssea que mostraram bons resultados em tarefas comuns de visão por computador. Baseado nos resultados obtidos foi desenvolvido/optimizado um novo modelo que é apresentado neste documento. Foi usado transfer learning e o treino de raiz nas redes neuronais seleccionadas obtendo uma taxa de erro de 7.89 meses na determinação da idade óssea em pacientes do sexo feminino e uma taxa de erro de 8,28 meses ao executar esta tarefa em homens."
  },
  {
    "keywords": [
      "Identificação eletrónica",
      "eIDAS",
      "ISO-IEC DIS 18013-5",
      "Sistema de identificação eletrónica configurável",
      "Aplicação",
      "Electronic identification",
      "Configurable electronic identification system",
      "Application",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Mobile ID como um serviço: generalização da arquitetura do standard ISO/IEC DIS 18013-5",
    "autor": "Parente, Filipa Correia",
    "data": "2022",
    "abstract": "O processo de identificação geralmente é usado para facilitar transações comerciais e governamentais. Apesar\nda existência de formas de identificação, maioritariamente presenciais, estas são pouco úteis para realização\nde negócios online.\nDe forma a encarar este problema, vários governos de diferentes países estão a criar sistemas nacionais\nde identificação eletrónica (eID), isto é, uma coleção de tecnologias e políticas que permitem aos cidadãos\nprovarem eletronicamente a sua identidade, ou um atributo da sua identidade para um sistema de informação.\nCom o aparecimento da eID, um dos principais problemas que surgiu foi a insuficiente interoperabilidade entre\nos sistemas de identificação dos diferentes países adotantes, especialmente devido à falta de uma base jurídica\ncomum.\nAo longo dos últimos anos surgiram soluções que, direta ou indiretamente, solucionam o problema de interoperabilidade,\ncomo é o caso do eIDAS. No entanto, o eIDAS é um regulamento seguido apenas pela União\nEuropeia, existindo também a necessidade de readaptar o sistema de identificação eletrónica dos diferentes\nestados membros para criar uma ligação entre os diferentes sistemas.\nComo alternativa às soluções já existentes, o comité ISO/IEC JTC 1 estabeleceu uma norma, aplicada especificamente\npara a Carta (ou licença) de Condução, mas que pode ser adaptada para outros documentos\nde identificação. Esta norma tem sido bem recebida pela maioria dos países e promete ser uma alternativa\nbastante viável para a implementação de um sistema de identificação eletrónica.\nAssim, o principal foco desta dissertação é a definição de uma arquitetura aplicacional genérica, baseada na\nnorma técnica ISO/IEC DIS 18013-5. Com base na arquitetura definida, como prova de conceito pretende-se\ncriar de um sistema de identificação eletrónica configurável, que permita ao utilizador final implementar o seu\nsistema de identificação eletrónica, conforme as suas necessidades."
  },
  {
    "keywords": [
      "Paraconsistency",
      "Transition systems",
      "Modal logic",
      "Quantum computation",
      "Paraconsistência",
      "Sistemas de transição",
      "Lógica modal",
      "Computação quântica",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Exploring paraconsistent logics for quantum programs",
    "autor": "Cruz, Ana Luzia",
    "data": "2021-12-04",
    "abstract": "Superconducting quantum circuits are a promising model for quantum computation, al though their physical implementation faces some adversities due to the hardly unavoidable\ndecoherence of superconducting quantum bits. This problem may be approached from a\nformal perspective, using logical reasoning to perform software correctness of programs\nexecuted in the non-ideal available hardware. This is the motivation for the work devel oped in this dissertation, which is ultimately an attempt to use the formalism of transition\nsystems to design logical tools for the engineering of quantum software.\nA transition system to capture the possibly unexpected behaviors of quantum circuits\nneeds to consider the phenomena of decoherence as a possible error factor. In this way, we\npropose a new family of transition systems, the Paraconsistent Labelled Transition Systems\n(PLTS), to describe processes that may behave differently from what is expected when facing\nspecific contexts. System states are connected through transitions which simultaneously\ncharacterize the possibility and impossibility of that being the system’s evolution. This\nkind of formalism may be used to represent processes whose evolution is impossible to\nbe sharply described and, thus, should be able to cope with inconsistencies, as well as\nwith vagueness or missing information. Besides giving the formal definition of PLTS, we\nestablish how they are related under the notions of morphism, simulation, bisimulation\nand trace equivalence.\nIt is a common practice to combine transition systems through universal constructions,\nin a suitable category, which forms a basis for a process description language. In this dis sertation, we define a category of PLTS and propose a number of constructions to combine\nthem, providing a basis for such a language.\nTransition systems are usually associated with modal logics which provide a formal set ting to express and prove their properties. We also propose a modal logic, more specifically,\na modal intuitionistic paraconsistent logic (MIPL), to talk about PLTS and express their\nproperties, studying how the equivalence relations defined for PLTS extend to relations on\nMIPL models and how the satisfaction of formulas is preserved along related models.\nFinally, we illustrate how superconducting quantum circuits may be represented by a\nPLTS and propose the use of PLTS equivalence relations, namely that of trace equivalence,\nto compare circuit effectiveness."
  },
  {
    "keywords": [
      "Decision-making process",
      "Organ transplantation",
      "Potential organ donors detection",
      "Deteção de potenciais dadores de órgãos",
      "Processo de tomada de decisão",
      "Transplantação de órgãos",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Real-time healthcare intelligence in organ transplantation",
    "autor": "Fernandes, Bruno Daniel Pereira",
    "data": "2016",
    "abstract": "Organ transplantation is the best and often the only treatment for patients with end-stage organ failure. However,\nthe universal shortage of deceased donors and the international variation in donation and transplantation\nactivities result in a worrying situation that must be addressed. As in most countries, Portugal has implemented\ndonation programs to answer the increasing need for transplants with the objective to identify all the\npossible and potential donors admitted to hospitals. These donors constitute the largest share of organ\ndonors in Portugal, but identifying a patient that may progress to brain death could be a complex task and\ncadaveric organs must be transplanted in a short period of time in order to achieve satisfactory results.\nTherefore, the urgent need of intelligent solutions that are able to support the decision-making process is\ncrucial in critical areas as the organ transplantation is.\nThe aim of this dissertation is firstly the knowledge acquisition on the potential organ donor criteria for further\ndetection and secondly the design and implementation of a software platform to assist the inefficient process\nof identification of potential organ donors. This will result in an increase of control of the screening method\nand consequently optimize the workflow of the pre-transplantation process.\nAccordingly, and after several meetings with the transplant team, a prior identification pattern was structured\nand used to characterize the development of the proposed solution, named Organite. Organite is defined as a\nsystem to support the transplantation process, based on Business Intelligence technologies. It is responsible\nfor the collection, management, storage, and signaling of potential organ donors using information from the\ndisparate Health Information Systems to provide real-time tracking of patients and optimize the transplant\nteam’s workflow.\nThe developed platform is currently implemented at Centro Hospitalar do Porto, Hospital de Santo António,\nEPE and displays a steady and competent behavior providing consequently a way to have more control of the information needed for the decision-making process. As a result, the number of transplantation records at\nCentro Hospitalar do Porto, Hospital de Santo António, EPE are expected to show more profitable outcomes."
  },
  {
    "keywords": [
      "Ray tracing",
      "Parallel computing",
      "Spatial coherence",
      "Computação paralela",
      "Coerência espacial",
      "519.674"
    ],
    "titulo": "Parallel interactive ray tracing and exploiting spatial coherence",
    "autor": "Cruz, Eduardo José Tanque de Pádua",
    "data": "2013-06-07",
    "abstract": "Ray tracing is a rendering technique that allows simulating a wide range of light transport phenomena, resulting on highly realistic computer generated imaging. Ray tracing is, however, computationally very demanding, compared to other techniques such as rasterization that achieves shorter rendering times by greatly simplifying the physics of light propagation, at the cost of less realistic images.\nThe complexity of the ray tracing algorithm makes it unusable for interactive applications on machines without dedicated hardware, such as GPUs. The extreme task independent nature of the algorithm offers great potential for parallel processing, increasing the available computational power by using additional resources. This thesis studies different approaches and enhancements on the decomposition of workload and load balancing in a distributed shared memory cluster in order to achieve interactive frame rates.\nThis thesis also studies approaches to enhance the ray tracing algorithm, by reducing the computational demand without decreasing the quality of the results. To achieve this goal, optimizations that depend on the rays’ processing order were implemented. An alternative to the traditional image plan traversal order, scan line, is studied, using space-filling curves.\nResults have shown linear speed-ups of the used ray tracer in a distributed shared memory cluster. They have also shown that spatial coherence can be used to increase the performance of the ray tracing algorithm and that the improvement depends of the traversal order of the image plane."
  },
  {
    "keywords": [
      "GPU",
      "GPGU",
      "Java",
      "High-performance",
      "OpenCL",
      "Aparapi",
      "SHOC",
      "Parboil",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Efficient execution of Java programs on GPU",
    "autor": "Raposo, Gonçalo Medeiros São Pedro",
    "data": "2022-01-13",
    "abstract": "With the overwhelming increase of demand of computational power made by fields as Big\nData, Deep Machine learning and Image processing the Graphics Processing Units (GPUs)\nhas been seen as a valuable tool to compute the main workload involved. Nonetheless,\nthese solutions have limited support for object-oriented languages that often require manual\nmemory handling which is an obstacle to bringing together the large community of object oriented programmers and the high-performance computing field.\nIn this master thesis, different memory optimizations and their impacts were studied\nin a GPU Java context using Aparapi. These include solutions for different identifiable\nbottlenecks of commonly used kernels exploiting its full capabilities by studying the GPU\nhardware and current techniques available. These results were set against common used\nC/OpenCL benchmarks and respective optimizations proving, that high-level languages can\nbe a solution to high-performance software demand."
  },
  {
    "keywords": [
      "Databases",
      "Data quality",
      "Data management",
      "Data mining",
      "Machine learning",
      "Bases de dados",
      "Qualidade de dados",
      "Gestão de dados",
      "Mineração de dados",
      "Aprendizagem máquina",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Historical data management in big databases",
    "autor": "Simão, José Pedro Ribeiro Nunes",
    "data": "2017",
    "abstract": "We are now living in a digital world where almost anything, or something is saved somewhere\nwith very few considerations for determining if that was in fact relevant to be saved\nor not. Hence, it is predictable that most information systems are facing an information\nmanagement problem. To overcome this issue, it is vital the creation of new and more specific\ndata management techniques that will enforce the established governance policies and\nmanage the information systems in order to maintain their ideal performance and quality.\nCurrently, a solution that is able to cope with this problem efficiently is “pure digital gold”,\nespecially for the biggest players that have to handle an astonishing amount of data, which\nneeds to be properly managed. Nevertheless, this is a problem of general interest for any\ndatabase administration, because even if shrinking the dimension of the information is not\na major concern in some cases, the data assessment efficiency and its quality assurance\nare certainly two subjects of great interest for any system administrator. This work tackles\nthe data management problem with a proposal for a solution that uses machine learning\ntechniques and other methods, trying to understand in an intelligent manner the data in a\ndatabase, according to its relevance for their users. Thus, identifying what is really important\nto who uses the system and being able to distinguish it from the rest of the data, is\na great way for creating new and efficient measures for managing data in an information\nsystem. Through this, it is possible to improve the quality of what is kept in the database\nas well as increase, or at least try to ensure, system performance. Basically, what its users\nexpect from it throughout its lifetime."
  },
  {
    "keywords": [
      "Ambient Intelligence",
      "Crowd Sensing",
      "Machine Learning",
      "Smart Cities",
      "Séries Temporais",
      "Time Series Problems",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Crowd sensing and forecasting for Smart Cities",
    "autor": "Kramer, David Daniel Pinto Coelho",
    "data": "2021-02-04",
    "abstract": "A utilização de inteligência sob forma de tecnologia no nosso dia-a-dia é uma realidade em crescimento e, portanto, devemos fazer uso da tecnologia disponível para melhorar várias áreas do nosso quotidiano. Por exemplo, a tecnologia atual permite a conceção de sensores inteligentes, mais especificamente sensores de multidão, para detetar passiva mente dispositivos como smartphones ou smartwatches através de probe requests emitidos por estes dispositivos que, por sua vez, fazem parte de um processo de comunicação que ocorre sempre que o Wi-Fi dos dispositivos está ativado. Adicionalmente, crowd sensing - uma solução de Ambient Intelligence (AmI) - é estudada hoje em dia em várias áreas com bons resultados. Portanto, esta dissertação visa investigar e utilizar sensores de multidão para capturar passivamente dados acerca da densidade de multidões, explorar as capacidades do sensor escolhido, analisar e processar os dados para obter melhores estimativas, e conceber e desenvolver modelos de Machine Learning (ML) para prever a densidade nas áreas sensorizadas. Áreas nas quais o sensor de multidão está inserido - AmI, Smart Cities, Wi-Fi Probing - são estudadas, juntamente com a análise de diferentes abordagens ao crowd sensing, assim como paradigmas e algoritmos de ML. Em seguida, é explicado como os dados foram capturados e analisados, seguido por uma experiência feita às capacidades do sensor. Além disso, é apresentado como os modelos de ML foram concebidos e otimizados. Finalmente, os resultados dos vários testes de ML são discutidos e o modelo com melhor desempenho é apresentado. A investigação e os resultados práticos abrem perspetivas importantes para a implementação deste tipo de soluções na nossa vida diária."
  },
  {
    "keywords": [
      "Ciências Médicas::Ciências da Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Plataforma de apoio à decisão nos cuidados de ginecologia e obstetrícia",
    "autor": "Pereira, Sónia Patrícia Pinto",
    "data": "2015",
    "abstract": "A grande quantidade de dados gerada todos os dias na indústria, e nomeadamente,\nna área da saúde, impulsiona a utilização de Tecnologias de\nInformação (TIs) para o seu registo, tratamento e exploração, com o intuito\nde adquirir conhecimento com valor assim como instrumentos para o apoio\nna tomada decisões.\nNa unidade de cuidados materno-infantis do Centro Hospitalar do Porto\n(CHP), o Centro Materno Infantil do Norte (CMIN), os pro ssionais de saúde\nlidam com utentes em condições delicadas e situações que requerem a tomada\nde medidas rápida e e ciente. Um vasto conhecimento dos processos de Ginecologia\ne Obstetrícia (GO) pode ser crucial para promover as boas práticas\nmédicas e evitar eventos adversos na mãe e no recém-nascido. A aplicação\nbem sucedida de Sistema de Apoio à Decisão Clínica (SADC) em ambiente\nclínico e a recetibilidade dos pro ssionais de saúde e de TI do CMIN à introdu\nção de novos conceitos e tecnologias, motivam o desenvolvimento de novos\nartefactos.\nNeste sentido, este projeto aplica os conceitos de Business Intelligence\n(BI) e Descoberta de Conhecimento sobre informação disponível nos Sistemas\nde Informação (SIs) da instituição, através de uma grande variedade\nde metodologias, métodos e tecnologias para construir soluções e apoiar os\nserviços prestados na unidade de cuidados materno-infantis.\nUm dos artefactos criados é o desenvolvimento de indicadores clínicos e\nde desempenho a partir da tecnologia de BI Pentaho Community Edition\n(CE). Este processo inicia-se com a extração, transformação e carregamento\nda informação numa estrutura multidimensional, o Data Warehouse (DW),\npermitindo a posterior representação da informação através da aplicação de BI. Esta plataforma integra indicadores dos módulos de GO, de triagem e da\nadmissão hospitalar do CMIN.\nNa componente da descoberta de conhecimento, vários estudos são efetuados,\ntendo em conta as preocupações dos pro ssionais de saúde e as necessidades\nda instituição, provando a viabilidade de utilizar técnicas de Data\nMining (DM) na construção de modelos de previsão no sector da saúde. Os\nnascimentos pré-termo, a seleção do tipo de parto mais adequado, a categoriza\nção das utentes e o seu percurso pela unidade de GO e os tempos de\nespera de pré e pós-triagem são alguns dos problemas analisados. Os estudos\nalcançam resultados promissores e clinicamente relevantes, permitindo a\nidenti cação de fatores de risco clínico. A título de exemplo, adquiriam-se\nmodelos de previsão para os nascimentos pré-termo com valores de sensibilidade\ne especi cidade de 89% e 93%.\nDe forma a presentear os pro ssionais de saúde com as soluções desenvolvidas,\nfoi criada uma plataforma de alto-nível que integra os produtos de\nBI e os modelos de previsão de DM, culminando o objetivo do projeto num\nartefacto  nal, que visa o apoio às práticas clínicas, a qualidade dos cuidados\nprestados e a consequente satisfação dos utentes."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Adaptive game content generation",
    "autor": "Oliveira,  Sérgio Lucas dos Santos",
    "data": "2017",
    "abstract": "From a simple entertainment activity to a learning tool, it is undeniable that video games\nare one of the most active and relevant areas in today’s society. Since their inception, video\ngames experienced an evolution unmatched by almost any other area and today’s video\ngames go beyond being simple software. They are authentic pieces of art that programmers,\ndesigners and artists work together to build.\nAs such, it presents no surprise that building a state of the art (AAA) video game is a\nvery expensive process. If one breaks down the budget it takes to develop a AAA a video\ngame, one finds that a sizable amount of money goes to developing content for that video\ngame.\nProcedural content generation is based on a strict set of rules and it offers an alternative\nto manual design of video game content, optimizing the process of development and thus\nreducing its cost.\nBecause procedural content generation relies on a set of rules, it is possible to take into\nconsideration the specific needs of a given player and dynamically adjust the game content\nto better suit its play style and increase the value s/he takes out of the game.\nThis work focuses on the development of a preliminary version of a new Procedural\nContent Generation (PCG) methodology that generates customized game content that is\ndependent on the player’s context.\nThe main goal of this methodology is to close the gap between players and developers,\noffering an easier way for the game developers to access information about each specific\nplayer, allowing them to craft generation algorithms that make use of individual player\ninformation.\nThis work also contemplates the development of two games that make use of said methodology\nand thus generate adaptive content."
  },
  {
    "keywords": [
      "Geofences",
      "Road Safety",
      "Smart Alerts and Notifications",
      "Smart Cities",
      "Vulnerable Road Users",
      "Avisos Inteligentes",
      "Segurança Rodoviária",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Smart city geofences for vulnerable road users",
    "autor": "Fonseca, Pedro Daniel Gomes",
    "data": "2019-12-13",
    "abstract": "Some road users have been targeted in a way to try and create systems that help increase\ntheir safety. We are referring to, in this instance, pedestrians, cyclists and even motorcyclists\nwhom we call Vulnerable Road Users (VRU) due to the fragility they present when compared\nto other users that circulate on the same roads, in particular cars. These systems fall within\nthe ambit of the Smart Cities that are increasingly growing all over the world.\nThe objective of this dissertation is to take advantage of geofencing technology, i.e., to\ncreate a virtual perimeter for a real geographical location in which it is desired to detect at\nany moment the entrance or exit of an user. To achieve such goals a mobile app was deve loped with the purpose of notifying the user as soon as he enters or exits such geofences.\nA web-based platform was also conceived and designed for city hall administrators where\ngeofences may be created, updated, deleted and classified.\nThe main focus went towards the creation of geofences in specific points of interest for\nVRUs, i.e., areas that may be considered dangerous for pedestrians when crossing the road,\ndue to bad visibility for instance, signaling road blocks or construction areas that may cause\ndisruption in traffic or even a road accident."
  },
  {
    "keywords": [
      "Doença de Alzheimer",
      "Diagnóstico assistido por computador",
      "Défice cognitivo ligeiro",
      "Ressonância magnética estrutural",
      "Aprendizagem profunda",
      "Redes neuronais convolucionais",
      "Alzheimer’s disease",
      "Computer aided diagnosis",
      "Mild cognitive impairment",
      "Structural MRI",
      "Deep learning",
      "Convolutional neural networks",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Diagnóstico da doença de Alzheimer com redes neuronais profundas",
    "autor": "Silva, Mateus Ferreira da",
    "data": "2022",
    "abstract": "A doença de Alzheimer é o tipo mais predominante de demência e, apesar de não existir\ncura para a mesma, o seu diagnóstico prematuro é fundamental para um tratamento efetivo\ne que permita retardar o progresso dos sintomas. Desta forma, nos últimos anos, tem\nsurgido um grande interesse em estudar e desenvolver sistemas automáticos de diagnóstico\nque usam como fonte de dados os exames médicos realizados pelos pacientes.\nEsta dissertação enquadra-se na temática da utilização de aprendizagem profunda para\ndiagnosticar a doença de Alzheimer. Pretende-se que seja avaliado o desempenho de\nredes neuronais profundas existentes da forma mais real possível, e que seja proposta uma\narquitetura com bom desempenho, que possa ser usada num sistema de diagnóstico assistido\npor computador. A capacidade da aprendizagem profunda em encontrar padrões ocultos em\nimagens médicas permite reduzir o erro de diagnóstico humano e auxiliar num diagnóstico\nmuito mais preciso.\nÉ pretendido que, com base numa única imagem por paciente a rede neuronal proposta\nseja capaz de fazer um diagnóstico sobre a doença de Alzheimer. Para isso, serão utilizadas\nimagens do cérebro obtidas pela técnica de Ressonância Magnética Estrutural, adquiridas a\npartir do conjunto de dados da Iniciativa de Neuroimagem da Doença de Alzheimer.\nPara a concretização do objetivo proposto, foram estudados e implementados, num cenário\nexperimental recorrendo a ferramentas simples, os métodos mais indicados para a realização\ndesta tarefa.\nOs resultados mostram que as Redes Neuronais Convolucionais (CNNs) permitem construir\nmodelos com enorme potencial, contudo, a sua utilização em ambientes reais ainda\nnão é muito viável nos dias de hoje."
  },
  {
    "keywords": [
      "Sistemas ciber-físicos",
      "Auto-diagnóstico",
      "Automação de testes",
      "Aplicação web",
      "Cyber-physical systems",
      "Self-diagnosis",
      "Test automation",
      "Web Application",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of a self-diagnosis tests system for integration in a cyber-physical system",
    "autor": "Pereira, Ricardo Barros",
    "data": "2021-08-10",
    "abstract": "Hoje, a CONTROLAR fornece para a Bosch a Intelligent Functional Test System Machine, um sistema\nciber-físico desenvolvido para realizar diferentes níveis de testes funcionais em dispositivos e componentes\nelectrónicos. A Bosch utiliza-a para testar o correto funcionamento dos auto-rádios produzidos. Durante\neste processo, os auto-rádios são submetidos a vários testes e o problema surge quando a máquina\ndetecta erros em vários auto-rádios consecutivos e não é possível saber se a própria máquina está com\nproblemas, pois não possui nenhum módulo que permita saber se está a funcionar corretamente ou não.\nA origem deste trabalho surge da necessidade de encontrar uma solução que resolva o problema\nenunciado, mas também, inovadora e com contribuições para o mundo da investigação em sistemas\nciber-físicos e sistemas de testes de autodiagnóstico. A solução é integrar um sistema de autodiagnóstico\nna máquina que possa testar o seu funcionamento para que a Bosch possa ter certeza se o problema está\nna máquina ou nos auto-rádios. Como a máquina é um sistema ciber-físico, permite a integração de um\nsistema de software que possa gerir a execução de testes, sendo capaz de detectar falhas nas máquinas.\nO trabalho aqui apresentado aborda o problema criando um novo sistema de testes de autodiagnóstico\nque garantirá a confiabilidade e integridade do sistema ciber-físico. Em detalhe, esta dissertação começa\npor expôr um estudo sobre o estado da arte atual de sistemas ciber-físicos, automação de testes, metodo logia de teste keyword-driven e mais alguns conceitos relacionados a linguagens específicas de domínio\nque serão relevantes para a solução final. São apresentadas a especificação e análise do sistema, a fim\nde definir bem os seus componentes. Uma nova arquitetura modular e extensível é proposta para siste mas de testes de autodiagnóstico, bem como uma arquitetura para estendê-lo e integrá-lo num sistema\nciber-físico. Foi proposto um novo sistema de testes de autodiagnóstico que aplica a arquitetura proposta\nprovando que é possível realizar o autodiagnóstico em tempo real do sistema ciber-físico e permitindo a\nintegração de qualquer tipo de teste. Para validar o sistema, foram realizados 28 casos de teste, abran gendo todas as suas funcionalidades. Os resultados mostram que todos os casos de teste passaram e,\nportanto, o sistema cumpre todos os objetivos propostos."
  },
  {
    "keywords": [
      "Benchmark",
      "Internet of things",
      "Fog",
      "Time series",
      "Databases",
      "Internet das coisas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "MulletBench: Multi-layer Edge Time Series Database Benchmark",
    "autor": "Pereira, Pedro Miguel de Leal Meireles",
    "data": "2023-12-13",
    "abstract": "Internet of Things (IoT) systems generate massive amounts of time series data that need to be stored\nfor historical analysis. As a result, Database Management Systems (DBMSs) for these scenarios\nhave particular requirements in their ability to ingest large amounts of data and to optimise aggregation,\nfiltering and time-ranged queries over this data, which are essential for historical analysis.\nThrough the use of Fog Computing, combining both Edge and Cloud layers, it is possible to achieve\nreduced latency and increased scalability, privacy and connectivity through the Edge, while still benefiting\nfrom the enhanced computing and storage power of the Cloud. This has led to the development of Fog\nDBMSs. Database benchmarking allows standardising performance assessment and comparison of different solutions. However, current time series database benchmarking tools are not designed for multi-layer architectures, such as the ones used by Edge-Cloud hybrid DBMSs. This thesis proposes MulletBench, a benchmarking tool that is able to evaluate the internal load balancing capabilities of a multi-layer Time Series Database Management System (TSDBMS). This is achieved by integrating automated deployment features, per-node and per-layer performance and system resource metrics, allowing for a more detailed analysis of the SUTs’ performance than previously possible. The performance of InfluxDB and IoTDB is evaluated using the developed tool, comparing their performance in multiple workloads and deployment scenarios. Results show that the Edge layer can be used to improve performance by distributing the workload over multiple layers and performing downsampling at the Edge layer, increasing overall throughput and reducing latency at the Cloud. These conclusions are enabled by MulletBench’s novel features, and would not have been possible with previously existing solutions."
  },
  {
    "keywords": [
      "61:681.3",
      "681.3:61",
      "616-073",
      "612.8"
    ],
    "titulo": "Modelação de padrões de conectividade cerebral funcional",
    "autor": "Magalhães, Ricardo José Silva",
    "data": "2013",
    "abstract": "O estudo da conectividade cerebral, da integração e segregação das funções das diferentes\npartes do cérebro será no futuro próximo uma das ferramentas mais importantes na\ncompreensão do cérebro humano. Contudo, a variedade de processos e dinâmicas lá\npresentes tornam esta tarefa extremamente complexa.\nAssim, o objectivo principal deste trabalho passa por desenvolver e testar um modelo capaz de\nrepresentar redes de conectividade cerebral. A teoria de grafos, em conjugação com\nmodalidades de neuroimagiologia como a ressonância magnética, tem-se mostrado uma\nferramenta extremamente valiosa neste sentido.\nO trabalho aqui apresentado foca-se em três pontos: o pré-processamento das imagens de\nressonância magnética; a definição dos elementos que constituem a rede, comparando\ndiferentes estratégias, e construção das redes; a utilização de métricas e conhecimentos de\nteoria de grafos para caracterizar e comparar as redes.\nUtilizando dados reais foi possível construir redes esparsas, eficientes, resilientes, com forte\ndivisão em comunidades e arquitetura small-world. Foi observado o efeito das diferentes\nestratégias nas características das redes, e mesmo na falta de fortes conclusões sobre qual a\nmais adequada, foi possível compreender a dificuldade inerente á comparação de redes\ncomplexas e dados passos importantes no sentido de melhorar essa comparação."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Monitorização e controlo em ambientes industriais através de rede de sensores sem fios",
    "autor": "Castro, Pedro Filipe Mendes de",
    "data": "2015",
    "abstract": "Ao longo dos últimos anos tem existido um elevado crescimento na utilização de\ndispositivos de comunicação sem fios, em que se destacam as redes de sensores sem\nfios, com elevado número de possíveis aplicações, nas mais diversas áreas, sendo cada\nvez mais uma realidade no quotidiano das pessoas e da indústria. As redes de sensores\nsem fios têm como principal objetivo o encaminhamento dos dados sensoriais\nrecolhidos por determinados nós da rede para outros nós da rede (e.g., dados\nrecolhidos pelos sensores de um dispositivo terminal e enviados para uma estação\nbase).\nNesta dissertação, foi realizado um projeto que consiste no estudo,\ndesenvolvimento e testes de uma rede sem fios ZigBee, para controlar e monitorizar\nequipamentos em ambiente industrial. A implementação desta rede surge como um\nprocesso de substituição da rede cablada existente e dos dispositivos que controlam\nos equipamentos. Esta implementação foi feita utilizando plataformas de hardware e\nsoftware da Texas Instruments e o IDE NetBeans. Numa fase inicial foram realizados\ntestes para avaliação da taxa de erros na transmissão num ambiente industrial, de\nforma a poder viabilizar o projeto. Foi desenvolvido código para dispositivos do tipo\nrouter ZigBee com a capacidade de controlar equipamentos de refrigeração, recolher\nos dados dos sensores que lhe estão associados, e enviá-los pela rede até ao seu\ndestino, diretamente ou por intermédio de outros routers. Foi desenvolvido também\num coordenador ZigBee ligado a um PC que, através de uma interface gráfica com o\nutilizador, tem a capacidade de monitorizar e controlar todos os routers existentes na\nrede."
  },
  {
    "keywords": [
      "Deductive verification",
      "Distributed systems and protocols",
      "Why3",
      "Why3do",
      "Verificação dedutiva",
      "Sistemas e protocolos distribuídos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Verification of distributed algorithms with the Why3 tool",
    "autor": "Cruz, Carla Isabel Novais",
    "data": "2022-04-11",
    "abstract": "Nowadays, there currently exist many working program verification tools however, the developed tools are mostly limited to the verification of sequential code, or else of multi-threaded shared-memory programs. Due to the importance that distributed systems and protocols play in many systems, they have been targeted by the program verification community since the beginning of this area. In this sense, they recently tried to create tools capable of deductive verification in the distributed setting (deductive verification techniques offer the highest degree of assurance) and claim to have achieved impressive results. Thus, this dissertation will explore the use of the Why3 deductive verification tool for the verification of dis tributed algorithms. It will comprise the definition of a dedicated Why3library, together with a representative set of case studies. The goal is to provide evidence that Why3 is a privileged tool for such a task, standing at a sweet spot regarding expressive power and practicality."
  },
  {
    "keywords": [
      "681.3.06"
    ],
    "titulo": "Fault injection for the evaluation of critical systems",
    "autor": "Cunha, João Mário Quintas",
    "data": "2013",
    "abstract": "Atualmente, os sistemas críticos estão cada vez mais presentes no nosso dia-a-dia, fazendo\naumentar a necessidade de os assegurar cada vez mais e reduzindo o risco de acidente ou\nfalha. A industria espacial e automóvel são exemplos de indústrias que usam esses sistemas\ne que necessitam de os ver assegurados. Consequentemente, têm de ser tomadas medidas\npara garantir a segurança de um sistema ao nível de software e hardware.\nA injeção de falhas é uma das respostas a esse problema, fazendo uso das suas diferentes\ntécnicas para poder avaliar e validar sistemas críticos. A injeção de falhas pode ser considerada\numa técnica de teste ao software, onde as falhas podem ser injetadas ao nível do software\nou hardware e cujos resultados podem ser monitorizados de forma a avaliar como é que o\nsistema reagiu a tais falhas. Scan-Chain Implemented Fault Injection é a técnica de injeção\nde falhas que proporciona uma maior acessibilidade, observabilidade e controlabilidade. Com\nesta técnica, os níveis de hardware e de integração de sistemas podem ser validados.\nO csXception® é um ambiente de injeção de falhas automatizado desenvolvido pela Critical\nSoftware S.A para avaliar e validar sistemas críticos. A sua arquitetura é dinâmica e baseada\nem plug-ins de injeção de falhas. Devido à crescente presença dos microcontroladores ARM®\nCortex-M3 na industria automóvel, surgiu a necessidade de criar um novo plug-in de injeção\nde falhas para o csXception®.\nAssim, o objectivo principal desta dissertação de mestrado é o desenvolvimento de um\nnovo plug-in de injeção de falhas para o csXception®, que permita injetar falhas em microcontroladores\nARM® Cortex-M3, contextualizar o novo plug-in com a norma ISO-26262 e utilizar\num caso de estudo para mostrar alguns dos resultados obtidos."
  },
  {
    "keywords": [
      "Wireless Sensor Networks",
      "Adaptive sensing",
      "Energy-aware sensing",
      "Intelligent Irrigation System",
      "CupCarbon",
      "Energy Consumption",
      "Redes de Sensores Sem Fios",
      "Sensing adaptativo",
      "Eficiência energética",
      "Sistema de Rega Inteligente",
      "Consumo energético",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Eficiência energética em redes de sensores sem fios: medição adaptativa num sistema de rega inteligente usando o CupCarbon",
    "autor": "Fortunato, Filipe Vieira",
    "data": "2021-02",
    "abstract": "Today, there are many cities that offer to citizens smart solutions to make their daily\nlives easier so that the available resources can be better managed and the global quality\nof life improved. These solutions generally rely on a variety of Wireless Sensor Networks\n(WSN), which are applied in a wide range of scenarios. Most of these solutions work\nwithout human intervention, therefore, there has been a lot of interest in increasing the\nlongevity of these sensor networks.\nIn this context, the main purpose of this work is to study and optimize an adaptive,\nenergy-aware sensing algorithm for WSNs, e-LiteSense [11], wich is an algorithm\ncapable of auto-regulate how data is sensed, adjusting it to each applicational scenario.\nThis work, resorts to a simulation scenario representing a case in real life, namely, an\nIntelligent Irrigation system. In this study, CupCarbon is used as a simulation tool\nto implement WSN-based system and the e-LiteSense algorithm. The aim is to adapt\nthe number of measurement events of environmental parameters so that the energy\nconsumption of the different nodes of the network can be reduced while maintaining\nthe correct evaluation of the measurement data and increasing the lifetime of the sensor\nnetwork. The versatility of the algorithm in relation to its effectiveness and ability to\nself-configure in different types of sensing scenarios is also evaluated."
  },
  {
    "keywords": [
      "Well-being Indexes",
      "Sentiment analysis",
      "Text Mining",
      "Online Analytical Processing (OLAF)",
      "Índices de Bem-estar",
      "Análise de sentimentos",
      "Processamento de textos",
      "Sistemas multidimensionais de dados (OLAP)",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Análise de sentimentos para a geração de índices de bem-estar",
    "autor": "Veloso, Ricardo Milhazes",
    "data": "2022-07-25",
    "abstract": "In recent years, due to constant social and economic crises, there has been some concern regarding the quality of life and satisfaction of the population. Then, a need to create measures or criteria that would allow assessing the population's well-being, arose. Usually, these criteria are quite complex, because any one of them can be analyzed through different perspectives or dimensions, since the quality of life of a human being depends on several factors, such as health and education. The analysis of such criteria can be done through indexes. To deal with the complexity of these indexes and to create conditions that facilitate decision making, multidimensional systems can be used. These allow for a broader analysis of well-being indexes and their underlying dimensions. In this dissertation work, we will explore this area by creating a well-being analysis system based on indexes that will be calculated through the analysis of feelings expressed in texts."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Efficient synchronization of state-based CRDTs",
    "autor": "Duarte, Vitor Manuel Enes",
    "data": "2017",
    "abstract": "Data consistency often needs to be sacrificed in order to ensure high-availability in large\nscale distributed systems. Conflict-free Replicated Data Types relax consistency by always allowing\nquery and update operations at the local replica without remote synchronization.\nConsistency is then re-established by a background mechanism that synchronizes the replicas\nin the system.\nIn state-based CRDTs replicas synchronize by periodically sending their local state to\nother replicas and by merging the received remote states with the local state. This synchronization\ncan become very costly and unacceptable as the local state grows.\nDelta-state-based CRDTs solve this problem by producing smaller messages to be propagated.\nHowever, it requires each replica to store additional metadata with the messages\nnot seen by its direct neighbors in the system. This metadata may not be available after a\nnetwork partition, since a replica can be forced to garbage-collect it (due to storage/memory\nlimitations), or when the set of direct neighbors of a replica changes (due to dynamic\nmemberships).\nIn this dissertation we further improve the synchronization of state-based CRDTs, by\nintroducing the concept of Join Decomposition of a state-based CRDT and explaining how\nit can be used to reduce the synchronization cost of this variant of CRDTs.\nWe validate our proposal experimentally on Google Cloud Platform by comparing the\nstate-based synchronization algorithm against the classic and improved versions of the\ndelta-state-based algorithm. The results of this comparison show that our proposed techniques\ncan greatly reduce state transmission, even under normal operation when the network\nis stable."
  },
  {
    "keywords": [
      "Base de dados",
      "Sistemas distribuídos",
      "Base de dados distribuída geo-replicada",
      "Simulação",
      "Simulação de eventos discreto",
      "Database",
      "Distributed systems",
      "Distributed geo-replicated database",
      "Simulation",
      "Discrete event simulation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Simulação de sistemas distribuídos de gestão de bases de dados",
    "autor": "Sousa, Paulo Silva",
    "data": "2023-11-27",
    "abstract": "Hoje em dia, graças à existência de várias aplicações em grande escala com acesso a grandes quantidades de informação, bases de dados monolíticas não são capazes de satisfazer as suas necessidades,\nquer a nível de disponibilidade, de escalabilidade ou de performance. Deste modo, necessitamos de\nsistemas distribuídos de gestão de bases de dados para conseguir satisfazer estas aplicações. Destes\nsistemas, são particularmente interessantes aqueles que se destinam a um grande número de servidores\nespalhados por diferentes zonas geográficas, devido à urgência de os aproximar das populações para obter uma melhor escalabilidade do sistema e uma melhor performance. Estes sistemas estão geralmente\ndivididos em duas famílias: uma que dá prioridade à coerência dos dados e uma que dá prioridade à\ndisponibilidade do serviço.\nApesar do interesse que estes sistemas despertam, existe um grande custo associado ao seu teste no\nmundo real, sendo necessário recorrer a modelos de simulação para reproduzir o seu comportamento.\nAlém disso, estes sistemas contém bastantes diferenças entre eles, sendo muitas vezes difícil de comparar\nas suas vantagens e desvantagens em contexto real.\nNesta tese desenvolvemos o SAGeo, um simulador de bases de dados geo-replicadas configurável,\ncapaz de avaliar e comparar o desempenho relativo de diversas bases de dados distribuídas. Para além\ndisso, configuramos este simulador para três algoritmos de bases de dados diferentes e apresentamos\ncomparações de resultados de diversas simulações realizadas."
  },
  {
    "keywords": [
      "HTTPS",
      "User",
      "Web security",
      "Utilizador",
      "Segurança na Web",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Navegação segura - análise do uso de HTTPS na perspectiva do utilizador final",
    "autor": "Machado, Carlos Eduardo Ribeiro",
    "data": "2020-08-05",
    "abstract": "The Internet emerged in the late sixties in a scenario marked by the race of world hegemony\nbetween USA and USSR. Besides military applications, it was also initially used by researchers,\nacademics, and college students, enabling file transfer between hosts. After the nineties the\nInternet reached the general public. It was then focused on other purposes, such as access to\nhypermedia, social networks, advertising and even products sale.\nGiven the diversification of these accesses, the adoption of protocols for safe browsing has be come essential to protect user’s information. Combined with the classification of encrypted traffic,\nusing appropriate techniques for this purpose, this paper aims to analyze the use of HTTPS pro tocol in various browsing scenarios once considered safe. Through testing scenarios, this research\nintends to verify changes and impacts that this protocol promotes regarding the data collection\nfrom the users during the Internet access experience."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Learning and testing stochastic discrete event",
    "autor": "Pedro, André Matos",
    "data": "2012-11-14",
    "abstract": "Sistemas de eventos discretos (DES) são uma importante subclasse de sistemas (à luz da teoria dos sistemas). Estes têm sido usados, particularmente na indústria para analisar e modelar um vasto conjunto de sistemas reais, tais como, sistemas de produção, sistemas de computador, sistemas de controlo de tráfego e sistemas híbridos.\n\nO nosso trabalho explora uma extensão de DES com ênfase nos processos estocásticos, comummente chamado como sistemas de eventos discretos estocásticos (SDES). Existe assim a necessidade de estabelecer uma abstração estocástica através do uso de processos semi-Markovianos generalizados (GSMP) para SDES.\n\nAssim, o objetivo do nosso trabalho é propor uma metodologia e um conjunto de algoritmos para aprendizagem de GSMP, usar técnicas de model-checking estatístico para a verificação e propor duas novas abordagens para teste de DES e SDES (respetivamente, não estocasticamente e estocasticamente).\n\nEste trabalho também introduz uma noção de modelação, analise e verificação de sistemas contínuos e modelos de perturbação no contexto da verificação por model-checking estatístico."
  },
  {
    "keywords": [
      "Automatic quantification of cells",
      "Central nervous system",
      "Deep learning",
      "Image processing",
      "Image segmentation",
      "Microglial cells",
      "Células microgliais",
      "Processamento de imagem",
      "Quantificação automática de células",
      "Segmentação de imagem",
      "Sistema nervoso central",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automatic quantification of microglial cells from brain images",
    "autor": "Lopes, Diogo Alexandre Rodrigues",
    "data": "2022",
    "abstract": "Microglia are a type of glial cell residing in the central nervous system and represent about 10 to\n15% of the brain cell population. These cells don’t produce electrical impulses and are responsible for\nfundamental physiological and pathological processes, as they represent the first line of immune defence\nwithin the central nervous system. Thus, the quantification of these cells is essential in a clinical context,\nas it allows better monitoring and planning of treatments for different pathologies.\nConventional cell counting involves a specific set of tools and devices developed for this purpose.\nThis process is time-consuming and imprecise due to being heavily dependent on the operator. Currently,\nmost processes are performed manually. However, other approaches have been studied and developed\nto improve the counting process, making it less time-consuming, more efficient and reduce the error\nassociated with factors external to the counting. That said, the objective of this dissertation is to study the\nbest approach to automate the quantification of microglial cells, ranging from classical to deep learning\nmethodologies. Combined with the appropriate image processing and analysis techniques, the classical\napproach proves to be an adequate solution. However, in recent years, approaches based on deep learning\nhave shown promising performance in various image analysis tasks, such as classification, detection and\nsegmentation.\nThe approaches developed to automate the quantification process were tested on a set of images built\nin partnership with researchers from the School of Medicine of the University of Minho. As for the classical\nmethodology approach, a protocol was developed within ImageJ, which was combined with image processing\ntechniques that allowed the automation of the counting process. Based on Convolutional Neural\nNetworks, the classification problem referring to a deep learning methodology obtained an accuracy of\n0.9021 and managed to classify the 661 images in 5 minutes and 44 seconds. The two approaches, considered\noptimal within each methodology, are competitive with the state-of-the-art methods, as they allowed\nfor the automation of the quantification process, and showed a significant improvement in reproducibility,\nefficiency and reduced error associated with human factors."
  },
  {
    "keywords": [
      "Quantum computing",
      "Grover’s algorithm",
      "Complexity",
      "Ray casting",
      "Computação quântica",
      "Algoritmo de Grover",
      "Complexidade",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "A quantum algorithm for ray casting using an orthographic camera",
    "autor": "Alves, Carolina Isabel Monteiro",
    "data": "2019-12-30",
    "abstract": "Quantum computing has the potential to provide better time complexities (than those achieved with classical computers) for challenging problems solved by classical comput ers or even provide a solution for problems out of reach of classical computers in terms of\ntime complexity (where the time consumed for the resolution of the problem is not prac tical, e.g. thousands of years). Here we’re not solving an unsolvable problem but trying\nto improve its time complexity. There are several problems in rendering that are good can didates to being solved in a quantum fashion. Although previous research has proposed\nof theoretical ways of doing this, here we present a practical solution. This work takes a\nfirst step in applying quantum computing to one of the most fundamental operations in\nrendering: ray casting. This technique allows computing visibility between two points in\na 3D model of the world which is described by a collection of geometric primitives. The\nalgorithm returns, for a given ray, which primitive it intersects closest to its origin. Without\na spatial acceleration structure, the complexity for this operation is O(N). The main goal\nof this work is to use the Grover’s Algorithm, a quantum search algorithm based on ampli tude amplification, to improve the complexity of this problem. This algorithm provides a\nquadratic speed up allowing for visibility evaluation for unstructured primitives in O(\n√\nN)\nsteps. Due to technological limitations associated with current quantum computers we had\nto simplify our problem’s structure and in this work the geometrical setup is limited to\nrectangles and parallel rays (orthographic projection)."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Bidirectional finite state machine based testing",
    "autor": "Vilaça, Diogo Filipe Silva",
    "data": "2018",
    "abstract": "This thesis aims to develop a new methodology that combines model-based testing and\nbidirectional transformations. More precisely, the method of software testing used is blackbox\ntesting, where the system under test is a black-box. Without knowledge of the blackbox’s\ninternal structures or implementation, the focus is on the inputs and outputs. To infer\na model for this black-box, machine learning algorithms are used by submitting test cases\nagainst the black-box and observing the correspondent output. The resulting model is a\nfinite state machine that produces the same outputs of the black-box when submitted the\nsame inputs used in its making. Usually, in this approach, new test cases are provided to\ninfer better models.\nIn this thesis, bidirectional techniques will be studied in order to guarantee the conformity\nbetween both the model and the instance evolution. This way, it is allowed not only\nthe evolution of the test cases and co-evolution of the model, but also the evolution of the\nmodel and the co-evolution of the test cases."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Simulação de protocolos de encaminhamento em redes oportunistas",
    "autor": "Pires,  João Paulo Vilaça",
    "data": "2016-10-31",
    "abstract": "Nos dias que correm existe um crescimento tecnológico que se traduz não só no\nnúmero de dispositivos, mas também nas suas capacidades de processamento. Cada vez\nmais é possível encontrar pedestres e veículos equipados com dispositivos capazes de\ncomunicar sem fios. E graças a esses desenvolvimentos, é necessário criar aplicações que\npermitam otimizar a utilização destes dispositivos.\nDe forma a ser possível prever o comportamento destas redes móveis, é necessário\nsimular a utilização destes dispositivos em ambientes urbanos. No entanto, os\nsimuladores existentes continuam a ser muito genéricos e incapazes de simular em grande\nescala ou simular protocolos específicos de encaminhamento. Foi com o objetivo de\nresponder a estes problemas que foi criado este simulador.\nEsta dissertação descreve o desenvolvimento de um simulador de comunicações\nem ambientes urbanos. Este simulador permite simular as movimentações de vários tipos\nde atores e permite simular a interação entre atores através de vários protocolos de\nencaminhamento. Estes protocolos terão que realizar a comunicação da forma mais\nrealista possível. Foram então implementados vários protocolos de encaminhamento, de\nmaneira a que seja possível existir essa interação. Para ter a certeza que os protocolos\nimplementados foram implementados da forma mais realista, estes protocolos foram\ntestados a nível de performance e de resultados, e os resultados obtidos foram comparados\ncom resultados obtidos por um dos simuladores mais usados para a simulação deste tipo\nde redes. Assim, com esta comparação, é possível concluir que os objetivos propostos\npara este projeto foram atingidos."
  },
  {
    "keywords": [
      "Placenta",
      "Curvas de crescimento",
      "Desenvolvimento",
      "Shiny",
      "Regressão de Quantis",
      "Growth curves",
      "Development",
      "Shiny",
      "Quantile regression",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Dynamic application to identify percentiles related to placenta parameters according to gestational age",
    "autor": "Alves, Samuel Gustavo Correia Nogueira",
    "data": "2022-03-24",
    "abstract": "Durante anos, o crescimento do feto e o seu desenvolvimento eram as medidas que os médicos usavam \npara estimar o desenvolvimento e crescimento do mesmo. No entanto em estudos recentes tem-se vindo \na provar que o desenvolvimento e crescimento da placenta é de grande importância para estudar a saúde \ne crescimento do feto.\nPara a correta análise do desenvolvimento da placenta, curvas de crescimento precisam de ser criadas \npara comparação com diferentes percentis. Métodos de regressão linear foram usados para a criação de \ncurvas de crescimento para a população Portuguesa (Nogueira et al., 2019).\nNeste trabalho, usando os mesmos dados usados por Nogueira et al. foram criadas curvas de \ncrescimento usando um método estatístico conhecido por regressão de quantis que permite criar curvas \nusando um método mais robusto que o anteriormente usado.\nFoi também objetivo deste trabalho a criação de uma aplicação que permita ao utilizador colocar os \nvalores de crescimento da placenta e poder comparar com as curvas de crescimento. Para isto, duas \naplicações foram criadas. A primeira denominada APP1 tenta simular as ligações a bases de dados \nexistentes em hospitais, tentando ao máximo recriar as condições presentes num hospital. A segunda \naplicação, APP2, é mais simples e só precisa de documentos .csv com os valores da placenta, e permite \na sua utilização sem necessidade de criação de algum tipo de ligação\n(https://samuelalves.shinyapps.io/APP2/).\nAmbos os objetivos foram conseguidos com sucesso, permitindo um avanço na área que cada vez mais \nse mostra de grande utilidade."
  },
  {
    "keywords": [
      "Sistemas de ajuda inteligentes",
      "Saúde no smartwatch",
      "Índices de bem-estar",
      "Indicadores de saúde",
      "Análise de dados",
      "Mineração de dados",
      "Reporting",
      "Dashboarding",
      "Smart help systems",
      "Smartwatch health",
      "Wellness indexes",
      "Health indicators",
      "Data analysis",
      "Data mining",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Geração de índices de saúde em tempo real",
    "autor": "Costa, Diogo Francisco Araújo Leite da",
    "data": "2022",
    "abstract": "Neste milénio, a sociedade global tem não só criado tecnologias que revolucionaram o mundo e a\nperceção do mesmo como tem existido uma facilidade muito maior de obter dispositivos que contém uma\nimensidão de funcionalidades e sensores a um preço cada vez mais baixo. Dessa forma, a monitorização\ne recolha de indicadores de saúde e médicos em tempo real é algo possível e a sua recolha através dum\nsmartwatch, smartphone ou dispositivo semelhante para posterior informação sobre o estado de saúde\natual do utilizador e dessa forma calcular um índice de saúde de bem-estar geral. Como consequência\nnatural, também é importante funcionar como uma ajuda pessoal para alertar em tempo real irregularidades\nno estado de saúde do utilizador. Toda esta informação utilizada de forma inteligente pode criar uma\ncomunidade de saúde e bem-estar essenciais para os atuais utilizadores e também futuros. Neste trabalho\nde dissertação idealizámos e implementámos um sistema de índices de bem-estar e saúde capaz de\nfornecer informação em tempo real sobre uma variedade de métricas de bem-estar sobre o utilizador, em\nque essa informação é extraída dos sensores do smartwatch do mesmo, fornecendo assim um sistema\nde notificações para alertar uma variação elevada no bem-estar do indíviduo mas também um histórico\ndo seu índice de bem-estar ao longo do tempo."
  },
  {
    "keywords": [
      "Hypatiamat",
      "Championships",
      "Certificates",
      "Interface",
      "Statistics",
      "Campeonatos",
      "Certificados",
      "Interface",
      "Estatísticas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Hypatiamat: Gestão de Campeonatos",
    "autor": "Vieira, Luís Pedro Oliveira de Castro",
    "data": "2023-10-24",
    "abstract": "Hypatiamat, namely through its Hypatiamat Association (AHM) has, among others, the\nobjective of contributing to awakening among students of various educational levels an interest\nfor mathematics and a better understanding of its nature; of promoting the development of\nmathematics teaching at all levels; of promoting the quality of teaching/learning of mathematics\nthrough the use and integration of new technologies in the classroom, through the resources\navailable on the Hypatiamat Platform, thus capitalizing on the familiarity and enthusiasm of\nstudents for more technological environments.\nConsidering that the first years of schooling are essential for the construction of knowledge\nin mathematics and for the development of transversal skills such as logical reasoning and\nproblem solving, and taking into account that the evolution of mathematical learning takes\nplace in a continuous and spiral dynamic, it is important to establish solid foundations so\nas not to compromise the learning of the following years. Note that failure in this subject is\noften rooted in incomplete or dysfunctional learning, resulting from a poor construction of the\nmathematics foundations.\nThe intervention of the Hypatiamat project, being directly aimed at the students, takes\nadvantage of their natural taste for technological environments. With this in mind, it proposes\nto provide teachers with tools that will allow them to incorporate, in their daily practice,\nmethodologies that use this type of environment, duly articulated with other methodologies.\nIt should be noted that learning is richer and more effective if the experiences and contexts\nprovided to students are more diversed, and it is in this perspective that the Hypatiamat\nProject offers a great diversity of materials and approaches, allowing teachers to work them\nnot only in a technological context, but also in a concrete one.\nIn this dissertation the objective is to update and improve the component corresponding\nto the management of championships of Hypatiamat, thus allowing users of the platform an\neasier and more intuitive interaction with it."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Middleware de acesso coerente a serviços de bases de dados na nuvem",
    "autor": "Ribeiro, Cláudia Fernandes",
    "data": "2017",
    "abstract": "O aumento da quantidade de dados a processar, em diversos domínios, levou à necessidade\nde escalar os sistemas de armazenamento. Além dos sistemas de bases de dados tradicionais,\nque têm suporte a transações com propriedades ACID (Atomicidade, Coerência, Isolamento,\nDurabilidade), surgiram sistemas com base em outros paradigmas, que oferecem\noperações mais simples, baseadas no modelo chave-valor. Nestes sistemas, abdicou-se do\nsuporte a transações com propriedades ACID para atingir a escalabilidade necessária.\nPor outro lado, apareceram os serviços de armazenamento na nuvem, seguindo o modelo\nchave-valor, em que o tarifário de utilização é baseado no número de operações aprovisionadas\ne o nível de coerência a que estas são executadas.\nNo entanto, continuam a haver aplicações que necessitam de aceder a dados com garantias\nde coerência. Para tal, surgiram camadas transacionais de interface com sistemas de\narmazenamento chave-valor que medeiam todos os acessos das aplicações ao serviço de\narmazenamento.\nEsta dissertação analisa os compromissos dos modelos de coerência, oferecidos por serviços\nde armazenamento na nuvem, e propõe uma arquitetura que tira partido da mediação\ndos acessos à nuvem para otimizar o custo e o desempenho. Esta proposta é avaliada com\num modelo de simulação, que permite demonstrar a sua validade."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Encaminhamento probabilístico de dados nomeados em redes tolerantes a atraso",
    "autor": "Lima, Fábio Joel Sá",
    "data": "2016-04-22",
    "abstract": "Atualmente a arquitetura na qual a Internet se baseia assenta na ideia de comunicação fima-fim\nsempre disponível. É esperado que uma ligação entre a origem e o destino da informação\nesteja sempre disponível. Contudo, existem situações onde tal não é possível, ou não é\ngarantido. Nessas situações é necessário ter uma abordagem diferente.\nFoi com esse intuito que nasceram as Redes Tolerantes a Atrasos (Delay Tolerant Networks,\nDTN). Neste tipo de redes, as ligações ponto a ponto são esporádicas, podendo acontecer em\ndeterminado momento, e no momento seguinte deixarem de estar disponíveis. Por exemplo,\nnuma rede em que os nós são móveis, quando dois nós se cruzam estabelecem ligação. No\nmomento seguinte, afastam-se e a ligação perde-se. As DTNs têm a capacidade de operar neste\ntipo de cenários, por exemplo.\nPara além desta assunção de que existe sempre uma ligação fim-a-fim disponível, a Internet,\natualmente baseia-se também no conceito de IPs, ou seja, endereço de origem e endereço de\ndestino. A informação em si não é tida em conta. Ultimamente tem sido tentada uma\nabordagem diferente. Uma abordagem na qual é tido em conta o conteúdo das mensagens. Um\nexemplo deste tipo de redes são as Redes de Dados Nomeados (Named Data Network, NDN). É\naplicado o paradigma de produtor/consumidor, no qual existem dois tipos distintos de nós. O\nprimeiro cria a informação e o segundo consome-a. Um nó que produza determinado tipo de\ninformação coloca-a na rede, os nós que tiverem interessados na mesma, devem então\nsubscrevê-la.\nO principal objetivo desta dissertação é acoplar estes dois tipos de redes. Mais\nprecisamente tentar colocar uma abordagem semelhante às NDNs em cenários onde se utilizam\nDTNs. Nomeadamente, é sugerida uma abordagem no que diz respeito à distribuição de dados\nnomeados em cenários onde as DTNs operam.\nPretende-se melhorar um protocolo de encaminhamento probabilístico que encaminha\nmensagens tendo em conta encontros prévios. Transportou-se para as redes de dados\nnomeados o paradigma das redes oportunistas de que se dois nós se encontram a probabilidade\nde se voltarem a encontrar aumenta. Havia-se aplicado esse conceito no contacto com\nconteúdos, agora esse conceito é também aplicado no contacto de interesses. Ou seja, os\nconteúdos são agora encaminhados consoante a probabilidade do nó recetor encontrar\ninteressados no mesmo."
  },
  {
    "keywords": [
      "Lagos de dados",
      "Transações",
      "Processamento híbrido transacional-analítico",
      "Bases de dados",
      "Sistemas distribuídos",
      "Data lakes",
      "Transactions",
      "HTAP (Hybrid Transactional Analytical Processing)",
      "Databases",
      "Distributed systems",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "HyLake: atualização de lagos de dados com granularidade fina",
    "autor": "Teixeira, Nelson José Dias",
    "data": "2021-12-03",
    "abstract": "Os lagos de dados, também conhecidos por data lakes, suportam a recolha de grandes quantidades de\ninformação em ficheiros imutáveis para processamento analítico. No entanto, tem surgido a necessidade\nde modificar e atualizar esta informação de forma fiável, seja porque os dados são recebidos de forma\nincremental (por exemplo, de sensores e outras fontes de eventos) ou para eliminar os mesmos (por\nexemplo, devido ao RGPD (Regulamento Geral sobre a Proteção de Dados)). As soluções atuais para o\nfazer não são no entanto ideais: o armazenamento em SGBD (Sistema de Gestão de Bases de Dados)\nNoSQL (Not only SQL) tem um grande impacto no desempenho analítico, enquanto que sistemas baseados\nem ficheiros, como o Delta Lake, permitem apenas atualizações de granularidade grossa.\nNeste trabalho aborda-se este problema propondo uma solução híbrida que combina o armazena mento de longo prazo em ficheiros com um armazenamento transitório num SGBD NoSQL de forma a\nobter as vantagens de ambos os sistemas. Para o efeito, é implementado uma prova de conceito usando\nSpark, com ficheiros Parquet, e MongoDB. Assim, com a introdução deste sistema pretende-se possibi litar a execução de transações frequentes e de granularidade fina para suportar uma carga de trabalho\nOLTP (Online Transaction Processing). Os resultados experimentais obtidos confirmam que esta proposta\nobtém desempenho analítico e transacional comparável a cada um dos sistemas isolados."
  },
  {
    "keywords": [
      "Alloy",
      "Electrum",
      "Model checking",
      "LTSmin",
      "Partial order reduction",
      "TLA+",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Towards model checking electrum specifications with LTSmin",
    "autor": "Cancelinha, Bruno Miguel Sousa",
    "data": "2019-12-23",
    "abstract": "Model checking é uma técnica comum de verificação; garante a consistência e integridade de\nqualquer sistema fazendo uma exploração exaustiva de todos os possíveis estados. Devido à\ngrande quantidade de intercalações possíveis entre eventos, modelos de sistemas distribuídos\nmuitas vezes acabam por gerar um número de estados muito grande. Nesta dissertação\nvamos explorar os efeitos de partial order reduction — uma técnica para mitigar os efeitos\nda explosão de estados — implementando uma linguagem semelhante ao Electrum com\nLTSmin. Vamos também propor um event layer por cima do Electrum e uma análise sintática\npara extrair informação necessária para que esta técnica possa ser implementada."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Data analytics in IoT FaaS with DataFlasks",
    "autor": "Araújo, Paulo Ricardo Cunha Correia",
    "data": "2018",
    "abstract": "The current exponential growth of data demands new strategies for processing and analyzing information.\nIncreased Internet usage, as well as the everyday appearance of new sources of data, is\ngenerating data volumes to be processed by Cloud applications that are growing much faster than\navailable Cloud computing power.\nThese issues, combined with the appearance of new devices with relatively low computational\npower (such as smartphones), have pushed for the development of new applications able to make\nuse of this power as a complement to the Cloud, pushing the frontier of computing applications,\ndata storage and services to the edge of the network.\nHowever, the environment in Edge computing is very unstable. It requires leveraging resources\nthat may not be continuously connected to a network and device failure is a certainty. The system\nhas to be aware of the processing capabilities of each node to achieve proper task distribution as it\nmay exist a high level of heterogeneity between the system devices.\nA recent approach for developing applications in the Cloud, named Function as a Service (FaaS),\nproposes a way to enable data processing in these environments. FaaS services adhere to the principles\nof serverless architectures, providing stateless computing containers that allow users to run\ncode without provisioning or managing servers.\nIn this dissertation we present OpenFlasks, a new approach to the management and processing\nof data in a decentralized manner across Cloud and Edge. We build upon these types of architectures\nand other data storage tools and combine them in a novel way to create a flexible system\ncapable of balancing data storage and data analytics needs in both environments. In addition, we\ncall for a new approach to provide task execution both in Edge and Cloud environments that is able\nto handle high churn and heterogeneity of the system.\nOur evaluation shows an increase in the percentage of task execution success under high churn\nenvironments of up to 18%withOpenFlasks relatively to other FaaS systems. In addition, it denotes\nimprovements in load balancing and average resource usage in the system for the execution of\nsimple analytics at the Edge."
  },
  {
    "keywords": [
      "QML",
      "VQC",
      "VQA",
      "Depth",
      "Ansatz",
      "Profundidade",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "ADAPT-VQC Adaptive Variational Quantum Classifier",
    "autor": "Mano, Márcio Eduardo Lima",
    "data": "2023-12-15",
    "abstract": "This dissertation explores the potential of Quantum Computing, particularly in the context of Variational\nQuantum Algorithms (VQA), like the Variational Quantum Classifier (VQC). It focuses on overcoming chal lenges such as noise in quantum circuits and optimization complexities.\nThe research introduces adaptive strategies for VQC, enabling dynamic adjustments to circuit depth\nand expressibility during training. This flexibility aims to improve classification accuracy on datasets.\nThe dissertation starts with a literature review of VQA algorithms, especially adaptive strategies, draw ing insights from various domains, including chemistry.\nNext, it details the proposed adaptive approaches for VQC and presents rigorous experiments to\nevaluate their performance across diverse datasets, comparing them to the standard VQC. The results\nshow promise with improved accuracy and reduced circuit depth.\nHowever, it’s important to note that this work primarily serves as an introduction to the concept of\nadaptive approaches for classification, focusing on enhancing VQC within its existing context.\nIn summary, this dissertation provides insights into enhancing VQC performance and contributes\nincrementally to quantum computation in the realm of classification. Adaptive VQC addresses challenges\nposed by noisy quantum devices and offers opportunities for further research in quantum classification\nalgorithms."
  },
  {
    "keywords": [
      "Programming languages",
      "Programming language characterization",
      "Programming anguage design",
      "Programming language identification",
      "Linguagens de programação",
      "Caracterização de linguagens de programação",
      "Design de linguagens de programação",
      "Identificação de linguagens de programação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Properties that better describe a programming language",
    "autor": "Alves, Júlio Miguel de Sá Lima Magalhães",
    "data": "2023-12-15",
    "abstract": "This document reports the development of a Master’s Thesis, included in the second year\nof the Master’s Degree in Informatics Engineering at Universidade do Minho in Braga,\nPortugal. The main goal for this project was to identify which characteristics influence the recognition and identification of a programming language, considering both its typical source code\nelements and its linguistic style. In other words, which elements contribute the most to\nthe characterization of a language? How many structural elements of a language may be\nmodified without losing its identity? In order to achieve these goals, a comprehensive bibliographic research was made, ranging from basic concepts such as programming languages and how they work, to several state-of-the-art studies that have been conducted in the same context of this project. Complementary to this research, a set of programming languages was also chosen as a study subject, which resulted in a detailed review and categorization of their characteristics.\nAfter the definition of a general approach, a survey was developed and conducted to\ngather programmers’ answers on how they identify and recognize programming languages.\nIn addition to the survey, a machine learning model was also used to evaluate how these\ntwo facets (human versus machine) compared to each other. This dual approach provided\ninsights into which syntactic and semantic elements have a greater influence on the identity\nof a programming language. This Master’s project resulted in an overall picture of programming languages’ characteristics and the relative influence they have on both programmers’ and AI-driven recognition. This result may serve as support for language engineers and project managers who wish to reduce attrition when defining or designing new languages for a project, domain, or context."
  },
  {
    "keywords": [
      "Aborrecimento",
      "Aprendizagem automática",
      "Computação afectiva",
      "Inteligência artificial",
      "Sensores",
      "Affective computing",
      "Artificial intelligence",
      "Boredom",
      "Machine learning",
      "Smartphone",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "A machine learning approach to boredom detection in smartphones",
    "autor": "Campos, Carlos José Gomes",
    "data": "2020-12-22",
    "abstract": "Desde tempos antigos que o ser humano tenta combater sentimentos negativos, como a tristeza\ne a solidão. Uma outra emoção que sempre perturbou a humanidade é o aborrecimento. Desta\ncausa nascem vários tipos de arte e também diversos desportos, sendo que estes continuam a\nser observados e/ou praticados até hoje. Nos dias de hoje, dado o facto que os smartphones se\ntornaram dispositivos utilizados a nível global, faz com que as pessoas sejam submetidas a cada\nvez mais estímulos. Assim sendo, quando as mesmas não estão ocupadas, sentem a necessidade\nde fazer algo que mantenha o seu cérebro activo. Por esta razão, detectar aborrecimento quando\nse usa um smartphone, abre caminho para melhorar os índices de sucesso dos sinais de estímulo,\ncom um sistema menos intrusivo e mais inteligente. Numa fase inicial, este trabalho de investigação\nfocou-se na recolha de dados. Desenvolveu-se uma aplicação inicial, para cumprir este objectivo.\nO principal propósito desta aplicação protótipo é a recolha da gama de valores dos sensores físicos\ne virtuais de um dispositivo móvel, e dados que possam ilustrar o comportamento digital durante\no seu uso. Posteriormente à construção do conjunto de dados, foi realizado uma série de técnicas\ne processos relacionados com Machine Learning para eleger o melhor modelo possível. Por fim, a\núltima etapa foi a elaboração da aplicação final, já com o modelo ideal incorporado, que é capaz\nde indicar quando o utilizador de um smartphone está aborrecido, com base nos valores indicados\npelos sensores e pelo estado do próprio dispositivo móvel. O modelo incorporado trata-se de uma\nRede Neuronal Artificial que tem a capacidade de prever o nível de aborrecimento da pessoa que\nestá a interagir com o telemóvel. Este modelo consegue prever o sentimento em causa com uma\nprecisão de 70%."
  },
  {
    "keywords": [
      "681.3:61",
      "61:681.3",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Avaliação da qualidade de informação no registo da cirurgia segura",
    "autor": "Lopes, Jorge Miguel Lobão",
    "data": "2014",
    "abstract": "Os sistemas de informação representam uma parte importante dentro das organizações e o\nseu papel, para o fornecimento de informação a tempo e onde é necessária, é crítico. Nos últimos\nanos, o recurso a estes sistemas tem aumentado exponencialmente, sendo possível encontra-los em\npraticamente todas as áreas da ação humana.\nA área da saúde não é alheia a esta tendência e tem-se observado um aumento bastante\nacentuado na quantidade de dados processados e armazenados.\nAssegurar que a informação essencial para o correto funcionamento da instituição está\ndisponível e tem um elevado grau de qualidade são missões da Informática Médica.\nAs ferramentas de Business Intelligence entram nesta área com uma elevada relevância, pois\npermitem a análise de dados e da informação que estes potencialmente representam, através do\nrecurso a diversas metodologias, aplicações e tecnologias.\nO registo clínico electrónico, para os dados do projeto “Cirurgia Segura Salva Vidas”, pode ser\npotencialmente vital para a qualidade e segurança do paciente no Bloco Operatório, uma vez que este\ndefine etapas e passos a serem realizados pelos profissionais envolvidos com o objetivo de reduzir o\nnúmero de eventos adversos.\nAvaliar a qualidade da informação deste registo torna-se, por isso, particularmente\ninteressante, principalmente quando se ponderam as potenciais consequências que a informação\nextraída deste registo pode ter na instituição.\nOs indicadores utilizados permitiram retirar conclusões sobre a qualidade da informação\nencontrada nos registo da cirurgia segura no CHAA e considera-se que a ferramenta de Business\nIntelligence, Pentaho Community, foi aplicada com sucesso no sector da saúde. As necessidades e\nfalhas de informação presentes no CHAA foram identificadas."
  },
  {
    "keywords": [
      "Personalização",
      "Comunicação",
      "Otimização",
      "Eficiência",
      "Personalization",
      "Communication",
      "Optimization",
      "Efficiency",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Your travel: plataforma de agendamento de viagens personalizadas",
    "autor": "Ribeiro, Miguel Mateus",
    "data": "2022-12-13",
    "abstract": "Hoje, no turismo, a tendência é hoje cada vez mais voltada para a personalização, para as experiências \nlocais, para a qualidade da viagem e não para a quantidade de cidades visitadas. Para o sucesso de uma \nviagem personalizada é muito importante dois fatores, a agência, que planeia a viagem e o guia. A \nagência, para além de ter de conhecer o destino a ser visitado, tem de ser organizada, seja relativamente \ná equipa que a constitui como com os parceiros com que trabalha para construir as Viagens. Para além, \ndisso esta também precisa de ter uma comunicação fácil com o cliente pois sem isso não tem como \nsaber quais são as preferências e motivos da viagem que este quer fazer. O principal objetivo desta \ndissertação é fazer uma plataforma web que possa fornecer as ferramentas necessárias á agência para \nque esta possa construir viagens personalizadas com sucesso. Para isso, o desenvolvimento desta foi\ncontou com a orientação da equipa de BackOffice da Your Tours e graças a isso foi possível detetar quais \nas maiores dificuldades que era necessário a plataforma resolver, relativamente a organização, \ncomunicação e planejamento e perceber quais as features que faltavam nas plataformas utilizadas por \nestes. Graças a isso, esta plataforma que possibilita fácil organização de trabalhos /tarefas e\ncomunicação entre a equipa do BackOffice, uma comunicação direta com o cliente para que este possa \nter uma participação ativa na construção da sua viagem e possa acompanhar todo o processo em tempo \nreal e um flow eficiente e otimizado de construção de viagens personalizadas com orçamentação \nautomática, cronogramas dinâmicos e construção de itinerários digitais fácil e eficiente."
  },
  {
    "keywords": [
      "Ambient intelligent system",
      "Attention",
      "Decision support system",
      "Teacher",
      "Students",
      "Sistema inteligente ambiente",
      "Atenção",
      "Sistema de suporte de decisão",
      "Professor",
      "Alunos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Modelling an intelligent interaction system for increasing the level of attentiveness and engagement",
    "autor": "Castro, David Barbosa",
    "data": "2018-12-14",
    "abstract": "One of the main factors for achieving school success is related to the level of attention and\ninterest that students manifest in the classroom. When in the classroom students perform\ntasks on electronic devices, and if the classes have a high number of students, the teacher\ndoes not have the idea if a student is really attentive and focused on the tasks defined\n(Pimenta et al. (2015)). Usually the teacher only realizes this situation when he evaluates\nthe students, which is in most of cases too late.\nHowever, if the teacher receives information about the attention and interest of each user\n(student) of a class in real time, he/she can adopt a set of teaching strategies in order to\nmaximize the results of his/her students. Hence it is possible to avoid and prevent some\nnegative behaviors in the classroom and increase the level of attention and consequently,\nschool success. In terms of school success, it is common sense that a high level of attention,\nallows to acquire better results.\nThis thesis intends to develop a framework which allows the teacher to visualize, in real\ntime, the attention level of each student, allowing him to adopt strategies for the students\nwith abnormal behaviors (Carneiro et al. (2015); Duraes et al. (2016b))."
  },
  {
    "keywords": [],
    "titulo": "Simulation of large scale pervasive displays networks",
    "autor": "Ferreira, Luís António Araújo",
    "data": "2012",
    "abstract": "As redes de ecrãs públicos de área alargada estão-se a tornar um paradigma\nemergente e representam uma transformação radical em relação à maneira\ncomo encaramos a disseminação da informação em locais públicos.\nEstas redes com o sua natureza ubíqua levantam alguns desafios para\nquem tem que as desenhar, instalar e usar. É bastante importante perceber\nquais são as principais compromissos a assumir quanto ao desenho das redes\nde ecrãs, principalmente em relação aos seus componentes e respectivos\nprotocolos, para desta forma podermos oferecer uma rede aberta, global e\nsobretudo escalável.\nA partir destas ideias o trabalho de caracterizar os componentes de rede\né um dos pontos essenciais para alcançar um desenvolvimento fundamentado\ndeste sistema. Também é fundamental ter uma avaliação dos desenvolvimentos\nrespeitantes à desempenho do sistema e à forma como o aumento do\nnumero de intervenientes no mesmo afecta o seu comportamento.\nAssim este trabalho, complementando essa caracterização e classificação\ninicial, pretende desenvolver uma ferramenta que permita às demais equipas\nmultidisciplinares criar cenários e modelos de simulação para confirmar se\nas suas decisões quanto aos padrões a implementar são os que melhor se\nadequam aos requisitos destas redes."
  },
  {
    "keywords": [
      "Active learning",
      "Data science",
      "Fraud detection",
      "Machine learning",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Active learning for fraud detection",
    "autor": "Leite, Miguel Lobo Pinto",
    "data": "2020-11-13",
    "abstract": "Um obstáculo comum em vários domínios no processo de preparação de um modelo de Machine Learning (ML) é a escassez de labels (i.e., etiquetas dos dados). Em aplicações reais, algures no processo de construção de um dataset existe um especialista a fazer anotação manual de cada instância dos dados para identificar a respetiva label. Dentro do domínio de deteção de fraude, que é normalmente tratado como um problema de ML supervisionado, a existência de analistas de fraude a reverem todas as transações que ocorrem representaria um nível de custos em recursos humanos inexequível. Isto leva a que apenas uma fração dos dados possam ser manualmente analisados. O sub-campo de ML conhecido como Active Learning (AL) surgiu em resposta a este problema. Em AL são implementados algoritmos que selecionam de forma eficiente quais as instâncias dos dados que devem ser analisadas de forma a otimizarem-se os custos de anotação dos dados. O objetivo principal deste processo é a criação de um modelo de previsão eficaz treinado com a menor quantidade de dados possível. Neste trabalho, apresentamos um estudo detalhado de diversas estratégias de AL em que realizamos experiências com dados de aplicações reais. Focamo-nos principalmente no cenário em que a anotação dos dados é iniciada a partir do primeiro dia de geração dos mesmos, não tendo à partida dados prévios para a construção de perfis dos utilizadores nem quaisquer labels. Apresentamos avaliações de novos algoritmos e configurações de AL, assim como métodos pré-existentes, através de múltiplas experiências. Estas experiências são realizadas num ambiente em streaming (tal como nos sistemas de produção em causa), em que as transações ao processadas em tempo real. Para além da escolha do algoritmo de AL existem outros parâmetros a definir na configuração geral. Realizamos estudos que nos permitem compreender quais os valores mais favoráveis de vários destes parâmetros, incluindo o impacto da escolha do método de pré-processamento de dados e do modelo de ML usado em avaliação. A maioria dos algoritmos de AL existentes na literatura exigem um conjunto de dados já com labels que tenha elementos de todas as classes existentes (e.g., transações legítimas e fraudulentas). Dado que no domínio da deteção de fraude é comum a ocorrência de transações fraudulentas ser rara, isto pode limitar quão rápido um algoritmo de AL totalmente supervisionado pode começar a ser utilizado nas primeiras iterações do processo. Em resposta a este problema nos apresentamos uma framework de AL em três fases que utiliza, num período intermédio, um algoritmo de AL que recorre à estrutura dos dados com labels sem utilizar as mesmas. Isto resulta num aumento da eficácia do sistema de AL. Dada a hipótese de que dois algoritmos de AL podem ser combinados de forma a produzir um que seja melhor que as suas partes, também desenvolvemos e estudamos vários métodos de combinação destes algoritmos. Realizamos uma comparação com uma grande quantidade de combinações que nos levam à conclusão de que tais combinações não aumentam a eficácia relativamente aos algoritmos individuais numa framework de três fases. Finalmente, realizamos um conjunto de experiências em larga escala que cobrem os diversos casos de uso da deteção de fraude. Os resultados indicam que AL é uma solução adequada para os casos de banking e merchant, principalmente quando utilizados algoritmos de AL baseados em incerteza. Contudo, o nosso estudo não demonstrou resultados positivos para um dataset de banking com ocorrências de fraude extremamente raras nem para o dataset de merchant acquirer."
  },
  {
    "keywords": [
      "Plataforma Internet Protocol television (IPTV)",
      "Desenvolvimento de aplicações",
      "Templates",
      "IPTV platform",
      "Applications",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aplicações para TV baseadas em templates",
    "autor": "Cunha, Victor Manuel da",
    "data": "2019-12-23",
    "abstract": "Altice Labs is developing a new Internet Protocol television (IPTV) platform to replace\nthe one it currently possesses, being based on standard technologies and open standards\nallowing for greater evolution and resilience. To check if it is capable of incorporating\nservices used in production, the company decided to implement one of these services on the\nnew platform. The chosen service was a back office service, designated by Landing Pages\n(LP) that allow the creation of applications based on predefined templates. Applications\nare then presented through an interpreter present on the client, as in their set-top boxes.\nThis master’s dissertation aims to implement this interpreter of LP applications on the new\nplatform, and also check if it is able to support the services that are currently available.\nThe implementation was made in JavaScript using Google Chrome as a web browser and the\nNW.js platform (node-webkit) as tests environments, since the specification of the set-top\nbox, where the platform should run, is still unknown.\nAn analysis of the service and its templates led to their division into groups. Each group\nwas then implemented sequentially in an iterative and incremental process. The implemen tation revealed that the platform is capable of incorporating the intended features but it\nhas flaws that must be corrected. This dissertation presents the process followed to recreate\nall templates on the new platform and corresponding analysis of its features on the new\nplatform. This process provides pointers on how the interpreter and platform should evolve\nto enable deployment of the services that the company Altice Labs intends to make available."
  },
  {
    "keywords": [
      "519.245",
      "669.715"
    ],
    "titulo": "Simulation of the nucleation of the precipitate Al3Sc in an Aluminum Scandium alloy using the kinetic Monte Carlo method",
    "autor": "Moura, Alfredo de",
    "data": "2012",
    "abstract": "As estruturas de precipitados desempenham uma função fundamental nas ciências dos materiais devido à capacidade de obstruir o movimento de deslocamentos dentro do material.\nEsta tese de mestrado debruça-se sobre uma aplicação baseada na mecânica estatística, nomeadamente o método Monte Carlo, no estudo e previsão do fenómeno de precipitação em ligas de alumínio. A liga de alumínio em estudo é a liga alumínio escândio.\nEsta tese aborda temas como a mecânica computacional, mecânica estatística, ciências dos materiais, difusão, e ainda métodos de mineração de dados (data mining). A tese descreve as condições que influenciam a precipitação e como controlar este fenómeno.\nO resultado desta tese é um conjunto de aplicações de software que permitem (i) efetuar a simulação de Monte Carlo, (ii) analisar os resultados usando a técnica de mineração DBSCAN e (iii) comparar os resultados da simulação com a teoria clássica da nucleação. Os resultados práticos obtidos com estas aplicações são:\n- Relatórios da simulação, da análise dos clusters/precipitados com o algoritmo DBSCAN e da aplicação da teoria clássica da nucleação;\n- Ficheiros para visualização 3D da simulação (em vários pontos ao longo do tempo);\n- Ficheiros para visualização 3D dos precipitados. Larry Gonick é um cartoonista que desenhou cartoons para a revista Discover. É autor de um vasto conjunto de livros das quais quero mencionar: “The Cartoon Guide to Statistics” da qual esta figura foi retirada e que me parece adequado na forma como ilustra vários temas que esta tese aborda: estatística, aleatoriedade, “salto” e “barreira”."
  },
  {
    "keywords": [
      "Confluence",
      "Desenvolvimento de software",
      "Documentação",
      "Forge",
      "Macro",
      "Relatório",
      "Testes de software",
      "Xray",
      "Documentation",
      "Report",
      "Software development",
      "Software testing",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Integração de uma aplicação de reporting para testes de software no confluence cloud",
    "autor": "Sá, João Pedro Coimbra Martins de",
    "data": "2023-11-27",
    "abstract": "A crescente evolução tecnológica verificada nas últimas décadas e a necessidade de criação e implementação de novas soluções de software, levam a que o papel dos testes de software assuma, cada vez mais,\numa maior relevância no respetivo ambiente de desenvolvimento. Boas ferramentas de gestão e monitorização de testes, assim como, de geração de relatórios de estados e resultados associados aos mesmos\ntestes, melhoram o processo de desenvolvimento tornando-o melhor e mais completo a todos os níveis.\nO principal objetivo da presente dissertação de mestrado é criar uma nova aplicação de geração de relatórios em páginas Confluence, utilizando como fonte de dados, os testes gerados a partir do Xray. O Xray\né uma aplicação de gestão e monitorização de testes de software em instâncias JIRA. O desenvolvimento\nda aplicação implica o estudo das diferentes estratégias de desenvolvimento de software disponíveis no\necossistema Atlassian.\nA aplicação Xray foi analisada em detalhe, com especial foco nos relatórios de testes de software\ndisponibilizados, o que permitiu a formulação de uma proposta de solução para o problema em questão. De seguida, foram analisadas diferentes frameworks de desenvolvimento de software para produtos\nAtlassian, culminando na escolha da ferramenta Forge face a todas as vantagens e desvantagens a ela\nassociadas.\nDe igual modo, é apresentado todo o processo de desenvolvimento da aplicação, assim como as\nestratégias adotadas para a correta implementação de cada um dos relatórios propostos."
  },
  {
    "keywords": [
      "Distributed systems",
      "Machine Learning",
      "Artificial intelligence",
      "Fault Tolerance",
      "Sistemas distribuídos",
      "Inteligência artificial",
      "Tolerância a faltas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Fault tolerant decentralized deep neural networks",
    "autor": "Padrão, João Carlos Faria",
    "data": "2021-02-04",
    "abstract": "Machine Learning is trending in computer science, especially Deep Learning. Training\nalgorithms that follow this approach to Machine Learning routinely deal with vast amounts\nof data. Processing these enormous quantities of data requires complex computation tasks\nthat can take a long time to produce results. Distributing computation efforts across multiple\nmachines makes sense in this context, as it allows conclusive results to be available in a\nshorter time frame.\nDistributing the training of a Deep Neural Network is not a trivial procedure. Various\narchitectures have been proposed, following two different paradigms. The most common one\nfollows a centralized approach, where a centralized entity, broadly named parameter server,\nsynchronizes and coordinates the updates generated by a number of workers. The alternative\ndiscards the centralized unit, assuming a decentralized architecture. The synchronization\nbetween the multiple workers is assured by communication techniques that average gradients\nbetween a node and its peers.\nHigh-end clusters are the ideal environment to deploy Deep Learning systems. Low\nlatency between nodes assures low idle times for workers, increasing the overall system\nperformance. These setups, however, are expensive and are only available to a limited\nnumber of entities. On the other end, there is a continuous growth of edge devices with\npotentially vast amounts of available computational resources.\nIn this dissertation, we aim to implement a fault tolerant decentralized Deep Neural Net work training framework, capable of handling the high latency and unreliability characteristic\nof edge networks. To manage communication between nodes, we employ decentralized\nalgorithms capable of estimating parameters globally"
  },
  {
    "keywords": [
      "Transparent flexible heating systems",
      "PEDOT:PSS",
      "Silver nanowires",
      "Solvents",
      "Post-treatments",
      "Films deposition",
      "Screen-printing",
      "Resistance",
      "Sistemas de aquecimento transparentes e flexíveis",
      "Nanofios de prata",
      "Solventes",
      "Pós-tratamentos",
      "Deposição de filmes",
      "Serigrafia",
      "Resistência",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Fabrication of PEDOT:PSS/silver nanowire based films for the development of transparent heating systems",
    "autor": "Ribeiro, Sara Coelho",
    "data": "2023-12-05",
    "abstract": "Transparent and flexible heating systems have become increasingly important for applications such\nas in defrosting windows, sensors, or heating displays. PEDOT:PSS and silver nanowire (AgNW) films offer\nthe possibility of high transparency, flexibility, and conductivity, making them promising substitutes for ITO,\nthe most used material in these systems. In this work, these films were studied with the aim of utilizing\nscreen-printing as a reproducible method to produce effective transparent and flexible heating systems\nwith low input voltages. With that purpose, AgNWs were synthesized and the effect of different factors\non the films’ resistances was studied: the films’ thickness; PEDOT:PSS modifications with water, PEG,\nglycerol, methanol, or DMSO; post-treatments with methanol, DMSO, CTAB or sodium borohydride, and\nthe utilization of sintered AgNWs, an ionic liquid, and PVA.\nThe results showed that depositing a layer of AgNWs on PEDOT:PSS decreased its resistance slightly,\nwhile another layer of PEDOT:PSS decreased it considerably. Modifying PEDOT:PSS and post-treating it\nwith methanol had an insignificant effect on the resistances, while post-treatment with DMSO generally\nlowered them. Utilizing sintered AgNWs, an ionic liquid, PVA, or post-treating the films in CTAB or sodium\nborohydride was ineffective. As such, it was concluded that the resistance depended mainly on the thick ness of the films. When characterized thermally with an input voltage of 12 V, most films showed an\ninsignificant increase in their temperature. Nevertheless, films with two manually deposited layers of PE DOT:PSS post-treated with methanol with a layer of AgNWs, and films with screen-printed PEDOT:PSS with\na big mesh size and a layer of AgNWs were able to increase their temperature values by almost 5 °C.\nIn conclusion, different methods were studied to improve the conductivity of films for transparent and\nflexible heating systems, which showed that the films’ thickness is a key factor in their resistance and,\nconsequently, in their heating abilities."
  },
  {
    "keywords": [
      "Virtual reality",
      "Intensive care",
      "Cognitive resilience",
      "Non-pharmacological therapies",
      "Realidade virtual",
      "Cuidados intensivos",
      "Resiliência cognitiva",
      "Terapias não-farmacológicas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Evaluating constrained users ability to interact with virtual reality applications",
    "autor": "Ribeiro, Tiago Ramos",
    "data": "2023-12-15",
    "abstract": "This Master’s Project presents a comprehensive exploration of a novel Virtual Reality (VR)\napplication designed to evaluate and enhance user performance within the context of\nconstraints experienced by individuals, including those confined to an Intensive Care Unit\n(ICU). The work unfolds through a detailed examination of the proposal, development, and\nassessment phases.\nThe proposal lays the foundation for the project, emphasizing the need for an immersive\ntechnology-based solution to assess ICU patients’ abilities. It includes a well-structured\nsystem architecture, deployment architecture, and data architecture. This framework guides\nthe subsequent phases, offering insights into the development and assessment processes.\nIn the development phase, the practical realization of the VR application is explored.\nIt highlights adjustments tailored to the specific needs of ICU patients, offering valuable\ninsights into user progress, reducing dependency on external assistance. Eight distinct tasks\nare detailed, categorized based on complexity and fundamental functionalities.\nThe assessment phase evaluates the real-world impact of the VR application through three\ninterventions. While limited to non-ICU environments, these interventions capture data from\nusers who share critical constraints with ICU patients. The assessment involves correlation\nanalysis of numerous variables, including age, cognitive function (assessed through the\nMini-Mental Status Examination), and prior VR experience. The results unearth significant\ncorrelations, shedding light on age-related differences, the influence of cognitive ability, and\nthe impact of prior VR exposure on task performance.\nThis comprehensive exploration represents an essential contribution to the burgeoning\nfield of VR applications for healthcare, specifically targeting constrained user groups like\nICU patients. The findings underscore the significance of considering user characteristics,\nprior experience, and cognitive function when designing VR interventions. Further research\nis warranted to refine assessment methodologies and expand the scope of real ICU patient\ntesting, ultimately paving the way for improved patient care and enhanced rehabilitation\npractices."
  },
  {
    "keywords": [
      "681.324",
      "681.3-7"
    ],
    "titulo": "Botnet detection: a numerical and heuristic analysis",
    "autor": "Mendonça, Luís Miguel Ferreira Costa",
    "data": "2012-04-20",
    "abstract": "Internet security has been targeted in innumerous ways throughout the ages and Internet cyber criminality has been changing its ways since the old days where attacks were greatly motivated by recognition and glory. A new era of cyber criminals are on the move. Real armies of robots (bots) swarm the internet perpetrating precise, objective and coordinated attacks on individuals and organizations. Many of these bots are now coordinated by real cybercrime organizations in an almost open-source driven development resulting in the fast proliferation of many bot variants with refined capabilities and increased detection complexity.\nOne example of such open-source development could be found during the year 2011 in the Russian criminal underground. The release of the Zeus botnet framework source-code led to the development of, at least, a new and improved botnet framework: Ice IX.\nConcerning attack tools, the combination of many well-known techniques has been making botnets an untraceable, effective, dynamic and powerful mean to perpetrate all kinds of malicious activities such as Distributed Denial of Service (DDoS) attacks, espionage, email spam, malware spreading, data theft, click and identity frauds, among others.\nEconomical and reputation damages are difficult to quantify but the scale is widening. It’s up to one’s own imagination to figure out how much was lost in April of 2007 when Estonia suffered a well-known distributed attack on its internet country-wide infrastructure. \nAmong the techniques available to mitigate the botnet threat, detection plays an important role. Despite recent year’s evolution in botnet detection technology, a definitive solution is far from being found. New constantly appearing bot and worm developments in areas such as host infection, deployment, maintenance, control and dissimulation of bots are permanently changing the detection vectors thought and developed.\nIn that way, research and implementation of anomaly-based botnet detection systems are fundamental to pinpoint and track all the continuously changing polymorphic botnets variants, which are impossible to identify by simple signature-based systems."
  },
  {
    "keywords": [
      "Saúde",
      "Comunidade",
      "Avaliação",
      "Móvel",
      "Health",
      "Community",
      "Rating",
      "Mobile",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "HealthAdvisor ecosystem mobile solutions",
    "autor": "Ribeiro, Henrique José Leão Costa Santos",
    "data": "2021",
    "abstract": "Apreciando plataformas on-line de partilha de opiniões ou experiências sobre diversos serviços pode-se\nreparar que existe a necessidade, por parte dos utilizadores, de saber opiniões de outros. Aqui insere-se\no sentido de comunidade que é prevalente na Internet. Desde redes sociais a videojogos este conceito\nfaz-se presente onde a criação de uma comunidade pode ser a chave para o sucesso de uma nova ideia.\nNeste caso a nova ideia trata-se de HealthAdvisor Mobile Solutions (HeAd-MS), integrante de HealthAdvisor\nEcosystem (HeAd-ES), HeAd-MS aponta para a criação de uma plataforma móvel onde é possível\npartilhar opiniões e avaliações sobre serviços de saúde existentes, estas opiniões e avaliações são feitas\npela comunidade e para a comunidade, gerando assim valor para os utilizadores numa espécie de\nsimbiose entre utilizador e plataforma.\nO que surge com a existência do HealthAdvisor Mobile Solutions trata-se de uma solução não só\nintegrante mas ainda parte vital de um ecossistema maior HealthAdvisor Ecosystem que tem como objetivo\na criação de uma comunidade como suporte principal e mais valia do sistema, potenciando o seu\ncrescimento e florescimento graças à sua transparência e simplicidade.\nCom o desenvolvimento desta dissertação o que se alcançou foi a criação de uma aplicação móvel,\ncross-platform, com foco na comunidade. Aqui um utilizador pode-se informar sobre serviços de saúde, saber\nopiniões/avaliações de outros utilizadores e ainda partilhar a sua própria opinião sobre esses mesmos\nserviços. A aplicação foi desenvolvida em React Native servida de dados por uma web REST API."
  },
  {
    "keywords": [
      "Critical systems",
      "Embedded systems",
      "Rust",
      "Sistemas críticos",
      "Sistemas embebidos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Exploring rust for embedded and critical systems",
    "autor": "Pinho, André Brandão de",
    "data": "2020-01-09",
    "abstract": "The C programming language is perhaps the most widespread in the design of safety-critical systems. However, its adoption suffers from disadvantages because it lacks in safety, entailing extensive and expensive verification processes. There are other programming languages, such as Ada and SPARK, that offer safety features that automatically comply with the current safety standards. Nevertheless, the software industry continues to use C after all and its associated verification process to build safety-critical software. Rust is a modern programming language that promises to soften such insecurities by design, thus improving on the development of safety-critical software. Due to its ownership model, Rust can ensure memory safety at compile time. Because it is a systems programming language, it is also a promising language for embedded systems. The main aim of the project reported in this dissertation is to understand how Rust can alleviate the certification process of safety-critical software, while evaluating its maturity for embedded systems. We analyse in which platforms Rust is available and compare embedded Rust to other languages used in the safety-critical domain. This analysis consists in comparing Rust safety features with coding guidelines commonly used in the software industry. Some case studies are carried out, such as preemptive and cooperative scheduling (both in C and Rust to better understand the programming differences in these languages), a driver for an accelerometer and finally a program that uses the scheduler, the accelerometer and the leds of a micro-controller, where one thread reads acceleration values and another thread turns leds on/off according to such readings. The dissertation ends with an overview of the results obtained. Not only a comparison is given with coding guidelines used in industry, but also concerning the case studies developed. It also anticipates some important work that could be added, as well as some details where Rust could be improved to become prominent in the industry of safety critical software."
  },
  {
    "keywords": [
      "618:681.3",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Apoio à decisão em ginecologia e obstetrícia",
    "autor": "Pereira, Eliana",
    "data": "2014",
    "abstract": "Hoje em dia, a tomada de decisões de forma rápida e eficaz é essencial\nnas organizações de saúde. Neste sentido, surgem os Sistemas de Apoio à\nDecisão, as plataformas de Business Intelligence e os Sistemas de Recomenda\nção. De forma a apoiar a decisão no âmbito da ginecologia e obstetrícia,\nno Centro Materno Infantil do Porto para além de existir o Processo Clínico\nEletrónico, encontra-se também implementado um sistema de pré-triagem\nque permite distinguir as utentes em dois níveis (Urgência e Consulta). No\nâmbito desta dissertação e tendo por base os sistemas existentes, foi realizado\num projeto com o objetivo de extrair conhecimento de uma forma\nautomática e em tempo real e desenvolver um sistema de prioridades para\na triagem obstétrica. Este projeto seguiu a metodologia de investigação Design\nResearch, cujo objetivo é orientar e validar a construção de artefactos.\nNeste sentido, foi desenvolvida e implementada uma plataforma de Business\nIntelligence e elaborada uma proposta de um sistema prioridades de triagem\nobstétrica. No primeiro caso, a plataforma desenvolvida permite obter\nindicadores em duas vertentes: indicadores de monitorização do sistema de\npré-triagem existente e indicadores obstétricos. No total, foram produzidos\ncerca de 50 indicadores. A nível técnico foi seguida a metodologia de Kimball\ne foi utilizado o Pentaho Community Edition como ferramenta Open Source\nde Business Intelligence, que lhe conferiu várias características, entre elas,\no Pervasive. Em relação ao segundo artefacto, inicialmente, procedeu-se à\nexploração, validação e divulgação do sistema de pré-triagem para a comunidade\nmédica e científica. Numa fase posterior foram desenvolvidos modelos\nde Data Mining, que permitiram aferir se o sistema de pré-triagem estava\ncalibrado para triar utentes em dois níveis (acuidades superiores a 81,75%).\nFoi também desenvolvido um algoritmo de simulação que permitiu aferir se\nseria necessário e viável o sistema de pré-triagem evoluir para um sistema de\ntriagem de prioridades com triagem de 5 níveis. Por último, foi desenvolvida\ne apresentada uma proposta de um sistema de triagem prioridades obstétrica.\nApós validação dos artefactos produzidos com as equipas clínica, verifcou-se\nque este trabalho contribui para o apoio à decisão nas áreas abordadas."
  },
  {
    "keywords": [
      "Massively Multiplayer Online Games",
      "Distributed System",
      "Gaming Architecture",
      "Online Game",
      "Game Server",
      "Interest Management",
      "Synchronizationx",
      "Synchronization",
      "Scalability",
      "Networking",
      "Replication",
      "Consistency Control",
      "Jogo MMO",
      "Sistema Distribuído",
      "Arquitetura de Jogo",
      "Jogo Online",
      "Servidor de Jogo",
      "Gestão de Interesse",
      "Sincronização",
      "Escalabilidade",
      "Rede",
      "Replicação",
      "Controlo de Consistência",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Distributed game",
    "autor": "Rocha, Márcio da Silva",
    "data": "2022-06-08",
    "abstract": "The demand for online games has risen over the years, expanding multiplayer support for new and different game\ngenres. Among them are Massively Multiplayer Online games, one of the most popular and successful game\ntypes in the industry. Nowadays, this industry is thriving, evolving alongside technological advancements and\nproducing billions in revenue, making it an economic importance. However, as the complexity of these games\ngrows, so do the challenges they face when constructing them.\nThis dissertation aims to implement a distributed game, through a proof of concept or an existing game, using a\ndistributed architecture to acquire knowledge in the construction of such complex systems and the effort involved\nin dealing with consistency, maintaining communication infrastructure, and managing data in a distributed way.\nIt is also intended that this project implements multiple mechanisms capable of autonomously helping manage\nand maintain the correct state of the system.\nTo evaluate the proposed solution, a detailed analysis is carried out with performance benchmark analysis,\nstress testing, followed by an examination of its security, scalability, and distribution’s resilience.\nOverall, the present research work allowed for a greater understanding of the technologies and approaches\nused in constructing a gaming system, establishing a new set of development opportunities to be further investi gated upon the constructed solution."
  },
  {
    "keywords": [
      "Aplicações de saúde",
      "Big Five Mini-Markers",
      "Dados biométricos",
      "Machine learning",
      "Teste psicológico",
      "Biometric data",
      "Heath apps",
      "Psychological tests",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Monitorização de saúde física e psicológica utilizando dispositivos móveis",
    "autor": "Pereira, Adriana",
    "data": "2019-12-23",
    "abstract": "Na última década, tem sido cada vez mais frequente a utilização de testes de \npersonalidade, tais como o OCEAN/BIG FIVE, desenvolvido por Goldenberg em 1992, para \navaliar aquilo que os psicólogos consideram ser as cinco principais dimensões da \npersonalidade (Extroversão, Agradabilidade, Abertura, Consciencioso e Estabilidade). \nAssim sendo, o propósito fundamental desta dissertação é o de conceber e desenvolver \numa aplicação móvel focada no tratamento de vários dados biométricos do utilizador assim \ncomo na sua personalidade. Para isso, será implementado na aplicação o teste de \npersonalidade, baseado no modelo de Saucier, que permite ao utilizador escolher de entre \nquarenta adjetivos, os que melhor se adequam a si. Aquando da realização do teste, que \nse repetirá ao longo do tempo, será tida em consideração a informação biométrica do \nutilizador e o seu histórico. O objetivo é o de oferecer, ao utilizador, a possibilidade de \nconhecer os traços que compõe a sua personalidade ao mesmo tempo que lhe é fornecido \ninformação biométrica, sendo possível analisar relações e correlações entre as cinco \ndimensões fundamentais da personalidade e os dados biométricos recolhidos, assim como \na evolução de cada um destes atributos ao longo do tempo."
  },
  {
    "keywords": [
      "V2X",
      "Privacy",
      "Simulation",
      "Vehicles",
      "Privacidade",
      "Simulação",
      "Veículos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Traceability and safety tradeoffs in modern vehicles",
    "autor": "Fernandes, Mariana Pereira",
    "data": "2022-07-27",
    "abstract": "In this dissertation, the efficiency of privacy protecting mechanisms in short-range vehicular communications,\nnamely Pseudonym Change Strategies, is investigated. To evaluate these strategies, a set of simulation tools is used, that allow for the assessment of several metrics, such as the privacy level obtained and the real pseudonym consumption, resulting from the use of a representative set of pseudonym change strategies. Most importantly,\nhybrid strategies were considered, which combine schemes that were previously analysed separately. The results show that combining mix-zones with another scheme provides better privacy in most cases. Lastly, we showcase and analyse the problems found in the process of trying to make the simulated scenarios more realistic, which easily comes into conflict with tool limitations and/or subtle and hard to anticipate interactions between different components."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Shadow mapping e ray-tracing",
    "autor": "Moderno, Dave Lage",
    "data": "2011-12-07",
    "abstract": "Shadow mapping has been one of the most used algorithms for real time calculation of shadows, since\nit is extremely simple and quick in calculating said shadows, but not always presents the best results.\nOn the other hand, ray-tracing presents pixel-perfect shadows, but it is more demanding from a\ncomputational point of view.\nShadow mapping has seen many proposals to increase its accuracy, while retaining its high\nperformance nature. Some of the methods proposed, based solely on the standard shadow mapping\ntechnique, do improve significantly the standard shadow mapping result at the expense of a minor\ndecrease in performance. Other approaches propose hybrid methods, using shadow mapping as a way\nof limiting the number of pixels that require ray-tracing. One of such approaches uses texel coherence\nto reduce the number of pixels that require testing.\nThese latter approaches establish the theme for this work. The goal is to narrow down as much as\npossible the amount of pixels that require a ray-tracer to determine its shadow status.\nThe first step was to identify the location of the errors present in a shadow map. The tests confirmed\nthe intuition that most of these errors should be located in the contours of the shadow areas.\nThe next step focuses on these contour areas and looks for ways to determine the correctness of a\npixel’s shadow status. Several methods were proposed to achieve this goal. Some methods were\ncapable of confirming pixels in shadow. Some were capable of correcting pixels in light.\nEach method, with the exception of texel coherence, uses a very selective ray-tracer, i.e. only very\nfew triangles are tested for intersection with a single light ray.\nSince each method has its strengths and weaknesses an algorithm was proposed, chaining all these\nmethods together. The first step is to determine the set of pixels in the contours of the shadow areas.\nThen each method is applied in turn, so that only the pixels the remaining unconfirmed/uncorrected\npass on to the next stage.\nAt the end of the algorithm a very large percentage of pixels in shadow were confirmed and a\nsignificant number of pixels in light were corrected. The remaining pixels could then be fed to a full\nray-tracer. The load of the ray-tracer is severely reduced under this approach making it an affordable solution to obtain pixel perfect shadows in the contours of the shadowed areas."
  },
  {
    "keywords": [
      "Data science",
      "Financial performance prediction",
      "Natural language processing",
      "Readability evaluation",
      "Stock markets",
      "Avaliação da legibilidade",
      "Ciência de Dados",
      "Mercado de ações",
      "Previsão do desempenho financeiro",
      "Processamento de linguagem natural",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Combining data and text mining techniques for automatic analysis of financial reports",
    "autor": "Pinto, Marcelo Queirós",
    "data": "2020-01-09",
    "abstract": "The application of Data Science techniques, specifically Natural Language Process ing (NLP) and Machine Learning, in financial markets is of immense interest to in vestors, as these techniques can have a potential economic impact. In particular, stock\nmarkets represent an opportunity that has been exploited in several ways, such as us ing market opinions (e.g., news, blogs) to predict the direction of price movement or even volatility.\nThis study analyses the 10-K documents of the S&P 100 index for 10 years (2008-2017), which contains the 102 largest companies in the United States of America. The\n10-K is an annual financial report required by the United States Securities and Ex change Commission (SEC), which describes the financial performance of a company.\nRecent research suggests that the readability of a company’s 10-K text document may\ninfluence its future financial performance, since the way the market perceives textual\ninformation also depends on the readability of that text. In this sense, this work aims\nto understand the relationship between 48 readability metrics applied to these reports\nand the corresponding future financial performance of these companies. A clustering\napproach was applied over these readability metrics, aiming to identify distinct and\nvaluable readability clusters. As an external evaluation, we assessed the information\nvalue of the clusters by analyzing 3 future crash risk metrics, that are often used to\nassess the companies’ financial performance."
  },
  {
    "keywords": [
      "Doença de Parkinson",
      "Marcha",
      "Biomarcar",
      "Padrões",
      "Sensores vestíveis",
      "Inteligência Artificial",
      "Parkinson’s Disease",
      "Gait",
      "Biomark",
      "Patterns",
      "Wearable sensors",
      "Artificial Intelligence",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Artificial intelligence-based software for recognizing parkinsonian gait patterns based on wearable miniaturized sensors",
    "autor": "Pinheiro, Pedro Gonçalo Santos Pires",
    "data": "2022-12-19",
    "abstract": "A Doença de Parkinson (DP) é uma doença degenerativa do sistema nervoso central, geralmente caracterizada por prejudicar vários aspetos da marcha dos pacientes, como bradicinesia, comprimento do passo\nencurtado e congelamento da marcha. As escalas de avaliação clínica são tipicamente usadas com base\nem exames para monitorizar esses sintomas motores associados à marcha. Além disso, estas avaliações\nsão baseadas na memória dos pacientes e pesquisas subjetivas, fornecendo dados tendenciosos. Assim,\nsão necessários dados de longo prazo sobre as atividades motoras diárias do paciente.\nAvanços tecnológicos forneceram dispositivos sensores pequenos e vestíveis capazes de capturar dados de longo prazo, podendo ser utilizados em ambientes domiciliares permitindo a captura de dados\nprecisos. A combinação desses sensores com inteligência artificial (IA) produz modelos capazes de biomarcar os níveis de doença, condições motoras e bem-estar dos pacientes, e de fornecer dados não\ntendenciosos sobre os padrões de marcha dos pacientes. A integração destes modelos num aplicativo\npara médicos facilitará gerir o estado de DP e tratamentos mais personalizados serão alcançados.\nTendo isto em conta, esta tese tem como objetivo usar dados de pacientes que apresentam deficiências de marcha para treinar modelos baseados em IA que sejam capazes de classificar níveis de doença,\ncondições motoras e qualidade de vida desses pacientes.\nPara isso, foram adquiridos dados de 40 pacientes com DP, com o objetivo de desenvolver 3 modelos\nde IA diferentes, um usado para classificar o nível de doença de um paciente na escala UPDRS-III, outro\npara classificar as condições motoras escala H&Y e outro usado para classificar a qualidade de vida.\nEsses modelos foram implementados numa APP para auxiliar os médicos durante as suas consultas.\nOs resultados obtidos foram positivos. O modelo UPDRS-III conseguiu uma acurácia de 91,67%,\numa sensibilidade de 90,43% e uma especificidade de 93,98%, enquanto o modelo H&Y alcançou uma\nacurácia de 88,98%, uma sensibilidade de 88,71%, e especificidade de 92,79%, sendo que o modelo\nPDQ-39 obteve acurácia de 84,19%, sensibilidade de 82,13% e especificidade de 90,24%."
  },
  {
    "keywords": [
      "Twitter",
      "Classificação de documentos",
      "Deep Learning",
      "Língua portuguesa",
      "Document classification",
      "Portuguese language"
    ],
    "titulo": "Twitter Observatory: developing tools to recover and classify information for the social network Twitter",
    "autor": "Elias, Constança Machado Aires Lobo",
    "data": "2022-12-19",
    "abstract": "As redes sociais tornaram-se na nova forma de comunicar e, consequentemente, uma importante\nfonte de informação. Mais concretamente, o Twitter, desde a sua criação, tornou-se numa das redes\nsociais mais utilizadas. Esta popularidade permitiu um aumento do número de investigações na área de\nText Mining usando o Twitter para diferentes aplicações, como saúde e política. Nesta área, a classificação de documentos tem sido aplicada a vários dados, nomeadamente tweets, para analisar tendências,\nentender o comportamento humano e prever determinados eventos. No entanto, nem sempre é possível\nter os datasets desejados para efectuar essa classificação e análise.\nPara resolver o problema encontrado, esta dissertação, proposta pela OmniumAI, pretende explorar\nas abordagens já existentes para a extração e classificação de dados do Twitter, focando-se principalmente na língua portuguesa. Para isso, foi desenvolvida uma API capaz de extrair tweets de acordo\ncom um determinado tópico de interesse, e criar datasets classificados automaticamente com labels de\nrelevância. Foi ainda desenvolvida uma pipeline de classificação de tweets com base nas abordagens\nde Deep Learning encontradas no Estado de Arte para a classificação de documentos. O produto final\nconsiste numa framework, Twitter Observatory, que permite aos utilizadores criar datasets de acordo com\num determinado tópico de interesse e analisar esses mesmos datasets.\nPara testar a framework desenvolvida, foram selecionados dois casos de estudo: COVID-19 e a Invasão Russa da Ucrânia em 2022. Relativamente a estes dois tópicos, dois datasets foram extraídos e\nclassificados de acordo com a relevância dos tweets, contendo, respetivamente, 2,268,575 e 219,887\ntweets em português. Foi feita uma análise exploratória destes dados e os resultados de classificação\nusando modelos de Deep Learning foram apresentados. Para validar esses resultados, foi utilizado o\ndataset existente CrisisLex, traduzido para português."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Assistive autonomous smart walker in a hospital environment as a rehabilitation tool",
    "autor": "Soares, Rui Jorge Cerqueira",
    "data": "2018",
    "abstract": "Ageing brings a lot of problems in the human body. One really common issue is the lost of\nsome mobility be it simply by losing control or by contracting some sort of illness. However,\nsome times these problems can be tackled with some physical therapy and a recovery can\nbe achieved, even if not full recovery.\nThis master dissertation tries to enable a medical device with the ability of adapting to\ndifferent types of disabilities, developing a system capable of tackling a range of mobility\ndisorders through therapy.\nThis work adds an autonomous mode where more severe problems can be treated leaving\nall the coordination to the walker letting the user focus on training their gate.\nIt also adds a shared control mode where the task of guiding the robot can be offloaded\nto the user, targeting patients in a more advanced recovery state forcing them to focus\non training their coordination and ambient perception besides from the treating their gate\nproblems.\nThe directed tests showed a system capable of adapting to any situation it was put\nthrough, with a robust set of emergency mechanism in case anything malfunctions.\nBased on the analysed results and the answers to the survey, we can indeed say that the\nsystem would serve as a suitable integration in the ASBGO*, walker enabling the walker\nwith more versatility."
  },
  {
    "keywords": [
      "Chest radiography",
      "Deep learning",
      "Convolutional neural networks",
      "Medical imaging analysis",
      "Visualization methods",
      "Dataset pre-processing",
      "Data augmentation techniques",
      "Diagnostic tool refinement",
      "Radiografia torácica",
      "Aprendizagem profunda",
      "Redes neurais convolucionais",
      "Análise de imagens médicas",
      "Métodos de visualização",
      "Pré-processamento de conjunto de dados",
      "Técnicas de aumento de dados",
      "Refinamento de ferramentas de diagnóstico",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Exploring visualization methods for CNNs in X-ray image classification",
    "autor": "Apostolyuk, Valeriy",
    "data": "2024-01-15",
    "abstract": "The chest radiography is one of the most frequently requested hospital exams, playing a pivotal role in\nthe diagnosis, management, and monitoring of respiratory diseases in patients presenting with respiratory\nsymptoms. Harnessing the power of deep learning methodologies has revolutionized the field of medical\nimaging analysis, enabling the development of robust models for accurate disease prediction and clas sification, encompassing an extensive array of conditions. The inherent strength of deep learning lies in\nits ability to automatically extract intricate features from complex datasets, thereby facilitating enhanced\naccuracy without the need for extensive human intervention.\nThis study delves into the intricate mechanisms of a chest X-ray classification model, highlighting\na noteworthy inclination toward seemingly inconsequential regions of the X-rays, particularly fixating on\nnon-diagnostic elements such as letters and symbols. While the elimination of these artifacts did not\nsubstantially alter the model’s classification outcomes, it raises pertinent inquiries about the potential\nbenefits of a preprocessed dataset devoid of such distractions.\nThis research also scrutinizes the visualization methods employed, unraveling an intriguing tendency\nof the model to focus on regions comprised entirely of black pixels in certain cases. This observation insti gated a comprehensive examination of the efficacy of the visualization methods, uncovering a substantial\nmisalignment between the generated hotspots and the annotations provided by radiologists. Notably, the\nstudy identifies that the size of the bounding box and the model’s classification success rate share a strong\ncorrelation, indicating that larger bounding boxes generally demonstrate better alignment. Additionally,\nwe establish a direct relationship between the model’s classification accuracy and the alignment with\nvisualization techniques.\nThis analysis not only deepens our understanding of the dynamics at play within chest X-ray analysis\nbut also underscores the essential considerations necessary for refining the training and visualization\nprocesses in machine learning-based diagnostic tools."
  },
  {
    "keywords": [
      "Localização geográfica",
      "Padrões de condução",
      "Sustentabilidade",
      "Geolocation",
      "Driving patterns",
      "Sustainability",
      "681.324:656.1",
      "656.1:681.324",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Assessing road traffic expression",
    "autor": "Sarmento, João Duarte Machado",
    "data": "2014-12-12",
    "abstract": "A utilização dos transportes rodoviários, nomeadamente o transporte rodoviário de pessoas, é um dos meios mais adoptados pela população para deslocação, tanto dentro da cidade como entre cidades. Como é sabido de senso comum, existem horas e percursos mais críticos. Estes, consequentemente, podem provocar desgastes ambientais e económicos tais como degradação do piso ou aumentos de consumos de combustível - que por sua vez aumentam as emissões de gases e também desgastes a nível pessoal, como mudanças de stress por estar no meio de um ambiente de trânsito intenso com possíveis horários a cumprir.\n\nEste trabalho foca-se na recolha e na análise de dados através de dispositivos móveis como Smartphones de forma a obter dados e informação que permita tirar conclusões sobre perfis de condução, caracterização de vias, e partilha de informação."
  },
  {
    "keywords": [
      "Distributed and parallel computing",
      "Discrete-event simulation",
      "Epidemic protocols",
      "Peer sampling service",
      "Performance",
      "Scalability",
      "Computação distribuída e paralela",
      "Simulação por eventos",
      "Protocolos epidémicos",
      "Serviço de amostragem de nós",
      "Performance",
      "Escalabilidade",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Simulation of epidemic protocols",
    "autor": "Sobral, Luís Filipe Cruz",
    "data": "2023-12-15",
    "abstract": "We live in a digital era, in a world connected by technology. The incredible capabilities of our mobile\nphones and computers let us communicate and get data from all over the globe, in the instance of a\nmillisecond. However, technological progress doesn’t stop. We persist in looking for faster connections,\ninnovative applications and platforms, more efficient, scalable, and resilient. Distributed systems are the\nfundamental basis driving this progress in several scientific and industry fields. Epidemic protocols are\ncrucial to ensure efficient data dissemination on these systems, providing fault tolerance, scalability, and\navailability. Its relevance grows as networks become more dynamic and distributed, playing a main role in\nensuring the reliability and efficient operation of these systems.\nProgress is not possible without studies and experimental evaluation of proposed algorithms. Although,\nas they are projected to systems compromising millions of nodes and processes, these studies are almost\nimpossible at this scale, so most rely on simulation. Discrete-event simulation is one of the major experi mental methodologies in several scientific and engineering domains. The used simulator is often seen as\na technical detail, and many researchers develop their custom tool. Simulation tools vary in complexity\nand application, catering to a wide range of industries and research domains. The choice of a specific tool\ndepends on the nature of the simulation, the problem being addressed, and the preferences and expertise\nof the user.\nIn this dissertation, we present, analyze, and compare a set of selected simulation tools, to choose\nthe one that better fits epidemic protocol simulations in P2P systems. After choosing the most adequate\nsimulation tool, we defined a generic simulation framework for epidemic protocols, and implementations\nof two different peer sampling services and one dissemination protocol. Leveraging this framework, we\nperform a extensive evaluation of the different protocols."
  },
  {
    "keywords": [
      "Data mining",
      "Árvores de decisão",
      "Regressão logística",
      "Redes neuronais",
      "Random forests",
      "Support vector machines",
      "Previsão de churn",
      "Sistemas de retalho",
      "Decision trees",
      "Logistic regression",
      "Neural networks",
      "Churn prediction",
      "Retail systems",
      "681.3",
      "658.8"
    ],
    "titulo": "Um modelo para previsão de churn na área do retalho",
    "autor": "Veloso, Fernando Jorge Machado",
    "data": "2013-03-20",
    "abstract": "O ambiente de grande competitividade característico do sector do retalho e crescente dificuldade na captação de novos clientes leva as empresas a apostar na implementação de estratégias adequadas para promover a satisfação dos clientes adquiridos para motivar a sua lealdade. É neste contexto que se começa a reconhecer a importância de combater o fenómeno de churn, ou seja, a perda de clientes. É necessário identificar os clientes que estão em risco de churn e, para isso, é necessário criar um método que o permita fazer com antecedência para que possam recair sobre eles as campanhas de retenção proactivas. Quanto mais eficaz for o método a identificar os clientes em riscos, maior será o retorno da aplicação da campanha. Muitos trabalhos têm sido desenvolvidos na área de previsão de churn nos mais diversos sectores. Contudo, na área do retalho a pesquisa têm sido muito limitada. Assim, com este trabalho de dissertação pretendeu-se estudar o fenómeno da perda de clientes com o objectivo de definir e implementar um modelo de churning para o sector do retalho recorrendo a técnicas de mineração de dados. Pretendeu-se fazer um levantamento das principais questões envolvidas na previsão de churn no retalho, na construção do conjunto de dados (assinaturas dos clientes) e na aplicação de técnicas de mineração de dados no processo de previsão. Nesse sentido, foram construídos alguns modelos para fazer a previsão de casos de churn baseados em cinco das técnicas de classificação mais utilizadas em trabalhos de previsão de churn: Árvores de Decisão, Regressão Logística, Redes Neuronais, Random Forests e SVM. A avaliação e comparação da performance dos modelos elaborados foi feita de acordo com várias medidas como accuracy, precision, sensitivity, specificity, f-measure e AUC e, para além disso, foi testado o impacto, na precisão do modelo, da alteração da\ndensidade de eventos de churn no conjunto de treino."
  },
  {
    "keywords": [
      "Ponto de controlo e restauro",
      "Adaptação dinâmica aos recursos",
      "Grelhas computacionais",
      "Programação orientada aos aspectos",
      "Checkpointing and restart",
      "Run-time adaptation",
      "Grid computing",
      "Aspect oriented programming",
      "681.3"
    ],
    "titulo": "Técnicas de ponto de controlo e adaptação em grelhas computacionais",
    "autor": "Medeiros, Bruno Silvestre",
    "data": "2011-09-26",
    "abstract": "A recente popularidade dos ambientes de grelhas introduziu a necessidade de suportar a execução robusta de aplicações numa gama alargada de recursos computacionais. Em contextos de grelhas computacionais, onde a fiabilidade e disponibilidade dos recursos não é garantida, as aplicações deverão ser capazes de suportar dois requisitos fundamentais: 1) tolerância a faltas; 2) adaptação aos recursos disponíveis. As técnicas tradicionais utilizam uma abordagem \"caixa-negra\", onde a camada intermédia de software (mediador) é a única responsável por assegurar estes dois requisitos. Estes tipos de abordagens possibilitam o suporte a estes serviços com uma intervenção mínima do programador, mas limitam a utilização de conhecimento sobre as características da aplicação, visando a otimização destes serviços. Nesta tese são apresentadas abordagens orientadas aos aspetos para suportar tolerância a faltas e adaptação dinâmica aos recursos em grelhas computacionais.\n\nNas abordagens propostas, as aplicações são aprimoradas com capacidades de tolerância a faltas e de adaptação dinâmica através da ativação de módulos adicionais. A abordagem de tolerância a faltas utiliza a estratégia de ponto de controlo e restauro, enquanto a adaptação dinâmica utiliza uma variação da técnica de sobre-decomposição. Ambas são portáveis entre sistemas operativos e restringem a quantidade de alterações necessárias no código base das aplicações. Além disso, as aplicações poderão adaptar de uma execução sequencial para uma configuração multi-cluster. A adaptação pode ser realizada efetuando o ponto de controlo da aplicação e restaurando-a em diferentes máquinas, ou então, realizada em plena execução da aplicação."
  },
  {
    "keywords": [
      "Controlo analítico",
      "Deteção de anomalias",
      "Estações de tratamento de águas residuais",
      "Machine Learning",
      "Analytical control",
      "Anomaly detection",
      "Wastewater treatment plants",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Deteção de anomalias em estações de tratamento de águas residuais",
    "autor": "Silva, Diogo Filipe Gigante da",
    "data": "2022-11-28",
    "abstract": "A iminente escassez de recursos naturais e o constante aumento populacional tem assolado o presente século. Tal crescente habitacional contribui para uma concentração nos grandes centros urbanos e, consequentemente, um maior nível de poluição quer em contextos habitacionais como industriais. Nesta vertente, as Estações de Tratamento de Águas Residuais, desempenham um papel crucial no controlo do nível de qualidade da água que é reutilizada ou descarregada para o exterior. Estas instalações recebem ininterruptamente cargas de afluentes extremamente poluentes que são provenientes da rede pública de esgotos e que carecem de um tratamento faseado para a purificação das mesmas. Porém, para garantir a qualidade da água que é reaproveitada ou devolvida ao meio ambiente, é necessária monitorização contínua destas estações de forma a permitir o processo de tomada de decisão. Posto isto, esta dissertação visa implementar modelos de Machine Learning com o intuito de detetar possíveis anomalias nas substâncias presentes no efluente destas infraestruturas. Assim sendo, são aplicados modelos como Isolation Forest (IF), One Class Support Vector Machine (OCSVM) e Long Short-Term Memory Autoencoder (LSTM-AE) para identificar os registos do Azoto Total, Nitratos e pH que possam ser anómalos. No caso em específico das LSTM-AE, são considerados três thresholds para classificar os registos, dos quais, dois utilizam valores estáticos e um consiste em valores dinâmicos. De entre os melhores modelos candidatos, no global, os modelos de IF e OCSVM alcançaram resultados superiores aos modelos baseados em LSTM-AE. No que diz respeito aos thresholds, as abordagens com valores estáticas de forma geral, atingiram resultados ligeiramente superiores. Em suma, os vários cenários aplicados permitiram concluir que os modelos concebidos conseguiram detetar as várias anomalias presentes nas substâncias referidas."
  },
  {
    "keywords": [
      "Skills for life",
      "Teacher support tools",
      "Student support tools",
      "Integration of teaching tools",
      "Competências de vida",
      "Ferramentas de suporte ao professor",
      "Ferramentas de suporte ao aluno",
      "Integração de ferramentas pedagógicas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "E•CVoltera, Competence Assessment System",
    "autor": "Lopes, Diana Cristina Abreu",
    "data": "2022-04-05",
    "abstract": "This document presents and discusses a project in the area of managing tools to support\nteaching. This project is part of the work in the second year of the Master degree in\nInformatics Engineering accomplished at Universidade do Minho in Braga, Portugal. The\nmain result of this Master’s project is an application that allows the Monitor to plan his\ncourses and activities; it also allows the participant to have a better performance in the\ncourse in which he is enrolled. From the point of view of Monitor, this tool is intended to\nfacilitate the organization of a course or activity, linking to each task the materials/tools\nnecessary to carry it out. From a Participant’s point of view, it aims to provide greater\ninvolvement of the participant in the course in which he is enrolled in order to have a\nbetter supported evolution. For that purpose, the Participant will be able to attend a course\nselected from the offer promoted by Escola Competências de Vida (E•CV), obtain more\npersonalized and immediate feedback, increasing his motivation and engagement to learn\nmore. Upon receiving this constant feedback, the participant feels more involved in the\nactivity, acquiring knowledge more naturally, thus potentiating a faster evolution. On the\nother hand, Monitors, that are responsible for teaching the corses, will find their work easier\nas all the tasks concerned with each activity (both to provide learning resources to the\nstudents or to assess/grade them) are concentrated under the same platform. The system\nhere described is called E•CVoltera. It was designed and fully implemented in order to\nsatisfy the requirements specified by Escola Competências de Vida, an academy created and\nmaintained by a group of professors/researchers of Escola de Psicologia of Universidade do\nMinho. The system was made accessible to the community as a web platform."
  },
  {
    "keywords": [
      "Epidemic multicast",
      "Machine Learning",
      "Peer-to-peer",
      "Aprendizagem automática",
      "Difusão epidémica",
      "Entre-pares"
    ],
    "titulo": "Optimization of epidemic multicast protocols",
    "autor": "Fernandes, Diogo André Teles",
    "data": "2021-08-10",
    "abstract": "Epidemic multicast protocols, also known as gossip protocols, offer fault tolerance and\ngood performance at large scale. Therefore, these are used in peer-to-peer (P2P) systems\non the Internet and in NoSQL data management systems. Research has shown there are\nmultiple variants of these protocols which are most efficient in certain environments and\napplications. Some protocols, such as Plumtree, even allow the application to configure\nto obtain different performance trade-offs. This dissertation aims at taking advantage of\nMachine Learning (ML) to configure these protocols, developing a solution that adapts in\nruntime to network conditions and evaluate it experimentally. The results obtained by using\nML models to control the transmission strategy used when forwarding messages show that\nit is possible to achieve a better trade-off between bandwidth used and the time to reach the\nentire network. Moreover, this does not endanger the characteristics of epidemic multicast\nprotocols, maintaining their reliability while becoming even more scalable."
  },
  {
    "keywords": [
      "681.324"
    ],
    "titulo": "Interoperabilidade e portabilidade em ambientes PaaS",
    "autor": "Cunha, David",
    "data": "2012-12-11",
    "abstract": "O Cloud Computing tem emergido como sendo um novo paradigma para entrega de serviçoes através da Internet. Neste mercado em expansão, o serviço de PaaS (Platform-as-a-Service) tem sido objeto de grande interesse por parte das mais variadas organizações permitindo o fácil deployment de aplicações sem necessidade de uma infraestrutura dedicada, instalação de dependências ou configuração de servidores. No entanto, cada fornecedor de solução PaaS acaba por gerar um lock-in do utilizador às suas características proprietárias, tecnologias ou APIs (Application Programming Interfaces). Além disso, dando como garantida a conectividade até aos clientes, a rede de operadores como seja o caso da Portugal Telecom (PT) acaba por servir apenas de dumb-pipe entre o fornecedor e os seus clientes.\nEste projeto foca-se na especificação, desenvolvimento e avaliação de uma camada de abstração que visa unificar os processos de gestão e aquisição de informação sobre aplicações e bases de dados criadas atraves de diversos PaaS, de modo a combater o lock-in existente no mercado. Neste sentido, um utilizador de PaaS pode selecionar a plataforma mais adequada para uma aplicação interagindo de forma idêntica com qualquer fornecedor suportado,tendo também a possibilidade de migrar aplicações entre fornecedores distintos. Assim sendo, um operador como a PT tem agora a possibilidade de agir como um mediador entre os utilizadores e os fornecedores de PaaS."
  },
  {
    "keywords": [
      "Acordo Distribuído Aproximado",
      "Escalabilidade",
      "Sistemas distribuídos",
      "Approximate distribution agreement",
      "Distributed system",
      "Scalability",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Análise e optimização de protocolos de Acordo Distribuído Aproximado",
    "autor": "Oliveira, Joaquim Manuel Gonçalves",
    "data": "2021-12-22",
    "abstract": "Esta dissertação aborda o acordo distribuído aproximado, no qual é pretendido que um grupo de processos decida um valor dentro de um intervalo com uma amplitude limitada. Na primeira fase procede-se ao levantamento dos algoritmos existentes na literatura,\nonde se incluem algoritmos que propõem a resolução no modelo assíncrono, no qual são\nconsideradas faltas bizantinas. Através de uma análise comparativa, são evidenciadas as \nprincipais diferenças entre os algoritmos. Posteriormente, após a seleção criteriosa de um \ndos algoritmos, e feita a sua análise detalhada sobre os fatores de escalabilidade em resultado \nda sua implementação. Com base nos resultados, são propostas alterações que promovem a\nperformance do algoritmo face ao aumento do número de processos no sistema."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Image recognition using deep learning",
    "autor": "Barbosa, Bruno Miguel da Silva",
    "data": "2018",
    "abstract": "Computer vision is a vast knowledge subject responsible for traducing digital images and\nvideos into a higher level of understandable information. Image recognition is one of the\nseveral tasks that are inserted in this subject and it can be subdivided in object recognition\n(also called as object classification), segmentation, identification and detection.\nSome of the available alternatives for image recognition are based on Machine Learning\n(ML) approaches. Deep Learning (DL) is a branch of ML that became very popular in the\nlast years due to its success in previously considered hard tasks. The lack of large amounts\nof data and efficient computational resources a few years ago, were a barrier for the expansion\nof DL. However, thanks to the current easy data access and due to development of\nmore powerful computational resources, including CPU and GPU too, the attention turned\nback on, and it became easier and faster to train a model than can distinguish different\ntypes of classes with a very low error rate. One interesting fact about DL is its ability to\nautomatically learn from data and understand the most differentiable features of it.\nFrom the point of view of the industry, many artificial vision inspection lines still do their\njobs relying on traditional computer vision methods/algorithms. Yet, with more complex\ndomains, for example like texture patterns, things can get more difficult. This is where DL\ncomes in.\nThis document begins with an introduction of DL for artificial vision. It starts by addressing\nthe theoretical fundamentals of DL for image recognition and then focuses on the\ngeneral aspects of Convolutional Neural Networks (CNN). Next, are reviewed the state of\nthe art network configurations that stood out in recently.\nA high-level toolkit for image recognition was created to simplify the whole process of\nbuilding DL models, from the data pre-processing to the trained model testing phase. It\nallowed to easily prepare a set of experiences that address some of the common practices\nused on CNNs and highlight the power of DL on image recognition related tasks.\nThis dissertation was developed under a business environment on a artificial vision company\ncalled Neadvance, Machine Vision, SA. The Neadvance, Machine Vision, SA is also\ninterested in researching the new trends related to DL for image recognition in order to\nknow how to apply them on their projects since it opens a new range of challenging opportunities."
  },
  {
    "keywords": [
      "Machine Learning",
      "Artificial Intelligence",
      "Neural network",
      "Neuroevolution",
      "Neuroevolution of augmenting topologies",
      "Genetic algorithm",
      "Reverse engineering",
      "Input emulation",
      "Process memory scanning",
      "Process memory reading",
      "Bot",
      "3D",
      "MMORPG",
      "PVE",
      "Video game",
      "Player emulation",
      "Exploit",
      "Scripting",
      "Automation",
      "Hooking",
      "DLL Injection",
      "Aprendizagem de máquina",
      "Inteligência Artificial",
      "Rede neuronal",
      "Neuroevolução",
      "NEAT",
      "Algoritmo genético",
      "Engenharia reversa",
      "Emulação de input",
      "Pesquisa de memoria",
      "Leitura de memória",
      "Video jogo",
      "Emulação de jogador",
      "Exploit",
      "Automação",
      "Hooking",
      "Injecção DLL",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of a bot like entity to emulate an user in a tridimensional virtual environment",
    "autor": "Neto, Luís Paulo Ferreira Gomes",
    "data": "2023-11-27",
    "abstract": "This dissertation explores a novel approach for the development of a virtual entity or artificial intelligence\ncapable of simulating user behavior within the immersive and expansive virtual realm of the World of\nWarcraft video game. Classified as a three-dimensional massively multiplayer online role-playing game,\nWorld of Warcraft serves as an exemplary context for studying and refining techniques that can be readily\nadapted to other applications. The research methodology employed in this study involves a systematic\nanalysis of the application’s process memory space, with a focus on identifying crucial memory data\nlocations. Furthermore, the investigation entails the identification and preservation of pathways leading to\nthe aforementioned memory data points, ensuring their efficient and viable accessibility. To enable the\ncreation of the virtual entity, the Neuroevolution of Augmenting Topologies technique is employed, which\nfacilitates the generation and intricate development of an artificial neural network—the entity’s simulated\nbrain. By utilizing the previously acquired memory data points as sensory inputs, and emulating the entity’s\nresponses as inputs within the running process, a comprehensive framework for emulating user behavior\nis established. The findings presented in this dissertation contribute to the advancement of knowledge\nin the field of virtual entity creation and artificial intelligence, offering practical implications for a range of\napplications beyond World of Warcraft."
  },
  {
    "keywords": [
      "Software engineering",
      "Machine learning",
      "MLOps",
      "Model",
      "Automation",
      "Engenharia de software",
      "Aprendizagem automática",
      "Modelo",
      "Automação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automation of machine learning pipelines for anomaly detection challenges",
    "autor": "Martins, Ricardo Rodrigues",
    "data": "2023-12-11",
    "abstract": "Machine Learning (ML) and Data Science can solve different real-world problems. Businesses are\nbecoming increasingly interested in these approaches, and as technology evolves, new challenges can\nbe identified, mostly regarding the ML models development, deployment cycle and data cleansing, which\ncan significantly decrease the accuracy and viability of ML software systems. Development and Operations\n(DevOps) practices have become popular in operating software systems at scale successfully, but they\nneed to be adapted to deliver the best results when applied to ML systems. This led to the emergence\nof Machine Learning and Operations (MLOps), a development culture specific for ML systems, derived\nfrom DevOps principles. What MLOps attempts to address is the unification of the development cycle of\nML based software systems while striving for automation and monitoring, in order to allow continuous\nintegration and delivery. With this thesis, the goal is to study different available frameworks and methods\nfor ML systems, in order to develop an automated ML pipeline to ingest and manipulate high volumes\nof data. A sensorial system, which simulates the interior of a vehicle, gathers enough data to feed the\npipeline. Alongside the development of the ML system, a visual interface which allows control over the\noverall system and its data is created."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "GPM: generic predictive machine an automated machine learning framework for the prediction of a business’ financial evolution and other generic supervised learning problems",
    "autor": "Malhadas, Daniel",
    "data": "2018-12-12",
    "abstract": "One of the biggest problems for business owners, of both big and small companies, is the\ncorrect forecast of its financial evolution. This means that the owners in question need to\nspend a lot of time studying the economic issues that involve these situations and/or pay\nspecialized workers to do a thorough study on the area. In addition, it becomes a constant,\nchronic and seasonal problem that demands to be solved. If there was a prediction system\ncapable of forecasting and advising the owner of the company with enough precision to be\nreliable, one could save time, money and, above all, test or simulate the future with different\nparameters. This project then aims to complete a framework capable of encapsulating all the\ndata-science work and knowledge needed to create good enough machine learning models\nthat provide these predictions even to those without data-science knowledge and to any\ngeneric business. This framework goes by the name Generic Predictive Machine (GPM). Its\nperformance, both in terms of accuracy of results and execution time in various different\nsituations, as well as the process leading to its development and the development itself are\nthoroughly documented. The problem of having to create a generic system arises as every\nbusiness’ data is inherently different and assumptions cannot be made on the developer’s\nend. Therefore the final application allows for the solving of a vast array of problems\neven if not related to economics and requires little knowledge about Machine Learning.\nThe problem as a whole serves too as a vehicle for a deeper study on machine learning\nitself. Touching upon important aspects like, how far can such a system learn beyond the\ncapabilities of the human who programmed it, how faster can it do it and what can it teach\nus about the inherent patterns in data that sometimes remain unnoticed to the human eye.\nIn the process of answering these questions many works on the area are referenced that\ntouch specific points of the theory behind machine learning, a final novel conclusion is then\nderived from the knowledge found among all related works as to take a step forward in the\ntheory of machine learning as a whole."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Monitorização da performance do aluno no processo de aprendizagem",
    "autor": "Oliveira, Ruben Eliseu da Cunha",
    "data": "2016",
    "abstract": "Nos dias de hoje verifica-se cada vez mais que vivemos num mundo onde a tecnologia tem grande relevância em todas as áreas. Com este constante avanço, pode salvar-se milhares de vidas todos os dias devido à sua união com a medicina, melhora-se a capacidade de prever o tempo na área da meteorologia, apoia-se a química com o desenvolvimento de software para o estudo molecular, entre outros. A tecnologia está em todo o lado, como se pode notar, e requer uma constante adaptação por parte do Ser Humano para melhorar o seu desenvolvimento. Uma área que já utiliza este avanço é a educação, e é onde este projeto se irá focar. Com a ajuda de um sistema de e-learning (Moodle) pretende-se criar um sistema inteligente para reduzir a distância entre os professores e os alunos durante a sua vida académica. Pretende-se, também, dar a possibilidade aos professores de perceber mais facilmente e mais objetivamente as dificuldades dos seus alunos através do desempenho que os mesmos apresentam durante os momentos de avaliação."
  },
  {
    "keywords": [
      "Cuidados de saúde",
      "Processamento de linguagem natural",
      "Aprendizagem automática",
      "Chatbot",
      "Perguntas e respostas",
      "Healthcare",
      "Natural language processing",
      "Machine learning",
      "Question answering",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Chatbot development to assist patients in health care services",
    "autor": "Barbosa, António Pedro Mesquita",
    "data": "2020-12-10",
    "abstract": "Dados de alta qualidade sobre tratamentos médicos e de informação técnica tornaram-se\nacessíveis, criando novas oportunidades de E-Saúde para a recuperação de um paciente.\nA implementação da aprendizagem automática nestas soluções provou ser essencial e\neficaz na elaboração de aplicações para o utilizador para aliviar a sobrecarga do sector\nde saúde. Atualmente, muitas interações com os utentes são realizadas via telefonemas\ne mensagens de texto. Os agentes de conversação podem responder a estas questões,\nfomentando uma rápida interação com os pacientes.\nO objetivo fundamental desta dissertação é prestar apoio aos pacientes, fornecendo\numa fonte de informação fidedigna que lhes permita instruir-se e esclarecer dúvidas\nsobre os procedimentos e repercussões dos seus problemas de saúde. Este propósito foi\nconcretizado não apenas através de uma plataforma Web intuitiva e acessível, composta\npor perguntas frequentes, mas também integrando um agente de conversação inteligente\npara responder a questões.\nPara este fim, cientificamente, foi necessário conduzir a investigação, implementação\ne viabilidade dos agentes de conversação no domínio fechado para os cuidados de\nsaúde. Constitui um importante contributo para a comunidade de desenvolvimento de\nchatbots, na qual se reúnem as últimas inovações e descobertas, bem os desafios actuais\nda aprendizagem automática, contribuindo para a consciencialização desta área."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Mecanismos de profiling para sistemas de avaliação de conhecimento",
    "autor": "Fernandes, Luís Miguel Moreira",
    "data": "2019-12-23",
    "abstract": "Nos dias que correm, é cada vez mais frequente o recurso à tecnologia para a resolução de problemas nas mais diversas atividades, nos mais variados setores. O setor da educação não é, pois, uma exceção. Nesse setor, um dos aspetos que tem vindo a obter alguma relevância aborda a temática dos ITS — Intelligent Tutoring Systems, apesar da timidez das diversas aproximações realizadas, em especial ao nível do ensino de nível académico. Atualmente, está em curso na Universidade do Minho o desenvolvimento de um ITS, denominado Leonardo, cujo objetivo principal é o auxílio dos alunos no seu processo de aprendizagem e formação. Este sistema pretende fornecer um acompanhamento personalizado ao utilizador, tanto em termos do seu processo de formação, como, posteriormente, em termos do processo de avaliação correspondente. Nesta dissertação apresenta-se o desenvolvimento de um sistema de profiling para apoio ao processo de avaliação, que possibilita a construção do perfil do utilizador (do estudante) à medida que este interage com o sistema ao longo do tempo. Este sistema de profiling representa um dos módulos do sistema Leonardo e, através das devidas interações com os restantes módulos, serve de base de apoio a todas as decisões que são tomadas no sistema, uma vez que estabelece o perfil do utilizador pelo qual o sistema se deve reger ao adaptar os conteúdos a disponibilizar."
  },
  {
    "keywords": [
      "Ciências Naturais::Ciências Biológicas"
    ],
    "titulo": "Development of a tool for partial automated annotation of metabolic reactions",
    "autor": "Queirós, Pedro Miguel Teixeira",
    "data": "2018-10-22",
    "abstract": "Metabolic network reconstructions provide the mathematical in-silico frame work for the study of metabolism through the simulation of generic or specific\nmetabolic pathways.\nPolyphenols are niche dietary compounds with a growing field of interest\nin the scientific community. Modelling with these compounds is a step for ward in the completion of metabolic reconstructions.\nAs polyphenols are metabolized by both the gut microbiome and the human\nhost, it is essential to understand the organism-specific metabolic mechanism\nbehind their degradation. Since information is spread out through several\ninformation sources and their metabolism is highly complex, it is extremely\nchallenging and time-consuming to manually annotate their metabolism.\nThe focus of the work here developed was thus the creation of a tool that\ncould speed up the data collection process for posterior manual curation, with\na focus on the addition of polyphenol metabolism into the largest and most\ncomprehensive reconstruction of human metabolism, ”Recon”. This resulted\nin the creation of the Database Reaction Automatic eXtraction (DRAX)\ntool, a biological database web scraper. DRAX was initially targeted at\npolyphenols but it also allows the collection of reactions for other metabolites\nand drugs.\nDRAX allows the comprehensive extraction of metabolite reactions through\nmetabolic pathway-based iterative web scraping. It will provide researchers\nwith a starting point for metabolism reconstruction, allowing a more efficient\naddition of novel metabolic pathways."
  },
  {
    "keywords": [
      "Serious games",
      "Speech recognition",
      "Jogos sérios",
      "Reconhecimento de fala",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Avaliação automática de testes de atenção e acuidade visual",
    "autor": "Pereira, Mariana de Oliveira",
    "data": "2023-03-14",
    "abstract": "In recent years, our research group on Language Processing, GEPL, has been collaborating\nwith Centro Neurosensorial de Braga, led by Dr. Ana Paula Azevedo.\nIn this context, some serious games were developed and installed for recognizing shapes,\nemotions and training central and peripheral vision. They are used in memory therapy,\ndeconcentration, dyslexia, and other problems that affect the acquisition of knowledge in\nlearning processes.The ideas that rose up along the literature review done on those areas,\nwill be exposed along the state-of-the-art chapter in this report.\nThis thesis proposes a system that will implement an error detection algorithm based on\nspeech-to-text analysis to check whether the spoken sequence contains errors or not. As the\nsystem is intended to be installed in the Neurosensory Center, the results will be presented\nvisually to help the therapist in their day-to-day work and monitor the actual use of the\nsystem."
  },
  {
    "keywords": [
      "Workflow",
      "Engine",
      "PROM",
      "BPMN",
      "Camunda",
      "Motor",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Adaptive questionnaires using an workflow engine",
    "autor": "Pereira, Diogo Paulo da Costa",
    "data": "2023-04-11",
    "abstract": "Nowadays medical monitoring is very important and something that helps us a lot. Some supporting tools\nare already used for this, such as Patient Reported Outcome Measures (PROMs) and Patient Reported\nExperience Measures (PREMs), which are standardized medical surveys used to evaluate the quality of\ncare or patient experience from the patient’s viewpoint. They can often be transformed into Computerized\nAdaptive Tests, which aim to determine the best set of questions to ask each patient based on their previous\nanswers. With these tools, it becomes possible for most patients to finish their forms, because they no\nlonger have extensive forms, which increases the response rate to them. This is beneficial for both the\npatients and the healthcare providers. In the case of the patients, they get a more careful follow-up that\nhelps them to have a better lifestyle and well-being. On the provider side, with additional information from\nthe patient, healthcare providers can provide better, more personalized care and have a larger data set\nfor future research. So, this dissertation is based on these improvements at the medical level and also in\nother improvements at the business level.\nFor this, a solution was developed that uses BPMN as a high level language, Camunda as an engine,\na user interface and the main component of the solution. This last one connects all the elements and\nmakes it a workflow engine capable of processing forms coded in BPMN diagrams."
  },
  {
    "keywords": [
      "Machine Learning",
      "Deep Learning",
      "Action recognition",
      "Violence detection",
      "Early fusion",
      "Late fusion",
      "Aprendizagem Máquina",
      "Aprendizagem Profunda",
      "Reconhecimento de ações",
      "Deteção de violência",
      "Fusão antecipada",
      "Fusão tardia",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Deep Learning for activity recognition in real-time video streams",
    "autor": "Reinolds, Francisco André Vieira",
    "data": "2022-04-05",
    "abstract": "In an ever more connected world, smart cities are becoming ever more present in our society. In these smart\ncities, use cases in which innovations that will benefit its inhabitants are also growing, improving their quality of life.\nOne of these areas is safety, in which Machine Learning (ML) models reveal potential in real-time video-stream\nanalysis in order to determine if violence exists in them.\nThese ML approaches concern the field of Computer Vision, a field responsible for traducing digital images\nand videos, and be able to extract knowledge and understandable information from them, in order to be used\nin diverse contexts. Some of the available alternatives to recognise actions in video streams are based on ML\napproaches, such as Deep Learning (DL), that grew in popularity in the last years, as it was realised that it had\nmassive potential in several applications that could benefit from having a machine recognising diverse human\nactions.\nIn this project, the creation of a ML model that can determine if violence exists in a video-stream is proposed.\nThis model will leverage technology being used in State of the Art methods, such as video classifiers, but also\naudio classifiers, and Early/Late Fusion (EF / LF) schemes that allow the merging different modalities, in the case\nof the present work: audio and video. Conclusions will also be drawn as to the accuracy rates of the different\ntypes of classifiers, to determine if any other type of classifiers should have more prominence in the State of the\nArt.\nThis document begins with an introduction to the work being conducted, in which both the its context, mo tivation and objectives are explained. Afterwards, the methodology used in order to more efficiently conduct\nthe research in this Thesis is clarified. Following that, the State of the Art concerning ML based approaches to\nAction Recognition and Violence Detection is explored. After being brought to date in what are the State of the\nArt approaches, one is able to move forward to the following chapter, in which the Training method that will be\nemployed to train the models that were seen as the best candidates to detect violence is detailed. Subsequently,\nthe selected models are scrutinized in an effort to better understand their architecture, and why they are suited\nto detect violence. Afterwards, the results achieved by these models are explored, in order to better comprehend\nhow well these performed. Lastly, the conclusions that were reached after conducting this research are stated,\nand possibilities for expanding this work further are also presented.\nThe obtained results prove the success and prevalence of video classifiers, and also show the efficacy of\nmodels that make use of some kind of fusion."
  },
  {
    "keywords": [
      "Anonymized traffic",
      "Traffic classification",
      "IDS",
      "Machine learning",
      "Tor",
      "Classificação de tráfego",
      "Tráfego anonimizado",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Detection and classification of anonymous traffic using machine learning",
    "autor": "Regueiras, Maria Miguel Albuquerque",
    "data": "2024-07-25",
    "abstract": "Traffic classification is a fundamental task for various aspects of network management and monitoring.\nKnowing traffic characteristics allows a better design of the network and services, contributing for an\nimproved performance and quality of service. Regarding anonymized traffic, this becomes an intricate\nprocedure due to packet encryption.\nThe presence of anonymous traffic in networks is noticeable, demonstrating user’s concern about\nprivacy and anonymity at different levels. The growing use of anonymity tools creates the need of a secure\nand efficient experience. Therefore, traffic classification is also relevant to developers of these tools (to\nimprove user service robustness) and Internet Service Providers (to understand what type of traffic is\ncirculating in the network).\nThis dissertation’s primary goal is to study the application of machine learning algorithms in multilevel\nanonymous traffic classification. This work proposes an adaptation of an intrusion detection system as\nproof of concept, using datasets containing mixed traffic (benign and anonymized). Furthermore, it is\npresented a comparison to empirical studies carried out previously."
  },
  {
    "keywords": [
      "Campus inteligente",
      "Detecção de objetos",
      "Monitorização de múltiplos objetos",
      "Reidentificação",
      "Monitorização de pessoas",
      "Smart campus",
      "Object detection",
      "Multiple object tracking",
      "Re-identification",
      "People tracking",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Multi-people tracking using a distributed camera network: application to a university campus",
    "autor": "Matos, Henrique Miguel Cardoso",
    "data": "2023-10-02",
    "abstract": "Nesta dissertação é explorado o tema da monitorização de múltiplos objetos no contexto de um ”smart\ncampus”, com foco no contexto específico num campus universitário, sendo este o tema principal do projeto de\ninvestigação Lab4USpace. A monitorização de múltiplos objetos, especialmente de pessoas, é relevante para\ndiversas aplicações, incluindo aplicações de vigilância, mobilidade e inteligência ambiental. No entanto, torna-se\nparticularmente desafiante no contexto de espaços abertos, às quais exigem soluções com múltiplas câmaras\ncom problemas inerentes, tais como a reidentificação.\nO objetivo desta dissertação é desenvolver um framework capaz de fornecer informações sobre o percurso de\nvárias pessoas ao longo do campus universitário usando um cenário com múltiplas câmaras. A solução visa não só\na monitorização de uma pessoa num único cenário, mas também em todo o campus, coberto por diversas câmaras\ncom ou sem sobreposição.\nEsta dissertação discute os diversos desafios enfrentados durante o desenvolvimento deste projeto, incluindo\npreocupações com a privacidade e segurança dos utilizadores do campus. Com isso, optou-se por não enviar\nimagens para nenhuma aplicação, tratando apenas das informações estritamente retiradas da monitorização das\npessoas. Um dos principais desafios foi desenvolver um framework que rastreie vários objetos num ambiente de um\n”smart campus”, abordando desafios de espaços abertos e problemas de reidentificação. Além disso, devido aos\nrecursos computacionais limitados, foi usado um computador de bordo para lidar com processamento de imagens\ne operações relacionadas às técnicas de visão computacional de maneira mais eficaz.\nO framework proposto utiliza modelos de deteção de objetos e algoritmos de monitorização em tempo real que\nforam comparados neste contexto específico. Depois de pesquisar outras alternativas, a estrutura usa o modelo\nYOLOv7-tiny para deteção de objetos, BoT-Sort para a monitorização dos vários objetos e Deep Person Reid para\na reidentificação. O programa foi desenvolvido em Python e juntamente a ele foi também criado um website para\nalterar as configurações do sistema de monitorização utilizando o framework Flask. Um message broker também\nfoi utilizado para a comunicação entre os diversos componentes do sistema.\nOs testes de validação demonstram a eficácia da framework proposta na monitorização das várias pessoas em\ntodo o campus. O sistema proposto contribui significativamente para o desenvolvimento de soluções de múltiplas\ncâmaras mais eficientes e eficazes para aplicações de ”smart campus”, com benefícios potenciais para a segurança,\nproteção e gestão do campus.\nNo geral, esta dissertação apresenta uma estrutura que rastreia de maneira eficaz várias pessoas num ambiente\nde ”smart campus”. A framework é uma contribuição importante para o desenvolvimento na área do ”smart\ncampus” e tem potencial para desenvolvimento futuro e aplicações para além do campus universitário."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Desenvolvimento de um sistema de software para gerir ocorrências em municípios",
    "autor": "Costa, Ricardo Filipe da Silva",
    "data": "2015",
    "abstract": "Reportar ocorrências relativas a espaços ou equipamentos públicos é uma tarefa essencial, pois a identificação e a resolução dos problemas devem ser breves, de modo a evitar novos como consequência da demora deste processo.\nAtualmente, os cidadãos quando são confrontados com alguma ocorrência têm dificuldade em saber como e onde participar a mesma para que esta seja resolvida.\nPretende-se, com o desenvolvimento deste projeto, criar um meio de comunicação facilitador entre os cidadãos e as autarquias, que vise não só auxiliar os cidadãos a reportarem as mais variadas ocorrências relativas aos espaços ou equipamentos públicos, mas também permitir aos municípios gerir e monitorizar ocorrências reportadas nas diversas áreas de intervenção dos serviços municipais.\nO resultado do desenvolvimento deste projeto enquadra-se na criação de uma aplicação móvel e de uma aplicação web."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sensing, coordination and actuation in office spaces",
    "autor": "Mendes, Fernando José Ribeiro",
    "data": "2017",
    "abstract": "Productivity in an office space is directly affected by atmospheric conditions. With the\ncapabilities of Internet of Things appliances, it’s possible to automate the surrounding environment\nand maintain optimal work conditions while, at the same time, integrating with\nthe team’s workflow. The direct control of the environment then becomes part of any office\nmanagement application used by the teams. This dissertation addresses the creation of a\nprototype capable of doing so. Starting with a Single Board Computer packed with atmospheric\nsensors, it describes the building blocks for office automation, creating a new architecture\nand communication protocol. Instead of implementing the code to interact with\nphysical appliances and their own intrinsic behaviour and interfaces, this protocol is easily\nextensible, allowing consumers to add custom nodes responsible for bridging that gap.\nThese custom nodes can themselves produce readings and automate physical appliances\nregardless of the nodes that are already taking part in the protocol. Having a protocol with\nthose properties, the prototype to be developed can produce relevant information about the\nsurrounding environment while leaving complex computations to a third party, which can\nbe technology enthusiasts or even appliance vendors."
  },
  {
    "keywords": [
      "Atestação",
      "Sistemas embedidos",
      "TEE",
      "TrustZone",
      "Attestation",
      "Embedded systems",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "TrustZone based attestation in secure runtime verification for embedded systems",
    "autor": "Quaresma, Miguel Miranda",
    "data": "2020-08-24",
    "abstract": "ARM TrustZone é um “Ambiente de Execução Confiável” disponibilizado em processadores da ARM, que\nequipam grande parte dos sistemas embebidos. Este mecanismo permite assegurar que componentes\ncríticos de uma aplicação executem num ambiente que garante a confidencialidade dos dados e integridade\ndo código, mesmo que componentes maliciosos estejam instalados no mesmo dispositivo. Neste projecto\npretende-se tirar partido do TrustZone no contexto de uma framework segura de monitorização em tempo\nreal de sistemas embebidos. Especificamente, pretende-se recorrer a components como o ARM Trusted\nFirmware, responsável pelo processo de secure boot em sistemas ARM, para desenvolver um mecanismo\nde atestação que providencie garantias de computação segura a entidades remotas."
  },
  {
    "keywords": [
      "681.5",
      "51:372.851",
      "372.851:51"
    ],
    "titulo": "Uma experiência educativa com robótica inteligente",
    "autor": "Borges, André Paulo Renato Pereira",
    "data": "2012-12-19",
    "abstract": "Este trabalho, ou este estudo, ou esta experiência tenta demonstrar que existem ferramentas diferentes das já utilizadas para ajudar os alunos a compreender o funcionamento exato da disciplina de Matemática. Assim sendo, todo este projeto concentra-se na obtenção de formas diferentes de ensinar matemática aos alunos. A robótica, como instrumento de ensino e demonstração, vai permitir aos alunos terem um contacto prático com as matérias que lhes são ensinadas.\nEste projeto decorre numa escola básica com alunos do 5o ano. Todos estes alunos são voluntários e eram alunos com notas medianas a esta disciplina no ano anterior.\nOs robôs utilizados, são robôs da Lego Mindstorms NXT, com o respectivo software de programação. Os alunos tinham de os programar consoante a matéria que iam dando nas aulas. Nenhum aluno tinha tido contacto com estes robôs e muitos deles nem sequer nunca tinham mexido em computadores. Esta articulação programa-aluno-professor foi feita com a Diretora de Turma e a professora de Matemática, que iam revelando a planificação das aulas para que os alunos pudessem experimentar na prática o que tinham aprendido."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Distinguishing kinships beyond identity and paternity",
    "autor": "Simões, Raquel",
    "data": "2017",
    "abstract": "In kinship testing powerful statistical results are usually obtained when genetic information\nis expected to be shared between a pair of samples, which happens in paternity and\nidentification testing. However, there are other pedigrees where genetic information sharing\nis not required, such as when a pair of full-siblings or avuncular, is analyzed. Studying\nthese pedigrees, where the sharing of genetic information is not mandatory, will be the\nfocus of this work. We will consider several kinship problems where two (exhaustive and\nmutually exclusive) hypotheses will be compared, through a statistical evaluation based on\nthe calculation of a likelihood ratio (LR) where the probabilities of genotypic configurations,\nassuming one or another kinship hypothesis, are compared. This analysis will allow the\nidentification of the proportion of cases where the statistical evaluation had weak results,\nand those where LR favored the false hypothesis of kinship for a widely used commercial\nkit of genetic markers, considering simulated profiles assuming the pedigrees in question.\nIn addition, we will compare the statistical gain of increasing the battery of analyzed markers\nand infer the impact of considering the genetic information given by the knowledge of\nthe genetic profile of a relative, as the undoubted mother in the case where the hypotheses\n”individuals A and B are related as full-siblings“ and ”the individuals A and B are\nunrelated“ are asked to be compared. Furthermore, a validation of the Familias software\nfor two individuals will be performed for the simplest assumptions - absence of mutation\nand absence of silent allele - through the implementation of the algebraic formulas already\nestablished."
  },
  {
    "keywords": [
      "Continuous integration",
      "Compare past executed iterations",
      "Web API",
      "Integração contínua",
      "Comparar iterações anteriores executadas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Methodology for validation and performance analysis of compilation toolchains",
    "autor": "Silva, Luís Manuel Gonçalves da",
    "data": "2024-07-25",
    "abstract": "A Toolchain is a fundamental element of software development and provides developers\nwith the resources necessary for the development, testing, and maintenance of applications.\nUnreliable test environments can cause major problems for the successful completion of\ntests. resulting in a decrease in performance and quality of the application. The disparity be tween tests executed locally and remotely by different development teams can exacerbate\nthis issue, leading to inconsistency in results. In order to guarantee reliable and comparable\nresults it is essential that all developers use the same environment and configuration when\nconducting tests. This consistency is particularly important to obtain an accurate depiction\nof the application’s capabilities, revealing any potential issues that may not be prevalent in\none environment but exist in another.\nThis research aims to create a streamlined and efficient testing methodology for Toolchain\ndevelopers, developed in collaboration with Synopsys, a leader in the semiconductor indus try, provides a range of ARC RISC processors supported by various commercial and open source operating systems and middleware. This work proposes a tool designed to simplify\nthe testing process for different tools in the Toolchain, allowing developers to easily execute,\ncompare, profile, explore, and report tests with confidence. The tool will enable direct local\ncomparisons, reducing the time and effort required for manual testing and reporting. By us ing this tool, developers can save valuable time while ensuring their tests are accurate and\nreliable. An improved testing methodology will be beneficial to all Toolchain developers, as it\nwill allow them to focus less on manual testing processes and more on developing innovative\nsolutions with confidence. Tests are an essential component of any Toolchain development\nprocess, so having an efficient testing methodology in place is key to ensuring the highest\nstandards of quality code. With the proposed tool, developers can rest assured that their\ntests are accurate, reliable and replicable."
  },
  {
    "keywords": [
      "Virtual reality",
      "Functional autonomy",
      "Rehabilitation",
      "Exergame",
      "Head-mounted devices",
      "Activities of daily living",
      "Realidade virtual",
      "Autonomia funcional",
      "Reabilitação",
      "Dispositivos montados na cabeça",
      "Actividades da vida quotidiana",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Structural cognitive training with immersive virtual reality",
    "autor": "Pereira, Nuno Afonso Araújo",
    "data": "2023-01-18",
    "abstract": "In this thesis, a systematic review was conducted on the study of the use of VRSs.\nVR is an immersive technology capable of simulating real life events through image,\nsound and headed mounted devices or technologies such as windows kinnect. These\ntechnologies can be used to evaluate the performance and evolution of IADLs in older\nadults. An electronic data search was conducted, during January 2022. The final\nanalysis includes 12 studies with 285 participants in total. The use of VRSs is an\ninnovative and feasible technique to support and improve the functional autonomy of\nolder adults living in the community compared to conventional treatment. Between\n20% to 25% of community-dwelling people over 75 years old have limitations in the\nability to perform ADLs. The ability to perform ADLs is extremely important as it\nenables individuals to have a good quality of life by creating a sense of competence,\nself-esteem, confidence, identify and realisation. In this thesis we present the concept\nof structural cognitive training, in which cognitive training tasks (executive functions\nand cognitive abilities) are combined with training of instrumental activities of daily\nliving. The methodology adequacy is assessed by the design of a digital game to train\nolder adults to conduct IADLs."
  },
  {
    "keywords": [
      "Q-Learning",
      "Policy",
      "Markov processes",
      "Neural networks",
      "Política",
      "Cadeias de Markov",
      "Redes neuronais",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Q-Learning applied to games: a reward focused study",
    "autor": "Ferreira, Pedro Henrique de Passos",
    "data": "2023-01-31",
    "abstract": "Q-Learning is one of the most popular reinforcement learning algorithms. It can solve different complex problems\nwith interesting tasks where decisions have to be made, all the while using the same algorithm with no interfer ence from the developer about specific strategies. This is achieved by processing a reward received after each\ndecision is made.\nIn order to evaluate the performance of Q-Learning on different problems, video games prove to be a great\nasset for testing purposes, as each game has its own unique mechanics and some kind of objective that needs\nto be learned. Furthermore, the results from testing different algorithms on the same conditions can be easily\ncompared.\nThis thesis presents a study on Q-Learning, from its origins and how it operates, showcasing various state of\nthe art techniques used to improve the algorithm and detailing the procedures that have become standard when\ntraining Q-Learning agents to play video games for the Atari 2600.\nOur implementation of the algorithm following the same techniques and procedures is ran on different video\ngames. The training performance is compared to the one obtained in articles that trained on the same games\nand attained state of the art performance.\nAdditionally, we explored crafting new reward schemes modifying game default rewards. Various custom\nrewards were created and combined to evaluate how they affect performance.\nDuring these tests, we found that the use of rewards that inform about both good and bad behaviour led to\nbetter performance, as opposed to rewards that only inform about good behaviour, which is done by default in\nsome games.\nIt was also found that the use of more game specific rewards could attain better results, but these also required\na more careful analysis of each game, not being easily transferable into other games.\nAs a more general approach, we tested reward changes that could incentivize exploration for games that were\nharder to navigate, and thus harder to learn from. We found that not only did these changes improve exploration,\nbut they also improved the performance obtained after some parameter tuning.\nThese algorithms are designed to teach the agent to accumulate rewards. But how does this relate to game\nscore? To assess this question, we present some preliminary experiments showing the relationship between the\nevolution of reward accumulation and game score."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desenvolvimento e exploração de uma nova geração de ferramentas de Business Intelligence para o apoio à decisão e a prática clínica em unidades hospitalares",
    "autor": "Esteves, Marisa Araújo",
    "data": "2016",
    "abstract": "Ao longo dos últimos anos, tem aumentado exponencialmente a utilização de Tecnologias\nde Informação (TIs) e de ferramentas computacionais em vários setores económicos, incluindo\no setor da saúde, por serem defendidas como tecnologias que podem transformar\ne melhorar radicalmente a prestação de cuidados de saúde.\nO principal objetivo das instituições de saúde é prestar os melhores cuidados de saúde\naos seus utentes, garantindo, assim, a prestação de serviços de qualidade e a consequente\nsatisfação dos utentes, bem como reduzir os custos e desperdícios desnecessários associados.\nPortanto, as decisões devem ser tomadas rapidamente mas, igualmente, eficazmente.\nAcredita-se, assim, que o futuro realmente bem sucedido das TIs no setor da saúde\npassa então pelo desenho e pela implementação de sistemas user-friendly, incluindo Sistemas\nde Apoio à Decisão Clínica (SADCs), personalizados e focados no paciente, bem como\na receptibilidade dos profissionais de saúde aos mesmos. Abrange, igualmente, o uso de\ntecnologias emergentes na sua conceção, incluindo Business Intelligence (BI), de modo a tirar\nreal partido da informação disponível.\nDeste modo, no âmbito deste projeto de dissertação, foram desenhadas, desenvolvidas\ne exploradas uma nova geração de ferramentas Web de Business Intelligence para o apoio à\ndecisão e a prática clínica em unidades hospitalares. Englobou, em particular, o desenvolvimento\nde uma plataforma de BI versátil, incluindo a sua aplicação a dois casos práticos\ndiferentes, notadamente no apoio à decisão nas listas de espera de consultas e de cirurgias,\nassim como nos cuidados de Ginecologia e Obstetrícia (GO), e de uma ferramenta de\ncodificação clínica ICD-9-CM (International Classification of Diseases, Ninth Revision, Clinical\nModification).\nAssim, as ferramentas foram projetadas de modo a auxiliar os profissionais de saúde do\nCentro Hospitalar do Porto (CHP) no seu trabalho diário, incluindo a lidar com pacientes\nem condições delicadas e determinadas situações que requerem uma tomada de decisão\neficiente, bem como a codificação clínica de episódios de altas hospitalares."
  },
  {
    "keywords": [
      "Variability",
      "Variational software",
      "Variational data structures",
      "Variational query languages",
      "Robotics",
      "Software product lines"
    ],
    "titulo": "Encoding and analysis of variational ROS computation graphs",
    "autor": "Moura, Pedro Rafael Paiva",
    "data": "2022-05-25",
    "abstract": "In robotic applications, it is common to develop several variants of the same system (also known as a software\nproduct line), for example, to support different configurations of a robot. ROS is the most popular framework for\ndeveloping robotic applications, where each application is implemented as a distributed system of computation\nnodes that communicate through message passing. HAROS is a framework for static analysis of ROS-based\ncode. It can extract an abstract model of a ROS system’s architecture (called the computation graph) and perform\nan analysis on that model. However, it can only analyse one configuration at a time.\nIn this thesis, we present three different approaches for encoding various ROS computation graphs in a single\nvariational data structure, which contains the information related to the whole system and not just a configura tion. Additionally, we also define a variational execution algorithm for each approach, along with a small query\nlanguage, so that we can query and perform some analysis on said data structures. Lastly, we evaluate these\nalgorithms and data structures so that we can reach some conclusions on which approaches work best, and in\nwhat conditions."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Exploring heterogeneous computing with advanced path tracing algorithms",
    "autor": "Oliveira, André David Gomes Monteiro",
    "data": "2017",
    "abstract": "Currently, most computing systems have access to more than one type of processing unit,\ntypically a multicore CPU device and a computing accelerator, such as a GPU. However,\nthe vast majority of the existing implementations of advanced path tracing algorithms only\ntake advantage of one of these processing units. The implementation of these algorithms in\nsuch heterogeneous platforms while efficiently using both types of computing units already\nproved to provide improved performance results.\nThis dissertation examines four path tracing algorithms (Path Tracing aka PT, Bidirectional\nPath Tracing aka BPT, Bidirectional Photon Mapping aka BPM and Vertex Connection\nand Merging aka VCM) and extends previous work by exploring a richer heterogeneous\nenvironment with more GPU accelerators and with manycore x86 devices (i.e., Xeon Phi\nKnights Corner), complemented with an insight into the challenges introduced by each\ncomputing architecture and their programming environment. It also shows how these are\ncombined together to perform heterogeneous computation managed by a simple scheduling\nalgorithm, created to take advantage of each device’s features.\nThis work proved that a fully heterogeneous approach to these four path tracing algorithms\nis feasible and the performance results are significantly improved."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Scheduling scientific workloads on an heterogeneous server",
    "autor": "Maia, John Camilo Ferreira",
    "data": "2016-12-03",
    "abstract": "The goal of this dissertation is to explore techniques to improve the efficiency and performance\nlevel of scientific applications on computing platforms that are equipped with multiple\nmulti-core devices and at least one many-core device, such as Intel MIC and/or NVidia GPU\ndevices. These platforms are known as heterogeneous servers, which are becoming increasingly\npopular both in research environments as in our daily gadgets.\nTo fully exploit the performance capabilities of the heterogeneous servers, it is crucial to\nhave an efficient workload distribution among the available devices; however the heterogeneity\nof the server and the workload irregularity dramatically increases the challenge.\nMost state of the art schedulers efficiently balance regular workloads among heterogeneous\ndevices, although some lack adequate mechanisms for irregular workloads. Scheduling these\ntype of workloads is particularly complex due to their unpredictability, namely on their execution\ntime. To overcome this issue, this dissertation presents an efficient dynamic adaptive\nscheduler that efficiently balances irregular workloads among multiple devices in a heterogeneous\nenvironment.\nTo validate the scheduling mechanism, the case study used in this thesis is an irregular\nscientific application that has a set of independent embarrassingly parallel tasks applied to a\nvery large number of input datasets, whose tasks durations have an unpredictable range larger\nthan 1:100. By dynamically adapting the size of the workloads that were distributed among\nthe multiple devices in run-time, the scheduler featured in this dissertation had an occupancy\nrate of every computing resources over 97% of the application’s run-time while generating an\noverhead well below 0.001%."
  },
  {
    "keywords": [
      "681.3.06"
    ],
    "titulo": "A bounded model checker for SPARK programs",
    "autor": "Lourenço, Cláudio Filipe Belo da Silva",
    "data": "2013-11-28",
    "abstract": "Formal verification of software has been an active topic in the area of computer science. Several techniques to verify software are now available, and many tools have been created over the years for\ndifferent languages and using different techniques. However, for SPARK, a programming language broadly used in critical systems, only deductive verification tools based on contracts are available. The main downside of this approach is the lack of a full automation.\n\nIn this dissertation we propose an automated verification tool for SPARK code, thus contributing to fill the gap identified above. Our tool bases on an alternative technique, called bounded model\nchecking, that sacrifices completeness in exchange for automation. Through grounding our work in the highly popular and successful CBMC tool for verification of C code, we investigate how to perform bounded model checking of SPARK programs, and, in particular, we present our implementation of  a bounded model checker for SPARK programs called SPARK-BMC.\n\nExperiments performed with our tool show that automatic verification of SPARK programs is feasible and useful, even though is not complete. As far as we know, there is no tool based on such an automated technique for SPARK. The tool is freely available and based on open-source technologies."
  },
  {
    "keywords": [
      "Controlo analítico",
      "Data Science",
      "Eficiência energética",
      "Estação de Tratamentos de Águas Residuais",
      "Analytical control",
      "Energy efficiency",
      "Wastewater Treatment Plant",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Data Science para o controlo analítico e eficiência energética numa ETAR",
    "autor": "Faria, Carlos Daniel Coutinho",
    "data": "2022-05-30",
    "abstract": "Atualmente, principalmente nos países mais desenvolvidos, as águas residuais são tratadas através das\nEstação de Tratamento de Águas Residuais (ETARs) de forma a tentar atenuar os efeitos da atividade\nhumana na poluição da água potável. As ETARs são infraestruturas que desempenham um papel funda mental e imprescindível para a sociedade, pois estas permitem que a água potável já utilizada em diversas\natividades e no uso doméstico possa retornar ao seu habitat natural nas melhores condições, podendo\nser reaproveitada.\nContudo, uma ETAR para realizar a sua função requer elevados consumos energéticos, devido a todo\no processo de tratamento que é auxiliado com um número elevado de equipamentos, desde das águas\nresiduais afluentes até ao seu destino final. Esses consumos oscilam consoante o tipo de afluente, sendo\nimportante analisar através de um controlo analítico, quais as substâncias que, quando presentes no\nafluente, requerem maior tratamento, e consequentemente maior consumo de energia.\nCom isto, o objetivo desta dissertação é, através do uso de Data Science, elaborar um forte controlo\nanalítico às substâncias do afluente e relacionar o mesmo com a oscilação do consumo energético,\ntornando as ETARs mais eficientes e sustentáveis."
  },
  {
    "keywords": [
      "Distributed systems",
      "Distributed consensus",
      "Approximate distributed agreement",
      "Multi-agent systems",
      "Fault tolerant systems",
      "Sistemas distribuídos",
      "Consenso distribuído",
      "Acordo distribuído aproximado",
      "Sistemas multi-agente",
      "Sistemas tolerantes a faltas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Approximate distributed agreement toolkit",
    "autor": "Conceição, Eduardo Lourenço da",
    "data": "2023-12-28",
    "abstract": "Distributed Agreement is a well known and researched problem, one whose solutions have\nvast application in Distributed Systems, as reaching agreement over a certain value or\nover the order of received messages is extremely important in many multi-agent contexts.\nApproximate agreement has long been relegated to the sidelines compared to exact agreement, with its most notable application being clock synchronisation. Other proposed\napplications stemming from control theory target multi-agent consensus, namely for sensor stabilisation, coordination in robotics, and trust estimation. Several proposals for\napproximate agreement follow the Mean-Subsequence-Reduce approach, simply applying\ndifferent functions at each phase. However, taking clock synchronisation as an example,\napplications do not fit neatly into each generic algorithm’s definition: instead they require adapting their internals. Our contribution is three-fold. First, we conduct a survey\non approximate agreement and related algorithms, delineating their characteristics thoroughly. Second, we identify additional configuration points, establishing a more general\ntemplate of MSR approximate agreement algorithms. We then show how this allows us\nto implement not only generic algorithms but also those tailored for specific purposes.\nFinally, we propose a toolkit for making approximate agreement practical, providing classical implementations as well as allow these to be configured for specific purposes. We\nvalidate the implementation with classical algorithms and clock synchronisation."
  },
  {
    "keywords": [
      "Aplicações",
      "EAI",
      "ESB",
      "Integração",
      "SOA",
      "Applications",
      "Integration",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desenvolvimento de uma plataforma de serviços para facilitar a integração de aplicações empresariais",
    "autor": "Leite, Rui Filipe Castro",
    "data": "2019-12-23",
    "abstract": "O número de soluções utilizadas pelas empresas para fornecer serviços aos clientes e para fazer a gestão dos processos internos é cada vez maior. Este aumento provocou o aparecimento de novos problemas no que diz respeito à manutenção e à integração de novos serviços, uma vez que grande parte das aplicações não conseguem viver isoladamente. Com o objetivo de conseguir a integração de diferentes aplicações, surgiu o conceito de Enterprise Service Bus (ESB) — uma infraestrutura de conectividade que permite a comunicação entre aplicações, que podem ter diferenças a nível das plataformas em que são executadas, das linguagens de programação em que são escritas e dos modelos de dados que utilizam. A Eurotux Informática, S.A., é uma empresa especialista no planeamento, integração e implementação de sistemas informáticos, onde, devido à existência de diferentes interdependências entre aplicações de apoio ao negócio, surgiu a necessidade de implementar uma solução de integração utilizando um Barramento de Serviços. Assim, nesta Dissertação de Mestrado, para além do estudo dos padrões de integração de aplicações, é apresentado o processo de planeamento e implementação de uma solução de integração de aplicações num contexto empresarial."
  },
  {
    "keywords": [
      "Traffic signs detection and recognition",
      "Key-point features",
      "Machine learning",
      "Convolutional neural networks",
      "Sinais de trânsito",
      "Deteção e reconhecimento de objectos",
      "Key-point features",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Tool for semi autonomous building  of traffic sign images repositories",
    "autor": "Miranda, Miguel Dias",
    "data": "2020-01-09",
    "abstract": "Either to improve the drivers knowledge about the road, or focusing on the current development of autonomous vehicles, most car manufacturers began offering driving assistance\nsystems in their vehicles. A crucial part of the monitoring task performed by those systems is\nthe detection and reaction over found traffic signs. Since the decision flow of those solutions\nis reliant on the information gathered from the found signs, these systems deeply rely on their\nrecognition stage.\nTo achieve high-accuracy classification rates at nearly real-time, recognition is usually implemented using machine learning techniques, such as state of the art Convolution Neural\nNetworks (CNNs). However, these methods demand a large amount of data for their learning\nprocess. Due to the lack of large traffic signs repositories, these systems are restricted to one\nof the few available datasets. A significant decrease in accuracy was observed when using a\nrecognition model trained with samples from a given country and latter used to classify signs\nfrom another country, thus supporting the need for country-specific repositories and trained\nmodels. Traffic signs reveal several differences when compared to the same functional sign\nover different countries. Those changes, although similar for the human perception, cause\na significant disturbance in the classification abilities learned by a given machine learning\nmodel.\nAiming to overcome this issue, this dissertation proposes a semi autonomous tool to create\ntraffic sign repositories, for almost any given country. The created dataset is intended to later\nbe applied to train country-specific models, with traffic sign images from the country where\nthe recognition model is going to be used. To achieve this, a pipeline based on an Ensemble\narchitecture joining several computer vision techniques is proposed. The combination of\nvarious methods allows to improve the recognition rates and, more importantly, decrease the\nnumber of false positives gathered in the produced repositories.\nFinally, the pipeline was used to create the first Portuguese traffic sign repository, having\ncurrently around 33000 labelled signs."
  },
  {
    "keywords": [
      "Cyber-physical system",
      "Indústria aeroespacial",
      "Indústria 4.0",
      "Internet of things",
      "Aerospace industry",
      "Industry 4.0",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "IoT platform for the aerospace industry",
    "autor": "Rodrigues, Daniel Camelo",
    "data": "2021",
    "abstract": "A tecnologia da informação tem avançado significativamente nas últimas décadas; desde o\ndesenvolvimento de hardware, cada vez mais compacto e eficiente, até ao desenvolvimento\nde aplicações e serviços com impacto mundial. Como consequência deste avanço, novos\nsistemas, mais complexos e sofisticados, tem surgido com o intuito de melhorar serviços já\nexistentes ou criar novos serviços. Os sistemas IoT são um exemplo deste avanço tecnológico.\nEstes sistemas permitem a monitorização, análise e controlo dos mais diversos ambientes\natravés da recolha e processamento de dados e consequente interação com o meio, por via\nde sensores e atuadores.\nA maturação destas tecnologias e a evolução industrial atraiu a atenção de vários países\ne empresas para o potencial da sua integração nos processos industriais. Como tal, várias\niniciativas relacionadas com o desenvolvimento e aplicação de sistemas IoT e CPS surgiram,\ncomo por exemplo a iniciativa I4.0. Sendo que, uma das áreas mais influenciadas por\nesta evolução é a indústrias de manufactura. Nos últimos anos esta indústria têm vindo\na adotar cada vez mais as tecnologias de IoT devido às suas vantagens, especialmente na\nmonitorização e controlo das linhas de produção. Isto é através da recolha e posterior\nanálise dos dados provenientes das linhas de montagem é possível identificar ineficiências,\nreduzir os custos de produção, aumentar o nível de controlo, entre outros.\nNeste caso de estudo, a medição do tempo dos estágios da linha de produção é essencial\npara a avaliação da produtividade da empresa. No entanto, o processo de medição é\nefetuado manualmente quando necessário, o que torna o processo mais demorado e mais\npropenso a erros. Assim sendo, o objetivo deste trabalho é o desenho, desenvolvimento e\nanálise de um sistema IoT que complemente os métodos de monitorização utilizados, a fim\nde melhorar a qualidade dos dados obtidos desta.\nPara a elaboração da solução, foram, em primeiro lugar, estudadas e comparadas algumas\nplataformas de desenvolvimento de soluções IoT e aplicações de visualização de dados.\nSeguidamente procedeu-se com o desenho e desenvolvimento da solução proposta, tendo\nem conta a recolha de dados, tanto dos sensores como da infraestrutura preexistente, e\nposterior análise e apresentação dos dados. Por fim, delineou-se um cenário de testes em\nque se simula o comportamento da linha de produção através de dados históricos do caso\nde estudo, a fim de testar e validar a solução desenvolvida."
  },
  {
    "keywords": [
      "Administração Pública",
      "Gestão documental",
      "Macroestrutura funcional",
      "Plataforma CLAV",
      "CLAV Plataform",
      "Documental management",
      "Funcional macrostructure",
      "Public administration",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "CLAV editor genérico de XML",
    "autor": "Marques, Cláudia Azevedo",
    "data": "2020-01-09",
    "abstract": "Nos dias de hoje, inúmeras instituições recebem, todos os dias, uma vasta quantidade de informação e, muitas das vezes em papel. Desta forma, existe uma preocupação cada vez maior quer no consumo excessivo de papel quer na gestão de uma grande quantidade de informação. Com o intuito de simplificar a gestão documental, o governo tem concebido algumas estratégias, particularmente na Administração Pública (AP), com base em normas e orientações provenientes da Comissão Europeia. Assim, surge uma dessas estratégias, o Projeto ”M51-CLAV- Arquivo digital: Plataforma modular de classificação e avaliação da informação pública”, da Direção-Geral do Livro, dos Arquivos e das Bibliotecas (DGLAB). O objetivo principal deste projeto é a automatização de alguns processos relativos a classificação e avaliação de documentação na Administração Pública Portuguesa, utilizando um referencial comum que permite o desenvolvimento de instrumentos de natureza transversal a aplicar em contexto organizacional. Desta forma, nesta dissertação, pretende-se ter fora da Plataforma ”CLAV - Classificação e avaliação da informação pública”, um ambiente que permita às pessoas manipular e editar a informação relativa aos processos de negócio da Administração Pública (AP). Desse modo, numa fase inicial foi feito um XML Schema, com base na Macroestrutura Funcional (MEF), com os dados da CLAV. Posteriormente, pretende-se ver de que forma é que alguns dos invariantes sobre os processos de negócio podem ser colocados a nível do schema, e depois implementados no Xonomy - um editor XML em JavaScript."
  },
  {
    "keywords": [
      "Machine Learning",
      "Visão por computador",
      "Couro",
      "Deteção",
      "Defeitos",
      "Computer vision",
      "Leather",
      "Detection",
      "Defects",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automatic defect detection in leather",
    "autor": "Soares, João Pedro Matos Ribeiro",
    "data": "2022-12-19",
    "abstract": "Esta dissertação desenvolve-se em torno do problema da deteção de defeitos em couro. A deteção\nde defeitos em couro é um problema tradicionalmente resolvido manualmente, usando avaliadores ex perientes na inspeção do couro. No entanto, como esta tarefa é lenta e suscetível ao erro humano, ao\nlongo dos últimos 20 anos tem-se procurado soluções que automatizem a tarefa. Assim, surgiram várias\nsoluções capazes de resolver o problema eficazmente utilizando técnicas de Machine Learning e Visão\npor Computador. No entanto, todas elas requerem um conjunto de dados de grande dimensão anotado e\nbalanceado entre as várias categorias. Assim, esta dissertação pretende automatizar o processo tradicio nal, usando técnicas de Machine Learning, mas sem recorrer a datasets anotados de grandes dimensões.\nPara tal, são exploradas técnicas de Novelty Detection, as quais permitem resolver a tarefa de inspeção\nde defeitos utilizando um conjunto de dados não supervsionado, pequeno e não balanceado. Nesta dis sertação foram analisadas e testadas as seguintes técnicas de novelty detection: MSE Autoencoder, SSIM\nAutoencoder, CFLOW, STFPM, Reverse, and DRAEM. Estas técnicas foram treinadas e testadas com dois\nconjuntos de dados diferentes: MVTEC e Neadvance. As técnicas analisadas detectam e localizam a mai oria dos defeitos das imagens do MVTEC. Contudo, têm dificuldades em detetar os defeitos das imagens\ndo dataset da Neadvance. Com base nos resultados obtidos, é proposta a melhor metodologia a usar\npara três diferentes cenários. No caso do poder computacional ser baixo, SSIM Autoencoder deve ser a\ntécnica usada. No caso onde há poder computational suficiente e os exemplos a analisar são de uma só\ncor, DRAEM deve ser a técnica escolhida. Em qualquer outro caso, o STFPM deve ser a opção escolhida."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Integration and customization of a management system for indoor autonomous vehicles",
    "autor": "Monteiro, João Carlos Silva",
    "data": "2018-07-30",
    "abstract": "This document reflects the work developed for a master’s dissertation in scope of Smart Autonomous\nMobile Units (SAMU) project. This project is inserted in the iFactory programme\nwhere exists a partnership between Bosch Car Multimedia (CM) and University of Minho\n(UM).\nThe SAMU project main objective is the creation and development of a system that supports\nthe internal movement of materials along the supply chain using autonomous vehicles\n(AVs). With this type of system is possible to increase the efficiency and productivity of the\nlogistics processes. In this context, the present document makes a brief description about\nthe project area of intervention and a summary about the current state of material flows in\nBosch BrgP.\nThe work developed in this dissertation is integrated in SAMU project and is fundamental\nto achieve SAMU system goals. Briefly, it consists of a solution responsible for managing\nand tracking a fleet of indoor autonomous vehicles. In addition, the integration process of\nSAMU system in Bosch infrastructure was necessary to validate the management system.\nThis document has a brief contextualization about SAMU project and the detailed description\nof all the work made in this dissertation. It is also presented the analysis of the\nstate of art for this type of systems, the problems that the project intends to address and\nthe challenges encountered throughout the work. Finally, it is described the solution found\nwith the mechanisms and strategies adopted, the results analysis and the final conclusions."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Programmer profiling through code analysis",
    "autor": "Novais, Daniel José Ferreira",
    "data": "2016",
    "abstract": "This document serves as a Master’s dissertation on a degree in Software Engineering, in\nthe area of Language Engineering.\nThe main goal of this work is to infer the profile of a programmer, through the analysis\nof his source code. After such analysis the programmer shall be placed on a scale that\ncharacterizes him on his language abilities.\nThere are several potential applications for such profiling, namely, the evaluation of a\nprogrammer’s skills and proficiency on a given language, or the continuous evaluation of a\nstudent’s progress on a programming course. Throughout the course of this project, and as\na proof of concept, a tool that allows the automatic profiling of a Java programmer should\nbe developed."
  },
  {
    "keywords": [
      "Arquitetura de Software",
      "Revisão",
      "Reestruturação",
      "Modelação",
      "Software Development Kit",
      "Aplicações de Terceiros",
      "Model Driven Architecture",
      "Software Architecture",
      "Revision",
      "Restructuring",
      "Modeling",
      "Third Party Applications",
      "681.3.06"
    ],
    "titulo": "Revisão e reestruturação da arquitetura da plataforma Vortal Next : Vortal Software Development Kit",
    "autor": "Santos, Bruno Miguel Almeida",
    "data": "2013-07-17",
    "abstract": "A evolução tecnológica das últimas décadas generalizou o uso de software para a substituição ou suporte de múltiplos processos das empresas e, evidenciou novas perspectivas para o desenvolvimento de soluções com altos níveis de performance, disponibilidade, escalabilidade e flexibilidade. No contexto Vortal (empresa líder no mercado de contratação electrónica português com a plataforma VortalNext>), esta generalização levou à necessidade da existência mecanismos que permitam aos seus clientes a personalização/criação de áreas de trabalho dedicadas.\nTendo esta necessidade como foco, são avaliados os diferentes componentes da plataforma Next>, a metodologia de desenvolvimento atualmente utilizada  (Model Driven Architecture) e quais as melhores aproximações para o desenvolvimento de aplicações no âmbito de uma plataforma web, focando as suas vantagens e desvantagens a nível arquitetural e aplicacional.\nConcluiu-se que todas as soluções estudadas são adequados ao desenvolvimento de aplicações web, sendo o seu grau de adequação variável com o contexto de utilização. São soluções diferentes relativamente à complexidade de implementação, aos recursos necessários, aos riscos envolvidos e à simplicidade de utilização por parte do grupo de utilizadores finais.\nPor fim, é apresentada a arquitetura de um Software Development Kit (são estudadas outras opções, sendo esta a que oferece mais estabilidade aplicacional e mais vantagens competitivas) e a sua integração no ecossistema aplicacional e arquitetural da plataforma maximizando, não apenas a flexibilidade e funcionalidade para o cliente final, como também a segurança, robustez e fiabilidade do ecossistema da plataforma. A arquitetura definida em conjunto com o modelo de negócio apresentado formam a linha de ação indicada para garantir a existência de aplicações personalizadas a serem executadas no ecossistema VortalNext>."
  },
  {
    "keywords": [
      "Sistema de ficheiros",
      "POSIX",
      "LSFS",
      "Gossip",
      "Escalabilidade",
      "Tolerância a faltas",
      "File system",
      "Scalability",
      "Fault tolerance",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Armazenamento confiável e em larga escala para aplicações compatíveis com POSIX",
    "autor": "Ferreira, Alexandre José Branco",
    "data": "2023-07-19",
    "abstract": "A Internet of Things (IoT) é uma das áreas tecnológicas que necessita de sistemas distribuídos que\nsuportem o armazenamento e acesso à enorme quantidade de dados constantemente produzidos por\ncentenas a milhares de dispositivos. Até agora, a maioria dos sistemas desenvolvidos encontravam-se\nadaptados a instalações em centro de dados, impulsionados pela adoção de serviços de computação em\nnuvem, porém, o sistema distribuído Large Scale File System (LSFS), veio mudar o paradigma atual e\nmover o armazenamento distribuído para infraestruturas totalmente descentralizadas. Este é um sistema\nde ficheiros peer-to-peer não estruturado, parcialmente compatível com a interface POSIX, que permite a\nrealização de leituras por parte de múltiplos utilizadores, mas escritas por parte de um só utilizador. Foi\nconstruído para atingir alta disponibilidade e resiliência e encontra-se preparado para escalar para infraes truturas do futuro. Todavia, o LSFS não apresenta operações essenciais de um sistema de ficheiros, como\na eliminação ou modificação de dados, a grande carga que é exercida sobre a rede tem consequências\nnegativas no sistema como um todo e a forma como este foi avaliado levanta várias questões.\nCom o propósito de resolver estes desafios, desenvolveu-se o improved Large Scale File System\n(iLSFS), um sistema de ficheiros que estende o sistema LSFS, dotando-o de uma melhor usabilidade, mas\npreservando todas as suas características fundamentais como a escalabilidade, a resiliência e a disponi bilidade. Para isso, o sistema adota soluções, como Tombstones, que viabilizam a eliminação de dados\ne a disponibilização de uma interface com maior compatibilidade com o standard POSIX, implementa\nmétodos, como Version Vectors, que permitem o controlo de concorrência entre múltiplos utilizadores, e\nmecanismos, como caches, que ajudam a mitigar o problema de saturação da rede.\nOs resultados obtidos, demonstram que o iLSFS, com todas as funcionalidades introduzidas, apre senta uma melhor usabilidade sem, no entanto, comprometer significativamente o desempenho. Quando\nintroduzido num caso de estudo real, demonstra-se que o sistema é capaz de escalar para ambientes de\nlarga escala, com centenas de nodos, e mesmo quando submetido a cenários de instabilidade, onde a\nocorrência de falhas aleatórias é a norma, o iLSFS mostra-se capaz de tolerar a falha de uma grande\nquantidade de nodos de armazenamento, sem que esta provoque uma disrupção do seu funcionamento."
  },
  {
    "keywords": [
      "Cultural heritage",
      "Virtual museums",
      "Museum of the Person",
      "Ontologies",
      "CIDOC-CRM",
      "FOAF",
      "DBpedia",
      "Herança cutural",
      "Museus virtuais",
      "Museu da Pessoa",
      "Ontologias",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Building the Museum of the Person based on a combined CIDOC-CRM - FOAF - DBpedia Ontology",
    "autor": "Araújo, Cristiana",
    "data": "2016-12-03",
    "abstract": "This document presents the work developed to fulfill the requirements for a Master Thesis in Software Engineering, in the areas of Virtual Museums, and Ontologies for knowledge representation and exploration.\nThe first objective of this thesis work was the creation of a specific ontology for the\ndocument repository of the Museum of the Person (Museu da Pessoa), using a standard for\nmuseums, CIDOC-CRM (Comit´e Internacional pour la Documentation - Conceptual Reference\nModel), complemented with FOAF (Friend-of-a-Friend) and DBpedia that provides specific\nconcepts and relations to deal with persons. This abstract ontology was then populated\nwith life stories collected previously through of interviews of common people.\nTwo different approaches have be proposed to create the web pages for the Virtual Museum\n(VM), but only the approach 1 was implemented. A TripleStore was used as database to\nstore all the information that constitutes the Museum assets; the VM was created consulting\nthe datastore through SPARQL (SPARQL Protocol and RDF Query Language) queries.\nIn the dissertation will be discussed the design decisions, and provided the technical\ndetails; the project outcomes will be illustrated.\nThe npMP site created can be accessed at http://npmp.epl.di.uminho.pt/ and complements\nthis reading."
  },
  {
    "keywords": [
      "Cibersegurança",
      "Estratégia de cibersegurança",
      "Ataques informáticos",
      "Legislação/“Standards”/Boas práticas",
      "Riscos de cibersegurança",
      "“Perfil-Alvo”",
      "Plano de cibersegurança",
      "Lacunas",
      "Implementação de ações",
      "NIST CSF v1.1",
      "C2M2 v2.0",
      "CIS Controls v8.0",
      "ISO",
      "Cybersecurity",
      "Cybersecurity strategy",
      "Cyberattacks",
      "Legislation/Standards/Best practices",
      "Cybersecurity risks",
      "“Target Profile”",
      "Cybersecurity plan",
      "Gaps",
      "Implementation of actions",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Estratégia de cibersegurança",
    "autor": "Faria, Nelson Correia",
    "data": "2022-12-20",
    "abstract": "No mundo de hoje, a cibersegurança, ou segurança digital, é cada vez mais relevante à medida que a tecnologia\navança, o que é constatável por regulares notícias de ataques informáticos a infraestruturas de organizações\nque afetam muitas vezes os serviços das empresas e podem até resultar em consequências gravíssimas a nível\neconómico e financeiro. Mesmo em Portugal, nota-se que o número de ataques informáticos a empresas ou grandes\ncorporações têm aumentado e deixado um rasto de destruição em muitas delas, o que pode levar até à falência da\nempresa, fruto da má reputação adquirida por esta, o que deixa por vezes a empresa para trás relativamente à sua\nconcorrência que apresenta melhores garantias em segurança.\nDeste modo, esta dissertação de mestrado tem o principal intuito de mostrar a importância das empresas\ndefinirem e implementarem um plano de cibersegurança, pelo que é desenvolvida uma estratégia de cibersegurança\ncapaz de mitigar ou eliminar potenciais consequências graves à infraestrutura de uma organização oriundas de\nincidentes de cibersegurança. Numa primeira fase, é analisado um conjunto de documentos relacionados ao tema\nem questão fundamentais para uma segunda fase onde são descritos um conjunto de 7 passos para ajudar as\nempresas a criarem um plano que reflita as medidas que estão e as que serão implementadas no âmbito da\ncibersegurança. Para isso, é seguida a NIST Cybersecurity Framework v1.1 (NIST CSF) como a base da estratégia,\no Cybersecurity Capability Maturity Model (C2M2) v2.0 como ferramenta de autoavaliação e os CIS Controls v8.0\ncomo controlos adicionais para reforço da cibersegurança, além de outras fontes relevantes na área, tais como os\nstandards desenvolvidos pelo International Organization for Standardization (ISO). Numa terceira fase, a Estratégia\nde Cibersegurança é aplicada a uma empresa portuguesa que atua na área dos serviços de confiança, o que\nconstitui uma evidência de que a estratégia definida pode ser aplicada em qualquer organização em Portugal.\nAssim, seguindo todos os princípios-base da cibersegurança, através da análise documental de boas práticas,\nlegislação, frameworks e standards de cibersegurança, é desenvolvida uma estratégia de cibersegurança dedicada\nàs organizações, que teve aplicação prática num caso concreto e pode ser seguida por qualquer organização que\ntenha a intenção de reforçar a sua infraestrutura digital em cibersegurança. O objetivo final é que a cibersegurança\nfique formalizada na organização com planos/políticas que contenham o nível desejado em cibersegurança (“Perfil-Alvo”) e se consiga gerir os riscos através da implementação de ações para combater as lacunas identificadas na\nempresa."
  },
  {
    "keywords": [
      "681.3.02:910",
      "910:681.3.02"
    ],
    "titulo": "Data warehouses espaciais: projeto e implementação",
    "autor": "Morgado, André Correia",
    "data": "2013",
    "abstract": "Segundo um estudo realizado pela empresa International Data Corporation (IDC) (Adrian Bridgwater, 2009), o mercado dos data warehouses tem tido um grande crescimento. Cada vez mais as empresas procuram guardar todos os dados relacionados com o seu negócio, de forma a obter o máximo de conhecimento possível, podendo, assim, tomar melhor decisões relacionadas com o seu negócio. Os data warehouses aparecem como uma ferramenta útil para suporte a processos de tomada de decisão. A capacidade dos data warehouses guardarem grandes quantidades de dados relativos ao negócio da empresa e permitirem aos agentes de decisão acederem de forma simples e fácil a esses dados, fazem deles uma ferramenta de eleição para o processo de tomada de decisão. A partir dos dados presentes num data warehouse pode-se efetuar relatórios e análises detalhadas. Apesar de serem ferramentas muito poderosas, os data warehouses ditos convencionais ainda contêm limitações relativamente à capacidade de guardar e analisar dados com características geográficas. Estas ferramentas capazes de lidar com este tipo de características são largamente utilizadas pelas empresas em muitos domínios de aplicação, como as telecomunicações ou a segurança, que com auxilio desta ferramenta conseguem descobrir qual o melhor local onde instalar antenas ou, então, por entidades Governamentais, de forma a descobrir as zonas do seu país, com a maior criminalidade. Ao longo desta dissertação, pretende-se entender o processo de construção de um data warehouse espacial desde a sua fase de levantamentos e análises de requisitos até à sua fase de implementação, sendo, por fim, transformado um data warehouse convencional num data warehouse espacial recorrendo a toda a informação obtida ao longo do processo."
  },
  {
    "keywords": [
      "Internet of things",
      "Casas inteligentes",
      "Deteção de conflitos",
      "Verificação formal",
      "Modal action logic interactors",
      "Smart homes",
      "Conflict detection",
      "Formal verification",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Deteção de conflitos em programas de home automation",
    "autor": "Veloso, Pedro Miguel Dias",
    "data": "2023-12-15",
    "abstract": "O desenvolvimento da internet, evidenciado nos últimos anos, proporcionou o aumento da comunicação entre pessoas e dispositivos. Esta evolução, resultante na internet of things (IoT), permitiu o\ndesenvolvimento de novas tecnologias e o progresso de tecnologias já existentes, aplicadas aos mais diversos contextos, como é o caso das habitações. Desta forma, surge a criação do conceito de casas\ninteligentes. Casas inteligentes, permitem monitorizar e controlar remotamente os seus dispositivos IoT.\nEstas características possibilitam que os dispositivos sejam controlados através de mecanismos de automatização. Este tipo de habitações permite a melhoria da qualidade de vida dos seus habitantes, bem\ncomo a redução dos recursos necessários ao seu funcionamento.\nGeralmente, a programação de sistemas para casas inteligentes é realizada através da aplicação de\ntécnicas de programação baseada em regras de ativação-condição-ação. Este estilo de programação, em\nconjunto com as plataformas de automação, torna acessível a todos os utilizadores, automatizarem e\ncoordenarem os seus dispositivos IoT. Assim, com a definição de um conjunto de regras do tipo “se uma\ncondição se verificar, então executa-se uma ação”, é possível automatizar o conjunto de dispositivos. Esta\nautomatização permite adicionar inteligência às habitações.\nO aumento da disponibilidade dos dispositivos, bem como a sua simplicidade de programação, resulta\nno aumenta da adesão a estes sistemas. Porém, a criação de sistemas complexos implica um conjunto\nde regras também complexo. À criação de um conjunto de regras complexo agrega-se a dificuldade de\ngarantir que não ocorram conflitos entre todas as regras criadas. A ocorrência de conflitos neste tipo de\nsistemas pode resultar na execução de ações erradas que prejudicam a experiência do utilizador ou que\ncomprometam a sua segurança.\nNesta dissertação, estuda-se a viabilidade da aplicação da ferramenta IVY Workbench, na deteção de\nconflitos que ocorrem neste tipo de sistemas. Esta ferramenta permite modelar um sistema, e verificar\num conjunto de propriedades sobre ele expressas."
  },
  {
    "keywords": [
      "Variáveis categóricas",
      "ACP",
      "ACPCAT",
      "R",
      "Shiny",
      "Categorical variables",
      "PCA",
      "CATPCA",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Análise de componentes principais para variáveis qualitativas: exploração em R",
    "autor": "Gomes, Andreia Filipa Araújo",
    "data": "2023-12-14",
    "abstract": "A análise de grandes conjuntos de dados categóricos é um problema recorrente nas ciências sociais, comportamentais e biológicas. Surge por isso a necessidade de diminuir estes dados conseguindo, contudo, que as perdas de informação sejam mínimas. Tendo por base este problema, emergiu o tema desta dissertação, cujo objetivo passa pela análise exploratória do software R em busca de ferramentas para trabalhar com a análise de componentes principais categórica (ACPCAT), que surge como alternativa à tradicional análise de componentes principais (ACP), e permite reduzir a dimensionalidade de variáveis medidas em escalas diferentes. De forma a compreender os princípios fundamentais deste método estatístico, foi feita uma busca\nde fontes bibliográficas, que permitiu, adicionalmente, destacar o pacote Gifi e pacote Homals como sendo os únicos que possuem funções que permitem aplicar a ACPCAT. Estes pacotes foram explorados utilizando o mesmo dataset de exemplo, sendo feita uma descrição detalhada dos seus argumentos e dos valores e gráficos obtidos como forma de comparação das suas funcionalidades e recursos. O pacote Gifi descende do pacote Homals como sendo uma versão mais fácil de manipular e mais flexível devido a uma diferença na formulação da sua função de perda e ao facto deste utilizar B-splines.\nDe modo a explorar as funcionalidades e limitações da função princals() do pacote Gifi e para estabelecer de que forma dados biológicos podem ser trabalhados, foi também executada a análise de um conjunto de dados sensoriais recolhidos de provas de vinhos. Com o intuito de permitir ao utilizador executar uma ACPCAT de forma simplificada e intuitiva foi criada uma aplicação web, que está disponível no endereço https://andreiagomes.shinyapps.io/Gify/ e pode ser acessada livremente de qualquer dispositivo desde que este tenha acesso à internet. A aplicação, de nome Gify, tem por base a função princals() do pacote Gifi e permite ao utilizador, mesmo que este não tenha nenhum tipo de conhecimento sobre o software R, carregar o seu conjunto de\ndados, definir os seus parâmetros de análise, executar a ACPCAT e consultar os resultados, sendo tudo isto processado recorrendo a botões de seleção e espaços de preenchimento."
  },
  {
    "keywords": [
      "SDN (Software-Defined Network)",
      "OpenFlow",
      "Mininet-wifi",
      "Ryu-controller",
      "Vanets",
      "SUMO (Simulation of Urban MObility)",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Software Defined Vehicular Networks (SDVN): traffic routing process management",
    "autor": "Tomás, Erikson Neves",
    "data": "2024-02-15",
    "abstract": "This work focuses on the innovation of vehicular networks through the application of software-defined networks\n(SDN) to optimize connectivity and decision-making in urban mobility scenarios. Using the Ryu controller, the\nMininet-WiFi emulation environment and the SUMO urban mobility simulator, this research establishes a complete\nand realistic experimental environment for the study of vehicular networks.\nThe Ryu controller plays a key role in the dynamic orchestration of network decisions, enabling continuous\nadaptation to changes in the topology and communication demands of vehicular networks. Mininet-WiFi offers the\nability to emulate mobility scenarios, making it possible to analyze connectivity and performance in dynamic urban\nenvironments. In addition, the SUMO simulator accurately replicates urban roads, providing realistic modeling of\nthe vehicle movements.\nThe combination of these tools allows a comprehensive evaluation of the performance of vehicular networks in\nurban environments, as well as the study of resource management strategies and real-time decision-making. This\nresearch contributes to the advancement of vehicular communication technologies and offers valuable insights for\nthe development of an efficient and safe urban mobility solutions.\nThis study highlights the importance of integrating SDN, mobility emulation and road simulation to improve\nconnectivity and quality of service in vehicular networks, providing a solid foundation for further researches in the\narea."
  },
  {
    "keywords": [
      "Virtual machines",
      "Stack machines",
      "Register machines",
      "Compilers",
      "Assemblers",
      "Máquinas virtuais",
      "Máquinas de stack",
      "Máquinas de registos",
      "Compiladores",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "EWVM - an Educational Web Virtual Machine",
    "autor": "Teixeira, Sofia Almeida",
    "data": "2022-12-19",
    "abstract": "The Language Processing Course at Minho’s University uses a Virtual Machine implemented in C with its interface being implemented with the GTK toolkit. However, it is neither very informative nor very easy to install.\nThe goal in this Master’s Project is to analyze and model the entire Virtual Machine’s system and build a Web application with a graphical interface. The new tool offers two main characteristics: compiling and reporting errors in programs written for the Virtual Machine; and animate its execution, displaying the internal state of the VM and providing the user an interface to control the execution.\nIn this document, a study of existing technologies will be carried out, focusing in detail on\nthe current virtual machine VM. After this analysis, a solution will be proposed, followed by\na detailed explanation of its implementation."
  },
  {
    "keywords": [
      "A. thaliana",
      "Seca",
      "Modelos metabólicos",
      "Algoritmos",
      "Multi-ómicas",
      "Drought",
      "Metabolic models",
      "Algorithms",
      "Multiomics",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development and implementation of methodologies for integrating omics data with genomic-scaled metabolic models",
    "autor": "Rocha, Miguel Alexandre Oliveira",
    "data": "2022-12-13",
    "abstract": "As plantas são organismos fotossintéticos multicelulares essenciais para a vida humana e têm um\nenorme impacto na nossa economia. Como o crescimento e a sobrevivência das plantas estão intrinse camente relacionados com o seu metabolismo, as suas respostas metabólicas às condições ambientais\nmerecem ser estudadas. Com esse objetivo nasceu a Biologia de Sistemas, que usa a computação de\nmodelos teóricos e matemáticos de modo a analisar sistemas biológicos como um todo. Estes modelos\najudam a perceber processos biológicos e a estudar mecanismos metabólicos em diferentes condições.\nVários métodos têm vindo a ser desenvolvidos de modo a criar modelos metabólicos mais precisos que\nintegram diferentes ómicas. Isto foi possível devido ao avanço das tecnologias de alto rendimento. Tais\nmodelos são capazes de gerar previsões de fluxo mais precisas.\nAssim, neste estudo, métodos e algoritmos para integrar diferentes dados ómicos foram implementa dos com um modelo metabólico. Três algoritmos foram estudados: GX-FBA, RIPTiDe e EXAMO. O modelo\nmetabólico selecionado foi o da Arabidopsis thaliana (AraGEM), com dados ómicos em condições de\nseca. Os resultados dos algoritmos foram comparados com a literatura e com os modelos gerados pelo\nGIMME. Inicialmente, os resultados do GX-FBA demonstravam-se promissores, com número de reações\ne metabolitos semelhante aos modelos do GIMME. No entanto, após estudar as reações contendo fluxo,\nGX-FBA aparentava não ser capaz de distinguir as condições de um modo significativo. Em relação ao\nRIPTiDe, os resultados foram surpreendentes, sendo capaz de diferenciar as duas condições, apesar de\ninicialmente apresentarem significativamente menos reações e metabolitos, e de partilhar com o GIMME\nreações importantes envolvidas na resposta do metabolismo à seca. Por fim, o EXAMO não conseguiu\ngerar modelos viáveis."
  },
  {
    "keywords": [
      "Learning resources",
      "Computational thinking",
      "Visual impaired students education",
      "Game-based learning",
      "Pedagogy",
      "Special education",
      "Recursos de aprendizagem",
      "Pensamento computacional",
      "Educação de alunos com deficiência visual",
      "Aprendizagem baseada no jogo",
      "Pedagogia",
      "Educação especial",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "PathIt: computational thinking training resource for visually impaired individuals",
    "autor": "Cunha, Angélica Soares",
    "data": "2023-12-28",
    "abstract": "The contemporary landscape of problem-solving requires individuals to possess robust\nComputational Thinking (CT) skills. Acquiring these skills is contingent on the availability\nof adequate training resources. This scarcity is particularly pronounced for visually impaired\nindividuals, as the majority of existing CT training materials are inaccessible due to their\nreliance on visual elements.To remedy this situation, PathIt is introduced, offering a non visual CT resource tailored to provide full accessibility to visual impaired individuals.\nPathIt encompasses a physical component that provides a tactile interface for users\ninteraction and a software platform offering a range of CT challenges with both visual and\nauditory outputs, thereby catering to a diverse audience, including those with and without\nvisual impairments.\nThis Master’s Dissertation encompasses the design, development, assessment, and pre ceding research that led to the creation of the PathIt system. The efficacy of PathIt as a\nCT training resource is rigorously evaluated across a spectrum of visual abilities and age\ngroups, showcasing its potential as a versatile tool for nurturing CT skills and underscoring\nits adaptability and inclusivity."
  },
  {
    "keywords": [
      "High-throughput sequencing",
      "High performance computing",
      "DNA barcoding",
      "DNA metabarcodin",
      "Códigos de barras ADN",
      "Sequenciação de ADN",
      "Computação de alto desempenho"
    ],
    "titulo": "An efficient and accurate framework for large-scale sequences of DNA barcodes",
    "autor": "Neto, Luís Manuel Pacheco",
    "data": "2021-12-02",
    "abstract": "DNA barcodes are short sequences of pre-defined gene regions that contain a sufficient\namount of intra- and inter-species genetic information. High-throughput sequencing techniques are currently used to identify large sequences of DNA barcodes in a species genome, in a relatively short time.\nDomain experts require adequate self-contained tools to accurately and efficiently process\nDNA barcode data in a reasonable time, taking advantage of current parallel and heterogeneous computing systems. They also expect to use these tools on different computing platforms, from laptops to high-performance servers, without requiring a broad knowledge in software engineering to develop efficient computational applications.\nThe main goal of this project was to develop a framework and associated user-friendly tools\nfor domain experts to efficiently support DNA barcoding studies, providing an abstraction\nof the performance issues.\n4SpecID is the key outcome of this work: an application software that integrates a\nsemi-automated auditing and annotation tool for reference libraries, to ensure the quality\nstandards of the compiled data, aiming to enable a grounded decision when identifying\nspecies from DNA barcodes. Its graphics interface aids the end user to specify the operations\nand it also simplifies data filtering and remote file handling.\nThe C++ ported version (from MATLAB) was fully tested and is more robust than\nthe original version. Architecture features common to laptop and compute servers were\nexploited, namely parallel programming techniques and memory models.\nThe presented validation and performance results show significant improvements on\nexecution times, not only on the sequential version, but also by using the available parallel\ncapabilities of the underlying computing platforms."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Modelação de consumo de energia em Linux",
    "autor": "Portela, Carlos Gustavo Ferreira Araújo",
    "data": "2016",
    "abstract": "Nestes últimos anos, a importância da eficiência energética nos sistemas\ninformáticos tem vindo a crescer exponencialmente desde a área móvel até à\ncomputação de alto desempenho. O crescimento do mercado dos gadgets e a sua\ncrescente dependência dos serviços de computação em Cloud são algumas das razões\npara este rápido avanço neste ramo tecnológico. A redução do consumo de energia e\no aumento da produtividade de um sistema é, por várias razões, uma preocupação,\ntanto por parte do cliente como do fabricante. A exploração aprofundada do tópico\npode contribuir para, por exemplo, o aumento da autonomia dos terminais móveis e\na redução dos custos energéticos dos data centers e do cliente particular.\nNeste sentido, a monitorização do consumo de energia de um sistema\ndesempenha um papel fundamental para se aprimorar a eficiência energética do\nmesmo. No entanto, o grande desafio atual da monitorização passa por categorizar o\nconsumo de energia a vários níveis como, por processo, por máquina virtual ou por\nsubsistema de hardware. Esta escala de granularidade na análise energética permite o\ndesenvolvimento de relatórios mais incisivos e conclusivos sobre a distribuição de\nconsumo de energia do sistema.\nNo âmbito desta dissertação e tendo por base estes motivantes fatores, foi\ndesenvolvido um modelo simples que visa estimar o consumo de energia do sistema\nna sua totalidade, categorizado por subsistema e, no caso do armazenamento\nsecundário, também categorizado por processo.\nEste documento apresenta um estudo sobre diferentes metodologias de medição\nassim como sobre as abordagens possíveis para o modelo em sistemas de teste com\ndiferentes tipos de armazenamento secundário e com uma das ultimas gerações de\nprocessador da Intel. Numa fase inicial foi feita uma investigação a vários aspetos\nreferentes à energia de um sistema, incluindo modelos de estimação do consumo de\nsistemas, métodos de medição de consumo e a precisão desses mesmos métodos.\nNo desenvolvimento do modelo, recorreu-se apenas a recursos existentes no\nsistema em causa para viabilizar um mais fácil investimento a larga escala. Por isso, o\nmodelo recorre a interfaces de gestão e monitorização de energia como o RAPL e\nACPI. Foram analisados os mais importantes aspetos energéticos de um sistema como\na distribuição do consumo de energia estático e dinâmico pelos subsistemas e avaliouse\na eficiência e o desempenho dos mesmos nas mais diversas atividades. Desta forma,\ngarantiu-se uma maior polivalência do modelo e, de um modo geral, uma maior\nprecisão do mesmo. O modelo foi validado com base em ferramentas disponibilizadas pelo sistema e\natravés de medições físicas. Os resultados obtidos parecem satisfatórios, tendo sido\nregistadas taxas de erro máximas de 5% no consumo total e de 10% no consumo do\narmazenamento secundário.\nEste modelo desenvolvido pode ser adaptado a outro sistema, no entanto,\nnecessita de executar uma ferramenta de calibragem que realiza todas as etapas que\nforam executadas na configuração usada para este projeto. Isto acontece\nessencialmente no subsistema de armazenamento secundário onde não se recorre a\nqualquer ferramenta existente para a estimação da sua energia. Desta forma, há uma\netapa inicial que consiste na exercitação do subsistema de armazenamento secundário\natravés de ferramentas de benchmarking e na recolha de dados estatísticos do custo\ndas operações. Em seguida, é feito um estudo sobre esses mesmos dados e são\natribuídos diferentes pesos energéticos para cada operação executada no subsistema.\nDepois, constrói-se o modelo e este é calibrado com recurso a interfaces de energia\ncomo o RAPL e ACPI. No fim, este modelo deve ser capaz de apresentar a fatura\nenergética de cada processo que utiliza o subsistema de armazenamento secundário.\nAlém disso, o modelo deve também estimar o consumo de energia total do sistema e\na sua distribuição pelos principais subsistemas."
  },
  {
    "keywords": [
      "ETAR",
      "Digestores anaeróbios",
      "16S rRNA",
      "Qiime2",
      "Taxonomia",
      "Diversidade",
      "Abundância relativa",
      "WWTP",
      "Anaerobic digesters",
      "Taxonomy",
      "Diversity",
      "Relative abundance",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Comparative analysis of microbial communities from full scale anaerobic digesters",
    "autor": "Inácio, Ana Carolina Matos",
    "data": "2023-12-14",
    "abstract": "Nas estações de tratamento de águas residuais (ETAR), a exploração da composição taxonómica e da\nabundância de comunidades microbianas tornou-se um esforço indispensável. É essencial para otimizar os\nprocessos de tratamento, monitorizar a saúde do sistema e garantir a conformidade com as normas ambientais.\nOs microrganismos desempenham um papel crucial na decomposição da matéria orgânica, na remoção de\ncontaminantes e na recuperação de recursos. O conhecimento das comunidades microbianas leva a estações de\ntratamento de águas residuais (ETARs) mais eficientes e resilientes, promovendo a saúde pública e a\nsustentabilidade ambiental. A utilização do sequenciamento do gene 16S rRNA para a análise de \ncomunidades microbianas em digestores anaeróbicos em grande escala em estações de ETARs tem vindo a \ncrescer. Essa metodologia envolve uma série de processos computacionais, abrangendo avaliação da qualidade da \nsequência, remoção de ruído, classificação taxonômica, alinhamento e construção de árvores filogenéticas.\nNotavelmente, o conjunto de software Quantitative Insights Into Microbial Ecology versão 2 (QIIME2) emergiu como \numa ferramenta valiosa, simplificando a análise de dados do gene marcador 16S rRNA. Facilita a análise ponta a \nponta de diversos conjuntos de dados de microbiomas e facilita estudos comparativos com dados disponíveis \npublicamente. QIIME2 equipa os investigadores com ferramentas para selecionar profundidades de \namostragem apropriadas para conduzir análises de diversidade alfa e beta.\nNesta tese, apresentamos uma análise comparativa abrangente das comunidades microbianas que habitam\ndigestores anaeróbios de grande escala em ETARs. O estudo utiliza a sequenciação de nova geração,\nnomeadamente a sequenciação de alto rendimento do gene 16S rRNA, e depois utiliza o Qiime2 para determinar e\nanalisar a taxonomia, a diversidade e a abundância relativa das comunidades de digestores em cada ETAR e entre\nelas. Ion torrent foi a tecnologia utilizada para a sequenciação de alto rendimento de nova geração do gene 16S\nrRNA. O fluxo de trabalho utilizado no qiime2 consistiu na importação de dados, no controlo de qualidade\n(denoising), clustering e só depois a análise da diversidade e da taxonomia para determinar a composição, a\ndiversidade e a abundância das comunidades. Os resultados mostraram que as comunidades microbianas\npermaneceram bastante estáveis durante diferentes pontos de amostragem, mas distantes quando comparadas\nentre diferentes digestores de lamas. Porém, alguns dos microrganismos que compõem a comunidade\nmetanogénica presentes nas comunidades foram Methanobacteriales, Methanomicrobiales, Methanosarcinales.\nEm relação à comunidade bacteriana, os microrganismos mais abundantes em um determinado digestor e\npresentes em todos os digestores foram atribuídos a Sedimentibacter, Phycicoccus, Thermovirgae, Cloacimonas e\nPhycicoccus."
  },
  {
    "keywords": [
      "Apache Kafka",
      "Microservices Architectures",
      "Intensive Care Units",
      "Big Data",
      "Internet of Things",
      "Health Information Systems",
      "Arquiteturas de microservices",
      "Unidades de Cuidados Intensivos",
      "Sistemas de Informação na saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Microservices architectures in healthcare with Apache Kafka",
    "autor": "Santos, Rui Fernando Carvas dos",
    "data": "2022-04-12",
    "abstract": "Over the past few years, we have seen an exponential increase in the amount of data produced. This increase in\ndata is due, in large part, to the massive use of sensors, as well as the immense amount of existing applications.\nDue to this factor, and in order to obtain relevant information through the data, companies, institutions and the\nscientific community are constantly looking for new solutions to be able to respond to the challenges.\nOne of the areas where evolution is most needed is the area of healthcare, an area on which we all depend\nas a society. Every day, traditional healthcare information systems produce a large amount of data, making it\ncomplex to manage. Much of this data is produced by IoT devices, such as vital signs monitors, and in many\ncases can be critical to the patient’s health, as in the case of Intensive Care Units.\nIn this sense, the main objective of this dissertation is to expose the advantages and disadvantages of the\napplicability of microservices architectures and the use of Apache Kafka in the health area, more specifically\nin Intensive Care Units where the information flow is critical. In order to support these objectives, a Proof of\nConcept was developed, based on a future real applicability, which will support the carrying out of analyzes and\ntests."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Gestão inteligente de estacionamento em ambiente urbano",
    "autor": "Oliveira, Filipe Manuel Gonçalves de",
    "data": "2018",
    "abstract": "Desde as últimas décadas que a população mundial tem vindo a aumentar de uma forma\nexponencial, originando e potenciando vários problemas de difícil resolução como, por\nexemplo, problemas de trânsito relacionados com o elevado fluxo de veículos, problemas\nde poluição, de alojamento, de acesso à saúde, de gestão de recursos, entre outros. Neste\nenquadramento surgiu a necessidade de se “virtualizar” as próprias cidades, levando ao\nconceito de Cidades Inteligentes (Smart Cities) com o principal objetivo de criar condições\nde sustentabilidade nas próprias cidades e disponibilizar um acesso mais flexível a informação\nútil para os seus cidadãos. O conceito de Smart Cities é conhecido pelo uso de várias\ntecnologias para melhorar as infraestruturas urbanas e tornar os centros urbanos mais eficientes\ntendo em conta as necessidades das populações. Estes projetos recorrem geralmente\na redes de sensores distribuídas de forma estratégica para a recolha de dados do meio. Geralmente,\nestes projetos passam por uma avaliação experimental do problema em estudo,\nrecorrendo na maioria das vezes a ferramentas de simulação, de que são exemplos o Cup-\nCarbon, o InterSCSimulator, o UrbanSim, entre outros.\nNeste contexto, neste projeto de mestrado pretende-se estudar um problema concreto no\nâmbito das Smart Cities, sendo este o problema da gestão inteligente de estacionamento em\nambiente urbano tendo em vista uma maior eficiência da ocupação dos espaços disponíveis\ne uma melhor experiência dos utilizadores no uso e partilha deste recurso muitas vezes\nescasso. O ambiente de simulação CupCarbon será utilizado para simulação e teste dos\ncenários de gestão de estacionamento propostos."
  },
  {
    "keywords": [
      "ASCON",
      "LWC",
      "PICNIC",
      "SPHINCS+",
      "SKINNY",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Ligthweight + post-quantum cryptographies",
    "autor": "Faria, Henrique José Carvalho",
    "data": "2021-11-12",
    "abstract": "O objectivo do trabalho proposto passa por analisar a viabilidade de esquemas de assinatura digital “low-algebra” derivados de um dos algoritmos SPHINCS+ ou PICNIC recorrendo cada um a primitivas LWC das\nfamílias ASCON e SKINNY.\nPara esse efeito comerçar-se-á por implementar cada um desses algoritmos em C seguindo-se uma seleção\nde quais as variáveis que melhor se adequam a cada esquema. Posteriormente realizar-se-á uma análise\nde complexidade computacional das eventuais implementações em “software”, “hardware” ou hibrida."
  },
  {
    "keywords": [
      "Computational thinking",
      "Visual programming language",
      "Block-based programming language",
      "Educational robotics",
      "Pensamento computacional",
      "Linguagem de programação visual",
      "Linguagem de programação baseada em blocos",
      "Robótica educacional",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Robi: a visual programming language for educational robotics",
    "autor": "Galvão, Gustavo Linhares",
    "data": "2022-04-05",
    "abstract": "This document presents a Master’s thesis with researches focused on the teaching of\ncomputational thinking and present the development details of Robi, a block-based visual\nprogramming language that is able to program a robot built with an Arduino Uno. These\nresearches had the purpose of evaluating if the development of Robi, a block-based program ming language that communicates with Arduino, would really be needed. The researches\nhave proved that from the popular programming environments that exist in the market,\nthat were investigated, none have the requirements that Robi requires. The platform will be\nused to teach computational think through a block-based programming environment and\neducational robotics. Robi development is motivated by the intersection between the costs\nof educational robotics kits and the existing block-based programming language, in which\nsimplicity and intuitiveness could be improved, so children with learning difficulties or even\nyounger children, in the context of educational robotics, can leverage the learning benefits\nthat the Robi environment can bring. The educational robotics kit used with the block-based\nprogramming environment developed, is the one based on Arduino Uno, a microcontroller\nboard that, together with electronic components, can be considered cheaper than some of\nthe famous educational robotics kits. The main goal of this project is to provide a simpler\nand more intuitive visual programming language platform to program a robot based on\nArduino Uno."
  },
  {
    "keywords": [
      "Refactoring",
      "Program slicing",
      "Code complexity metrics",
      "Low-code",
      "OutSystems",
      "Graph theory",
      "Refatorização",
      "Program slicing",
      "Métricas de complexidade",
      "Low-code",
      "OutSystems",
      "Teoria de Grafos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Suporte para refatorização automática de lógica de negócio baseada em modelos",
    "autor": "Fernandes, Tiago Fernando Santos Braga",
    "data": "2017-12-28",
    "abstract": "Software’s structure profoundly affects its development and maintenance costs. Poor\nsoftware’s structure may lead to well-known design flaws, such as large modules or long\nmethods.\nA possible approach to reduce a module’s complexity is the Extract Method refactoring\ntechnique. This technique allows the decomposition of a large and complex method into\nsmaller and simpler ones, while reducing the original method’s size and improving its\nreadability and comprehension.\nNowadays, it’s almost mandatory that Integrated Development Environments (IDEs) support\nthis and other refactoring techniques. Despite the wide availability of the extract method\noperation on IDEs, the identification of portions of code that are worthwhile to refactor still\nrelies mostly on developer knowledge and expertise.\nThus, the purpose of this dissertation is to empower the OutSystems platform with a system that is able to analyse modules complexity and automatically suggest Extract Method\nrefactoring opportunities."
  },
  {
    "keywords": [
      "Fluid simulation",
      "Smoothed-particle hydrodynamics",
      "SPH",
      "Particle-based method",
      "Hash map",
      "Z-order",
      "Data structures",
      "Simulação de fluidos",
      "Método baseado em partículas",
      "Estruturas de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Data structure centered SPH performance evaluation",
    "autor": "Barbosa, Paulo Alexandre Ferreira",
    "data": "2022-03-21",
    "abstract": "Smoothed-particle Hydrodynamics (SPH) is a particle-based simulation considered by many to be the main\ncandidate for fluid simulation. This model was developed by R.A. Gingold and J.J. Monaghan in 1977 and had\nthe purpose of solving astrophysical problems. Over the years, Monaghan has revisited SPH (1985, 1988, 1992\nand so on) and it also gained traction with other researchers who discovered new applications for the model such\nas ballistics, volcanology, oceanography, and so on. Among the fields there is one we are particularly interested\nin, and that is Fluid Simulation.\nThis work aims to implement SPH using efficient data structures that allow a real-time simulation to run on the\nGraphics Processing Unit (GPU). According to the literature, the z-order indexing method and the hash map are\nthe most suitable structures for this purpose. It is intended to see its impact and in which situations one will be\nbetter suited to use than the other.\nWith said implementation, several tests were performed in order to analyze the robustness and stability of the\nmethod. With these tests it was possible to compare the two data structures used.\nThe implemented SPH showed realistic and robust results in most cases, being able to handle multiple scenes\nof varying levels of complexity. Despite the good results, it showed some difficulties in maintaining stability in\nsome boundaries (boundaries with great curvature or sharp edges) and also showed some difficulties in scenes\nwith two fluids with different densities.\nAs for the data structures, it was possible to observe that both are efficient and support real-time simulations\nwith more than 1 million particles (using a NVIDIA RTX 3080). In the case of z-order, it proved to be the method\nwith the best performance when compared to the hash map under the same conditions, that is, scenes with the\nsame number of particles and the same simulation volume. This is due to the larger data locality that z-order has.\nOn the other hand the hash map was a bit slower (when compared with the z-order under the same conditions)\nbut allowed for greater freedom when creating a scene. When comparing the two methods with the same number\nof particles but different simulation volumes we can see that the hash map catches up with the z-order method\nas the particles spread across the simulation.\nWith the two data structures analyzed it is possible to draw some conclusions. The z-order method is recom mended when we have a limited and relatively small simulation volume. In case there is no simulation volume,\nor it is very large, it is recommended to use a hash map since the performance deficit seems to disappear as the\nsimulation volume gets bigger and the particles spread across the volume."
  },
  {
    "keywords": [
      "Deep learning",
      "Machine learning",
      "Document classification",
      "Text mining",
      "Aprendizagem máquina",
      "Classificação de documentos",
      "Mineração de texto",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Development of a tool based on deep learning able to classify biomedical literature",
    "autor": "Alves, Nuno Miguel Caetano",
    "data": "2020-11-16",
    "abstract": "In the last decades, the scientific community has produced huge amounts of publications about\nthe most varied biomedical topics, making the search for relevant information a really difficult\ntask for every researcher. Some approaches have been followed to develop tools that can\nfacilitate this process. For instance, PubMed implemented in 2017 a Machine Learning model to\nsort documents by their relevance. Nevertheless, even the authors consider that their system\nwould benefit from the implementation of a Deep Learning model, which for now needs more\nstudies.\nIn this context, a package called BioTMPy1 was developed in this work, to perform document\nclassification of biomedical literature using the Python programming language. The package\nis divided into different modules to provide to the user functions to read documents in different\nformats, perform preprocessing and data analysis and to train, optimize and evaluate Machine\nand Deep learning models. Our package also provides intuitive pipelines that can be easily\nadapted for the user needs, illustrating how to implement complex deep learning models.\nThe developed package was applied to a dataset from a challenge of the BioCreative forum,\nfrom 2019, about protein-protein interactions altered by mutations, an important topic for the\nadvances related to precision medicine. Using this dataset, it was possible to observe a slightly\nbetter performance of BioWordVec pre-trained embeddings over GloVe, ”pubmed pmc” and\n”pubmed ncbi” embeddings. Also, with the evaluation of the developed models on the test set,\nwe managed to overcome the challenge’s best submission, by using a model with BioBERT and\na bidirectional LSTM on top, resulting in a difference of 7.25% for average precision, 3.22% for\nprecision, 2.99% for recall and 3.15% for the f1-score.\nAlso, a web server was developed to provide access to the best Deep Learning model\ntrained in this work. The overall pipeline here developed can be applied to other case studies in\ndifferent topics, provided there is a set of documents annotated as relevant and non-relevant,\nallowing to train the models."
  },
  {
    "keywords": [
      "Deep learning",
      "Document classification",
      "Machine learning",
      "Biomedical text mining",
      "Text mining",
      "Aprendizagem profunda",
      "Classificação de documentos",
      "Aprendizagem máquina",
      "Mineração de texto biomédico",
      "Mineração de texto",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Development of a recommendation system for scientific literature based on deep learning",
    "autor": "Silva, Tiago Rafael Ferreira Miranda da",
    "data": "2022-12-15",
    "abstract": "The previous few decades have seen an enormous volume of articles from the scientific commu nity on the most diverse biomedical topics, making it extremely challenging for researchers to\nfind relevant information. Methods like Machine Learning (ML) and Deep Learning (DL) have\nbeen used to create tools that can speed up this process. In that context, this work focuses\non examining the performance of different ML and DL techniques when classifying biomedical\ndocuments, mainly regarding their relevance to given topics. To evaluate the different techniques,\nthe dataset from the BioCreative VI Track 4 challenge was used. The objective of the challenge\nwas to identify documents related to protein-protein interactions altered by mutations, a topic\nextremely important in precision medicine. Protein-protein interactions play a crucial role in the\ncellular mechanisms of all living organisms, and mutations in these interaction sites could be\nindicative of diseases.\nTo handle the data to be used in training, some text processing methods were implemented\nin the Omnia package from OmniumAI, the host company of this work. Several preprocessing\nand feature extraction methods were implemented, such as removing stopwords and TF-IDF,\nwhich may be used in other case studies. They can be used either with generic text or biomedical\ntext. These methods, in conjunction with ML pipelines already developed by the Omnia team,\nallowed the training of several traditional ML models.\nWe were able to achieve a small improvement on performance, compared to the challenge\nbaseline, when applying these traditional ML models on the same dataset. Regarding DL, testing\nwith a CNN model, it was clear that the BioWordVec pre-trained embedding achieved the best\nperformance of all pre-trained embeddings. Additionally, we explored the application of more\ncomplex DL models. These models achieved a better performance than the best challenge\nsubmission. BioLinkBERT managed an improvement of 0.4 percent points on precision, 4.9\npercent points on recall, and 2.2 percent points on F1."
  },
  {
    "keywords": [
      "681.3-7",
      "336.71:681.324",
      "681.324:336.71"
    ],
    "titulo": "Protocolo CAP em Smartphones: análise e implementação do protocolo de autenticação Chip Authentication Program",
    "autor": "Martins, Tiago Filipe Maia Campos",
    "data": "2012-01-31",
    "abstract": "sociedade atual tem vindo a adotar a conveniência dos serviços bancários online. Estes, no entanto, tem enfrentado um problema crescente de fraudes. Uma das soluções que emergiu no mercado para tentar contrariar esta tendência é o protocolo Chip Authentication Program (CAP), que fornece ao sistema mecanismos de autenticação forte, baseados em dois fatores de autenticação. Contudo, este implica a utilização de um dispositivo dedicado, que, pelo investimento financeiro avultado envolvido e pela falta de aceitação dos clientes em transportar um dispositivo adicional sempre que pretendem usufruir dos serviços bancários online, tem funcionado como barreira à massificação deste protocolo.\n\nCom a presença cada vez mais relevante dos smartphones na sociedade e com o surgimento de smart cards para estes dispositivos, os objetivos desta dissertação focaram-se, fundamentalmente, em chamar a atenção para a potencialidade destas tecnologias para aplicações com requisitos de segurança críticos e em incentivar a utilização do protocolo CAP, ultrapassando os principais entraves à sua adoção e contribuindo com uma solução que permita reduzir as fraudes em serviços bancários online.\n\nO resultado desta dissertação é um sistema baseado no protocolo CAP, que reúne as características que melhor caracterizam um smartphone com as propriedades de segurança que os smart cards acrescentam a um sistema. A utilização do smartphone, um dispositivo bem mais ubíquo do que o dispositivo dedicado tipicamente usado no protocolo CAP, permite reduzir o investimento necessário, uma vez que não é necessário fornecer dispositivos aos clientes, e reflecte-se também numa maior aceitação por parte do utilizador final que não tem de transportar um dispositivo adicional para usufruir dos serviços bancários online."
  },
  {
    "keywords": [
      "Linguistic",
      "Natural language processing",
      "Attribute grammar",
      "Linguística",
      "Processamento de língua natural",
      "Gramáticas de atributo",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Applying attribute grammars to teach linguistic rules",
    "autor": "Sousa, Manuel Gouveia Carneiro de",
    "data": "2021-08-10",
    "abstract": "This document presents the topic “Applying Attribute Grammars to teach Linguistic\nRules”, at Universidade do Minho in Braga, Portugal. This thesis is focused on using the\nformalisms of attribute grammars in order to create a tool to help linguistic students learn\nthe different rules of a natural language. The system developed, named Lyntax, consists\nin a processor for a domain specific language which intends to enable the user to specify\ndifferent kinds of sentence structures, and afterwards, test various phrases against said\nstructures. The processor validates and evaluates the input given, generating a grammar\nwhich is specific to a previously chosen sentence. Lastly, using ANTLR, a parser is generated\nfor that specific grammar referred above. The processor built by ANTLR also creates a\nsyntax tree that is presented to the user for analysis purposes.\nAn interface that supports the specification of the language (written in Lyntax DSL) was\nbuilt, also allowing the use of the processor and the generation of the specific grammar,\nexempting the user from knowing the details of the process.\nWithin this document, the focus will be primarly dedicated to the analysis of the system\nand how each block was built. Different examples of the processor in action will be shown\nand explained."
  },
  {
    "keywords": [
      "Machine learning",
      "Recommender systems",
      "Meal planning",
      "Decision support systems",
      "Sistemas de recomendação",
      "Planeamento de ementas",
      "Sistemas de suporte a decisão",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Intelligence on nutrition in healthcare and continuous care",
    "autor": "Miranda, Rui Pedro Mesquita",
    "data": "2019-07-16",
    "abstract": "In the healthcare industry, the patient’s nutrition is a key factor in their treatment process,\nas every user has their own specific nutritional needs and requirements. For example, after\na major surgery, a patient should eat products with high fiber while avoiding processed\nfoods and dairy. An appropriate nutrition policy can therefore complement the patient’s\nrecovery process, alleviating possible symptoms.\nFood recommender systems are platforms that offer personalised suggestions of recipes\nto users. These systems are often implemented in food recipe websites, offering similar sug gestions. They are also used for improving the user’s health and recommending healthier\nrecipes while keeping their preferences in consideration. However, there is an absence of\nusage of recipe recommendation systems in the healthcare sector. Multiple challenges in\nrepresenting the domain of food, coupled with the patient’s needs, make it complicated to\nimplement these systems in healthcare services and continuous care.\nIn the context of this master’s dissertation, the aim was to design, develop, and explore\na new generation platform for the provision, planning, and reservation of food plans, com prised of web and mobile tools. A key feature of this platform is the suggestion of meal\nplans to each department, taking into account the patient’s nutritional requirements.\nData regarding the user’s nutritional requirements were collected and analysed, as well\nas feedback from health professionals and users from the social cafeteria. The collected\ninformation supported the development of a food recommendation system. These tools\nwill help nutrition professionals at the Santa Casa da Misericórdia of Vila Verde in their work,\nnamely with the making of meal plans for multiple departments, each with their specific\nnutritional requirements."
  },
  {
    "keywords": [
      "Apache ignite",
      "Disco",
      "In-memory data grid",
      "Memória",
      "Recursos Humanos",
      "Disk",
      "Human Resources",
      "Memory",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "In memory data grid clusters HR employee data fusion",
    "autor": "Mota, João Diogo Mendes Teixeira da",
    "data": "2022-12-15",
    "abstract": "Com o aumento constante da quantidade e complexidade dos dados, vem a necessidade de serem estudadas\nnovas tecnologias de forma a acompanhar este crescimento sem pôr em causa o desempenho.\nA presente dissertação foca-se numa destas soluções, os In-Memory Data Grid (IMDG). Um IMDG define-se\ncomo sendo uma base de dados com armazenamento primário em memória volátil colocada numa camada\nimediatamente a cima de uma base de dados tradicional (armazenamento persistente). Além disso, permite a\ndistribuição de recursos e computações por diversos nodos. Assim, no decorrer da dissertação, foram estudados\nos principais conceitos desta solução, bem como algumas das mais prestigiadas tecnologias na área.\nApós estabelecido que o Apache Ignite seria a tecnologia que poderia trazer mais vantagens, esta foi aplicada\na um caso real. Assim foi executado o cálculo de uma matriz de segurança, que permite atribuir as permissões\nde visualização de dados entre membros de uma empresa, bem como a execução de uma simples interrogação\nde forma a efetuar uma análise de desempenho entre o IMDG e a solução previamente existente baseada em\nSQLServer.\nNo que diz respeito ao cálculo da matriz de segurança, os valores de desempenho foram limitados,\nmaioritariamente devido às limitações de Structured Query Language (SQL) da ferramenta, obtendo um\ndesempenho três vezes inferior comparativamente com a solução anterior. Por outro lado, com a interrogação\nselecionada, o Ignite mostrou um melhor desempenho (na utilização de uma grid com 4 nodos, esta interrogação\napresentou, para uma thread, uma média de 2266.92ms vs. 8099.64ms no SQLServer)."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Local analysis strategies for exudate detection  in fundus images",
    "autor": "Pereira, Joana Daniela da Silva",
    "data": "2017",
    "abstract": "Diabetic Retinopathy (DR) is a common complication of diabetes, which is among the major causes of vision loss in the world. An early detection of the disease is the key to avoid the patient’s blindness. However, at the initial phase of the disease, the vision impairment is not easily percieved by the patient. Therefore, regular follow-up exams are recommended in order to detect anomalous patterns in the patient’s retina. Exudates are one of the most prevalent signs during the early stage of DR and, therefore, its early detection is vital to prevent the patient’s blindness. However, the manual detection of exudates by experts is laborious and time-consuming. Thus, automated screening techniques for exudate detection have great significance in saving cost, time and labor, allowing the ophthalmologists to make the treatment decision timely. In this sense, one of the main objectives of this thesis is to develop and compare different strategies to locally extract information of fundus images for detecting exudates.\nSeveral methods related to the automatic detection of exudates have been proposed in the literature however, these methods focus their efforts in the segmentation of exudates or require the extraction of features from a lesion candidate map. On the other hand, in the methodologies proposed in this thesis, the characterization of healthy and damaged retinal areas is performed by applying image descriptors in a local way, avoiding the segmentation step and the generation of candidate maps.\nA system based on local feature extraction and Support Vector Machine classification is used to develop and compare different strategies for automated detection of exudates. The main novelty of this work is allowing the detection of exudates using non-regular regions to perform the local feature extraction. To accomplish this objective, different methods for generating superpixels are applied to the fundus images of E-OPHTA database and texture and morphological features are extracted for each of the resulting regions. Finally, each region is classified according to healthy and pathological classes, during the classification stage.\nThe strategies proposed in order to generate superpixels rely on applying the marker-controlled watershed transformation to a spatially regularized gradient. From these strategies, two different types of superpixels are created: c-Waterpixels and m-Waterpixels. In the end, an elaborated comparison between the proposed methods for generating m and c-waterpixels and the state-of-the-art method for generating SLIC superpixels is performed.\nAdditionally, a system based on Convolutional Neural Networks (CNN) is explored to discriminate between healthy and pathological regions in fundus images. Transfer learning is applied to fine-tune some of the most important state-of-the-art CNN architectures. Exudates usually represent less than one percent of the total number of pixels that compose the retinal image. This is the reason why, in both the systems presented in this thesis, the fundus images are divided in superpixels and the classification is performed for each of the regions.\nLastly, an exhaustive comparison between the two created systems to automatically detect exudates is performed. In other words, the classification results obtained through the system involving CNNs are compared with the ones obtained by applying the approach based on feature extraction and subsequent classification using machine learning algorithms."
  },
  {
    "keywords": [
      "APIs",
      "Web application",
      "Irrigation system",
      "Water",
      "Machine Learning",
      "Aplicação Web",
      "Sistema de rega",
      "Água",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Smart irrigation system: otimização do sistema de rega em espaços verdes",
    "autor": "Barros, Paulo Filipe Moreira",
    "data": "2022-03-17",
    "abstract": "An optimization of irrigation systems and better management of green spaces is essential nowadays, as one\nof our main resources, water, is often wasted and the soil does not contain the necessary nutrients, which can\nlead to death of vegetation. This work presents a solution where, using a set of public APIs through which the\nenvironment data is collected, it is possible to intelligently and autonomously activate or deactivate the irrigation\nsystem, taking into account a group of previously defined metrics. The system is also prepared to receive\nreal data from sensors implemented in the field. A web application is also developed so that these data are\npresented in a clear and intuitive way, in order to support decision-making by the owner of a particular land.\nFinally, a machine learning algorithm was created that, based on the history of rain occurrence, tries to predict\nthe occurrence of precipitation for a particular day, thus contributing for a more efficient solution."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Agilizar o deployment de aplicações modernas",
    "autor": "Dionísio, Nuno Silvino Santos",
    "data": "2017",
    "abstract": "Para se alcançar um desenvolvimento ágil, as equipas de desenvolvimento devem estar\nmunidas de ferramentas que facilitem a execução e a automatização dos processos ao\nlongo do ciclo de vida de um produto de software. Porém não são as tecnologias por si só\nque tornam as equipas e as organizações ágeis.\nAs metodologias ágeis são essenciais para garantir o sucesso dos projetos de software,\npois permite, numa fase embrionária, a participação dos stakeholders no processo, viabilizando\na rápida deteção de problemas nos requisitos e no produto que se pretende desenvolver.\nPara potenciar todo o investimento colocado nas tecnologias é necessário alterar o modo\ncomo as organizações operam, e adotar práticas e processos que permitam maximizar\ntodos os recursos existentes.\nEste documento propõe uma abordagem que consiste em criar uma harmonia entre o\nprocesso de desenvolvimento e operacional, recorrendo a tecnologias de última geração."
  },
  {
    "keywords": [
      "Cyber-physical System",
      "Functional Mockup Interface",
      "Machine Learning",
      "Python",
      "Sistemas ciber-físicos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Using Machine Learning to automatically infer an approximation of a physical system",
    "autor": "Silva, Afonso João Borges Cabral Cerejeira da",
    "data": "2022-08-05",
    "abstract": "The development of Cyber-physical Systems (CPSs) models is a complex process which requires deep multi-disciplinary knowledge of the intended topic to model. Added to this complexity is the difficulty of combining multiple models, sometimes without access to their source code, and make them communicate in a harmonious and integrated way in order to represent the vicissitudes of the environment where the physical system is inserted into. Functional Mockup Interface is a set of C headers that define a protocol that allows the interoperability of different models, independently of the programming languages and tools that generated them. A model that implements this interface is called Functional Mockup Unit (FMU). \nThis dissertation explores the usage of Machine Learning to generate automatically a FMU from parsing a dataset containing the inputs and outputs obtained during the observation of a physical system. A Command-line Interface (CLI) tool named AutoFMU is also presented here, and it accepts as parameters a set of CSV tables and the names of the column that correspond to the inputs and outputs, using several supervised learning algorithms to infer the relationships between these variables. Its invocation results in a file containing a valid FMU ready to be used.\n In order to assess its feasibility in a real context, the tool AutoFMU was used to generate approximations of a controller of a line follower robot. The generated models were then simulated in the INTO-CPS program and the robot movements under the purview of the new controller were observed. The values generated by the new models were also compared with the datasets of the original physical unit."
  },
  {
    "keywords": [
      "681.3.06",
      "658.0"
    ],
    "titulo": "Test automation framework",
    "autor": "Fonseca, Ricardo Gomes da",
    "data": "2013",
    "abstract": "Primavera has invested a significant and costly man power in developing business-specific\nsoftware solutions. Such solutions share a significant part of boilerplate code, namely the\nuser interface. To minimize costs and, thus, improving it's software engineers productivity,\nPrimavera BSS has invested many resources developing a Framework that allows for the\nnext family of Primavera Products to be generated. The developed tool allows the Primavera\nSoftware Factory to easily adopt Software Development Processes based on Agile methodologies.\nThe goal of this Dissertation is to add a new software component to this framework,\na test automation component, that allows automated execution of tests to be performed on\nProducts modelled on the Framework."
  },
  {
    "keywords": [
      "Analysis",
      "Data management",
      "I/O patterns",
      "LTTng",
      "Performance",
      "Tracing",
      "Análise",
      "Captura",
      "Desempenho",
      "Gestão de dados",
      "Padrões E/S",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Analysis of I/O patterns for data management systems",
    "autor": "Rodrigues, Pedro Miguel Borges",
    "data": "2023-10-17",
    "abstract": "The exponential growth of digital information that has been witnessed in recent years requires a continuous\nevolution and optimization of data management systems, such as databases and storage solutions.\nIn order to provide efficient processing and storage capabilities for large amounts of data, data man agement systems must adopt different optimizations (e.g., caching, replication, data reduction) that in crease their complexity. As a result, developing, configuring and maintaining a data management system\nbecomes increasingly difficult and costly.\nTracing and analyzing the interactions and exchanges between components of these systems is funda mental to uncover performance, correctness and dependability issues almost unavoidable in any complex\nsolution. On the other hand, this presents several challenges, such as minimizing the impact on applica tions’ performance and storage space, improving tracing accuracy and achieving real-time analysis, that\nmust be explored.\nWith this thesis, we present a tracing and analysis pipeline capable of capturing and analyzing the I/O\npatterns of these data-centric systems in order to better understand their behavior, using LTTng as tracing\ntool.\nIn particular, the proposed solution includes a tracing component that efficiently collects disk and\nnetwork I/O metrics originated by the target application. This component is the major focus of this thesis\nand allows for the capture of system calls that the application executes, as well as their arguments, in a\nnon-intrusive and almost real-time way. The rest of the pipeline facilitates the analysis and visualization\nof captured events through search queries and diagrams, allowing the user to find potential performance\nand optimization problems.\nIn the end, we demonstrate that the proposed solution allows for the identification of inefficient and\nredundant I/O patterns in production applications without causing significant impacts on the runtime\nperformance of the application and allowing for near real-time analysis."
  },
  {
    "keywords": [
      "Microservice architecture",
      "Monolithic decomposition",
      "Topic modelling",
      "Software clustering",
      "Arquitetura de microserviços",
      "Decomposição de monólitos",
      "Modelação de tópicos",
      "Clustering de software",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Identification of microservices from monolithic applications through topic modelling",
    "autor": "Brito, Miguel António Ferrão",
    "data": "2020-12-22",
    "abstract": "Microservices emerged as one of the most popular architectural patterns in the recent\nyears given the increased need to scale, grow and flexibilize software projects accompanied\nby the growth in cloud computing and DevOps. Many software applications are being\nsubmitted to a process of migration from its monolithic architecture to a more modular,\nscalable and flexible architecture of microservices. This process is slow and, depending on\nthe project’s complexity, it may take months or even years to complete.\nThis dissertation proposes a new approach on microservices identification by resorting\nto topic modelling in order to identify services according to domain terms. This approach\nin combination with clustering techniques produces a set of services based on the original\nsoftware. The proposed methodology is implemented as an open-source tool for exploration\nof monolithic architectures and identification of microservices. An extensive quantitative\nanalysis using the state of the art metrics on independence of functionality and modularity\nof services was conducted on 200 open-source projects collected from GitHub. Cohesion at\nmessage and domain level metrics showed medians of roughly 0.6. Interfaces per service\nexhibited a median of 1.5 with a compact interquartile range. Structural and conceptual\nmodularity revealed medians of 0.2 and 0.4 respectively. Further analysis to understand if\nthe methodology works better for smaller/larger projects revealed an overall stability and\nsimilar performance across metrics.\nOur first results are positive demonstrating beneficial identification of services due to\noverall metrics’ results."
  },
  {
    "keywords": [
      "Ontology",
      "Ontology matching techniques",
      "Ontology matching tools",
      "Ontology alignment",
      "Ontologias",
      "Técnicas de concordância de ontologias",
      "Ferramentas de concordância de ontologias"
    ],
    "titulo": "OMT, an Ontology Matching System",
    "autor": "Gomes, João Pedro Rodrigues",
    "data": "2023-02-20",
    "abstract": "In recent years ontologies have become an integral part of storing information in a\nstructured and formal manner and a way of sharing said information. With this rise in usage,\nit was only a matter of time before different people tried to use ontologies to represent the\nsame knowledge domain. The area of Ontology Matching was created with the purpose of\nfinding correspondences between different ontologies that represented information in the\nsame domain area.\nThis document reports a Master’s work that started with the study of already existing\nontology matching techniques and tools in order to gain knowledge on what techniques\nexist, as well as understand the advantages and disadvantages of each one. Using the\nknowledge obtained from the study of the bibliography research, a new web-based tool\ncalled OMT was created to automatically merge two given ontologies.\nThe OMT tool processes ontologies written in different ontology representation languages,\nsuch as the OWL family or any language written according to the RDF web standards. The\nOMT tool provides the user with basic information about the submitted ontologies and after\nthe matching occurs, provides the user with a simplified version of the results focusing on\nthe number of objects that were matched and merged. The user can also download a Log\nFile, if he so chooses. This Log File contains a detailed description of the matching process\nand the reasoning behind the decisions the OMT tool made. The OMT tool was tested\nthroughout its development phase against various different potential inputs to assess its\naccuracy. Lastly, a web application was developed to host the OMT tool in order to facilitate\nthe access and use of the tool for the users."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Database preservation toolkit: a relational database conversion and normalization tool",
    "autor": "Ferreira, Bruno Alexandre Alves",
    "data": "2016",
    "abstract": "Databases are one of the main technologies supporting organizations’ information assets,\nand very often these databases contain information that is irreplaceable or prohibitively\nexpensive to reacquire. The digital preservation field attempts to maintain this kind of\ninformation accessible and authentic for multiple decades, but the complexity commonly\nfound in databases and the incompatibilities between database systems make it difficult to\npreserve this kind of digital object.\nThe Database Preservation Toolkit is a software that automates the migration of relational\ndatabases to the second version of the Software Independent Archiving of Relational\nDatabases format. Furthermore, this flexible tool that supports the current most popular\nRelational Database Management Systems can also convert a preserved database back to\na Database Management System, allowing for some special usage scenarios in an archival\ncontext. The conversion of databases between different formats, whilst retaining the databases’\nsignificant properties, poses a number of interesting issues, which are described in\nthis document, along with their current solutions.\nTo complement the conversion software, the Database Visualization Toolkit is introduced,\na software tool that provides access to preserved databases, enabling a consumer to quickly\nsearch and explore a database without knowing any query language. The viewer is capable\nof handling big databases as well, promptly presenting results of searching and filtering\noperations on millions of records.\nThis work covers the challenges of relational database preservation, and the development\nof a format and tools that play an important role in successfully preserving this kind of\ninformation."
  },
  {
    "keywords": [
      "Bacteriophages",
      "Phages",
      "Host prediction",
      "Bacterial strain",
      "Machine learning",
      "Bacteriófago",
      "Fago",
      "Previsão de hospedeiro",
      "Estirpe bacteriana",
      "Aprendizagem máquina",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Bacteriophage-host determinants: identification of bacteriophage receptors through machine learning techniques",
    "autor": "Araújo, Pedro Henrique Matela Aidos Manso de",
    "data": "2021",
    "abstract": "Bacterial resistance to antibiotics is nowadays becoming a major concern. Several reports indicate\nthat bacteria are developing resistance mechanisms to various antibiotics. Moreover, the processes involved\nin the development of new antibiotics are lengthy and expensive. Therefore, an alternative to antibiotics\nis needed. One promising alternative are bacteriophages, viruses that specifically infect bacteria,\ncausing their lysis. Hence, it would be interesting to discover which bacteria a specific phage recognizes.\nThe bacterial receptors determine phage specificity, using tail spikes/fibres as receptor binding proteins\nto detect carbohydrates or proteins, in bacterial surface. Studying interactions between phage tail spikes/-\nfibres and bacterial receptors can allow the identification of interaction pairs. Machine learning algorithms\ncan be used to find patterns in these interactions and build models to make predictions.\nIn this work, PhageHost, a tool that predicts hosts at a strain level, for three species, E. coli, K.\npneumoniae and A. baumannii was developed. Several data was extracted from GenBank, retrieving\ngeneral, protein and coding information, for both phages and bacteria. The protein data was used to\nbuild an important phage protein function database, that allowed the classification of protein functions,\nnamely, phage tail spikes/fibres. In the end, several machine learning models with relevant protein features\nwere created to predict phage-host strain interactions. Compared with previously performed works, these\nmodels show better predictive power and the ability to perform strain-level predictions. For the best model,\na Matthews correlation coefficient (MCC) of 96.6% and an F-score of 98.3% were obtained. These best\npredictive models were implemented online, in a server under the name PhageHost (https://galaxy.bio.di.\numinho.pt)."
  },
  {
    "keywords": [
      "Produção de filme",
      "Edição de guião",
      "Reconhecimento de palavras",
      "Realidade virtual",
      "Multimedia",
      "Movie making",
      "Screenplay editing",
      "Word recognition",
      "Virtual reality",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of film production software for a clinical application of advanced MRI technologies",
    "autor": "Santos, Pedro Afonso Rodrigues",
    "data": "2022-12-19",
    "abstract": "A Imagem de ressonância magnética (IRM) é uma técnica que visualiza as estruturas internas do corpo através\nde campos magnéticos poderosos e ondas de rádio, e ao contrário dos Raios-X, um exame de ressonância magnética\nnão usa radiação. Normalmente esta técnica é usada para detecção de doenças, monitoramento de tratamento e\ndiagnóstico. Tem um grande impacto na Neurociência desde que começou a ser desenvolvida nos anos 70 e 80,\nespecialmente porque a Ressonância Magnética funcional (IRMf) e a Ressonância Magnética de difusão (IRMd)\naumentam a eficiência da ressonância magnética, embora as técnicas de IRMf e IRMd tenham tido muito pouco\nimpacto nas aplicações clínicas.\nOs estudos e resultados recentes da IRMf têm sido progressivamente aprovados por médicos e pesquisadores,\npois são capazes de fornecer informações únicas sobre as funções cerebrais, no entanto, quando se trata de\nmelhorar o atendimento clínico, os resultados têm sido muito limitados e a utilização de estímulos audiovisuais\navançados de IRMf foram quase nulos. Uma das abordagens recentes que é adequada para criar um estímulo\naudiovisual avançado de IRMf, e é útil em aplicações clínicas (por exemplo, Transtorno de Déficit de Atenção e\nHiperatividade (TDAH)) é a IRMf cinematográfica. Um dos problemas principais das técnicas cinematográficas de\nIRMf é identificar uma determinada patologia que varia muito entre todos os filmes e patologias, ou seja, encontrar\ne fazer um filme adequado para cada patologia é muito caro e demora muito tempo.\nNeste projeto, iremos explicar o desenvolvimento de programas de software que estão interligados a um cérebro-máquina para que os scripts possam ser transformados em animação por computador."
  },
  {
    "keywords": [
      "eDNA",
      "Sampler",
      "Depth",
      "DNA ambiental",
      "Recolhedor de amostras",
      "Profundidade",
      "Engenharia e Tecnologia::Nanotecnologia"
    ],
    "titulo": "Deep sea biological sampling",
    "autor": "Gil, Carlos Jorge Dias",
    "data": "2023-11-21",
    "abstract": "With the advancement of Environmental DNA (eDNA) monitoring methods, there is a \ngrowing interest in developing new, simple, and cost-effective devices to collect eDNA from \nseawater, particularly from the deep sea.\nThis thesis provides a theoretical introduction to the Environmental DNA concept and \nexisting methods for sample collection, followed by a state-of-the-art review highlighting the \nkey features of each analysed device.\nIn the scope of this work, a novel device was designed and constructed, capable of \nfiltering water at different depths and preserving the filters for subsequent analysis, by the \naddition of ethanol. The device can collect up to four independent filters. To clean the \nequipment, the user simply needs to fill a container a decontaminating liquid (10% sodium \nhypochlorite (NaClO) (bleach), per example) and place the device in cleaning mode.\nThe constructed equipment consists of two aquarium pumps, one of them serving for \nthe intake of external liquid, while the other one directs the alcohol from a reservoir to the \nsystem; and a valve system electronically controlled by a microcontroller that determines the \nstate of each component based on the depth measured by a pressure sensor during sample \ncollection. In cleaning mode, the microcontroller opens all valves, except the alcohol valve,\nsequentially and activates the intake pump for ten minutes. Once the ten minutes have \nelapsed, the pump is turned off, and the valves are closed sequentially.\nFinally, conclusions are presented, weaknesses of the equipment are revealed.\nAdditionally, indications for possible future projects, aiming to optimize the device, are \npresented."
  },
  {
    "keywords": [
      "Setor hoteleiro",
      "Inovação",
      "Cloud",
      "Design science research",
      "Agile",
      "Hospitality industry",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Interfaces hotelaria",
    "autor": "Martins, José Nuno Baptista",
    "data": "2023-12-28",
    "abstract": "Atualmente, o setor hoteleiro vive uma intensa expansão devido ao crescimento da atividade turística.\nEste constitui um sector de atividades económicas onde as tecnologias têm vindo a manifestar-se como\nprimordiais para a eficiência das organizações. De modo a enfrentar a competitividade, as organizações\ndevem adquirir recursos necessários de maneira a tornarem-se os benfeitores do mercado em que estão\ninseridas.\nNesta dissertação descreve-se o desenvolvimento de duas plataformas na área de hotelaria que visam\nintegrar o leque dos produtos cloud comercializados pela [Wintouch]. As duas plataformas inserem-se em\ndois domínios distintos mas complementares. A primeira deverá conter as funcionalidades necessárias\npara ajudar na gestão eficientemente das operações de um hotel. Para além disso, deverá, de acordo\ncom o tamanho e a natureza do hotel, conseguir adaptar-se às as necessidades individuais de cada\nestabelecimento, mostrando-se flexível e abrangente. Este componente deverá estar integrado com a\nsolução de gestão comercial fornecida pela [Wintouch]. A segunda plataforma deverá complementar a\nprimeira, e visa ajudar as governantas de um hotel com a gestão da limpeza do mesmo, bem como\na estabelecer um canal de comunicação e registo entre as governantas e os outros departamentos da\nunidade hoteleira.\nTratando-se de plataformas que têm como fim serem comercializadas no mercado, estas terão de\ncumprir todos os requisitos de performance e funcionalidade associados, de modo a cumprir os padrões\nde qualidade esperados pelos parceiros comerciais da [Wintouch].\nEste documento demonstra a aplicação de uma design science research na abordagem de um problema na área de hotelaria. O foco na conceção e desenvolvimento de soluções inovadoras permitiu a\ncriação de uma solução prática e aplicável ao problema. Além disso, a utilização da metodologia Agile no\nprocesso de desenvolvimento assegurou que o trabalho fosse desenvolvido de forma incremental, com feedback e melhoria contínua. Esta abordagem permitiu uma forma eficiente e eficaz de gestão, resultando\nnum projeto estruturado e abrangente."
  },
  {
    "keywords": [
      "API Gateway",
      "Autenticação",
      "Autenticação.gov",
      "CLAV",
      "Swagger",
      "Authentication",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "CLAV: API de dados e autenticação",
    "autor": "Martins, José Carlos Lima",
    "data": "2020-11-13",
    "abstract": "A Administração Pública portuguesa tem desmaterializado processos e tem promovido a adoção de\nsistemas de gestão documental eletrónica bem como a digitalização de documentos destinados a serem\narquivados. Estas medidas pretendem atingir a otimização de processos, a modernização de procedimentos\nadministrativos e a redução de papel.\nCom o propósito de atingir estes objetivos e simplificar a gestão documental na Administração Pública,\na Classificação e Avaliação da Informação Pública (CLAV) nasce como uma das medidas. A CLAV tem\ncomo finalidade a classificação e a avaliação da informação pública por forma a auxiliar os sistemas de\ninformação das entidades públicas alertando-as quando determinado documento deve ser arquivado ou\neliminado. Para tal esta possui um referencial comum, a Lista Consolidada, com as funções e processos\nde negócio das entidades públicas associadas a um catálogo de legislação e de organismos.\nNos últimos dois anos, a CLAV tem vindo a ser desenvolvida no departamento de informática da\nUniversidade do Minho em estreita colaboração com a equipa de investigação da área na Direção-Geral do\nLivro, dos Arquivos e das Bibliotecas.\nÀ data de início deste trabalho, a CLAV era constituída por dois servidores de bases de dados que tinham\ncomo interlocutor o servidor da API de dados da CLAV. Era com este servidor da API de dados que toda a\ninteração com o exterior passava: acesso de aplicações de terceiras partes e acessos da interface cliente\ndesenvolvida para a CLAV.\nNesta dissertação, o grande objetivo era fazer evoluir a arquitetura aplicacional dando resposta a uma\nsérie de requisitos e tentando simplificar ao máximo o processo da sua manutenção futura.\nNesse sentido, especificou-se e implementou-se um serviço para a proteção da API de dados da\nCLAV, especificou-se a documentação desta API de dados, definiram-se os formatos de exportação e\nimplementaram-se os exportadores desta API por forma a permitir uma maior interoperabilidade dos dados,\nimplementou-se a autenticação com a Chave Móvel Digital recorrendo ao Autenticação.gov, criaram-se os\nmecanismos necessários à migração de HTTP para HTTPS e, por fim, adicionou-se uma API Gateway na\nCLAV por forma a simplificar o funcionamento e gestão da plataforma.\nTodos estes desenvolvimentos estão em produção e podem ser observados acedendo ao sítio Web oficial\nda CLAV: https://clav.dglab.gov.pt"
  },
  {
    "keywords": [
      "Ontology",
      "Domain specific language",
      "Automatic code generation",
      "Ontologia",
      "Linguagem de domínio específico",
      "Geração automática de código",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "ONTODL+: an ontology description language and its compiler",
    "autor": "Dias, Alexandre Costa",
    "data": "2021-10-27",
    "abstract": "Ontologies are very powerful tools when it comes to handling knowledge. They offer a good\nsolution to exchange, store, search and infer large volumes of information. Throughout the\nyears various solutions for knowledge-based systems use ontologies at their core.\nOntoDL has been developed as a Domain Specific Language using ANTLR4, to allow for\nthe specification of ontologies. This language has already been used by experts of various\nfields has a way to use computer-based solutions to solve their problems.\nIn this thesis, included on the second year of the Master degree in Informatics Engineering,\nOntoDL+ was created as an expansion of the original OntoDL. Both the language and\nits compiler have been improved. The language was extended to improve usability and\nproductivity for its users, while ensuring an easy to learn and understand language. The\ncompiler was expanded to translate the language specifications to a vaster array of languages,\nincreasing the potential uses of the DSL with the features provided by the languages.\nThe compiler and some examples of the DSL can be downloaded at the website https:\n//epl.di.uminho.pt/∼gepl/GEPL DS/OntoDL/ created for the application and presented in\nthe final chapters of the thesis."
  },
  {
    "keywords": [
      "681.3-7"
    ],
    "titulo": "An evaluation of key-agreement protocols based on weak-secrets",
    "autor": "Fernandes, Tiago Miguel Soares",
    "data": "2013-09-27",
    "abstract": "Two agents want to securely communicate on a insecure channel in the presence\nof an adversary. For that they agree in a strong cryptographic key based on\na weak-source of randomness stemming from the physical network characteristics\nwhere these agents communicate. In this dissertation we evaluate the tradeo s between\ntwo protocols: an information theoretically-secure Authenticated Key Agreement\n(AKA) [7] that was speci cally designed for this scenario; and a Password-\nAuthenticated Key Exchange (PAKE) protocol [11] whose security guarantees are\nbased on computational arguments. To this end, we carry out an analysis of the\nconcrete security of both protocols, considering in both cases that the goal is to\nagree on a fresh 128-bit secret key."
  },
  {
    "keywords": [
      "Aplicação web",
      "Gestão",
      "Franchising",
      "Rede de franchising",
      "Franchisador",
      "Franchisado",
      "Encomenda",
      "Auditoria",
      "Web application",
      "Management",
      "Franchising network",
      "Franchisor",
      "Franchisee",
      "Order",
      "Audit",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desenvolvimento de uma plataforma de gestão de franchisings",
    "autor": "Duarte, Luís Pedro Martins",
    "data": "2023-11-29",
    "abstract": "Com a constante evolução da economia e da sociedade, surge a crescente preocupação em\nestabelecer e manter modelos de negócios que não sejam apenas lucrativos, mas também\nsustentáveis a longo prazo. Entre esses modelos, destaca-se o sistema de franchising, que,\ncomo qualquer empreendimento, enfrenta desafios complexos e singulares no quotidiano da\nsua gestão.\nOra, na presente dissertação, apresenta-se o desenvolvimento de uma plataforma de gestão\nde franchisings, criada em colaboração com a empresa Wintouch. Este sistema representará\num valioso complemento à oferta de produtos já disponibilizados pela referida empresa\nsobre o mercado de software de gestão comercial.\nDessa forma, foi concebida uma aplicação web, e todos os componentes associados, que\nvisam abordar os diversos domínios que envolvem a gestão de franchisings. Essa solução foi\nprojetada para atender às necessidades de dois tipos distintos de utilizadores intervenientes\nnum negócio de franchising, nomeadamente, os franchisadores e os franchisados. Ainda, a\nplataforma desenvolvida abrange uma ampla gama de funcionalidades, desde a manutenção\nda rede de franchising até a disponibilização de sistemas para controlar as encomendas e de\nmonitorização do desempenho dos franchisados.\nPor fim, considerando a ambição e complexidade inerentes a este projeto de desenvol vimento de software, foram adotados um planeamento e estratégia bem definidos, que\ncompreenderam três fases distintas: uma investigação inicial sobre o tema em questão, a\nmodelação cuidadosa e robusta da solução a ser desenvolvida e, por fim, o desenvolvimento\nefetivo da plataforma. Todas essas fases estão minuciosamente documentadas ao longo\ndeste relatório."
  },
  {
    "keywords": [
      "Atomius",
      "Analysis",
      "Production",
      "Security",
      "Application",
      "Software development",
      "Análise",
      "Produção",
      "Segurança",
      "Aplicação",
      "Desenvolvimento de software",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Atomius: an application to support Bosch internal processes",
    "autor": "Rocha, Tânia Filipa Amorim da",
    "data": "2022",
    "abstract": "Nowadays, customer satisfaction is highly relevant in the business area, as it is only possible to design, produce and supply a product if a customer or partner is interested and invests in it. And so, it is extremely\nimportant to manage with as much rigor and care the product to be placed on the market as the buyer,\ncostumer or group that will use it.\nAtomius is an application used by Bosch’s chemical team to manage products which need to be\nanalyzed at a microscopic level and in detail in order to verify if a product has any defects, if it is damaged\nor if it has any other type of imperfection.\nThis analysis aims to confirm any problem with equipment, whether notified by the factory, whether\npartner or customer, will have a cost for the company depending on its origin.\nThis dissertation, written in a business context, describes the process of defining and monitoring the\ndevelopment of an interactive platform to support the analysis of products uploaded due to the need of\nverifying their composition. It allows chemical components to be associated with each product, as well as\norganize them into groups by state of their analysis and draw conclusions on these results.\nThis allows drawing conclusions about the origin of the defect or imperfection, leading to the correction\nand verification of the remaining products at the production level or verifying if it is a unique case to be\nresolved."
  },
  {
    "keywords": [
      "Indoor positioning",
      "Floorplans service",
      "GeoJSON",
      "Mobile application",
      "Webservices",
      "Posicionamento indoor",
      "Serviço de plantas",
      "Aplicação mobile",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Where@UM – Where is the classroom for my next lecture? – The problem of the space’s geometry",
    "autor": "Araújo, Ulisses Tiago Simões",
    "data": "2022-12-13",
    "abstract": "In recent years more and more complex structures have been built. Buildings and locations which\nusers must navigate efficiently so they can reach their appointments in a timely fashion, such as hospitals,\nuniversities and airports. Unfortunately technologies such as GPS are not well adapted to indoor locations\nand therefore do not provide a solution to this problem. Indoor mapping has been subject to increased\namounts of research in the past few years and a plethora of different solutions have started to arise\nalthough none completely fulfill every requirement this problem presents. This thesis is done in conjunction\nwith others with the final objective of creating the prototype of a mobile application and system that will be\nable to precisely locate where a user is inside of an indoor location through showing them their location on\na floorplan. It will more specifically focus on the modeling aspect of the space geometry in an efficient way\nthat can be used by this application. The purpose of this dissertation is to document the research done\nto choose the most appropriate data format, the development of a conversion method of the available\ndata to the chosen format, and the development of web services and mobile application components that\nwill provide this information to the end user. Additionally the development of a web application that, with\nthe results obtained throughout this investigation process, helps keep track of the progress of the radio\nmapping will also be documented."
  },
  {
    "keywords": [
      "Impact detection",
      "Artificial intelligence",
      "Deep learning",
      "Neural network",
      "Signal processing",
      "Detecção de impactos",
      "Inteligência artificial",
      "Redes neuronais",
      "Processamento síntese de sinal",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Detection and classification of small impacts on vehicles based on deep learning algorithms",
    "autor": "Nascimento, Bruno Manuel Macedo",
    "data": "2022",
    "abstract": "This thesis explores the detection of impacts that cause damage based on data retrieved by an accelerometer\nplaced inside a vehicle and subsequently classified by deep learning algorithms. The real\nworld application of this work inserts itself in the car sharing market, by providing an automated service\nthat allows constant monitoring on the vehicle status.\nThe proposed solution was set as an alternative to the current machine learning algorithms in use.\nPrevious research showed that deep learning algorithms are achieving better performance results when\ncompared to non deep learning algorithms.\nWe use data retrieved from two types of events: Normal driving and damage causing situations to test\nif the models are capable of generalising damage events. The approach to achieve this objective consisted\nin exploring and testing different algorithms: Multi Layer Perceptron (MLP), Convolutional Neural Network\n(CNN) and Recurrent Neural Network (RNN).\nResults revealed promising performance, with the MLP reaching a 82% true positive rate. Despite not\nmatching the result obtained by the current non deep learning algorithm allows us to assess that deep\nlearning is a strong alternative in the long term as more data is collected."
  },
  {
    "keywords": [
      "616-07:681.3",
      "681.3:616-07",
      "Ciências Médicas::Medicina Clínica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Reconhecimento de voz multilingue para controlo de procedimentos endoscópicos",
    "autor": "Afonso, Simão Pedro Oliveira",
    "data": "2014",
    "abstract": "Os exames endoscópicos são prescritos em grandes quantidades, pois são eficazes no diagnóstico,\nbaratos quando comparados com outros exames e estarem generalizados há muito tempo, pois podem\nser realizados em quase todos os hospitais. O resultado deste exame é normalmente um relatório que\ninclui anotações médicas complementadas com algumas imagens retiradas durante o exame.\nAlguns dos exames realizados são apenas feitos para confirmar informação já recolhida, o que leva a uma\nduplicação de esforços desnecessária e desperdício de recursos. Os profissionais de saúde podem\ndescartar informação relevante ao não conseguirem anotar em pormenor uma região de interesse para\nposterior análise mais cuidada.\nO objetivo deste trabalho consiste na criação de um sistema que consiga resolver o problema apresentado\nanteriormente, usando tecnologia de reconhecimento de voz. Este sistema deve reconhecer um pequeno\nvocabulário, independentemente do falante, usado para anotar regiões de interesse nos exames.\nO sistema MyEndoscopy atua como uma cloud privada, que contém vários dispositivos que usam e\nprovidenciam serviços entre si. O dispositivo central deste sistema é a MIVbox, que se liga ao endoscópio\ne permite a captura digital do sinal de vídeo que este gera. A principal funcionalidade providenciada por\neste sistema é a capacidade de armazenar indefinidamente os vídeos completos que são produzidos\ndurante exames endoscópicos, bem como disponibilizar estes vídeos e outros dados para outros\nprofissionais de saúde que os necessitem de consultar.\nNesta dissertação apresenta-se um módulo de reconhecimento de voz para línguas portuguesa e inglesa,\ndenominado MIVcontrol, totalmente integrado no sistema MyEndoscopy. Este módulo reconhece um\npequeno vocabulário, que consiste em comandos usado para controlar os outros módulos. O MIVcontrol é\napresentado como uma alternativa a sistemas similares baseados na cloud, que resolve certos problemas\nrelacionados com proteção de dados e segurança.\nFoi realizado um estudo sobre o módulo desenvolvido para determinar a sua eficácia em comparação ao\nestado da arte. Na sequência desse estudo conclui-se que o sistema tinha uma taxa de erro comparável a\nsistemas similares para outras línguas, e que como resultado é passível de ser usado em ambientes reais."
  },
  {
    "keywords": [
      "Scheduling",
      "Edge computing",
      "Containers",
      "Kubeedge",
      "Escalonamento",
      "Computação em borda",
      "Containers",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Orchestration and distribution of services in hybrid cloud/edge environments",
    "autor": "Vilaça, João Pedro Machado",
    "data": "2022-04-01",
    "abstract": "The Edge Computing paradigm aims at leveraging the computational and storage capabilities of Internet of Things (IoT) devices, while resorting to Cloud Computing services for more demanding processing\ntasks that cannot be done at commodity devices. However, deploying distributed services across Edge\nand Cloud nodes raises new challenges that must be addressed. Namely, the choice of what nodes run\neach service component may be critical for ensuring an efficient service for users. For example, if two\ncritical components, that must frequently exchange data, are placed in different geographic locations, the\nwhole performance of the service will be affected. Therefore, these geographically dispersed environments\ndemand new orchestration and distribution systems for hybrid Cloud and Edge environments, based on\ngeographic location, service demand, business objectives, laws, and regulations.\nThis thesis proposes Geolocate, a generic scheduler for workload orchestration and distribution across\nheterogeneous and geographically distant nodes. In more detail, it provides the design and implementation\nof a scheduling and placement algorithm based on nodes’ geographic location and resource availability\nand a fully functional prototype, integrating Geolocale with KubeEdge, an edge computing orchestration\nplatform based on Kubernetes.\nThe experimental results show that as the network latency and amount of data being transmitted\nbetween nodes increases, so does the response time for applications resorting to these distributed deployments. Our evaluation of an e-commerce application shows that the use of Geolocate can reduce, relative\nto KubeEdge’s default-scheduler, the average response time for requests by about 85%."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Monitorização de serviços baseada em contexto",
    "autor": "Silva, Ricardo Emanuel Fernandes da",
    "data": "2016",
    "abstract": "A monitorização de redes é uma tarefa essencial na gestão e engenharia das redes de comunicações atuais. A conﬁguração de um sistema de monitorização de rede deve considerar as exigências da tarefa de rede a medir e os parâmetros de medição correspondentes, compartilhando algumas necessidades comuns na medição de infraestruturas, mas com especiﬁcidades de acordo com o tipo de serviço prestado e recursos envolvidos. Por exemplo, medir o volume de tráfego da rede tem requisitos de medição distintos da deteção de ataques Distributed Denial-of-Service. Da mesma forma, os parâmetros e as exigências temporais para medir serviços de dados e vídeo são diferentes. Assim, é essencial ter uma visão global dos distintos aspetos relacionados com a monitorização de serviços para uma melhor compreensão dos pontos-chave e promover a qualidade dos serviços prestados. A consciência do contexto é, portanto, um aspeto importante nos sistemas de monitorização de serviços auto-conﬁguráveis. O tema desta dissertação enquadra-se no contexto anteriormente descrito na medida em que vai ser estudado e deﬁnido um modelo de monitorização de serviços baseados em contexto. O objetivo principal prende-se com a identiﬁcação dos requisitos e arquitetura necessários num sistema de monitorização baseado em contexto. Por conseguinte, pretende- se identiﬁcar e avaliar tecnologias apropriadas para lidar com o desenvolvimento de uma ontologia que visa apoiar um sistema de monitorização baseado em amostragem. Por ﬁm, após o estudo dos métodos e tecnologias existentes, um sistema ontológico é desenvolvido utilizando a tecnologia selecionada que forma a base de um sistema de monitorização baseado em amostragem e contexto."
  },
  {
    "keywords": [
      "Bases de dados orientadas a grafos",
      "CYPHER",
      "OWL",
      "RDF",
      "SPARQL",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "SPARQL versus CYPHER: um estudo comparativo",
    "autor": "Moreira, Ezequiel José Veloso Ferreira",
    "data": "2020-11-13",
    "abstract": "Com a crescente necessidade de armazenar dados sobre forma digital as ontologias\ntornam-se cada vez mais relevantes como maneira simples de expressar conhecimento.\nAssim bases de dados capazes de guardar este tipo de estruturas de dados de modo\neficaz, nomeadamente bases de dados orientadas a grafos, têm visto a sua utilização\naumentar.\nNesta dissertação foram estudados dois motores de base de dados deste tipo: o\nGraphDB (Ontotext (2020a)) e o Neo4j (Neo4j (2020)). GraphDB foi criado para\narmazenamento de ontologias Web: OWL (Group (2012)), SKOS (Group (2009)) e RDF\n(Group (2014)), podendo estas ser interrogadas através de SPARQL (W3C (2013a)),\nenquanto Neo4J foi desenhado para armazenar informação fortemente relacionada:\ngrafos de informação dos quais as ontologias fazem parte, sendo CYPHER (Neo4J\n(2020b)) a linguagem de query utilizada para a sua exploração.\nNesta dissertação estudou-se a viabilidade de armazenar ontologias em Neo4J e\nexplorá-las utilizando CYPHER.\nAo longo do processo da resolução deste problema foi determinado um segundo\nobjectivo: criar uma camada tecnológica que permite o uso de SPARQL para interrogar\no Neo4J.\nNesse sentido foi realizado um estudo comparativo das duas linguagens,\nimplementou-se um compilador capaz de traduzir um subconjunto de queries SPARQL\nem CYPHER e foi desenvolvida uma bateria de testes que permitem fazer o benchmark ing da tecnologia criada.\nFinalmente foi construído um protótipo Web que implementa uma frontend sobre o\nNeo4J de modo a permitir não só armazenar ontologias como interrogá-las através de\nSPARQL."
  },
  {
    "keywords": [
      "616.718.1",
      "616-07:681.3",
      "681.3:616-07",
      "Ciências Médicas::Medicina Clínica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Apoio ao diagnóstico das articulações coxofemorais para despiste de displasia congénita da anca",
    "autor": "Gomes, Hugo Manuel Peixoto",
    "data": "2014",
    "abstract": "A displasia congénita da anca é uma doença esquelética congénita comum em recém nascidos. O seu\ndiagnóstico é importante para evitar complicações tardias no crescimento e locomoção. Por ser um exame\ntão complexo e de grande responsabilidade, os diagnósticos feitos pelos profissionais são muitas vezes\nassociados a um grau elevado de incerteza na decisão, provocando receio na realização de exames do\ngénero. Os atos complementares de diagnóstico, neste caso a construção de ferramentas de apoio, são\nsem dúvida o maior passo para reduzir ou eliminar este problema. Desta forma, com profissionais mais\ninstruídos, consegue-se um diagnóstico mais seguro e fiável.\nSão apresentadas recomendações para a realização do exame, englobando parâmetros como a realização\ndo exame clínico, do exame de ecografia e da leitura de imagens de ecografia. As imagens de ecografia\ntêm imenso ruído e para permitir um melhor processamento foram experimentadas operações básicas de\nprocessamento de imagem. É também proposto um relatório normalizado para este exame. O benefício da\nimplementação do relatório é a sua ligação ao sistema de machine learning em que informações\ncolocadas nos campos de preenchimento do relatório seriam transformadas em metainformação das\nimagens de ecografia guardadas também no relatório, funcionando como a alimentação do sistema. Este\nsistema permitiria avaliar e classificar imagens de ecografia de um exame às articulações coxo-femorais.\nPara além destas ferramentas descritas, é proposto uma para otimizar em termos práticos o exame - um\nsistema de comandos por voz com ligação ao ecógrafo para que o profissional não tenha de desviar a\natenção para carregar num simples botão do ecógrafo para assinalar frames essenciais para o diagnóstico.\nA adoção de ferramentas de apoio ao diagnóstico da displasia congénita da anca que permitam melhorar\na prestação dos cuidados de saúde é uma necessidade. As ferramentas apresentadas são um contributo e\nrepresentam o início de novas abordagens ao despiste desta anomalia."
  },
  {
    "keywords": [
      "Ciências Médicas::Ciências da Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Deteção e classificação de regiões de interesse em vídeos de endoscopia",
    "autor": "Gomes, Margarida Maria Freitas de Oliveira",
    "data": "2015",
    "abstract": "As técnicas de Endoscopia Digestiva Alta e de Colonoscopia são fundamentais na prestação de Cuidados de\nSaúde Primários, pois permitem ao profissional de saúde validar o diagnóstico e prescrever o tratamento\nmais adequando. Estas técnicas geram vários tipos de resultados, dos quais se destacam os vídeos\nendoscópicos, uma vez que desempenham um papel preponderante no rastreio de patologias ou de lesões\nque possam estar presentes no Trato Digestivo.\nAs tarefas de visualização e análise dos vídeos endoscópicos, subsequentes à realização dos exames, podem\nvariar entre os 2 a 32 minutos para a Endoscopia Digestiva Alta e entre os 20 minutos a 1 hora para a\nColonoscopia, para cada vídeo. Tal implica uma demora significativa na interpretação dos dados, com\nconsequências ao nível da fadiga e de diagnósticos erróneos por parte do profissional de saúde. Para além\ndeste problema, identifica-se um outro, relacionado com falta qualidade da imagem captada durante os\nexames. Esta pode muitas vezes encontrar-se desfocada, podendo obstar a presença de uma dada patologia\nou lesão.\nÉ com base no panorama descrito previamente que se justifica o desenvolvimento de soluções inovadoras\nque permitam colmatar os problemas acima identificados, particularmente, o processamento e a análise de\nvídeos endoscópicos de longa duração e identificação de informação não relevante para o diagnóstico.\nUma das soluções engloba a eliminação de frames capturados fora do Trato Digestivo e permitiu obter\nvídeos endoscópicos reduzidos e, consequentemente, uma poupança de tempo utilizado nas tarefas de\nvisualização e análise dos mesmos, na ordem dos 45,6 %, para o caso das Endoscopias Digestivas Altas e\nde 56 %, para as Colonoscopias. A solução referente à eliminação de frames desfocados permitiu não só ter\nganhos de tempo, 4,6 %, para Endoscopias Digestivas Altas, e 4,8 %, para Colonoscopias, como de tamanho\nde armazenamento dos vídeos endoscópicos reduzidos, de 4,1 %, para Endoscopias Digestivas Altas, e de\n4 %, para Colonoscopias. Em ambas as soluções foi identificado um fator limitativo, o aumento do bit rate,\nno entanto os valores obtidos não vão influenciar o diagnóstico por parte do profissional de saúde."
  },
  {
    "keywords": [
      "Segurança da informação",
      "Gestão de segurança da informação",
      "Sistemas de Confiança Seguros",
      "Gestão de risco",
      "Gestão de incidentes",
      "Infraestrutura de chave pública",
      "Regulamento eIDAS",
      "Information security",
      "Information security management",
      "Trustworthy secure systems",
      "Risk management",
      "Incident management",
      "Public key infraestructure",
      "eIDAS Regulation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Gestão de segurança de informação para sistemas de Confiança Seguros",
    "autor": "Cunha, Joana Fernandes",
    "data": "2022-01-13",
    "abstract": "Com o aumento da nossa dependência nos sistemas de informação também aumenta a\nnecessidade de sistemas mais seguros e resilientes.\nA pandemia que vivemos, há mais de um ano, veio agravar a situação e mostrou que\ntemos de preparar os sistemas que suportam o nosso dia-a-dia para situações inesperadas e\nque podem comprometer o seu bom funcionamento.\nPara proteger os sistemas é importante aplicar medidas preventivas. Existem standards que\ndefinem as melhores práticas para a segurança dos sistemas, que podem ser implementados\npelas organizações para melhor se prepararem contra situações adversas. Destacam-se os\nstandards desenvolvidos pelo International Organization for Standardization (ISO), na área de\ngestão de segurança de informação, e pelo National Institute of Standards and Technology\n(NIST), na área de sistemas de confiança seguros.\nCada vez mais a preocupação com a segurança da informação tem-se reflectido na\nlegislação e regulamentação Europeia e Portuguesa.\nEsta dissertação pretende analisar as melhores práticas na área da segurança de informação,\natravés dessa análise, propor uma abordagem para a sua implementação e utilizá-la num\ncaso prático, sendo este a infraestrutura de chave pública do Cartão de Cidadão.\nDesta forma, ao longo desta dissertação são analisados os standards relevantes desen volvidos pelo ISO e NIST. Além disso, com o objectivo de contextualizar o caso prático é\nanalisada a regulamentação e legislação aplicável às infraestruturas de chave pública na\nEuropa e em Portugal bem como as componentes da infraestrutura de chave pública do\nCartão de Cidadão.\nCom esta análise, foi possível apresentar uma abordagem que reduz a complexidade do\nprocesso de implementação dos standards e colocá-la em prática num projecto de reestrutura ção e actualização da gestão de segurança da informação da infraestrutura de chave pública\ndo Cartão de Cidadão."
  },
  {
    "keywords": [
      "681.324"
    ],
    "titulo": "Knowledge extraction from the behaviour of players in a web browser game",
    "autor": "Alves, João Miguel Pereira",
    "data": "2013",
    "abstract": "The analysis of the player’s behaviour is a requirement with growing popularity in the traditional\ncomputer games segment and has been proven to aid the developers create better and more\nprofitable games. There is now interest in trying to replicate this attainment in a less conventional\ngenre of games known as web browser games.\nThe main objective of this work is to analyse and create a technique for the analysis of the\nbehaviour of the players inside a web browser game. For this analysis a system to automatically\ncollect, process and store the relevant data for the referred analysis was developed. The web\nbrowser game used as a case study for this work is developed by 5DLab and is called Wack-a-\nDoo. The work developed focused on creating short-term prediction models using the information\ncollected during the first days of playing for each player. The objectives of these models are to\npredict the time played or the conversion state of the players. With the study of the created\nmodels it was possible to extract results that provide potentially useful information to increase the\nprofitability of Wack-a-Doo."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Using deep learning  for unobtrusive sleep stage classification",
    "autor": "Prata, Marco André Ramos Dias",
    "data": "2018-12-13",
    "abstract": "Sleep represents a fundamental role to our well-being and today, as sleep disorders become more \nand more common, there is a growing necessity to monitor our sleep quality daily. Unobtrusive \nautomatic sleep stage classification has made a tremendous breakthrough in this subject allowing \nregular users to monitor their sleep with day-to-day wearables, such as Fitbit Charge 2 tracker, \ncontrary to the traditional manual sleep scoring based on polysomnography (PSG). Using\ncardiorespiratory signals to sleep stage has attracted increased attention as these signals can be \nobtained through unobtrusive techniques and have potential for continuous daily application. \nTherefore, in this thesis, deep learning frameworks based on Long-short-memory networks (LSTMs) \nand Convolutional Neural Networks (CNNs) are used to sleep stage classify, either just using \nrespiratory effort signals, for example obtained from respiratory inductance plethysmography (RIP), \nor using the combination of respiratory and cardiac features, often based on heart rate variability \n(HRV) calculated from electrocardiogram (ECG). The dataset used was the SIESTA dataset that \ncontains a total of 294 subjects (588 PSG recordings) of which 197 are healthy subjects, 51 suffer \nfrom obstructive sleep apnea syndrome (OSA), and the remaining from a variety of sleep or sleep related disorders. The classification problem was divided in a three-class and four-class sleep stage \nclassification problem. \nAs for the results, it was obtained with respiratory data for three stages classification (Wake, rapid eye-movement (REM) and non-REM stages) a Cohen’s kappa (𝜅) of 0.46 for the overall pool of \nsubjects (All), 0.50 for healthy subjects and 0.34 for OSA subjects. For four stages classification \n(Wake, REM, light sleep (N1/N2) and deep sleep (N3/N4) stages) it was obtained a Cohen’s Kappa \n(𝜅) of 0.40 for the subject pool containing all subjects (All), 0.44 for healthy subjects and 0.31 for \nOSA. With cardiorespiratory data, for four stages classification, it was obtained a 𝜅 of 0.40 for the \noverall subject pool (All), 0.44 for healthy subjects and 0.30 for OSA subjects. With three stages, a \n𝜅 of 0.46 for All subjects, 0.51 for healthy and 0.32 for OSA subjects. These results demonstrate \nthat, with the developed frameworks, it is possible to achieve fairly good results as they are similar, \nin some cases moderately higher, to the current state-of-the-art but fail to generalize well, as \nsignificant differences can be found between subject types (All, Healthy and OSA)."
  },
  {
    "keywords": [
      "System calls analysis",
      "Ransowmare",
      "Behaviour analysis",
      "Information security",
      "Linux security",
      "Análise de chamadas ao sistema",
      "Ransowmare",
      "Análise comportamental",
      "Segurança da informação",
      "Segurança em Linux",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Ransomware behaviour analysis in Linux environment",
    "autor": "Dias, Ricardo Cunha",
    "data": "2024-06-28",
    "abstract": "n the current spectrum of the world, there’s been a significant increase in cybersecurity threats, one of\neach is ransomware threats from whom Linux environments have become a more recent target compared\nto Windows environments and the area of study of ransomware in Linuxsystems have been relatively under studied compared to their Windows counterparts, despite the growing diversity of Linux operating systems\nand their significance in various infrastructures. To address this research gap, this dissertation conducts\na comprehensive investigation into the behaviour of ransomware in a compromised Linux system. The\nstudy employs both Static and Dynamic analyses to gain insights into the behaviours and characteristics\nof three distinct ransomware payload families, Avos, RansomExx, and REvil.\nTo test the analysis, a customized sandbox environment has been developed using VirtualBox (Oracle,\n2023) and the Lubuntu operating system. A set of automated tests have been created with shell and\npython scripts to carry out Dynamic analysis tests.\nThe Static Analysis uncovered that these payload families contained visible strings associated with\nmalicious activities, and some of the executables presented obfuscation to challenge reverse engineering\nefforts. This suggests a strong likelihood that the payload’s primary objective is malicious.\nIn the Dynamic Analysis, examining ransomware payloads in action yielded valuable insights. Notably,\nthe analysis uncovered that the overall behaviour is significantly similar between all three families studied.\nThe research also identified the influence of the number of files and file size on the ransomware behaviour,\nparticularly in terms of execution time and the number of executions by the system calls. This effect\nhighlights the importance of specific system calls, such as write, read, lseek, stat, and newfstat. This\nsystem calls plays a crucial role in the behaviour of ransomware payloads across the families studied, and\nthey also highlight the similarities with file-intensive programs with no maliciousness.\nIntricate relationships are also observed between lseek,read, and write system calls, indicating efficient\nfile manipulation by the ransomware.\nSome patterns found that might help distinguish the payloads from non-malicious behaviour are the\nintriguing pattern discovered between error behaviour and the system call futex. In the payloads where the system call futex is present, the majority of the errors that occurred during the payloads execution are\nrelated to this system call this pattern although not as evident is also replicated in the clock_nanosleep\nsystem call behaviour.\nA distinguishable pattern found in the rename and chmod system callsis that the number of executions\nis the same as the number of files present in the directory encrypted.\nThis research also allowed to detect in the studied families, a process behaviour that showed specific\nmalicious intentions targeting VMware systems, confirming the findings from (VMware, 2022).\nIn the realm of future research, this study paves the way for further exploration of the significance of\nspecific system calls in ransomware payloads and their responses to various environmental factors. This\nunderstanding can significantly enhance ransomware detection methods in Linux systems. Moreover, it\nsets the stage for future investigations into Linux ransomware across diverse scenarios, including different\nLinux operating systems, containerized environments, and a wide array of ransomware payloads, such as\nLocker Ransomware."
  },
  {
    "keywords": [
      "614:681.3",
      "681.3:614",
      "61:001.4",
      "001.4:61",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Nomenclaturas e ontologias : plataformas de eHealth e mHealth",
    "autor": "Amorim, Magda Gonçalves",
    "data": "2014",
    "abstract": "Os Sistemas de Informação (SI) das Unidades de Saúde recorrem, cada\nvez mais, a ferramentas informáticas para gerir a grande quantidade de informação, e assim garantir a qualidade e a segurança da mesma.\nA interoperabilidade semântica é uma urgência nos Sistemas de Informação de Saúde (SIS), pois, através de normas, permite a uniformização dos\ntermos médicos, assegurando registos clínicos com informação  fiável, sem\nredundância e ambiguidade, conferindo qualidade e segurança à informação.\nSem terminologias médicas a prestação de cuidados de saúde pode tornar\numa tarefa complexa e conduzir a erros médicos, pelo que a utilização\ndas mesmas é fulcral para o registo de diagnósticos, procedimentos e peças\nanatómicas no Registo Clínico Eletrónico (RCE) de cada utente.\nComo tal, o desenvolvimento de uma plataforma de interoperabilidade\nsemântica vai permitir uniformizar termos médicos e conduzir à diminuição\nde erros. Recorrendo às tecnologias mais avançadas de eHealth e mHealth,\npretende-se implementar o Systematized Nomenclature of Medicine Clinical\nTerms (SNOMED CT) em contexto hospitalar real, num serviço de Anatomia\nPatológica.\nA solução consiste numa aplicação independente da plataforma de registo\nde relatórios médicos, utilizando Web Services, que proporcionam a interação humana com diferentes interfaces para diferentes tipos de dispositivos\neletrónicos."
  },
  {
    "keywords": [
      "Wettability",
      "Adesão celular",
      "Grafeno",
      "Fluorinação",
      "Cell adhesion",
      "Graphene",
      "Fluorination",
      "Engenharia e Tecnologia::Nanotecnologia"
    ],
    "titulo": "Fluorination of graphene for controlled wetting properties and improved cell adhesion",
    "autor": "Moreira, Gabriel Cabral de Freitas Lopes",
    "data": "2024-06-18",
    "abstract": "Estudos de interfaces celulares são fundamentais para a compreensão do comportamento e das interações celulares em sistemas biológicos complexos, particularmente em neurociência, onde as interfaces entre células neuronais e seu microambiente são essenciais para a investigação da função cerebral, desenvolvimento e transtornos neurológicos. Avanços recentes em nanotecnologia facilitaram o desenvolvimento de biomateriais ideais para interagir com células neuronais, com o grafeno emergindo como um candidato promissor. Para ser considerado amigável às células neuronais, uma interface ideal deve possuir alta wettability e condutividade elétrica. Embora o grafeno pristino tenha alta condutividade intrínseca, as aplicações biológicas são limitadas pelo comportamento de baixa hidrofilia. Funcionalização com grupos químicos específicos permite ajustar a wettability do grafeno e ampliar suas aplicações.\nEste trabalho tem como objetivo desenvolver interfaces inovadoras de grafeno para células neuronais. Para aprimorar as suas propriedades de superfície (i.e., wettability), filmes de grafeno foram funcionalizados com grupos químicos de flúor após serem transferidos para substratos de politereftalato de etileno (PET) flexíveis, eletricamente isolantes e biocompatíveis. Os efeitos de fluorinação na wettability do grafeno ainda são enigmáticos devido a resultados inconsistentes na literatura. Portanto, a fluorinação e a caracterização do grafeno em diferentes substratos (Si e Si/SiO2) foram realizadas para uma melhor compreensão desses efeitos. Além disso, métodos para a geração seletiva de áreas de grafeno funcionalizado foram explorados para aplicação de controle celular aprimorado.\nPara obter a amostra de substrato ideal, foi necessário testar vários métodos e parâmetros de preparação e identificá-los como promissores em relação às propriedades finais da amostra. O ângulo de contato com a água (WCA) e o método de quatro pontas foram usados para monitorar a wettability e a resistência de folha das amostras, respetivamente. Além disso, a composição química e a estrutura cristalina foram caracterizadas por espectroscopia Raman e fotoeletrônica de raios-X, e os efeitos na topografia foram analisados com perfilômetro de contato e microscopia de força atômica. Testes de células cancerígenas foram realizados por nossos colaboradores no INL para resultados preliminares em adesão celular."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "An elasticity controller for applications orquestrated with Cloudify",
    "autor": "Fernandez Afonso, Carlos Eduardo",
    "data": "2016",
    "abstract": "Cloud computing is a widely adopted model for the management of computing resources.\nElasticity, an important characteristic of cloud computing, is the ability to allocate and\nrelease computing resources according to demand. An elasticity controller has the responsibility\nof performing decisions regarding when resources are provisioned or released\nand which types of resource are necessary. Developers have expectations of allocating the\nleast amount of resources required and reaching optimal states using the least number\nof costly actions. Therefore, the task of controlling elasticity is challenging, especially in\ncases where the elastic application is composed of components with complex dependencies\namong themselves.\nIn this documentation, we introduce CEController, an elasticity controller for applications\norchestrated with Cloudify. CEController introduces a novel elasticity strategy that\ntakes into account dependencies between components and differences between metric dimensions.\nCEcontroller is evaluated in an environment created to test the controller, which\nincludes the adaptation of a previously used application, and use of a load generation tool.\nFinally, we discuss the results obtained using CEController in a web application and discuss\nthe results."
  },
  {
    "keywords": [
      "Centros de dados",
      "Sistemas de bases de dados",
      "Document stores",
      "Consumo de energia em sistemas de bases de dados",
      "Green queries",
      "Data centers",
      "Database systems",
      "Energy consumption database systems",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Avaliação do consumo energético de consultas em document stores",
    "autor": "Duarte, Duarte Nuno Ferreira",
    "data": "2016",
    "abstract": "Neste momento marcante do meu percurso académico quero agradecer a todos aqueles que, de alguma forma, contribuíram para a concretização dos meus objetivos. Assim, dirijo as primeiras palavras ao meu orientador, o Professor Doutor Orlando Belo, pela sua disponibilidade e pelo seu apoio constante. A sua motivação, as suas sugestões e as suas críticas foram fundamentais. A sua experiência recheada de sabedoria foi um elemento-chave ao longo do desenvolvimento da presente dissertação. A minha mãe merece um agradecimento especial, pela compreensão e pelo apoio diário. A conﬁança que esta deposita em mim foi importante nas mais diversas tomadas de decisão ao longo da minha jornada académica. Sem ela, as vitórias alcançadas ao longo de todos estes anos de aprendizagem não seriam possíveis. Devo à minha mãe tudo o que sou e tenho. Ela está sempre ao meu lado, é um exemplo de ser e de estar. Dirijo, ainda, um agradecimento à Bárbara, pela paciência, motivação, apoio e amizade. Pelo seu empenho e pela sua valorização do meu trabalho. Por me apoiar acima de tudo nos momentos mais difíceis. Pelas suas sugestões de melhoria que nunca me deixaram desistir e que me mostraram sempre o melhor lado de todas as coisas. Por ﬁm, um último agradecimento, aos meus colegas e amigos, que me apoiaram e que me ajudaram. Estes estiveram presentes quer nos momentos de partilha de conhecimento, quer nos momentos de lazer."
  },
  {
    "keywords": [
      "681.3.062"
    ],
    "titulo": "Interpretador SMIL de alta performance para controlo de apresentações multimédia num servidor de streaming de media para dispositivos móveis",
    "autor": "Cadinha, João Pedro Domingues",
    "data": "2009-12-18",
    "abstract": "O Synchronized Multimedia Integration Language (SMIL) é um padrão definido pelo World Wide Web Consortium (W3C), baseado na eXtended Markup Language (XML), usada no controlo de apresentações multimédia. Esta linguagem é usada principalmente nos Serviços de Mensagens Multimédia mas, actualmente, também é usada nos High Definition DVD para interactividade e para vídeos na Internet.\nEste documento descreve de que forma o Ambulant Player, um animador de SMIL open-source, foi modificado para controlar apresentação multimédia fornecidas por um Media Server para dispositivos móveis. A implementação modular do Ambulant baseada em code factories permitiu que o seu módulo de visualização fosse substituído por um mais simples. Este novo módulo envia mensagens para um Media Server em vez de reproduzir os elementos de média. Usando o Ambulant Player como base para este interpretador tornou o desenvolvimento mais rápido e permitiu obter uma ferramenta que respeite a recomendação do W3C de uma forma simples.\nComo resultado final obteve-se um interpretador que demonstrou ser robusto, suportando cento e oitenta sessões concorrentes e servindo cerca de sessenta mil sessões sem erros."
  },
  {
    "keywords": [
      "681.324",
      "618.2"
    ],
    "titulo": "Apoio à prática obstétrica em plataformas móveis",
    "autor": "Valente, Samuel de Meneses",
    "data": "2013",
    "abstract": "Dispositivos móveis como smartphones e tablets estão a conquistar meritoriamente\num espaço de destaque no nosso dia-a-dia, tanto a nível pessoal como profissional.\nSimples atividades como ler um livro ou um jornal tornaram-se num ato muito mais\ncómodo a partir do momento que podemos recorrer a um destes dispositivos, tornandonos\ncada vez mais dependentes das facilidades que a tecnologia nos proporciona.\nNa prática obstétrica, esta interação Humano-Computador faculta aos profissionais de\nsaúde o acesso de forma ubíqua a diversas informações sobre os pacientes, fornecendolhes\nferramentas que facilitam o acompanhamento profissional aos utentes, podendo\nrevelar-se muito útil para o bom funcionamento do serviço. Existem já algumas aplicações\ndesenvolvidas no âmbito do auxílio à prática obstétrica, e apesar de algumas oferecerem\nfuncionalidades interessantes, nenhuma oferece um conjunto de ferramentas robusto o\nsuficiente para ser essencial no dia-a-dia de qualquer profissional de saúde da área. Dada\na especificidade aplicação, a impressão global é que ainda não existe uma aplicação que\nse apresente ao obstetra como uma ferramenta completa na prática clínica, sendo\nportanto o seu desenvolvimento uma mais-valia.\nAntes de desenvolver uma aplicação para dispositivos móveis, é necessário dominar\nalgumas linguagens de programação específicas – caso se pretenda desenvolver uma\naplicação para a plataforma iOS, é necessário dominar Objetive C; caso se pretenda\ndesenvolver uma aplicação para a plataforma Android, é necessário dominar Java. Ao ser\ndesenvolvido um produto para posterior distribuição, tem de ser definida qual a plataforma\nalvo. No entanto, a definição de qual a plataforma a eleger como alvo é um assunto\nbastante delicado dado que não é benéfico do ponto de vista comercial atender apenas\num conjunto de potenciais interessados no produto. Mas, por outro lado, proceder ao\ndesenvolvimento para diferentes plataformas implica um custo enorme ao nível da\nprodutividade pelo facto de ser necessário desenvolver versões do produto em linguagens\ndiferentes de forma a serem compatíveis com as diferentes plataformas móveis. Portanto,\né bastante benéfico enveredar por uma abordagem multiplataforma que permita\ndesenvolver apenas um produto e distribui-lo por várias plataformas."
  },
  {
    "keywords": [
      "Serviço chave móvel digital",
      "Assinatura eletrónica qualificada",
      "PDF",
      "Android",
      "iOS",
      "Qualified eletronic signature",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Conveniência e segurança com SCMD",
    "autor": "Vieira, Nuno Cabral",
    "data": "2019",
    "abstract": "Em Portugal existe o Serviço Chave Móvel Digital (SCMD), que permite a qualquer cidadão efetuar a assinatura eletrónica qualificada remota de dados. Atualmente é disponibilizada,\npublicamente a todos os cidadãos, a aplicação Autenticação.gov, que oferece um\nconjunto de funcionalidades, sendo uma delas a assinatura de documentos PDF utilizando\no SCMD. Mas esta é limitada às plataformas Windows, Linux e Mac. Desse modo, esta\ndissertação de mestrado tem como objectivo desenvolver uma aplicação que assine documentos\nPDF, com o SCMD, em Android e iOS.\nPara desenvolver a aplicação, é utilizada a framework de desenvolvimento de aplicações\nmóveis Android e iOS, React Native e a API nativa em conjunto com a ferramenta de\ngeração, programação e manipulação de documentos PDF, IText 7, para desenvolver as\noperações de assinatura e a comunicação com o SCMD, resultando numa aplicação móvel,\nque apesar das limitações enfrentadas, assina documentos PDF, com o SCMD.\nCom a aplicação desenvolvida, é possível assinar documentos PDF com o SCMD, nos\nquais as assinaturas são claramente visíveis, válidas e estão em conformidade com a especificação técnica dos standards de assinaturas eletrónicas avançadas de PDFs. Tendo isto em\nconsideração, a aplicação desenvolvida oferece assinaturas mais robustas, é mais rápida a\nconcluir todo o processo e produz assinaturas de menor tamanho em comparação com a\naplicação existente."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Efficient computational methods to index crystallographic (S)TEM images and ED patterns",
    "autor": "Silva, André Sá",
    "data": "2018-12-20",
    "abstract": "Electron Microscopy (EM) of nanomaterials relies on grey-scale images to display the material’s atomic arrangement, and a high resolution EM can simultaneously capture multiple\natomic structures into a single image. However, the extraction of useful information from\nthese images is still limited to the determination of the material’s orientations, an underutilisation of the powerful features of an EM equipment and less productive EM sessions.\nThis is due to the compute-intense tasks that have not been automated yet.\nThis dissertation aims to significantly reduce the time required to extract useful data from\nEM images and to remove the user bias when analysing high resolution (S)TEM images, by\nautomating most user routine tasks and integrating them into a software tool, Im2Cr.\nThe deployed Im2Cr tool aimed to aid an EM user to find the most probable atomic\nstructure orientation of a nanomaterial in a single 2D image from a set of pre-defined\nmaterials, with a minimal user interaction.\nIm2Cr was designed and built with a simple and intuitive Graphical User Interface (GUI)\nthat runs on a common modern laptop. It takes as input a high resolution (S)TEM image\nand multiple CIF files with candidate atomic structures to describe the material under\nobservation. After performing the Fourier Transform (FT) on selected Regions Of Interest\n(ROI) in the image, the tool automatically detects periodic information related to the atom’s\npositions by the brighter spots on the image FT. With a set of geometric computations it\ntries to match the theoretical values computed with the measured ones by assigning a\ncustom made merit index. This quantitative evaluation avoids possible user bias and/or\nerrors on image characterisation. Im2Cr outputs at the end a report with the best matching\ncrystallographic structure, its orientation and the indexation table.\nThis tool was successfully tested for robustness and execution efficiency in a wide range\nof high resolution (S)TEM images from crystalline nanomaterials, with domain size ranging\nfrom 4 to 100 nm. The autonomous indexation with preset parameters has a very high\nsuccess rate and runs in a small fraction of typical (S)TEM images acquisition time by\ntaking advantage of the inherent hardware parallelism. Alternatively, the user can change\nsome relevant parameters related to the ROI selection on the (S)TEM image and on the FT\npeaks detection.\nIm2Cr promising results point to the possibility of real-time image analysis with reduced\nuser interaction, allowing for an increased (S)TEM characterisation yield and also enabling\nthe interpretation of complex images, such as those from nanocrystalline materials imaged\nin high-order zone axis orientations."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Atendimento a utentes de unidades de saúde: uma abordagem baseada em realidade aumentada",
    "autor": "Guerra, Marta Alexandra Serapicos",
    "data": "2017",
    "abstract": "As necessidades e expectativas dos clientes acompanham o avanço tecnológico que ocorre de forma exponencial, o que gera um ambiente muito competitivo entre as instituições que precisam de manter os clientes habituais e chamar novos. Deste modo, para que as instituições consigam permanecer e evoluir têm de conseguir um elevado grau de qualidade que só é possível caso se mantenham atentos às constantes mudanças sociais e tecnológicas. Na área da saúde este facto não é exceção e, adicionalmente, o fator qualidade é ainda mais importante, já que se lida com a vida dos pacientes. A saúde do utente não está dependente apenas do topo das tecnologias relacionadas com os cuidados de saúde diretos, portanto todas as tecnologias que permitem uma melhor organização dos dados do paciente, bem como um aumento na eficiência de todos os processos envolventes devem ser consideradas.\nDiversas instituições de saúde encontraram nos sistemas self-service a solução para tornar o processo de atendimento ao utente mais eficiente. No entanto, nas horas de maior afluência estes sistemas não são suficientes e as filas de espera persistem, causando interferências no bem-estar dos pacientes e no desenrolar de outros processos. Tirando proveito das oportunidades que esta quarta revolução industrial revela, problemas deste tipo podem ser minimizados, a experiência do paciente pode tornar-se mais rica e a instituição de saúde pode tomar um lugar de notoriedade. É neste conceito que surge esta dissertação, onde se conceptualiza uma solução para integrar um sistema de quiosques self-service, já existente em algumas unidades de saúde, não só para otimizar o atendimento dos utentes, mas também para modernizar a instituição. É desenvolvido também um possível protótipo desta solução. Para o seu desenvolvimento foi imperativo a utilização de conceitos de tecnologias promissoras, destacando-se a tecnologia mobile, realidade aumentada e WiFi, de modo a que a solução final agregue todas as funcionalidades dos quiosques com todas as vantagens das tecnologias utilizadas."
  },
  {
    "keywords": [
      "Deteção de anomalias",
      "Machine learning",
      "Open charge point protocol",
      "Previsão de anomalias",
      "Redes de carregamento",
      "Veículos elétricos",
      "Anomaly detection",
      "Anomaly forecasting",
      "Charging networks",
      "Electric vehicles",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Anomaly detection for preventive and predictive maintenance systems",
    "autor": "Eiras, Eduardo Veloso",
    "data": "2024-01-15",
    "abstract": "Dado o aumento da preocupação relativamente às alterações climáticas, e políticas de redução dos\nefeitos das alterações climáticas, verifica-se cada vez mais um aumento na procura por veículos elétricos.\nEsta procura, implica alterações na rede de abastecimento de veículos com a necessidade de incluir\nestações de carregamento. Estas alterações, criaram condições para que novas empresas pudessem\nsurgir, como é o caso da We Can Charge. Porém, este aumento, implica também novos desafios na\nmonitorização, manutenção e especificamente na deteção de anomalias nas mesmas.\nAtualmente, a We Can Charge utiliza o Open Charge Point Protocol (OCPP). Este é um protocolo e\nstandard de código aberto, que permite a comunicação entre os diversos componentes de uma rede de\ncarregamento de veículos elétricos, permitindo obter informação relevante sobre os mesmos pela análise\nda informação enviada nos pacotes de comunicação. Através de uma ferramenta dedicada, é possível à\nWe Can Charge, detetar a ocorrência de anomalias. Apesar disto, a deteção das anomalias não é imediata\ne usa uma quantidade limitada da informação fornecida pelo protocolo, não utilizando a informação do\nestado dos conectores e transações de carregamento.\nAssim, o principal objetivo desta dissertação é analisar e utilizar os dados recolhidos sobre o estado\ndos seus conectores e transações de carregamento, aplicando-os a algoritmos de machine learning. Para\ntal, efetuamos a criação de vários modelos, um modelo de deteção em tempo real, onde obtivemos os\nmelhores resultados utilizando o algoritmo Isolation Forest, e dois modelos de previsão baseados em\nredes LSTMs, um relativo ao estado e outro ao número de erros reportados nos dados relativos ao estado\ndos conectores. Combinando estes modelos, foi possível a criação da Connectors Forecasting Network\n(CNF), que nos permite a previsão de anomalias futuras nas estações de carregamento."
  },
  {
    "keywords": [
      "Identidade móvel",
      "Identidade digital",
      "PID",
      "mDL",
      "(Q)EEA",
      "Carteira digital",
      "Mobile identity",
      "Digital identity",
      "EUDIW",
      "SD-JWT",
      "SD-JWT-VC",
      "Digital Wallet",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Serviço de emissão de documentos para EUDI Wallet no formato SD-JWT",
    "autor": "Sá, Tomás Reis Ferreira de",
    "data": "2024-06-11",
    "abstract": "Atualmente vivemos numa sociedade que sofreu uma grande explosão tecnológica ao longo dos anos.\nGraças a isso, muitos progressos ocorreram e como um dos resultados, surgiram os documentos de\nidentificação em formato digital. Neste momento, na União Europeia, vários Estados-Membros possuem\numa infraestrutura nacional que permite a emissão de documentos digitais e métodos que permitem\nautenticação digital, como no caso de Portugal com a infraestrutura Autenticacao.gov. Mas as aplicações\ne infraestuturas desenvolvidas por cada Estado-Membro apenas funcionam num contexto nacional. Para\nresolver esse problema, foi apresentado um projeto piloto denominado por European Digital Identity Wallet\n(EUDIW). A EU Digital Identity Wallet, permitirá que os cidadãos e as empresas europeias partilhem dados\nde identificação de uma forma segura e conveniente. Um dos formatos propostos para os documentos\ndigitais é o Selective Disclosure JWT (SD-JWT), uma variante de JSON Web Token (JWT) que possibilita que\nalguns dos atributos presentes nele podem ser seletivamente divulgáveis, possibilitando ao titular do JWT\no controlo de quais atributos deseja partilhar com as diferentes Relying Parties que o cidadão interagir.\nNesta dissertação será apresentado o serviço que suportará a emissão dos documentos e atestados\ndigitais, EUDI Wallet Provider Backend, nomeadamente a componente que possibilitará a emissão desses\ndocumentos em formato SD-JWT em conformidade com as regulamentações e standards que deverão\nser cumpridas para certificar a salvaguarda e controlo do dados por parte do utilizador final."
  },
  {
    "keywords": [
      "HTM5",
      "Segurança",
      "Black Box",
      "681.3",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "HTML5 - análise dos riscos de segurança e testes de penetração a aplicações web",
    "autor": "Gonçalves, Manuel Francisco Mendes",
    "data": "2014-07-18",
    "abstract": "A nova versão do HTML traz melhorias significativas relativamente à construcão de\naplicacões web mais ricas. Contudo, com as novas funcionalidades vêm acoplados\na elas sempre novos riscos de segurança que precisam ser analisados e colmatados.\n\nAnteriormente ao HTML5 já existiam determinadas ameaças de segurança que\nafetavam as aplicacões web (tais como SQLInjection, XSS, CSRF, etc), e que\nganham um novo potencial devido aos novos recursos do HTML. Este estudo\nfoca precisamente a análise dessas ameaças bem conhecidas, em conjunto com a\nanálise dos riscos de segurança associados às novas funcionalidades do HTML5,\nassim como a apresentacão de regras para atenuacão das mesmas.\n\nAdicionalmente são apresentados um conjunto de módulos para detecão de\nvulnerabilidades HTML5 em aplicacões web. As quais são originadas devido à\nmá utilizacão do HTML5 durante a fase de desenvolvimento. Esse conjunto de\nmódulos corresponde a uma extensão adicionada a um Black Box Web Application\nSecurity Scanner bem conhecido da OWASP designado ZAP.\n\nIsso implicou adicionar também algumas funcionalidades HTML5 a uma aplicacão\nweb também da OWASP designada Wave ZAP, cujo objetivo é ser utilizada para\nrealizar testes de penetracão a fim de testar esses novos módulos do ZAP."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Validation of quantum simulations: assessing efficiency and reliability in experimental implementations",
    "autor": "Rodrigues, Afonso Miguel Fernandes",
    "data": "2018",
    "abstract": "Quantum simulation is one of the most relevant applications of quantum computation for the\nnear future, due to its scientific impact and also because quantum simulation algorithms are\ntypically less demanding than generalized quantum computations. Ultimately, the success of a\nquantum simulation depends on the amount and reliability of information one is able to extract\nfrom the results. In such a context, this work reviews the theory behind quantum simulation,\nwith a focus on digital quantum simulation. The concepts of efficiency and reliability in\nquantum simulations are discussed, particularly for implementations of digital simulation\nalgorithms in state-of-the-art quantum computers. A review of approaches for quantum\ncharacterization, verification and validation techniques (QCVV) is also presented. A digital\nquantum simulation of the Schrödinger equation for a single particle in 1 spatial dimension was\nexperimentally implemented and analyzed, along with a quantum state tomography procedure\nfor characterization of the final quantum state and evaluation of simulation reliability.\nFrom the literature, it is shown that digital quantum simulation is theoretically sound and\nexperimentally feasible, with several applications in a wide range of physics-related fields.\nNonetheless, a number of conditions arise that must be observed for a truly efficient implementation\nof a digital quantum simulation, from theoretical conception to experimental\ncircuit design. The review of QCVV techniques highlights the need for characterization and\nvalidation techniques that could be efficiently implemented for current models of quantum\ncomputation, particularly in instances where classical verification is not tractable. However,\nthere are proposals for efficient verification procedures when a set of parameters defining the\nfinal result of the simulation is known.\nThe experimental simulation demonstrated partial success in comparison with an ideal\nquantum simulation. From the results it is apparent that better coherence times, better\nreliability and finer control are as decisive for the advancement of quantum computing power\nas the more-publicized number of qubits of a given device."
  },
  {
    "keywords": [
      "Microservices",
      "Software architectures",
      "Patterns",
      "Strategies",
      "Scalability",
      "Microsserviços",
      "Arquiteturas de software",
      "Padrões",
      "Estratégias",
      "Escalabilidade",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Patterns and development strategies used on a microservices architecture",
    "autor": "Oliveira, Hugo Manuel Coelho de",
    "data": "2021-10-27",
    "abstract": "Microservices are a modern architecture style that divides a single application into small, indepen dently deployable services, each running in its own process and communicating through lightweight\nmechanisms. However, there is still a lack of research on the design and development of microservices\napplications.\nThe development of applications using microservice-based architectures requires a variety of es sential factors that must be kept in mind to achieve good and future proof results.\nGiven the growing demand for scaling applications and the growth of cloud infrastructures, mi croservices emerged as one of the most prominent architectural advancements in recent years. They\nare still in their early stages of integration, and for that reason this architecture style has yet to be\nwidely studied.\nWith that in mind, this dissertation aims to close this gap by providing the key elements that should\nbe considered when designing and building solutions based in microservices. It begins by researching\nand studying these architectures and finishes with a implementation of microservices based on a case\nstudy."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Utilização dos templates e modelos do Django para desenvolver aplicações web de elevado desempenho",
    "autor": "Fernandes, João Miguel Gonçalves",
    "data": "2018-12-12",
    "abstract": "This document describes the development of high performance Web applications using\nDjango framework. Initially, the operation and usage mode of Django are introduced, as\nwell as several Web applications’ latency reduction techniques. The work carried out fo cused on the design, implementation and performance optimization of a Web application,\nwhich consists of an article sharing system. The development process followed the Scrum\nmethodology. During development, several technologies were explored, such as Memcached,\nCelery and Varnish, which enabled the implementation of certain performance optimi zation strategies. The latency of several operations was measured, before and after the\napplication of optimization techniques, in order to ensure that one was moving in the right\ndirection. The optimization of the application’s performance was performed at various le vels, including the transfer of content across the network and the backend services. HTTP\ncaching, data compression and minification tecniques, as well as static content replication\nusing Content Delivery Networks, were used. Partial update of the application’s pages on\nthe front-end and asynchronous processing techniques were applied. The database utili zation was optimized by creating indexes and by taking advantage of a NoSQL solution.\nMemory caching strategies, with distinct granularities, were implemented to store templa tes and application objects. Furthermore, asynchronous task queues were used to perform\nsome costly operations. All of the aforementioned techniques favorably contributed to the\nWeb application’s latency decrease. Django only supports the application of some of these\ntechniques, because it operates on the back-end. Since performance must be optimized at\nvarious levels, it was necessary to use other tools besides Django."
  },
  {
    "keywords": [
      "Sistema de pagamentos",
      "Blockchain",
      "Smart contracts",
      "Ethereum",
      "Solidity",
      "Payments system",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistema de pagamentos descentralizado para e-commerce na Blockchain",
    "autor": "Vaz, Ricardo Oliveira",
    "data": "2023-11-27",
    "abstract": "A tecnologia blockchain tem evoluído a um ritmo incrível desde da criação da Bitcoin em 2008, por Satoshi\nNakamoto, fazendo com que esta tecnologia seja um dos assuntos mais falados aquando a escrita desta\ndissertação.\nSão já imensas as aplicações e industrias em que a tecnologia blockchain é usada: NFTs (Non-Fungible\nTokens), Banking, Secure Data Sharing, Music Royalties, IoT, AML Tracking, Voting, Real Estate, Supply\nChain, Insurance, Energy, Cross-Border Payments (Payment Gateways) - funcionalidade em que se focará esta\ndissertação - entre outras funcionalidades.\nO termo Payment Gateway (PG) é utilizado para descrever um sistema de pagamentos que pode ser encontrado\nno checkout das lojas online dos comerciantes, este tipo de sistemas tem como objetivo facilitar os pagamentos\npara compradores e comerciantes. Assim sendo, Crypto Payment Gateway (CPG) é o termo utilizado para\nsistemas que, ao contrário dos anteriores, se focam nos pagamentos em criptomoeda, conseguindo assim\nremover intermediários no processo.\nSão já várias as soluções de Payment Gateways existentes, no entanto, estes sistemas partilham um grande\nproblema, não possuem uma solução que permita aos compradores o uso das suas criptomoedas para efetuar\ncompras ou proceder a pagamentos. É então que os CPGs entram para resolver essa questão, contudo também\nestes possuem limitações, entre as quais se destaca a centralização. Assim sendo, quer PG quer CPG são de\nalguma forma controlados por uma entidade centralizada, o que pode ser visto como uma desvantagem para o\nutilizador, pela falta de transparência nos processos. Por forma a resolver os problemas mencionados, foi criado\num sistema de pagamentos descentralizado.\nPara tal foram desenvolvidos smart contracts e criadas interfaces para facilitar a interação com os mesmos, foi\nainda implementado um sistema para obter as informações dos eventos dos smart contracts. Posteriormente,\nfoi desenvolvido um protótipo de uma Loja Online de um Comerciante, de forma a demonstrar como o projeto\nfuncionaria se implementado em contexto real."
  },
  {
    "keywords": [
      "616.127",
      "616-079"
    ],
    "titulo": "Analysis of the myocardial function using tagging MR",
    "autor": "Leitão, Ana Jorge Rodrigues de Moura",
    "data": "2013",
    "abstract": "Heart diseases can often manifest themselves by irregularities in the movement of the heart\nmuscle. To assess the function of the myocardium, a method based in the Optic Flow\nConstrain Equation (OFCE) is applied in tagging MR images. The sequence of tagging\nMR images allows us to detect deviations in deformation and strain through time. However,\nthe application of the OFCE implies the assumption of spatial phase conservation.\nTherefore, harmonic filters in the Fourier domain were used in each frame of the sequence\nto remove the variation of intensity trough time.\nIn order to achieve a model capable of distinguishing a malfunction from normal function\nof the cardiac wall it is necessary to acknowledge what is the ground truth and which\nfactors can affect the results. This study explores several scenarios using synthetic data that\nmimic tagged MR images in order to discover which variables can optimize the OFCE.\nThis work allows us to analyze up to what extension the OFCE can be applied to a\ncardiac motion simulator (CMS) based on Waks et al. [1], capable of reproducing the\nnormal function of the heart. After a series of tests with simulated data and the respective\ncomparison with real volunteers data, it is possible to assess quantitatively the method\nused."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Vulnerabilities fast scan: tackling SAST performance issues with Machine Learning",
    "autor": "Ferreira, Samuel Gonçalves",
    "data": "2020-05-06",
    "abstract": "Nowadays, cybernetic attacks are a real threat that can compromise any individual,\norganization or company’s integrity. Every day new cases are reported, that show the\nreal damage cyber criminals can cause. Sensitive data exposure, identity theft, service\nmalfunctioning or shutdown are just a few of the most common threats, which in many\ncases might impact companies either with financial loss or by damaging their reputation.\nPopulation, in general, is becoming each time more aware of the risks of using electronic\ndevices connected to the web and so are companies. With the rise of this awareness, over the\nlast years, cyber security has become a major concern for Software companies.\nThis threat also led to the birth of the Software vulnerability detection market. Companies\nstarted commercializing Software and advisory to other companies, in order to keep them\nless exposed to cybernetic risks. There are many mechanisms and technologies used by\nthese companies to identify vulnerabilities in applications. The most popular technology\nused to detect vulnerabilities is SAST (Static Application Security Testing) as it focus on\nthe detection of vulnerabilities at the early stages of Software development. However, this\nrequires the analysis of the source code, which in many cases, is huge and thus such analysis\nis too time consuming.\nBeing that the context and motivation for this dissertation, the goal is to investigate the\npossibility of performing source code analysis in a faster way, relying on machine learning\napproaches. Code embeddings, classification algorithms and clustering algorithms were the\nmain approaches explored in this work.\nAlong the project, it was realized that some approaches performed better than others, in the\ntask of detecting software vulnerabilities. Clustering algorithms, according to the performed\nexperiments, are not suitable for the problem. Classification algorithms produced results that\ncan be considered worthy of further investigation, but did not meet the established goals.\nAfter some failed attempts, this project demonstrated that it is possible to train a prediction\nmodel, based on code2seq approach, capable of detecting vulnerabilities in source code,\nwith better performance and accuracy than classic SAST solutions (according to a specific set\nof experiments). Moreover, the used approach allows to easily extend the developed work to\nfind vulnerabilities in any programming language."
  },
  {
    "keywords": [
      "Aplicação móvel",
      "Aplicação móvel multi-plataforma",
      "Sistemas de suporte a operações",
      "Levantamento de dados",
      "Inventário",
      "Realidade aumentada",
      "Mobile application",
      "Multi-platform mobile application",
      "Operations support systems",
      "Data collection",
      "Inventory",
      "Augmented reality",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aplicação móvel para inventário",
    "autor": "Silva, Ricardo Loureiro da",
    "data": "2023-07-27",
    "abstract": "Esta dissertação foi desenvolvida sobre no contexto de um projeto proposto pela empresa Altice Labs,\npara o desenvolvimento de um aplicação móvel para o catálogo e gestão de equipamentos e infraestruturas dispersos no terreno, que integrará o sistema de gestão de inventário, provisionamento de clientes\ne projeto de rede, Netwin. Estudou-se também a possibilidade de complementar a solução através da\nutilização de tecnologia de Realidade Aumentada.\nO projeto surge devido à necessidade de uma solução adaptada à utilização no terreno. O sistema\nNetwin possui uma interface Web, mas esta não se destina à utilização no exterior por dispositivos móveis, não só por não ser capaz de funcionar offline, mas também pela dificuldade que a complexidade\ne extensão desta impõem a um operacional que apenas pretenda catalogar/consultar infraestrutura e\nequipamento de rede.\nO projeto foi dividido em duas partes: A primeira do projeto consiste na conceção de uma aplicação\nque permite cumprir um conjunto de use cases definidos, que passam pelas operações de catálogo/consulta de infraestrutura e equipamento de rede. A segunda parte corresponde a um estudo, especificação\ne prototipagem de algumas soluções de Realidade Aumentada utilizando diferentes ferramentas e abordagens de modo a investigar a viabilidade destas no projeto e de que modo podem acompanhar a aplicação.\nA solução permite aos operacionais no terreno consultar/catalogar e atualizar o estado dos equipamentos, o que permite que o sistema tenha uma visão mais completa e atualizada sobre o estado da\nrede."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Portal Pedidos: plataforma de gestão de pedidos de meios complementares de diagnóstico e terapêutica",
    "autor": "Silva, Inês da Costa e",
    "data": "2019-11-08",
    "abstract": "Over the last few years, there has been a development of Information Technologies (IT)\nand its applicability has had repercussions in the most varied domains. The health\nsector has been no exception, with significant repercussions in terms of improving the\nquality and effectiveness of healthcare provided by the various organizations, the security of data maintenance and transmission, and the increased interoperability between\nthe various Hospital Information Systems (HIS).\nThe present dissertation project comes in the context of the need for maintenance\nand updating of University Hospital Center of Porto computer services. In this sense,\nthe objective was to develop a new web application, called Portal Pedidos, through\nwhich will be managed the whole process of requesting Complementary Diagnostic\nand Therapeutic Means. With reference to users’ perceptions of the current state of\nthe platform and the needs identified by them, the developments to be implemented\nshould ensure the optimization of the existing essential functionality and the introduction of gains that enable earnings in terms of usability, adaptation to devices and\nincreased data structuring performance. The application developed also allows the\navailability of information, previously monolithic and exclusive of the application in\nuse, with other services within the hospital unit through the implementation oriented\nto a web service.\nFor the purpose of application development, the prior exploration and selection of\nthe technologies to be adopted was crucial in order to guarantee an appropriate option\nto achieve the established objectives. The strategy adopted was the use of innovative\nweb technologies, specifically using the ReactJs for frontend and Node.js for backend.\nThe adoption of these technologies enabled the implementation of the pre-established\nrequirements and the incorporation of the contributions that emerged throughout the\nprocess."
  },
  {
    "keywords": [
      "Armazenamento",
      "Espaço de utilizador",
      "Tolerância a faltas",
      "SPDK",
      "Propagação de Contexto",
      "Storage",
      "User space",
      "Fault tolerance",
      "Context propagation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Realistic fault assessment in SPDK-enabled storage stacks",
    "autor": "Miranda, Alexandre Esteves",
    "data": "2023-07-24",
    "abstract": "A eficiência e desempenho das operações de Entrada/Saída(E/S) são aspetos fundamentais na implementação de sistemas de armazenamento. A maioria das soluções atuais são implementadas em\nkernel, obrigando a trocas de contexto entre espaço de utilizador e kernel por parte das aplicações. Estas trocas de contexto são custosas e, por isso, limitam o desempenho do sistema de armazenamento.\nA plataforma Storage Performance Development Kit (SPDK) disponibiliza uma forma de construir estes\nsistemas evitando o acesso a kernel, realizando todas as operações de E/S necessárias diretamente do\nespaço de utilizador para o disco físico.\nContudo, os dados continuam a ter que ser guardados com garantias de persistência. Assim sendo,\nos sistemas construídos com SPDK devem se tolerantes a faltas para garantir resiliência em cenários de\nfalta. A inexistência de uma ferramenta capaz de testar essa resiliência em sistemas de armazenamento\nconstruídos com SPDK é um problema para os programadores que querem testar a resiliência dos seus\nsistemas.\nDe forma a resolver este problema, esta dissertação propõe o Fault Injector in SPDK (FISPDK), uma\nferramenta que estende o SPDK e fornece injeção de faltas determinística ao nível do block device. Para\ninjetar faltas deterministicamente, FISPDK utiliza diferenciação de pedidos E/S de forma a identificar quais\npedidos devem (ou não) ser injetados com uma falta. Para isso, o FISPDK implementa mecanismos de\npropagação de contexto, que permitem passar informação da aplicação para os níveis mais baixos das\npilhas de E/S, e é baseado numa extensão da block device Application Programming Interface (API)\noriginal do SPDK. Para providenciar injeção de faltas, o FISPDK apresenta um block device virtual que\ninterceta pedidos E/S e injeta corrupção de dados ou atraso neles. O block device virtual pode ser\nconfigurado pelos utilizadores para apontar os tipos de faltas e quais os pedidos que devem ser injetados\ncom essas faltas.\nUma avaliação compreensiva do FISPDK demonstra que a nossa solução consegue injetar faltas de\nforma determinística e avaliar a tolerância a faltas de sistemas de armazenamento que usam SPDK, sem\nadicionar uma sobrecarga significativa à pilha de armazenamento."
  },
  {
    "keywords": [
      "Aplicações WEB",
      "Teste de software",
      "Testes baseados em modelos",
      "Interface gráfica com o utilizador",
      "Testes de carga",
      "Web applications",
      "Software testing",
      "Model-based testing",
      "Graphical user interface",
      "Load tests",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automação de testes de carga a partir da interface gráfica com o utilizador",
    "autor": "Teixeira, Bruno Miguel Fernandes",
    "data": "2022-12-15",
    "abstract": "Com a evolução tecnológica, as aplicações WEB têm um papel crucial na comunidade. Nesse sentido\né essencial que estas acompanhem o desenvolvimento tecnológico e sejam cada vez mais plataformas\nconfiáveis e disponíveis. Um dos componentes imprescindíveis para o sucesso de um sistema interativo\né a interface gráfica com o utilizador (GUI, em inglês Graphical User Interface), que, neste caso, são\nacedidas através de web browsers. Com o aumento das capacidades dos browsers, cada vez mais\naplicações fazem uso dessas capacidades, existindo uma componente lógica que é executada no próprio\nbrowser. Desse modo, é fundamental analisar o impacto, a nível computacional, resultante da execução\nda componente lógica no próprio browser. Uma forma de o fazer é através de testes de carga que\nsão executados a partir da interface com o utilizador, permitindo identificar possíveis falhas, tais como\nproblemas de implementação, tempos de resposta elevados ou gargalos de desempenho.\nNo entanto, é indiscutível que as aplicações são cada vez mais complexas e, por sua vez, o processo\nde testes torna-se mais difícil e demorado, existindo uma necessidade crescente da automatização do\nmesmo. Os testes baseados em modelos (MBT, em inglês Model-Based Testing) suportam a geração e\nexecução automática de testes a partir de um modelo do sistema. O MBT aplicado às interfaces gráficas\npermite uma avaliação mais exaustiva da aplicação, dado que permitem uma simulação da interação do\nutilizador com o sistema.\nEsta dissertação tem como objetivo desenvolver uma solução que, tendo como componentes principais o processo de testes baseados em modelos, testes a interfaces gráficas e testes de carga, permita\ncom o menor esforço possível gerar e executar testes de carga a partir da interface gráfica."
  },
  {
    "keywords": [
      "Vulnerability",
      "Attention models",
      "Atatic analysis",
      "Security",
      "Vulnerabilidade",
      "Modelos de atenção",
      "Análise estática",
      "Segurança",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Fast scan, an improved approach using machine learning for vulnerability identification",
    "autor": "Baptista, Tiago João Fernandes",
    "data": "2022-03-03",
    "abstract": "This document presents a Master Thesis in the Integrated Master’s in Informatics Engi neering focused on the automatic identification of vulnerabilities, that was accomplished at\nUniversidade do Minho in Braga, Portugal.\nThis thesis work aims at developing a machine learning based tool for automatic iden tification of vulnerabilities on programs (source, high level code), that uses an abstract\nsyntax11tree representation. It is based on FastScan, using code2seq approach. Fastscan\nis a recently developed system aimed capable of detecting vulnerabilities in source code\nusing machine learning techniques. Nevertheless, FastScan is not able of identifying the\nvulnerability type. In the presented work the main goal is to go further and develop a\nmethod to identify specific types of vulnerabilities. As will be shown, the goal will be\nachieved by changing the method of receiving and processing in a different way the input\ndata and developing an architecture that brings together multiple models to predict different\nspecific vulnerabilities. The best f1 metric obtained is 93% resulting in a precision of 90% and\naccuracy of 85%, according to the performed tests and regarding a trained model to predict\nvulnerabilities of the injection type. These results were obtained with the contribution given\nby the optimization of the model’s hyperparameters and also the use of the Search Cluster\nfrom University of Minho that greatly diminished the necessary time to perform training\nand testing. It is important to refer that overfitting was detected in the late stages of the tests,\nso this results do not represent the true value in real context. Also an interface is presented,\nit allows to better interact with the models and analyse the scan results."
  },
  {
    "keywords": [
      "681.324",
      "621.39"
    ],
    "titulo": "Gestão remota para pontos de acesso de redes sem fios",
    "autor": "Silva, Tiago Fontes Carvalho Duque da",
    "data": "2013",
    "abstract": "No âmbito de uma rede de um provedor de internet sem fios, faz todo o sentido afirmar\nque é essencial a existência de um sistema de monitorização com capacidades de acesso\nremoto e funcionalidades automatizadas. Desta forma, consegue-se reduzir a carga nos administradores\nda rede, bem como melhorar o tempo de resposta a vários eventos, tais como\nperda de rendimento da rede e aumento de colisões. Procura-se também que este sistema tenha\nbaixas percentagens de uso da largura de banda. Para atingir esta finalidade, recorre-se a\ntecnologias normalizadas facilmente disponibilizadas como o SNMP ou NETCONF. Depois\nde um breve estudo comparativo entre as tecnologias referidas, serão analisadas em detalhe\nas MIBs mais relevantes relativamente a pontos de acesso sem fios. A existência de nodos\nescondidos, pela sua importância na degradação da largura de banda de redes sem fios, foi\nestudada em particular. Um dos algoritmos mais relevantes para a mitigação deste problema\nutiliza dinamicamente o mecanismo RTS/CTS através da monitorização de parâmetros, tais\ncomo o número de retransmissões e número de tramas com erros, activando-o tendo em conta\nos valores dos parâmetros monitorizados, evitando a introdução de overhead na rede devido\nao seu uso desnecessário. Tal algoritmo foi introduzido na aplicação de gestão implementada\ne testada, sendo que os resultados obtidos não permitiram concluir da relevante bondade\ndeste mecanismo quando aplicado somente do lado ponto de acesso."
  },
  {
    "keywords": [
      "Interface",
      "Implementação",
      "Design",
      "Componente",
      "React",
      "Framework",
      "Implementation",
      "Component",
      "Software architecture",
      "Methodology",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistematização do desenvolvimento de interfaces web",
    "autor": "Sousa, Nelson Tiago Silva",
    "data": "2022-05-16",
    "abstract": "Esta dissertação aborda o processo de implementação de interfaces web, mais concretamente, utilizando a framework React. As interfaces de utilizador são peças fundamentais de qualquer produto\ncomputacional interativo. Uma boa interface consegue conquistar o utilizador e fazer com que este utilize\no produto, enquanto uma interface de menor qualidade pode ser a causa para a pouca utilização de um\nsoftware. Por este motivo, existem abordagens e metodologias focadas na criação de interfaces, para\nproporcionarem uma boa experiência ao utilizador e fazer com que este utilize o software desenvolvido.\nApós a conceção da interface, é necessário proceder à sua implementação. Para isso existem diversas\ntecnologias e abordagens. Entre as diferentes tecnologias há ainda múltiplas frameworks de desenvolvimento, cada uma com as suas características específicas, o que dificulta, por exemplo, a transição de\numa tecnologia para outra. O ideal seria tornar o processo de desenvolvimento de uma interface o mais\nindependente possível da tecnologia a ser utilizada.\nTendo em vista a resolução deste problema a dissertação apresenta duas contribuições principais.\nUm processo de interpretação do design e da sua divisão em componentes, com o objetivo de maximizar a reutilização de código e consequentemente a eficácia no processo de implementação. A divisão\ndo design é feita através de uma abordagem atómica, onde componentes mais atómicos se juntam e\nformam componentes mais complexos.\nA criação de uma arquitetura genérica capaz de representar uma aplicação React, com o objetivo de\nfornecer uma visão de mais alto nível, mostrando todas as diferentes entidades que existem na arquitetura\nde uma aplicação, e também a forma como estas entidades se relacionam. Isto permite uma separação\nde responsabilidades, separando a definição da interface, da sua lógica de negócio, e da interação com\nserviços externos.\nAlém disso, a arquitetura genérica serviu de ponto de partida para a criação de uma estrutura de\norganização do código capaz de suportar o crescimento dos projetos ao longo do tempo. Estrutura que\nfacilita, e sistematiza, o trabalho dos programadores, dado que estes ficam a saber exatamente onde têm\nde inserir determinados novos ficheiros, ou onde está um qualquer ficheiro que precisa de ser alterado\nquando é necessário atualizar um componente da interface.\nPor último, para provar que os conceitos descritos anteriormente são aplicáveis, para ajudar os programadores a aplicá-los, e para sistematizar o processo de implementação, criou-se uma ferramenta de geração de código. A ferramenta permite criar diferentes partes da arquitetura genérica automaticamente.\nÉ também possível gerar um componente React partindo de um protótipo de uma interface."
  },
  {
    "keywords": [
      "Electric vehicles",
      "Network of charging stations",
      "Smart cities",
      "Sustainability",
      "Cidades inteligentes",
      "Rede de pontos de carregamentos",
      "Sustentabilidade",
      "Veículos elétricos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Optimizing the location of electric vehicle charging points using machine learning",
    "autor": "Pereira, Ana Filipa Rodrigues",
    "data": "2024-01-15",
    "abstract": "One of the major challenges that today’s society deals with the assurance of a sustainable future for present and coming generations. Therefore, Large cities and urban areas are becoming more committed to adopting new sustainability paradigms and are doing so by employing technological solutions. The increasing advancement of technology has made it possible to implement a number of innovative approaches that have a highly beneficial impact on both the environment and the administration of cities.\nThis aspect enables the transition from conventional metropolises to smart cities. Electric vehicles are among the most relevant and popular examples of technological innovations in recent years. The tran sition to a new electric transportation paradigm offers a viable approach to reducing humanity’s ecological footprint due to the low emissions of polluting gases and the substitution of fossil fuels with rechargeable\nlithium batteries. In light of this, this dissertation aims to optimize the placement of new charging stations in Portuguese cities using various Machine Learning techniques. To do this, We Can Charge data was used to implement and evaluate multiple Machine Learning models in order to discover which one would determine more accurately the daily hours of usage of a potential charging station. Overall, the Random Forest was the model with the best performance. Additionally, Artificial Intelligence explainability\ntechniques were implemented to identify the key factors influencing the charging stations usage. For the districts of Portugal analysed, the number of times a charging station changes states daily, from being occupied by a vehicle to being free and vice-versa, was the most significant factor affecting the daily hours of usage of charging stations."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Remote management of applications: deployment of applications and configurations using a rule system",
    "autor": "Gomes, Fábio André Araújo",
    "data": "2016-12-21",
    "abstract": "Users expect access to programs and business information anywhere in the simplest way\npossible using a device. With the diversification of devices, the standard is disappearing\nand we are going towards a more heterogeneous world of mobile devices. With this divergence\nincreasing, it gets more difficult to update, support and control applications through\nall these new platforms. Therefore it is important to facilitate these tasks.\nThe solution to these problems lies on the Mobile Device Management (MDM) programs\nthat can control what devices install and configure, providing remote tasks and access. This\ndissertation aims not to compete with the current products on the market, but to propose\na different way to distribute content to the devices registered on the platform using a Rule\nsystem. This system will prioritize the newest rules by the device and its location characteristics.\nAs so, providing a different way of grouping devices and distributing content to them."
  },
  {
    "keywords": [
      "DLT",
      "Infraestructure",
      "Privacy",
      "Security",
      "Personal data",
      "Management",
      "Infraestrutura",
      "Privacidade",
      "Segurança",
      "Dados pessoais",
      "Gestão",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistema para gestão de permissão de acesso a atributos de identificação pessoal",
    "autor": "Vila, Luís Pedro da Silva",
    "data": "2023-12-28",
    "abstract": "With the increased use of applications that substitute physical elements of our daily lives, comes the necessity\nof sharing personal data with third parties so that the use of such applications is possible and viable. The access\nto these personal data from the applications should only be possible with explicit permission from the users, which\nhe should always be able to manage, so the privacy of his personal data is respected and the user has control\nover the access and use of it. As a response to this situation, arises the idea of developing a system that allows\nusers more control over the use of their data, as well as allowing the information of the permission to access\nto personal attributes to be stored in a secure and trustworthy manner. This system will use a DLT (Distributed\nLedger Technology) infrastructure, with which is intended to be possible to keep information of the access to\npersonal attributes of each user, in a reliable, transparent manner, and assuring the possibility to audit over the\ninformation stored on the infrastructure."
  },
  {
    "keywords": [
      "OpenEHR",
      "e-Health",
      "Interoperability",
      "Health Information Systems",
      "JSON",
      "API",
      "Interoperabilidade",
      "Sistemas de Informação em Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "A novel system for managing OpenEHR structures",
    "autor": "Dias, Miguel André Rocha",
    "data": "2023-07-18",
    "abstract": "The term e-Health has been increasingly used in research projects in the health industry. This concept\nencompasses the development and application of software and hardware solutions for the collection,\nstorage, manipulation, and communication of data in an efficient way, with an objective of continuously\nimproving the provision of health care.\nThe development of Health Information Systems has increasingly lacked the establishment of high\nlevels of interoperability in its semantic, syntactic, and technical aspects. The development of systems\nthat allow promoting interoperability between HIS within the same institution or even between HIS from\ndifferent institutions is on the global agenda as a common concern for all countries.\nThe use of globally recognized standards has been increasingly common, ensuring that both the\ninformation structure and its meaning remain intact, regardless of the flow they follow. Thus arises the\nmotivation to develop a system that contributes to the continuous improvement of interoperability in health,\nthrough the use of the openEHR standard.\nThe following dissertation presents a novel way to handle clinical data by creating an artifact enabling\nthe conversion of openEHR standardized data into a JSON object. The web application showcases a new\nway for a user to check for openEHR data while the API can be utilized by developers to work with openEHR\ndata in a more accessible and supported manner with other programming tools. To carry out the work a\nthorough examination of web development tools to build both the backend and frontend of the app was\nessential, as well as coming up with the most accurate regex expressions that are able to extract data from\nopenEHR files. The research and engineering effort put through the project was successful in showcasing\nthis novel approach implementing yet another tool to help out healthcare professionals and biomedical\nsoftware engineers."
  },
  {
    "keywords": [
      "Distance learning",
      "E-learning",
      "Web",
      "Formare",
      "Web forms",
      "React",
      "API",
      "Ensino à distância",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Modernising an e-learning platform with a new user interface",
    "autor": "Félix, João Pedro Neves",
    "data": "2023-11-27",
    "abstract": "Distanced learning has a rich history and its evolution is closely connected to technology. It started with\nhand-written letters, later through radios, CD-ROMs, and now with the internet. With these advancements,\nthe definition of e-learning appears - learning through the use of electronic devices. This new learning\nmethodology comes with its own list of advantages (accessibility, lower cost, flexibility, etc.) and disad vantages (low motivation, social isolation, effectiveness, etc.). Larger institutions, such as universities and\norganisations, find e-learning very appealing mostly due to its long-term cost-efficiency.\nWeb accessibility and usability are important aspects of e-learning, rooted in the fact that the web is\none of the most used means for sharing content. Ensuring that educational materials are accessible to all\nlearners, regardless of their abilities is paramount to e-learning. Developers need to understand how can\nthey make websites more accessible and usable, considering aspects such as design and implementation.\nE-learning is now widely used, therefore a market appeared around it, paving the way for platforms such\nas Blackboard and Moodle. These platforms have carved their niche in the realm of digital education, each\nwith its own strengths and unique features. By looking at what they offer we can deepen our understanding\nof what e-learning tools have to offer and what functionalities users value most.\nFormare is an e-learning web platform created by Altice Labs. The company feels the platform is built\nusing obsolete technologies (web forms) and wants to modernize the front-end layer of the application.\nConnecting Formare’s functionalities to a new web interface (using React) is not a straightforward process,\nso an API that implements the back office and connects it to the front end was introduced.\nThis dissertation documents the implementation process of modernizing an e-learning platform with\na new user interface, going in-depth about the problems and solutions found throughout development."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Desenvolvimento ágil de uma aplicação web para filatelistas",
    "autor": "Guterres, Paulo Jorge Patrocínio",
    "data": "2016",
    "abstract": "Ao longo dos últimos anos, a área da filatelia tem vindo a perder interessados, nomeadamente\nnas gerações mais jovens. Essa questão, aliada ao facto do mercado online estar em\nplena afirmação, leva a que este desinteresse se torne evidente e cada vez mais acentuado,\nrefletindo-se num decréscimo do negócio das lojas físicas de filatelia.\nNeste contexto, o objetivo da dissertação era desenvolver uma aplicação Web para filatelistas\nque permitisse e facilitasse a comunicação entre colecionadores para comprar, vender\nou trocar artigos filatélicos. Pretendia-se que a aplicação fosse desenvolvida segundo uma\nmetodologia ágil. A adoção da metodologia ágil Scrum foi fundamentada numa revisão\nbibliográfica das metodologias existentes mais representativas. Embora fosse sugerida a\nutilização de uma metodologia ágil, também se analisaram algumas metodologias tradicionais.\nOs resultados alcançados evidenciam que a falta de experiência na utilização da metodologia\nScrum pode criar alguns problemas à equipa de Scrum, especialmente nas fases\niniciais do desenvolvimento. No mesmo sentido, uma vez que a metodologia Scrum se\nfoca na produção de incrementos de produtos funcionais e menos na documentação, leva a\nque a documentação final da aplicação seja mais difícil caso não seja feita incrementalmente\nao longo dos sprints. Por outro lado, pelo facto da metodologia Scrum funcionar em ciclos\nrepetidos de desenvolvimento completo, permite que seja obtido feedback regular e valioso\ndas várias partes interessadas, nomeadamente do product owner. Deste modo, é muito mais\ngarantido que o produto final agrada às partes interessadas.\nPode afirmar-se que a aplicação Web desenvolvida é um produto viável mínimo, pois\ndisponibiliza as funcionalidades suficientes para ser adotada por filatelistas e por outros\ninteressados na área de negócio da filatelia."
  },
  {
    "keywords": [
      "Ambiente Cloud/Edge",
      "Sincronização",
      "Replicação",
      "Federação de dados",
      "Análise de dados exploratória",
      "Cloud/Edge environment",
      "Synchronization",
      "Replication",
      "Data federation",
      "Exploratory data analysis",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Data Lakes em ambientes híbridos Cloud/Edge",
    "autor": "Costa, Daniel Vilar da",
    "data": "2022-04-05",
    "abstract": "A análise dos dados tem sido, tradicionalmente, realizada em servidores na nuvem, onde a capacidade de\narmazenamento e de processamento são quase ilimitadas. Em contrapartida, os dispositivos periféricos\ntêm severas limitações tanto de armazenamento como de processamento. No entanto, estes dispositivos\nencontram-se mais próximos do local onde os dados são gerados. Por causa disso, estes são, usualmente,\nutilizados para cargas de trabalho transacionais onde a confiabilidade e interatividade são fulcrais.\nDevido às limitações dos dispositivos periféricos, os dados são, geralmente, extraídos periodicamente\npara a nuvem onde são depois armazenados e processados. De modo a permitir a análise exploratória de\ndados heterogéneos, é comum utilizar uma infraestrutura Data Lake que permite gerir dados em formato\nbruto de múltiplas fontes. No entanto, transferir todos os dados coletados para a nuvem é inviável devido\nà limitada capacidade da rede que não tem conseguido acompanhar o crescimento do volume de dados\ncoletados.\nEsta dissertação ultrapassa estes desafios ao implementar um componente middleware capaz de\narmazenar os dados previamente transmitidos na nuvem e propaga partes da interrogação para a periferia.\nDeste modo, consegue-se reduzir o volume de dados transferido ao enviar, idealmente, apenas uma vez\nos dados necessários para responder aos pedidos. Além disso, esta solução equilibra o impacto na rede\ne o custo computacional na periferia de modo a minimizar o tempo de execução."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Selective reprogramming of WSNs: energetic study and functionality optimization",
    "autor": "Abdah, Hadeel Mohamad-Ali",
    "data": "2016",
    "abstract": "Wireless sensors networks consist of large numbers of small, battery-powered, self-organizing computing motes. Nowadays, these networks are considered ideal candidates for a wide range of applications such as environmental monitoring, military operations and other application fields where it is hard to maintain a continuous presence of human beings.\nOnline remote reprogramming is usually carried out to update the code running on nodes due to factors such as changes in the environment or application. Remote reprogramming might be applied to the whole network or just to a subset of nodes (selective reprogramming), either way it is crucial to provide reliability for such procedure. Therefore, most of the approaches oriented to remote reprogramming resort to flooding the whole network, leading to a major waste of energy in network nodes.\nWhen dealing with selective reprogramming, the waste of energy increases steeply even when just a small number of nodes need to get the update messages. These messages may be received and retransmitted from all nodes in the network resulting in a waste of resources.\nThis research identifies multiple scenarios for selective reprogramming and proposes a different energy-aware approach for each one trying to reduce energy consumption in the network by taking advantage of multiple and complementary solutions such as wise routing, clustering and the ability to manage nodes sleeping time instead of using the typical flooding approach.\nThese approaches were tested and compared with typical flooding and Deluge solutions. The results show a significant reduction of the power consumption, thus, making the selective remote reprogramming more energy-efficient."
  },
  {
    "keywords": [
      "Chatbot",
      "Artificial intelligence",
      "Machine learning",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Chatbot for digital marketing and customer support: an artificial intelligence approach",
    "autor": "Vaz, Humberto João Alves",
    "data": "2019",
    "abstract": "Human interaction with machines has never been so frequent as nowadays. In order to\nreduce the redundant workload of a human being that answers repeated and trivial questions\nregarding customer support on a digital marketing website, this work has the purpose\nof replacing this tedious job with an informatics tool, a dialogue tool.\nA dialogue tool like a Chatbot that could handle customer support to a digital marketing\nwebsite, provides the opportunity of placing human resources on ”non mechanical tasks”.\nGiven that Chatbots exchange messages directly with customers, they could collect required\nprotocol information in all the interactions. In spite of the possibility of needing human\nassistance, he will not need to ask these standard questions and will improve its efficiency.\nBy automating these required dialogues to answer questions about certain products, that\nwould otherwise be responded by a human, the organizations will have the opportunity to\nplace human resources in another sectors that are not so easily automated."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Plataforma para análise de tráfego e otimização dos recursos de uma infraestrutura de comunicação",
    "autor": "Pereira, Marco André Alves",
    "data": "2017",
    "abstract": "Uma infraestrutura de comunicação nem sempre é fácil de gerir e o nível de dificuldade\naumenta com a dimensão e o número de dispositivos presentes na infraestrutura de rede. A\nmonitorização de uma rede é um processo fundamental na medida em que previne/deteta\neventuais problemas e dispõe de diversas ferramentas auxiliadoras do trabalho de um administrador\nde redes de computadores.\nEste trabalho tem como objetivo primordial o desenvolvimento de uma plataforma de monitorização de infraestruturas de rede de modo a que seja viável observar os tipos de aplicações\nque estão a gerar tráfego na rede, analisar o respetivo impacto do tráfego gerado, bem como\napresentar detalhes sobre os dispositivos que estão a utilizar essas mesmas aplicações, entre\noutras diversas funcionalidades. Esta informação será exposta num Dashboard intuitivo de\nforma a que um utilizador, mesmo sem ter conhecimentos técnicos aprofundados na área das\nredes de computadores, seja capaz de interpretar com facilidade o estado da infraestrutura de\nrede. O Dashboard será desenvolvido de forma a possibilitar uma rápida perceção dos pontos\ncríticos que existem na rede (e.g. pontos de congestão) e aplicações e dispositivos que estão na\norigem dos mesmos. Assim, será possível detetar em tempo real possíveis anomalias e tomar\nmedidas que as contenham, tendo por base as aplicações e os dispositivos que as originaram.\nTambém serão emitidos alertas referentes a essas mesmas anomalias.\nPosteriormente, será estudado e desenvolvido um módulo de otimização da rede que integrará na plataforma desenvolvida, consistindo na determinação de alterações que aproveitem\nao máximo os recursos da rede e, consequentemente, aumentem o desempenho da infraestrutura,\nde acordo com o tráfego que nela circula. Estas alterações poderão ser implementadas\npela plataforma, nos casos em que haja essa possibilidade, ou assumir a forma de\nrecomendações enviadas ao administrador da rede (e.g. horários em que se devam realizar\ndeterminadas operações na rede, tais como backups entre outras possibilidades).\nA plataforma será desenvolvida, aplicada e testada tento em conta o contexto específico\nde uma intranet de uma empresa do sector têxtil. Esta empresa necessita deste tipo de\nplataforma de forma a melhorar o desempenho da sua infraestrutura de rede."
  },
  {
    "keywords": [
      "Escalabilidade",
      "Java",
      "Paralelo",
      "Perfil de execução",
      "Parallel",
      "Profiling",
      "Scalability",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Profiling tools for Java",
    "autor": "Gomes, Vítor Domingos Araújo",
    "data": "2022-05-06",
    "abstract": "Atualmente, Java é uma das linguagens de programação mais populares. Esta popularidade é parcialmente\ndevida à sua portabilidade que advém do facto do código Java ser compilado para bytecode que poderá ser\nexecutado por uma máquina virtual Java (JVM) compatível em qualquer sistema. A JVM pode depois interpretar diretamente ou compilar para código máquina a aplicação Java. No entanto, esta execução sobre uma máquina virtual cria alguns obstáculos à obtenção do perfil de execução de aplicações.\nPerfis de execução são valiosos para quem procura compreender o comportamento de uma aplicação pela\nrecolha de métricas sobre a sua execução. A obtenção de perfis corretos é importante, mas a sua obtenção e\nanálise pode ser desafiante, particularmente para aplicações paralelas.\nEsta dissertação sugere um fluxo de trabalho de otimização a aplicar na procura de aumentos na escalabilidade de aplicações Java paralelas. Este fluxo sugerido foi concebido para facilitar a descoberta dos problemas\nde desempenho que afetam uma dada aplicação paralela e sugerir ações a tomar para os investigar a fundo.\nO fluxo de trabalho utiliza a noção de possible speedups para quantificar o impacto de problemas de desempenho diferentes. A ideia de possible speedups passa por estimar o speedup que uma aplicação poderia\natingir se um problema de desempenho específico fosse completamente removido. Esta estimativa é calculada\nutilizando as métricas recolhidas durante uma análise ao perfil de uma aplicação paralela e de uma versão\nsequencial da mesma aplicação.\nO conjunto de problemas de desempenho considerados incluem o desequilíbrio da carga de trabalho, sobre carga de paralelismo devido ao aumento no número de instruções executadas, sobrecarga de sincronização,\ngargalos de desempenho no acesso à memória e a fração de trabalho sequencial. Estes problemas foram considerados as causas mais comuns de limitações à escalabilidade de aplicações paralelas. Para investigar mais a\nfundo o efeito destes problemas numa aplicação paralela, são sugeridos alguns modos de visualização do perfil\nde execução de uma aplicação dependendo do problema que mais limita a sua escalabilidade. As visualizações\nsugeridas consistem maioritariamente de diferentes tipos de flame graphs do perfil de uma aplicação.\nDuas ferramentas foram desenvolvidas para ajudar a aplicar este fluxo de trabalho na otimização de aplicações Java paralelas. Uma destas ferramentas utiliza o async-profiler para recolher perfis de execução de\numa dada aplicação Java. A outra ferramenta utiliza os perfis recolhidos pela primeira ferramenta para estimar\npossible speedups e produzir todas as visualizações mencionadas no fluxo de trabalho sugerido.\nPor fim, o fluxo de trabalho foi validado com alguns casos de estudo. O caso de estudo principal consistiu\nna otimização iterativa de um algoritmo K-means, partindo de uma implementação sequencial e resultando no\naumento gradual da escalabilidade da aplicação. Casos de estudo adicionais também foram apresentados para\nilustrar possibilidades não abordadas no caso de estudo principal."
  },
  {
    "keywords": [
      "Medical image analysis",
      "Deep learning",
      "Artificial intelligence",
      "Breast cancer diagnosis",
      "Análise de imagem médica",
      "Inteligência artificial",
      "Diagnóstico de cancro da mama",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Intelligent medical image analysis: a Deep Learning approach to breast cancer diagnosis",
    "autor": "Fontes, João Pedro Pereira",
    "data": "2018",
    "abstract": "Once medical images were scanned and uploaded to a computer, researchers began\nto create automated medical imaging systems. From the 1970’s to the 1990’s,\nmedical imaging was performed with sequential application of low-level pixel processing\nand mathematical modeling to solve specific tasks such as organ segmentation.\nAt the end of the 1990’s, supervised techniques began to appear, where\ndata extracted from the images were used to train models and classification systems.\nOne example is the use of automated classifiers to build support systems\nfor cancer detection and diagnosis. This pattern recognition and / or Machine\nLearning approach is still very popular and represented a shift from systems that\nwere completely human-engineered to computer-trained systems with the use of\nspecific (manually drawn) features and automatically extracted from the training\ndata (example). The following step would be enabling the algorithms to directly\nlearn characteristics of the pixels of the images. This is the basic concept of Deep\nLearning algorithms: multi-layered models that transform input data (images) into\noutputs (e.g. the presence or absence of pathological lesions or cancer).\nThis study intends to present ways of using Deep Learning algorithms in the analysis\nof medical images, like the particular case of pathological lesions representative\nof breast cancer phenotypes."
  },
  {
    "keywords": [
      "Curriculum vitae (CV)",
      "CV screening",
      "Machine learning (ML)",
      "Natural language processing (NLP)",
      "Aprendizagem automática",
      "Curriculum vitae (CV)",
      "Processamento de linguagem natural (PLN)",
      "Triagem de currículos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automation of companies’ recruitment process: development of an algorithm capable of ranking CVs according to job offers",
    "autor": "Rocha, Beatriz de Freitas",
    "data": "2022",
    "abstract": "This document presents a Thesis and describes the underlying work which was developed\nalong the second year of the Master Degree in Informatics Engineering offered by Departamento\nde Informática of Universidade do Minho and accomplished at Syone SBS Software –\nTecnologia e Serviços de Informática, S.A..\nIn the past few years, some attempts to automatically screening CVs with resource to\nNatural Language Processing have been made not only to save recruiters’ time, but also\nto spare them the most tedious task of the recruitment process and, consequently, smooth\ntheir job. However, the majority is still very primitive, misclassifies a lot of CVs and needs a\ndeeper study.\nTherefore, the aim of this Master’s Project is precisely to develop an algorithm that is\ncapable of automatically ranking candidates’ CVs according to their similarity regarding the\njob offer they applied for.\nThus, a general architecture was proposed where CVs and job offers are preprocessed, in\norder to obtain the respective texts proper to be further processed. That said, two different\napproaches were followed, in order to find the similarity between the documents in question.\nTo do so, the first approach resorted to several Machine Learning algorithms and similarity\nmeasures, while the second approach structured the initial documents to compare their\nrespective information.\nAfter that, tests were conducted to evaluate both approaches and enable the comparison\nbetween them. Finally, the conclusions were drawn and also reported in this dissertation."
  },
  {
    "keywords": [
      "Data warehousing",
      "Sistemas de ETL",
      "Especificação e verificação de sistemas de ETL",
      "Alloy",
      "ETL systems",
      "Specification and Validation of ETL",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Especificação e validação de processos ETL em Alloy",
    "autor": "Capelo, Mariana Almeida Brandão",
    "data": "2018",
    "abstract": "O desenvolvimento de processos ETL é uma tarefa dispendiosa e complexa. Não admira, pois, o\ncuidado que os seus implementadores têm, em particular, durante as suas fases de planeamento e\nanálise. Muito trabalho tem sido desenvolvido em prol do estabelecimento de novos e melhores\nmétodos e técnicas de modelação conceptual e lógica destes processos. Todavia, ainda ocorrem\ninúmeros problemas durante as primeiras fases de execução dos processos de ETL, muitos deles\nprovocados por erros de análise, de desenvolvimento, ou de simples esquecimento. Como tal, é vital\nque antes da entrada destes processos em produção, eles sejam submetidos a algum tipo de\nmecanismo que permita validá-los e comprovar a sua correção, relativamente àquilo que se espera\nque eles realizem. A utilização da linguagem Alloy na especificação e validação de processos ETL\noferece esse tipo de validação. Neste trabalho de dissertação, suportado por um caso de estudo\nespecífico, Alloy é estudada, utilizada e avaliada quanto à sua aplicação na especificação formal e\nvalidação de processos ETL."
  },
  {
    "keywords": [
      "Aprendizagem automática",
      "Sensor da condição do piso",
      "Dispositivo edge",
      "Fusão sensorial",
      "Condições meteorológicas",
      "Machine learning",
      "Road condition sensor",
      "Edge device",
      "Sensor fusion",
      "Weather conditions",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Classificador da condição do piso para um sistema de condução autónoma",
    "autor": "Peixoto, Flávio Joel Martins",
    "data": "2020-12-16",
    "abstract": "O presente trabalho de dissertação surgiu no contexto do projeto Sensible Car, uma parceria entre a Bosch e a Universidade do Minho (UM), onde se está a desenvolver um sensor da condição do piso (RCS) em que circula um veículo automóvel. Com esta dissertação pretendia-se verificar se a aplicação de dados meteorológicos aliados à informação ótica apresenta vantagens na classificação da condição do piso, para além da utilização da informação ótica. Também se pretendia demonstrar se, com recursos computacionais limitados, é possível implementar um classificador de piso com a fiabilidade e a capacidade de resposta exigidas. Para atingir os objetivos propostos aplicou-se aprendizagem automática com supervisão e utilizaram-se dados de treino que combinam (i) os rácios da intensidade da luz recebida (após reflexão no piso) sobre a intensidade da luz emitida pelos dispositivos óticos do RCS, para quatro comprimentos de onda distintos, com (ii) dados meteorológicos. Os dados óticos são essenciais para a circulação com segurança em veículos com condução autónoma. Isto porque a deteção da condição do piso onde se circula permite ao veículo tomar melhores decisões em tempo real. Para além de se comparar o desempenho de cada modelo treinado só com dados óticos, com o desempenho do mesmo modelo treinado com dados resultantes da fusão entre dados óticos e meteorológicos, testaram-se diversos modelos, para selecionar o que mais se adequa à classificação da condição do piso. Numa fase inicial, selecionaram-se os modelos que apresentaram melhor desempenho, i.e. melhor, precisão e recall, na classificação de amostras dos vários tipos de piso. Os modelos aqui selecionados foram SVM Gaussiano (0.96 de precisão e 0.93 de recall), Regressão Logística (0.91 e 0.88), Árvore de Decisão (0.91 e 0.85) e XGBoost (0.94 e 0.94). Posteriormente, implementaram-se e testaram-se os melhores modelos no dispositivo Nvidia Jetson Nano. Nesta fase, além de se confirmar as percentagens de acerto dos modelos a classificar a condição do piso, verificou-se se eram capazes de classificar as amostras ao ritmo a que o sensor de condição de piso gera os dados óticos. Os resultados obtidos mostraram que os modelos desenvolvidos são capazes de acompanhar o ritmo de geração de dados do sensor da condição do piso, em que o modelo SVM faz 1040 classificações por segundo, a Regressão Logística faz 2080, a Árvore de Decisão efetua 1950 e o XGBoost faz 223. O modelo selecionado no fim foi o SVM Gaussiano, pois apesar de não ser o modelo com maior número de classificações por segundo, é o que possui o melhor desempenho geral na classificação da condição do piso."
  },
  {
    "keywords": [
      "Quorum sensing",
      "Formação de biofilms",
      "TRN",
      "PPI",
      "Integração",
      "Cross-talking",
      "Biofilm formation",
      "Integration",
      "681.3:57",
      "57:681.3"
    ],
    "titulo": "Mining quorum sensing in pathogenic P. aeruginosa and C. albicans",
    "autor": "Santos, Nadine Castelhano",
    "data": "2013",
    "abstract": "O estudo do fenómeno de cross-talking entre P. aeruginosa e C. albicans tem sido focado\nessencialmente nos processos de quorum sensing e formação de biofilmes, identificando-se os\ngenes e proteínas envolvidas. Contudo, mesmo já existindo um grande conhecimento dos genes\ne proteínas envolvidas, ainda existe uma lacuna na integração dos mesmos nas redes biológicas\ndos respectivos microorganismos. O número e qualidade das redes biológicas existentes\n(Transcriptional Regulatory Network - TRN and Protein-protein Interactions - PPI) para os\nfenómenos chave tais como, quorum sensing, formação de biofilmes, resistência a antibióticos e\npatogenecidade, não evidenciam a importância de alguns destes genes no fenómeno de crosstalking.\nEsta tese apresenta-se como a primeira tentativa de colocar em evidência os genes e\nproteínas envolvidos em cross-talking, associando-os aos parceiros de interacção nas respectivas\nredes biológicas de cada microorganismo. Primeiramente, utilizou-se um processo de integração\nde redes para os dois microrganismos, levando ao aumento do conhecimento geral sobre os\nprocessos de patogenecidade dos dois microorganismos, e por fim os genes envolvidos em\ncross-talking, identificados na literatura, foram evidenciados nestas redes integradas. Com esta\ntese pretende-se dar algumas pistas sobre como é que estes genes de cross-talking estão\nenvolvidos em importantes processos biológicos e também de algum modo apontar novos\npotenciais drug targets."
  },
  {
    "keywords": [
      "Ontologias",
      "Extração de ontologias",
      "Processamento de linguagem natural",
      "Ontology learning",
      "Padrões léxico-sintáticos",
      "Dependency parsing",
      "Textos não estruturados",
      "Base de dados orientada a grafos",
      "Neo4J",
      "Spacy",
      "Keywords ontology",
      "Ontology extraction",
      "Natural language processing",
      "Lexico-syntactic patterns",
      "Dependency parsing",
      "Unstructured texts",
      "Graph database",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Uma ontologia para a descrição de conteúdos de testamentos",
    "autor": "Yusupov, Shahzod",
    "data": "2022-07-25",
    "abstract": "Cada vez é mais notória a importância que as ontologias têm vindo a ganhar no que toca\nao desenvolvimento de sistemas baseados em conhecimento. Para além de ainda haver\nalguma dificuldade em compreender o seu modo de implementação, a sua construção\nmanual é muito dispendiosa tanto a nível de recursos como de tempo e, após a construção,\né necessário manter a ontologia atualizada consoante os novos requisitos que poderão\nsurgir. Nesta dissertação apresentamos, numa primeira parte, a definição de ontologia, a sua\nutilidade e algumas das metodologias que podem ser utilizadas na sua construção manual,\nanalisando a sua evolução ao longo do tempo. Após esta introdução, apresentamos algumas\ntécnicas de construção (semi-)automática de ontologias a partir de textos e abordamos\no conceito de ontology learning, bem como tudo aquilo que este processo envolve. Além\ndisso, enunciaremos alguns dos sistemas que fazem uso dessas mesmas técnicas. Por fim,\napresentamos o trabalho desenvolvido na extração de uma ontologia a partir de um conjunto\nde textos relativos a testamentos antigos, que foram editados por Barros e Alves (2019) em\nO Livro dos Testamentos – Picote, 1780-1803, detalhando o processo de extração realizado\npara a ontologia pretendida, bem como apresentando as técnicas e ferramentas utlizadas.\nNeste processo, queremos relevar a importância da utilização de padrões léxico-sintáticos e\no dependency parsing, que contribuíram de forma efetiva para a obtenção dos resultados que\nalcançámos."
  },
  {
    "keywords": [
      "Clouds",
      "Computer graphics",
      "Cloud rendering",
      "Atmosphere",
      "Volumetric",
      "Cloud modeling",
      "Nuvens",
      "Computação gráfica",
      "Renderização de nuvens",
      "Atmosfera",
      "Volumes",
      "Modelação de nuvens",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Cloud modeling and rendering",
    "autor": "Costa, Mariana Lino Lopes",
    "data": "2023-07-13",
    "abstract": "Clouds play an important role in enhancing the realism of ambient and outdoor scenes in Computer\nGraphics (CG). However, rendering clouds and certain atmospheric elements remains a major challenge\nin this field. Their representation involves processes such as modeling, photorealistic rendering, and\nanimation.\nThe main objective of this project was to explore techniques for modeling and rendering clouds. To\nthis end, code was developed in GLSL shader language to implement clouds, using the NAU3D application\nfor code compilation.\nDifferent types of cloud modeling were studied, based on textures, geometric shapes, noise, real\nand/or satellite images. Volumetric clouds were implemented, following the approach of Schneider and\nVos [2015], Hillaire [2016a], and Högfeldt [2016]. A volume was used in which the voxels were filled with\ninformation stored in a Weather texture.\nCloud rendering was performed using the Ray marching algorithm. For cloud lighting, a study of differ ent light events was conducted independently of the participating media. Lighting in this implementation\nwas adapted from formulas presented in Jarosz [2008] and Scratchapixel [2022b], taking into account\nthat the participating media is the cloud. Various light phenomena were studied, including in-scattering.\nTo address this effect, several phase functions were tested.\nTests were also conducted to evaluate computational costs, such as the time spent on GPU rendering\nof the implemented cloud shader and the required FPS for rendering the atmosphere with clouds. These\ntests were performed to understand the balance between computational cost and visual results, allowing\nfor customization of parameters based on this balance. The visual results obtained did not reach the level\nof state-of-the-art realistic clouds in CG, but there is room for improvement in the implemented clouds."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Efficient processing of ATLAS events analysis in homogeneous and heterogeneous platforms with accelerator devices",
    "autor": "Pereira, André Martins",
    "data": "2013-09-16",
    "abstract": "Most event data analysis tasks in the ATLAS project require both intensive data access and processing, where some tasks are typically I/O bound while others are compute bound. This dissertation work mainly focus improving the code efficiency of the compute bound stages of the ATLAS detector data analysis, complementing a parallel dissertation work that addresses the I/O bound issues.\nThe main goal of the work was to design, implement, validate and evaluate an improved and more robust data analysis task, originally developed by the LIP research group at the University of Minho. This involved tuning the performance of both Top Quark and Higgs Boson reconstruction of events, within the ATLAS framework, to run on homogeneous systems with multiple CPUs and on heterogeneous computing platforms. The latter are based on multicore CPU devices coupled to PCI-E boards with many-core devices, such as the Intel Xeon Phi or the NVidia Fermi GPU devices.\nOnce the critical areas of the event analysis were identified and restructured, two parallelization approaches for homogeneous systems and two for heterogeneous systems were developed and evaluated to identify their limitations and the restrictions imposed by the LipMiniAnalysis library, an integral part of every application developed at LIP. To efficiently use multiple CPU resources, an application scheduler was also developed to extract parallelism from simultaneously execution of both sequential and parallel applications when processing large sets of input data files.\nA key achieved outcome of this work is a set of guidelines for LIP researchers to efficiently use the available computing resources in current and future complex parallel environments, taking advantage of the acquired expertise during this dissertation work. Further improvements on LIP libraries can be achieved by developing a tool to automatically extract parallelism of LIP applications, complemented by the application scheduler and additional suggested approaches."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "A web-based social environment for Alloy",
    "autor": "Pereira, José Manuel Costa",
    "data": "2017",
    "abstract": "Alloy is a declarative specification language which describes rules and complex structural\nbehaviors. Alloy Analyzer is used to analyze this specifications, this tool generates concrete\ninstances from the invariants specified in a model, it simulates sequences of defined\noperations and verifies whether properties introduced are valid or not. Currently, the tool\nis available as a runnable .jar and it contains a trivial GUI to interact with it. Being such,\nit requires JAVA installed. It’s in the best interest of the community to achieve and easier\naccess to this tool through a web platform that shall support it in real time and also allow\nsharing models developed in it by users. Formal methods of software development are\ngrowing and they would also benefit from the constructive feedback obtained through this\nplatform regarding the Alloy language/tool."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "RODA-in: a generic tool for the mass creation of submission information packages",
    "autor": "Pereira, André Diogo Ribeiro Assunção",
    "data": "2016-11-25",
    "abstract": "Digital preservation is the sum of activities necessary to ensure the long-term access to\ndigital information. The OAIS standard(ISO, 2012a) was developed in order to ease the\ncommunication between the various entities involved in the preservation of digital objects\nand regulate the long-lasting storage of digital information.\nThe preservation process begins when the producer creates Submission Information Packages\n(SIP) and uploads them to the archive’s repository. To create these packages, the\nproducer must choose which files to archive and provide extra information (metadata) to\ndescribe and to allow finding the information. As the production of digital content increases\nexponentially, the creation of SIP by current methods can be too onerous and even\nunfeasible.\nThis work focuses on creating a semi-automatic way of producing SIPs by employing\na simple and well-defined workflow. Using the file system as the source of content, the\nproducer defines aggregation and metadata association rules and specify how the SIPs are\ncreated. The application that was developed to support this work, RODA-in, was designed\nto be able to create thousands of SIPs with gigabytes of data in an easy to use way. Additionally,\nit has multiple features that ease the work of the producer, such as metadata\ntemplating and mass edition."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Optimization in code generation to reduce energy consumption",
    "autor": "Branco, David Luis Moniz",
    "data": "2018",
    "abstract": "In recent years we have witnessed a great technological advance accompanied by an equally\nimpressive increase in energy consumption, causing problems of both financial and environmental\norder. In order to counteract this tendency, Green Computing emerges with a\nnumber of measures for a more efficient use of computing resources without a great loss of\nperformance.\nThis essay is a study of several elements of Information Technology analyzed from the\npoint of view of energy efficiency. With special emphasis on microprocessors, modern\ncompiler design, development tools and optimization of code generation, a wide range\nof information is gathered on very relevant subjects through perspectives still not very\nconsidered by the community in general.\nAlso presented are two experimental studies that analyze the optimization of generated\ncode for a set of benchmark programs in several programming languages with the aim of\napraise the otimization impact on improving their energy consumption efficiency. A software\nmeasurement framework was also developed that, together with the methodologies\npresented in both studies, allows obtaining very precise and pertinent results for analysis.\nFinally, a ranking was produced for 18 development tools, considering the execution time\nand energy consumption of the executables generated through their compilation profiles.\nThis study also intends to contribute to an energy efficient technological advancement.\nAll the work developed here may also serve as motivation so that these and other aspects\nof Information Technology may be seen through a greener perspective."
  },
  {
    "keywords": [
      "681.3.062",
      "681.3-7"
    ],
    "titulo": "Linguagens de domínio específico para software criptográfico",
    "autor": "Miranda, Luís Paulo Ferreira",
    "data": "2012-04-26",
    "abstract": "A criptografia desempenha um papel importante na nossa sociedade, visto que é utilizada em sistemas de computação designados como críticos, que têm que funcionar mesmo na presença de erros. Áreas como os sistemas bancários ou de saúde usam software e hardware, que têm que funcionar em todas as circunstâncias. O principal objetivo para o uso de criptografia nesses sistemas, é o de garantir a segurança da informação, que em muitos casos é sensível.\n\nNos últimos anos, foram surgindo linguagens de programação que se focam num domínio específico, chamadas de linguagens de domínio específico (DSLs). No domínio da criptografia, apareceram as linguagens Cryptol e CAO, ambas ambicionando aumentar a produtividade dos programadores, mas também aumentar a comunicação entre estes e os especialistas do domínio.\n\nO Cryptol é uma linguagem funcional e tem um conjunto de ferramentas associadas, compostas por um conjunto de ferramentas de verificação e de compilação para linguagens como C ou VHDL, que é uma linguagem descritiva de hardware. O CAO é uma linguagem imperativa, com uma sintaxe idêntica à do C, e tem também um conjunto de ferramentas associado, que permite a introdução de operações de alto nível na linguagem, por exemplo.\n\nNeste trabalho, essas duas linguagens foram abordadas, em particular as suas funcionalidades, e como podem ser usadas para implementar um algoritmo através da sua especificação. Além disso, foi desenvolvida uma ferramenta de compilação que pretende transformar código fonte CAO em código Cryptol, de forma a compilá-lo para VHDL posteriormente.\n\nPor fim, um caso de estudo que foca curvas elípticas para criptografia, foi utilizado para comparar as duas DSLs e também para testar a ferramenta desenvolvida."
  },
  {
    "keywords": [
      "Estações de tratamento de águas residuais",
      "Lamas ativadas",
      "Analise de imagem",
      "Wastewater treatment plants",
      "Activated sludge",
      "Image analysis",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Plataforma online para caracterizar comunidades microbianas de estações de tratamento de água e efluentes",
    "autor": "Oliveira, Daniel Filipe Braga de",
    "data": "2017",
    "abstract": "As Estações de Tratamento de Águas Residuais que utilizam o sistema de lamas ativadas\ncaraterizam-se por serem um complexo ecossistema, responsável pelo tratamento de águas\nresiduais, através de processos biológicos que metabolizam substâncias orgânicas e inorgânicas em produtos mais toleráveis a nível ambiental. A comunidade de microrganismos\ndesse sistema requer uma apertada monitorização e controlo, recorrendo a inspecções\nregulares por microscopia para evitar perdas económicas e danos ambientais provocados\npor populações bacterianas que não estejam balanceadas.\nA inovação tecnológica das últimas décadas permitiu que a monitorização que era realizada\natravés de um operador humano, passasse a ser desempenhada por computadores,\natravés por exemplo de programas como o ”Flocos e Filamentos” que consegue analisar\nmorfologicamente as populações bacterianas com o tratamento de uma ou mais imagens\ndigitais grayscale 8 bits, adquiridas de reatores laboratoriais ou Estações de Tratamento de\nÁguas Residuais.\nNesta dissertação de mestrado, foi construída uma plataforma online, que integrou o\nprograma ”Flocos e Filamentos” otimizado e compilado. A plataforma permite facilitar a\nmonitorização deste processo biológico, sendo acessível através de dispositivos moveis e computadores."
  },
  {
    "keywords": [
      "681.3:620.9",
      "620.9:681.3"
    ],
    "titulo": "Intelligent systems for energetic sustainability",
    "autor": "Cuevas, David José Farinha",
    "data": "2012",
    "abstract": "Hoje em dia, a sociedade ainda possui uma grande tendência para um\nconsumismo desleixado e irracional, especialmente nas nossas casas, levando\nassim a consumos desnecessários de energia, e consequentemente, ao dano do\nnossos sistema ambiente.\nÉ pretendido, com este trabalho, desenvolver um sistema inteligente de\ngestão de consumos energéticos e baseado na aprendizagem das acções das\npessoas que interagem com um determinado espaço. Este sistema irá actuar\nsobre uma rede de sensores, de onde é extraída toda a informação necessária\npara a aprendizaegm e gestão dos recursos. Com isto, é possível saber, de\numa forma aproximada, que ações é que os utilizadores tomam, como por\nexemplo, a que horas do dia é que um dado utilizador entra num dado espaço.\nAtravés dos sensores ambientais, como por exemplo, um sensor de\nluminosidade, poderemos também determinar quais as condições necessárias\npara saber também quando é que um dado utilizador vai desligar ou ligar um\ninterruptor ou até ligar um ar condicionado. Desta maneira, é possível conseguir\nadaptar o espaço que está a ser monitorizado, de modo a ser possível\nobter o melhor consumo energético e com isso tornar esse mesmo espaço um\nambiente sustentável.\nEsta solução deverá também informar o utilizador sobre que electrodomésticos\n(ou dispositivos que estão a ser monitorizados) são os mais consumidores\n(também deverá ter a percepção de qual é o que gasta menos) de forma a\neste poder regular o seu consumo numa fase inicial e posteriormente permitir\nao sistema aprender."
  },
  {
    "keywords": [
      "Predictive analysis",
      "SAP HCP",
      "R",
      "Análise preditiva"
    ],
    "titulo": "Solução de análise preditiva para gestão de tarefas e avarias",
    "autor": "Silva, Flávia Daniela da",
    "data": "2020-02-14",
    "abstract": "This dissertation reports on a Masters’ Project in the field of Computing Engineering.\nThe objective of this project is based on the forecast of possible shortages of goods and\npossible damage that the vending machines can have, optimizing the profitability of the\ndevices, as well as the management of the tasks of the employees. For the execution of this\nproject we intend to design and develop a support system of predictive analysis that allows\nto expand the functionalities provided by the existing application. The existing application\nmeets all the requirements initially presented by the company’s customer, but does not take\nadvantage of all the capabilities that the SAP Hana Cloud Platform (SAP HCP) has available.\nIt is possible and intended in this phase to add new functionalities in order to monetize the\ndevices including the predictive analytical capability."
  },
  {
    "keywords": [
      "Formal methods",
      "Runtime monitoring",
      "Temporal logics",
      "SALT",
      "SpeAR",
      "Métodos formais",
      "Lógicas temporais",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Early validation of system requirements and design",
    "autor": "Miranda, Marcelo",
    "data": "2020-01-09",
    "abstract": "Modern society is relying more and more on electronic devices, most of which are em bedded systems and are sometimes responsible for performing safety-critical tasks. As\nthe complexity of such systems increases due to concurrency concerns and real-time con straints, their design is more prone to errors which can lead to catastrophic outcomes.\nIn order to reduce the risk of such outcomes, a model-based methodology is commonly\nused. The model describes the behaviour of the system and is subject to verification tech niques such as simulation and model checking in order to verify it behaves according to\nthe requirements. Common problems that arise with this methodology is the ambiguity of\nrequirements written in natural language and the translation of a requirement to a property\nthat can be verified along with the model.\nThis thesis proposes a tool that, after the translation of the requirements to temporal\nformalism, allows the automatic generation of monitors in order to verify the model. Our\ntarget platform is Simulink, which is widely used in this domain to model, simulate and\nanalyze dynamic systems."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Cloud-based analytics for monitoring and classification of arrhythmias",
    "autor": "Brito, Cláudia Vanessa Martins de",
    "data": "2018",
    "abstract": "Real-time monitoring has become one of the most important and clinically relevant tasks\nin medical settings, yet one of the most repetitive and tiresome tasks is the analysis of\n24-hour ECG records. One way to automate this long task is to convert this process into\na real-time process with the automatic classification of the heart rate and with this, the\nclassification of arrhythmias.\nThus, this master thesis focuses on the study of Deep Learning models for the classification\nof arrhythmias and data processing tools for the streaming and processing of data in\nreal time. Consequently, this master’s thesis comprises several phases. In the first place,\na more theoretical part is presented which is the ground truth of the use of the tools later\nused for the development of the system. The development of the system includes a\nmore practical part of data streaming composed by an IoT middleware, Apache Kafka as\nan intermediate agent between this middleware and Apache Spark, and ElasticSearch for\nreal-time data storage for visualization. On the other hand, in the main scope of this thesis,\ntwo models of Deep Learning were created, one for the classification of arrhythmias and\nanother one for their forecast.\nThe results obtained are promising with the arrhythmia classification yielding 98% accuracy\nin the classification of each beat in one of the four classes used. When the model\nwas tested in data obtained directly from the Hospital of Braga, it was not possible to obtain\nsuch good results, however, the model after new training was able to obtain accuracy\nvalues of 81% for the testing dataset. This deep learning model was also tested with the\nintegration of Apache Spark, in order to create data parallelism and increase the speed of\nthe deep learning process, which tends to be very time consuming, without neglecting its\nperformance.\nThe development of the model for the prediction of arrhythmias was done based on Long-\nShort Term Memory layers, in order to create a neural network with memory, the results\nobtained in the records with 30 minutes were not high. Despite the less good results in the\nfirst dataset, when the model was tested in the 24-hour records, the results obtained were\nquite high which demonstrated that the model can predict if it is based on a longer record.\nNevertheless, these results were obtained individually because the Electrocardiograms can\nbe an object of human identification.\nBased on the results obtained it was possible to conclude that more tests should be done\nincreasing the spectrum of arrhythmias to be classified so that this process becomes fully automatic, without neglecting the precision of the results since human lives may depend\non it."
  },
  {
    "keywords": [
      "681.3",
      "659.1"
    ],
    "titulo": "Techniques for Place Aware Advertising",
    "autor": "Mota, Ricardo Agostinho Miranda",
    "data": "2013-03-14",
    "abstract": "Nowadays, the high percentage of mobile devices equipped with GPS and Bluetooth, as well as the growing number of public Wi-Fi networks, makes the public environment surrounding us immensely rich in information. The advertising industry sees this as an opportunity to transform the way advertising is done in public by adopting increasingly pervasive and intelligent digital signage screen systems.\nThe objective of this work is to develop an ad serving solution for a digital signage network that explores user profile information, as well as, audience screen activities to deliver targeted ads. The architecture of an ad server was conceptualized and developed with its own recommender system, offering a solution covering all different aspects of the ad serving process, including a web platform for campaign managing."
  },
  {
    "keywords": [
      "37.018.43",
      "681.324",
      "371.26",
      "616.891.6"
    ],
    "titulo": "Avaliação como momento de aprendizagem : análise de Stress num ambiente de e-Learning",
    "autor": "Gonçalves, Sérgio Manuel de Carvalho",
    "data": "2013",
    "abstract": "A avaliação é um momento determinante na elaboração de estratégias de sucesso da aprendizagem. Em ambientes presenciais o educador pode observar o comportamento dos seus alunos e determinar caminhos que facilitem a avaliação e não induzam o stress e as consequências negativas no resultado da aprendizagem.\nNos ambientes de aprendizagem de e-Learning torna-se impossível o contacto direto e, como tal, terão de existir formas facilitadoras de detetar e prevenir as situações de stress nos momentos de avaliação.\nUrge portanto analisar o stress e determinar estratégias de resolução dos problemas que são causados por ele. Neste trabalho pretende-se desenvolver um módulo de análise de stress em momentos de avaliação em contextos de aprendizagem em linha que possa indicar ao educador os momentos mais propícios para intervir assim como os conteúdos que causam maiores dificuldades. Desta forma o educador poderá intervir de forma mais eficiente junto dos alunos que mais precisem."
  },
  {
    "keywords": [
      "Deduplication",
      "Storage",
      "Inline",
      "Offline",
      "Hybrid",
      "Deduplicação",
      "Armazenamento",
      "Híbrido"
    ],
    "titulo": "HIODS: hybrid inline and offline deduplication system",
    "autor": "Pedrosa, Carlos Pinto",
    "data": "2021-02-22",
    "abstract": "Deduplication is a technique that allows finding and removing duplicate data at storage\nsystems. With the current exponential growth of digital information, this mechanism is\nbecoming more and more desirable for reducing the infrastructural costs of persisting such\ndata. Therefore, deduplication is now being widely applied to several storage appliances\nserving applications with different requirements (e.g., archival, backup, primary storage).\nHowever, deduplication requires additional processing logic for each storage request in\norder to detect and eliminate duplicate content. Traditionally, this processing is done in\nthe I/O critical path (inline), thus introducing a performance penalty on the throughput\nand latency of requests being served by the storage appliance. An alternative solution is to\ndo this process as a background task, thus outside of the I/O critical path (offline), at the\ncost of requiring additional storage space as duplicate content is not found and eliminated\nimmediately. However, the choice of what type of strategy to use is typically done manually\nand does not take into consideration changes in the applications' workloads.\nThis dissertation proposes HIODS, a hybrid deduplication solution capable of automati cally changing between inline and offline deduplication according to the requirements (e.g.,\ndesired storage I/O throughput goal) of applications and their dynamic workloads. The\ngoal is to choose the best strategy that fulfills the targeted I/O performance objectives while\noptimizing deduplication space savings.\nFinally, a prototype of HIODS is implemented and evaluated extensively with different\nstorage workloads. Results show that HIODS is able to change its deduplication mode dy namically, according to the storage workload being served, while balancing I/O performance\nand space savings requirements efficiently."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Java stream optimization through program fusion",
    "autor": "Ribeiro, Francisco José Torres",
    "data": "2018",
    "abstract": "Combining different programs or code fragments is a natural way to build larger programs.\nThis allows programmers to better separate a complex problem into simple parts.\nFurthermore, by writing programs in a modular way, we increase code reusability.\nHowever, these simple parts need to be connected somehow. These connections are done\nvia intermediate structures that communicate results between the different components,\nharming performance because of the overhead introduced by the allocation and deallocation\nof multiple structures.\nFusion, a very commonly used technique in functional programming, aims to remove the\ncreation of these unnecessary structures, as they don’t take part in the final result.\nWith the introduction of streams and lambda expressions, Java made its way into a more\nfunctional programming style. Yet, these mechanisms lack optimization and the adaptation\nof fusion techniques used by some compilers for functional languages could benefit the\nperformance of Java streams.\nIn this thesis, we study how functional fusion can be adapted to Java Streams."
  },
  {
    "keywords": [
      "Sistema de recomendações",
      "Comércio virtual",
      "Ciência de Dados",
      "Inteligência de negócio",
      "Aprendizagem automática",
      "Tomada de decisões baseadas em dados",
      "Recommender system",
      "E-commerce",
      "Data science",
      "Business intelligence",
      "Machine learning",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Building a hybrid recommender engine for e-commerce",
    "autor": "Peixoto, Vítor Emanuel Carvalho",
    "data": "2021-04-06",
    "abstract": "A história pode ser resumida em um punhado de eventos que influenciaram a evolução humana: a nossa capacidade de controlar o fogo, a invenção da roda ou a implementação da linha de montagem. Todas estas descobertas tiveram um enorme impacto no futuro da nossa raça e do próprio mundo. \nAgora encontramo-nos novamente noutra revolução: a revolução dos dados. Facilmente impercetível, esta nova perspetiva está a mudar de todas as formas possíveis como interagimos com a Internet e, pela primeira vez na história, como a internet interage connosco. \nEste novo tipo de interações é definido por conexões entre utilizadores e bens consumíveis (produtos, artigos, filmes, etc.). Através destas conexões, conhecimento pode ser encontrado. Esta é a definição de mineração de dados. Com o aumento da oferta que as plataformas de comércio virtual oferecem hoje, é necessária uma ferramenta que auxilie a conexão entre clientes e produtos. Os sistemas de recomendação surgem como o fator dominante na criação de novas conexões entre oferta e procura. \nEsta dissertação segue a investigação e implementação de um motor de recomendações adaptável a várias plataformas de comércio virtual de diversas áreas de negócio. Consequentemente, trata-se de um sistema escalável, genérico e customizável, com o objetivo de aumentar a interação dos clientes, através de uma experiência personalizada com múltiplos tipos de recomendações espalhados pelas plataformas. \nEste projeto irá seguir uma metodologia CRISP-DM e o motor deverá implementar algoritmos aceites mas adaptados aos dados e à estrutura das plataformas de comércio virtual. As recomendações serão também avaliadas de acordo com métricas apropriadas ao projeto, e os seus resultados discutidos."
  },
  {
    "keywords": [
      "Cidades inteligentes",
      "Qualidade de vida",
      "Business intelligence",
      "Operational intelligence",
      "Dashboards",
      "Índice de bem-estar",
      "Aplicações em tempo real",
      "Smart cities",
      "Quality of life",
      "Business intelligence",
      "Operational intelligence",
      "Well-being index",
      "Real-time applications",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Dashboards para índices de bem-estar citadinos",
    "autor": "Rebelo, Bruno Filipe Fernandes",
    "data": "2017",
    "abstract": "A grande deslocalização de pessoas de áreas rurais para zonas urbanas obrigou à criação de novas e melhores infraestruturas nos ambientes citadinos, de forma a se poder assegurar padrões de qualidade de vida adequados. A criação de cidades inteligentes tem como objetivo melhorar a qualidade de vida das pessoas e do meio que as rodeia, garantido uma melhor resposta dos serviços com que interagem em ambientes citadinos. Os habitantes das cidades costumam tecer opiniões concretas sobre a sua cidade. A possibilidade de recolher e conciliar essas opiniões é muito interessante para quem tem que reger tais ambientes, pois estas podem permitir a identificação de alguns pontos fortes e fracos dos vários aspetos de uma cidade. Com esse conhecimento, um gestor poderá, de forma mais suportada, com informação proveniente dos habitantes da cidade, saber qual o efeito das suas decisões através do recurso a um conjunto de dashboards que incluam índices de bem-estar, conjugando os vários elementos recolhidos e refletindo a apreciação das pessoas sobre a cidade, em tempo real. Os dashboards citadinos, quando dimensionados e implementados de forma adequada, constituem um instrumento importante para avaliação da qualidade de vida numa cidade. Neste trabalho de dissertação, apresentar-se-á a forma como tais dashboards podem ser projetados e implementados, suportados por um caso de aplicação concreta que discutiremos com detalhe."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Towards an efficient OLAP engine based on linear algebra",
    "autor": "Afonso, João Miguel",
    "data": "2018",
    "abstract": "Relational database engines associated to the widely used Structured Query Language\n(SQL) are suffering unsatisfactory performance results in complex business queries, due\nto ever increasing volumes of stored data. To retrieve and process data in a more efficient\nway, Online Analytical Processing (OLAP) models have been proposed with an increased\nfocus on attributes (measures and dimensions) over records.\nOLAP is based on a row-oriented theory, while a columnar-oriented theory could considerably\nimprove the performance of analytical systems. The Typed Linear Algebra (TLA)\napproach is an example of such theory: it encodes each database attribute in a distinct matrix.\nThese matrices are combined in a single Linear Algebra (LA) expression to obtain the\nresult of a query.\nThis dissertation combines concepts of relational databases, OLAP, TLA and performance\nengineering to design, implement and validate an efficient TLA-DB engine: SQL queries are\nconverted into its equivalent LA expression, using Type Diagrams (TDs), which represent\neach matrix as an arrow pointing from the number of columns to the number of rows, TDs\nare converted to a LA expression encoded in Linear Algebra Query language (LAQ) and\nthe LAQ script of a query is automatically coded in C Plus Plus (C++).\nAn efficient TLA-DB engine required the encoding of the sparse matrices in an adequate\nformat, namely Compressed Sparse Column (CSC), while the operations specified in LAQ\nexpressions had their performance improved by optimised algorithms and an optimised\nquery processor.\nThe functionality of the resulting LAQ engine was validated with several TPC Benchmark\nH (TPC-H) queries for various dataset sizes. A comparative evaluation of the TLA-DB with\ntwo popular Database Management Systems (DBMSs), PostgreSQL and MySQL, showed\nthat the developed framework outperforms both DBMSs in most TPC-H queries."
  },
  {
    "keywords": [
      "Data Mining",
      "Regressão",
      "Support Vector Machines",
      "Support Vector Regression",
      "Sequential Minimal Optimization",
      "Estações de Tratamento de Águas Residuais",
      "Carência bioquímica de oxigénio",
      "Sólidos suspensos totais",
      "Previsão de comportamento de ETARs",
      "Regression",
      "Wastewater Treatment Plants",
      "Biochemical oxygen demand",
      "Total suspended solids",
      "Prediction of WWTPs performance",
      "628.1:681.3",
      "681.3:628.1"
    ],
    "titulo": "Support Vector Machines na previsão do comportamento de uma ETAR",
    "autor": "Ribeiro, Daniel José Silva",
    "data": "2013-03-20",
    "abstract": "O Data Mining é um processo de exploração de grandes quantidades de dados, com um potencial enorme para ajudar as empresas na extração de conhecimento que está oculto nos mais diversos sistemas de dados. Esta tecnologia é utilizada pelas empresas nos mais variados domínios, com o intuito de as ajudar em atividades de tomada de decisões. Entre os diversos campos de aplicações encontramos o domínio da Biologia e do Ambiente, em particular, as questões relacionadas com as Estações de Tratamento de Águas Residuais (ETAR). As ETAR são infraestruturas essenciais para manter o equilíbrio do meio-ambiente, sendo caracterizadas por terem várias fases de tratamento, nas quais são removidas impurezas como sólidos, matéria orgânica e nutrientes. Todo este processo dinâmico e complexo deve ser processado de forma eficiente, permitindo que o efluente final que nelas é tratado tenha a melhor qualidade possível. A previsão da qualidade da água tratada, com base nos vários fluxos que dão entrada nas ETAR, permite medir a eficácia do tratamento e, assim, obter alguma informação útil para um melhor controle de toda a infraestrutura. A ETAR em estudo neste trabalho de dissertação, localiza-se no Norte de Portugal e serve uma população de cerca de 45 mil habitantes. Os dados fornecidos para alimentação dos processos de interação levados a cabo são referentes a tratamentos realizados nessa ETAR\ndurante o período de um ano. Este estudo pretendeu explorar técnicas de Data Mining preditivas, nomeadamente modelos de regressão, por forma a prever com eficácia os valores dos parâmetros de qualidade da ETAR. As medidas de qualidade do tratamento analisadas neste estudo, basearam-se nos parâmetros de previsão Carência Bioquímica de Oxigénio (CBO) e Sólidos Suspensos Totais (SST). Por sua vez, as técnicas de regressão adotadas neste trabalho são baseadas em Support Vector Machines, mais concretamente nos algoritmos Support Vector Regression e numas das suas variantes: Sequential Minimal Optimization. Este conjunto de técnicas tem sido aplicadas com sucesso em diferentes áreas, inclusive em alguns trabalhos relacionados com as ETAR. Pretendeu-se assim, à custa da utilização destas técnicas de previsão, definir um modelo comportamental para a ETAR em questão, por forma a analisar a sua capacidade preditiva neste tipo de sistemas complexos. Neste problema, as fases de análise e preparação dos dados mostraram-se determinantes na obtenção dos resultados alcançados. Analisaram-se ainda as diversas tarefas de modelação desenvolvidas neste estudo. Os modelos\ndesenvolvidos demonstraram uma boa capacidade preditiva, especialmente na previsão do parâmetro do efluente final CBO. As técnicas de previsão utilizadas, para além da capacidade de modelação preditiva não linear, permitem ainda uma análise aos atributos mais influentes à qualidade dos parâmetros de previsão."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aplicações web seguras em Django",
    "autor": "Teixeira, Adriano Dias",
    "data": "2018-12-19",
    "abstract": "The main objective of this dissertation was the development of a secure Web application\nwith the Django framework, through the implementation of a set of selected security mea sures. The application that was selected to apply the security measures was an e-commerce\nplatform. Another objective of the present work was to analyze the support, offered by\nDjango, to the development of secure Web applications. The literature review helped us to\nidentify Web application security threats as well as their possible organization in 5 classes:\n(i) code injection threats, (ii) authentication control threats, (iii) access control threats, (iv)\nthreats to data confidentiality, and (v) threats to the availability of the service. This kno wledge was crucial for the selection of the protection measures to implement in the Web\napplication. The application was developed according to the Scrum agile methodology.\nThe main problems encountered in using Scrum were (i) the need to adapt the methodo logy to a context in which the Scrum team has few members, and (ii) managing the conflict\nbetween the need to document sprints and the Scrum practice that favors productivity over\ndocumentation. The adopted compromise was to document each sprint before starting the\nnext one. On the other hand, using Scrum improved the definition and fulfillment of the\nobjectives, and allowed for the improvement of the development process itself. These be nefits result from Scrum following an iterative approach geared to the rapid production\nof functional product increments. From the set of functionalities identified during the re quirement elicitation phase, 18 user stories were successfully implemented, resulting in a\nminimum viable product, i. e., a functional product that implements the top most priority\nrequirements for potential users. After implementing and testing the application, it was\nfound that the level of security achieved is high, since 16 of the 19 implemented security\nmeasures are effective in protecting against the respective attacks. The support provided\nby Django, to the implementation of the selected security measures, reached a value close\nto 68 %. Code injection protection is the security measure best supported by Django. At\nthe opposite extreme are the threats to the availability of the service, against which Django\ndoes not offer any support."
  },
  {
    "keywords": [
      "Low-code",
      "Mendix",
      "MS PowerApps",
      "Outsystems",
      "Plataformas de desenvolvimento lowcode",
      "Low code development platforms",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Comparação de plataformas low-code",
    "autor": "Marques, Mariana Gonçalves",
    "data": "2023-01-05",
    "abstract": "A mudança contínua do mercado de software, acompanhado pelo constante aparecimento de novas\ntecnologias, pressiona as equipas de tecnologia a entregarem os seus projetos mais rapidamente sem\ncomprometer a qualidade do produto. Assim, o surgimento de plataformas low-code (LCP) tornou-se\ninevitável e rapidamente se alastrou pelo mercado, sendo estimado que em 2023, 50% das médias e\ngrandes empresas recorram a este tipo de plataformas.\nO desenvolvimento low-code de software é um paradigma emergente, que combina a utilização mínima de código com interfaces gráficas interativas, que possibilita o desenvolvimento rápido de aplicações.\nAssim, esta dissertação tem como objetivo o estudo de plataformas low-code, passando para um\nestudo pormenorizado das plataformas Mendix, MS PowerApps e Outsystems. Este estudo consiste na\nanálise de várias características destas plataformas, sendo feitas também algumas comparações entre\nestas. Para além disso, é ainda feito um estudo empírico usando vários participantes de várias universidades. Deste modo, este estudo pode também ser usado para avaliar a usabilidade das plataformas."
  },
  {
    "keywords": [
      "Crash consistency",
      "Injeção de faltas",
      "Reprodutibilidade",
      "Fault Injection",
      "Reproducibility",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Injeção de faltas reprodutível em sistemas de armazenamento local",
    "autor": "Ramos, Maria José Costa",
    "data": "2024-02-19",
    "abstract": "A era digital trouxe a incorporação dos computadores em vários setores do nosso quotidiano. Tem-se\nverificado, ao longo dos anos, uma crescente dependência na tecnologia, o que se traduz na necessidade\nde sistemas cada vez mais resilientes, rápidos, seguros e disponíveis.\nConsequentemente, a complexidade dos sistemas tem vindo a crescer, o que leva à implementação\nde mecanismos de persistência e atualização de dados mais difíceis de testar. Para elevar o nível de\ndificuldade, os diversos sistemas de ficheiros possuem particularidades que afetam de forma significativa\nestes mecanismos e a consistência das aplicações após a ocorrência de falhas de energia e dos sistemas\noperativos.\nEvitar a perda total ou parcial dos dados deve ser um ponto fulcral para sistemas de armazenamento\ncom fortes garantias de durabilidade e coerência. Na literatura, os sistemas capazes de voltar a um estado\ncoerente após uma falha de energia são chamados de crash-consistent. Muito do trabalho relacionado\nfoca-se na crash consistency dos sistemas de ficheiros e, quando se foca nas aplicações, verifica-se a\nfalta de ferramentas para reprodução de bugs. Quando um utilizador reporta problemas encontrados\nnuma dada aplicação, é útil para as equipas de desenvolvimento terem uma ferramenta que rapidamente\nreproduz o bug e ajude na sua correção.\nEsta dissertação propõe o LazyFS+, um sistema de ficheiros que simula a perda total e parcial de\ndados através da injeção de faltas baseada em software. A nossa solução possui uma cache interna, o\nque permite uma gestão determinística dos dados das aplicações e a injeção de faltas reprodutível. Um\ndos seus pontos fortes é o facto de imitar comportamentos que alguns sistemas de ficheiros apresentam,\ncomo a reordenação de escritas, sem se prender a uma implementação específica. Para além disso,\nfacilita a análise das operações executadas pela aplicação e permite obter informação sobre os dados\nnão sincronizados.\nO LazyFS+ provou-se útil através da reprodução de bugs já conhecidos e da identificação desses\nmesmos bugs em versões antigas das aplicações, nunca antes reportados. Para além disso, foi possível\nencontrar novos bugs, quer em versões antigas, quer em versões mais recentes das aplicações. Estes\núltimos foram reportados com todos os passos de reprodução realizados com o LazyFS+. Como resultado,\natualmente este está a ser integrado nos testes do sistema de armazenamento chave-valor etcd."
  },
  {
    "keywords": [
      "Curva ROC",
      "AROC",
      "Covariáveis",
      "Shiny",
      "Bioestatística",
      "Informática médica",
      "Classificação estatística",
      "Software",
      "ROC curve",
      "Covariates",
      "Biostatistics",
      "Medical informatics",
      "Statistical classification",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "RealROC: a shiny based application for ROC curve study with covariate adjustment",
    "autor": "Costa, Francisco Luís do Amaral Ribeiro Machado e",
    "data": "2020-11-16",
    "abstract": "A curva ROC (Receiver operating characteristic) é uma ferramenta analítica eficaz para testes clínicos. A análise permite visualizar a variação de sensibilidade e especificidade para uma dada região de corte através de um simples, mas robusto gráfico bidimensional. Num contexto biológico, testes podem ser influenciados por múltiplas variáveis externas e como tal a análise ROC pode não ser a ideal ou gerar resultados incompletos. É então necessário saber que variáveis afetam determinado teste clínico de forma a determinar os melhores parâmetros para determinado teste ou até descartar determinada metodologia mediante a situação. O ajuste da curva ROC a covariáveis permite a normalização do efeito das mesmas ou diretamente ajustar a curva para os seus efeitos. Software direcionado ao ajuste da curva ROC é, infelizmente, escasso e muitas vezes difícil de manusear por utilizadores não especializados. Recentemente o pacote AROC foi lançando para R que disponibiliza vários recursos para estes ajustamentos, no entanto a dificuldade de utilização mantém-se. A combinação deste pacote com a estrutura Shiny, um pacote que permite o desenvolvimento de aplicações interativas, tem por objetivo a criação de um programa grátis e acessível que permita uma análise mais aprofundada disponível para todos os investigadores. RealROC foi capaz de replicar resultados de um caso de estudo que analisou a influência do sexo no sistema de pontuação CRIB e respetiva previsão de mortalidade, demonstrando a usabilidade e acessibilidade do programa que será disponibilizado online e potencialmente contribuir para novos desenvolvimentos na área."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Polyglot: sistema poliglota de processamento de dados",
    "autor": "Gonçalves, Hugo Manuel Ramos Vilas Boas",
    "data": "2018",
    "abstract": "O aumento exponencial do volume de dados gerados no mundo tecnológico atual é incontestável. A necessidade de armazenar e processar esses grandes volumes de dados levou a\nindústria a optar por soluções de armazenamento e processamento na nuvem. Além disto,\nos desenvolvedores optam cada vez mais por sistemas de base de dados que permitem\nmelhor desempenho e também tirar partido da variedade estrutural dos dados face aos\nsistemas relacionais tradicionais. Estes sistemas que estão a surgir apresentam modelos\nde dados baseados em estruturas como, p.e., grafos ou índices chave-valor, e oferecem interfaces\nque podem ser apenas duas operações (PUT/GET) ou, à semelhança dos sistemas\nrelacionais com o SQL, ter linguagens de interrogação específica.\nContudo, a migração de praticamente todos os componentes das infraestruturas das\naplicações para a nuvem implica que os dados sejam processados e armazenados em infraestruturas\nde terceiros, ficando muitas vezes a privacidade destes comprometida. Por\noutro lado, um dado problema pode ter dados com estruturas diferentes ou partes diferentes\nde uma aplicação podem ter necessidades diferentes quanto aos dados e, por isso, a\ndiversidade entre sistemas de armazenamento leva uma grande complexidade em desenvolver\nsistemas que usem várias fontes de dados diferentes e heterogéneas eficientemente.\nAssim, esta dissertação pretende dar uma resposta à problemática da gestão de dados de\nforma privada nas aplicações web, potencializando a utilização de múltiplos sistemas de\nfontes de dados heterogéneas. Em específico, esta dissertação apresenta uma nova arquitetura,\nà qual se chamou Polyglot, que permite a manutenção da privacidade dos dados, enquanto\nao mesmo tempo possibilita a utilização de múltiplas fontes de dados heterogéneas\ne tira partido da nuvem para grande parte do processamento. Esta arquitetura é também\nimplementada sob a forma de um protótipo direcionado a um sistema de monitorização,\nque consiste no caso de estudo desta dissertação. Este protótipo permite comprovar a validade\nda arquitetura, sendo que a implementação feita demonstra todas as funcionalidades\nessenciais ao funcionamento do sistema. Mais ainda, este protótipo é também avaliado a\nnível de desempenho e utilização de recursos, permitindo demonstrar a viabilidade deste\nsistema para uma utilização em cenários reais. Por último exploram-se algumas das funcionalidades\nmais relevantes que se poderiam adicionar ao sistema e os ganhos que estas\ntrariam face à implementação atual, demonstrando o potencial do protótipo."
  },
  {
    "keywords": [
      "Evolution of mycobacterium tuberculosis",
      "Bioinformatics",
      "Phylogenetics",
      "Horizontal gene transfer (HGT)",
      "Evolução de Mycobacterium tuberculosis",
      "Bioinformática",
      "Filogenética",
      "Transferência horizontal de genes",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "A comprehensive phylogenetic analysis of Mycobacterium tuberculosis protein-coding genes: insights into evolution and virulence",
    "autor": "Pereira, Daniela Sofia Gaspar",
    "data": "2018",
    "abstract": "Understanding the evolutionary relationships between organisms is a complex issue that has gained importance not only in evolution but also in clinical and biological inferences, only possible with technological advances that require new analytical tools. In this work, the main focus is to study the genome of Mycobacterium tuberculosis, the causal agent of tuberculosis, establishing a detailed evolutionary framework.The secondary objective, regardless of the focus on the evolution of Mtb, is that the pipeline created can be applied to any other organism. This dissertation presents some basic facts about tuberculosis, an infectious bacterial disease caused by the Mycobacterium tuberculosis complex, its constitution, evolution, pathology, drug resistance and genetic variation. Our objectives were contextualized considering the most up-to-date tools of alignment and phylogenetics, an area in constant progress due to the growing needs of bioinformatics tools in the area of genomics and evolution. Taxonomic and genetic data were compiled from all organisms with complete genomes in NCBI. This database was subsequently cured by eliminating redundant genomes, i.e., containing only one representative element of each species with the complete proteome. A search of each Mycobacterium tuberculosis protein in this local database using BLAST allowed the detection of probable homologs in a large number of taxonomically informative organisms. The search results were limited to two hundred homologues, which were aligned using MUSCLE. Phylogenetic trees, based on maximum likelihood were constructed for the approximately four thousand Mycobacterium tuberculosis proteins. The phylogenetic relationship and monophyly of Mycobacterium tuberculosis with the remaining bacteria of the same genus(Mycobacterium) and the same family (Corynebacteriaceae) were studied to understand possible processes of acquisition of genes by horizontal transference. Finally, positive selection processes were studied by searching for excess or deficit of non-synonymous mutations in relation to the synonymous (Ka / Ks) using the CODEML software, in order to identify branches with an accelerated evolution in the establishment of the pathogenic species Mycobacterium tuberculosis. These genes may form the basis of the physiological and biochemical characteristics that make this bacterium pathogenic to humans."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Service and auto-configuration framework for secure Ad-hoc environments in Android and Linux",
    "autor": "Azevedo, Nuno Filipe Solinho de",
    "data": "2011-11-20",
    "abstract": "Ad-hoc networks can be useful in many contexts because they can be spontaneously created and do not require any sort of infrastructure. They can be useful for small groups when no other network is accessible. They can also be used in wider areas as a low cost replacement for wireless infrastructure networks with multiple dedicated access points. Despite this, ad-hoc networks are not a very popular option for most users.\n\nUnfortunately, ad-hoc networks are not as user friendly as infrastructure networks. The latter ones usually provide standardized mechanisms that perform the essential configurations for the correct functioning of the network. Ad-hoc networks do not have standardized mechanisms adapted to them. Each wireless network manager supports a different set of configuration mechanisms. There is usually no problem when every machine uses the same operating system but when different ones are used, users may need to manually perform the required configurations. Another cause for this low popularity is the lack of useful and easy to use applications. These applications are usually hosted on the Internet, as it provides a larger variety of business models.\n\nTo tackle these problems, new forms of automatically configuring machines and providing services should be explored. These services must be easy to develop, in order to attract the developers that would develop them. The designed solutions must also be adapted to ad-hoc environments. Another important aspect that must be addressed is security. In some contexts, such as public and corporate environments, security can be essential to provide authentication and even to allow the correct functioning of the\nnetwork."
  },
  {
    "keywords": [
      "681.3",
      "616.891.6"
    ],
    "titulo": "Analysis of the influence of stress on the interaction with the computer",
    "autor": "Catalão, Fábio Alexandre Marques",
    "data": "2013-06-08",
    "abstract": "The monitoring of different physical and cognitive functions of the human being has been the subject of numerous studies in recent years. However, most of these monitor-ing systems use invasive and very expensive techniques, which complicate its use in research projects and real scenarios alike. Some studies are trying to obtain relevant data from common devices like personal computers or Smartphones, but this area has not been properly explored yet.\nIn this project, it was proposed to perform an analysis of the interaction of the users with a computer using the mouse and the keyboard in order to obtain relevant conclu-sions about the effects of stress on the individual. The hypothesis presented here is in-teresting due to the use of non-invasive techniques to retrieve data as well as the use of common and inexpensive hardware instead of specific and expensive one."
  },
  {
    "keywords": [
      "Bases de dados de grafos",
      "Bases de dados relacionais",
      "Master data management",
      "Recursos Humanos",
      "Graph databases",
      "Human Resources",
      "Relational databases",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Graph databases for HR relationships",
    "autor": "Cunha, Carolina Alves da",
    "data": "2022-12-21",
    "abstract": "As bases de dados relacionais não foram desenhadas para tratar de dados interligados, em oposição às tecnologias de grafos. A modelação de Recursos Humanos trabalha com estruturas altamente relacionadas entre si, pelo que a substituição de bases de dados relacionais por bases de dados de grafos neste contexto poderia melhorar a robustez e desempenho das aplicações. \nNa presente dissertação, pretendeu-se comparar as tecnologias existentes (relacionais, não relacionais e grafos) e, dentro da tecnologia de grafos, avaliar qual a mais adequada em contexto de Recursos Humanos. A revisão de literatura revelou que as bases de dados de grafos são mais eficientes para dados interligados do que as bases de dados relacionais e não relacionais. De todos os modelos de grafos analisados, o Neo4j foi o sistema de gestão de bases de dados que reuniu, num âmbito geral, as melhores características e, por este motivo, foi utilizado como prova de conceito. Foram realizadas três interrogações (duas com obtenção de dados de diferentes relações e uma com obtenção de dados de uma única relação), tendo-se obtido tempos de resposta de 41.48, 18.58 e 62.14ms vs. 804.68, 103.08 e 318.42ms entre bases de dados de grafos e relacionais, respetivamente. \nOs resultados obtidos revelaram melhor desempenho do Neo4j na maioria das situações avaliadas. Em situações sem junções entre diferentes relações num ambiente relacional, o desempenho do SQL foi superior ao do Neo4j. Adicionalmente, verificou-se quebra significativa de desempenho do Neo4j quando foi analisado mais de metade do grafo. \nAs bases de dados de grafos apresentaram um melhor desempenho no tratamento de bases de dados altamente relacionadas."
  },
  {
    "keywords": [
      "Paralelismo",
      "Análise de dados",
      "PROOF",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Estudo de viabilidade de paralelização de códigos de análise de dados em PROOF",
    "autor": "Silva, Rafael Caldeira",
    "data": "2014-12-18",
    "abstract": "Esta dissertação surge no contexto das análises de dados gerados pelo LHC (Large Hadron Collider), do esperado crescimento do volume de dados produzidos depois da atualização de 2013-2014 e do atual paradigma pseudo-paralelo destas aplicações no LIP-Minho (Laboratório de Instrumentação e física experimental de Partículas, delegação Minho). \nO trabalho surgiu como um estudo da utilização do PROOF (Parallel ROOT Facilities) como plataforma para habilitar a extração automática de paralelismo nas aplicações de análises de dados do LIP-Minho. \nNa consideração que as análises em estudo têm uma estrutura semelhante que é susceptível de ser paralelizada, partimos de um caso de estudo para a familiarização e experimentação do ambiente PROOF. \nFace às dificuldades de adaptação da aplicação para utilização do sistema PROOF, desenvolvemos e testamos uma nova estrutura de classes, chamada event, que pode eliminar uma série de problemas na fase de desenvolvimento. Esta proposta é suportada por um gerador de código esqueleto de aplicações deste tipo, o makeEvent. \nOs testes efetuados comprovam a possibilidade de usar a estrutura event como alternativa à API TSelector, sem perda de desempenho e com a possibilidade de alcançar speedups superlineares no ambiente de cluster utilizado. \nNo caso de códigos de análise de dados com alguma dimensão e complexidade, o processo de adaptação para um modelo compatível com o sistema PROOF pode ser uma tarefa morosa e exigente que pode não ser trivial. Por este motivo, propomos como trabalho futuro a criação de uma biblioteca que trate das tarefas habituais no processo de análise dos dados. Prevê-se também que a aplicação makeEvent permita a seleção apenas dos branches utilizados na classe event, reduzindo significativamente o tempo de execução de análises de dados que carregam desnecessariamente todos os branches de uma tree. \nA conclusão a que chegamos é a da viabilidade da utilização da estrutura event, e consequentemente do makeEvent, como uma alternativa possível para a extração de paralelismo automático das análises de dados em estudo, recorrendo à plataforma PROOF."
  },
  {
    "keywords": [
      "681.3.06",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Improving program comprehension tools for domain specific languages",
    "autor": "Oliveira, Nuno Ernesto Salgado",
    "data": "2009",
    "abstract": "Since the dawn of times, curiosity and necessity to improve the quality of their\nlife, led humans to find means to understand everything surrounding them, aiming\nat improving it. Whereas the creating abilities of some was growing, the capacity\nto comprehend of others follow their steps. Disassembling physical objects to comprehend\nthe connections between the pieces in order to understand how they work\ntogether is a common human behavior. With the computers arrival, humans felt\nthe necessity of applying the same techniques (disassemble to comprehend) to their\nprograms.\nTraditionally, these programs are written resorting to general-purpose programming\nlanguages. Hence, techniques and artifacts, used to aid on program comprehension,\nwere built to facilitate the work of software programmers on maintaining\nand improving programs that were developed by others. Generally, these generic\nlanguages deal with concepts at a level that the human brain can hardly understand.\nSo understanding programs written in this languages is an hard task, because the\ndistance between the concepts at the program level and the concepts at the problem\nlevel is too big.\nThus, as in politics, justice, medicine, etc. groups of words are regularly used\nfacilitating the comprehension between people, also in programming, languages that\naddress a specific domain were created. These programming languages raise the\nabstraction of the program domain, shortening the gap to the concepts of the problem\ndomain.\nTools and techniques for program comprehension commonly address the program\ndomain and they took little advantage of the problem domain. In this master’s thesis,\nthe hypothesis that it is easier to comprehend a program when the underlying problem\nand program domains are known and a bridge between them is established, is\nassumed. Then, a program comprehension technique for domain specific languages,\nis conceived, proposed and discussed. The main objective is to take advantage from\nthe large knowledge about the problem domain inherent to the domain specific language,\nand to improve traditional program comprehension tools that only dealt, until\nthen, with the program domain. This will create connections between both program\nand problem domains. The final result will show, visually, what happens internally\nat the program domain level, synchronized with what happens externally, at problem\nlevel."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Retinal image quality assessment using deep convolutional neural networks",
    "autor": "Rodrigues, Ana Rita Vieira",
    "data": "2018",
    "abstract": "Diabetic Retinopathy (DR) and diabetic macular edema (DME) are the damages caused to the retina and are complications that can affect the diabetic population. Diabetic retinopathy (DR), is the most common disease due to the presence of exudates and has three levels of severity, such as mild, moderate and severe, depending on the exudates distribution in the retina. For screening of diabetic retinopathy or a population-based clinical study, a large number of digital fundus images are captured and to be possible to recognize the signs of DR and DME, it is necessary that the images have quality, because low-quality images may force the patient to return for a second examination, wasting time and possibly delaying treatment.\nThese images are evaluated by trained human experts, which can be a time-consuming and expensive task due to the number of images that need to be examined. Therefore, this is a field that would be hugely benefited with the development of an automated eye fundus quality assessment and analysis systems. It can potentially facilitate health care in remote regions and in developing countries where reading skills are scarce. Deep Learning is a kind of Machine Learning method that involves learning multi-level representations that begin with raw data entry and gradually moves to more abstract levels through non-linear transformations. With enough training data and sufficiently deep architectures, neural networks, such as Convolutional Neural Networks (CNN), can learn very complex functions and discover complex structures in the data. Thus, Deep Learning emerges as a powerful tool for medical image analysis and evaluation of retinal image quality using computer-aided diagnosis.\nTherefore, the aim of this study is to automatically assess all the three quality parameters alone (focus, illumination and color), and then an overall quality of fundus images assessment, classifying the images into the classes “accept” or “reject with a Deep Learning approach using convolutional neural networks (CNN). For the overall classification, the following results were obtained: test accuracy=97.89%, SN=97.9%, AUC=0.98 and 𝐹1-score=97.91%."
  },
  {
    "keywords": [
      "Energy efficiency",
      "DBMS",
      "Green software",
      "Green computing",
      "Program analysis",
      "SGDBS",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Energy consumption on database management systems",
    "autor": "Monteiro, Tiago André Araújo",
    "data": "2021-07-26",
    "abstract": "In recent years, with the growth of energy consumption by computing devices, energy efficiency is a crucial concern in the IT area due to its economics and environmental impact. The recent but widespread use of powerful computing devices, namely smartphones, which rely on \"the cloud\" to store large amounts of information (like, for example, photos and videos), is demanding the construction and maintenance of large data centers. \nSuch data centers run large-scale internet-based systems like cloud services. As a consequence, the energy consumed by data centers is growing fast, which is a crucial concern in the IT area due to its economics and environmental impact. \nThe growing reliance on cloud construction services is one of the main reasons for the rapid rise in research and development of energy efficient software and hardware for data centers. Nowadays, the most popular usage of data centers is the Database Management Systems (DBMS) that, normally, are responsible for the access, management, manipulation, and organization of data. While there have been advances and studies in energy-awareness in this area, there isn't enough knowledge on the energy efficiency provided by different database systems. \nThis master thesis intends to tackle this lack of knowledge by analyzing the energy consumption of DBMS software. Through benchmarks that simulate real usage environments, this research plays a key role in improving the knowledge on the energy efficiency of DBMS. We analyze four systems, namely MySQL, Postgres, MariaDB, and Redis. Moreover, we use the HammerDB benchmark framework for the simulation of DBMS in a real environment. Thus, to have a precise knowledge of the energy consumption of DBMS, we analyze the energy consumption in various subsystems of the computer, namely like CPU, DRAM, GPU, and Disk. Moreover, we present further analysis of the energy consumption per performance ratio in all subsystems levels. \nOur results show that, indeed, there are significant differences in the energy consumption of which DBMS and that in some scenarios, the one with better run time performance is not what consumes more energy."
  },
  {
    "keywords": [
      "SILAB",
      "ROS2",
      "Driving simulator",
      "Middleware",
      "DSM-2W",
      "Simulador de condução",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Integration of ROS2 with a simulation environment",
    "autor": "Capa, Luís Filipe Costa",
    "data": "2022-04-29",
    "abstract": "Currently, the University of Minho owns a driving simulator, from now on referred to as Driving Simulator Mockup 2-Wheeler (DSM-2W), which mimics a real driving environment for motorcycles. This simulator can reproduce diverse driving scenarios, like driving on different roads, traffic, and weather conditions, and is mostly used to test how the driver reacts to stimulus from subsystems under test in a particular scenario.\nThe simulator has several components, namely, the Mock-up, which represents the motorcycle physically, the\nsoftware responsible for the simulation environment, that is also projected on a screen, called SILAB [1] as\nwell as several other subsystems and respective software, which all together form a complex distributed system.\nSILAB creates realistic graphic environments, has different models to control the behavior of other drivers and\npedestrians, generates 3D sounds, and facilitates the personalization of the simulation scenario.\nRobot Operating System 2 (ROS2) [2] provides a set of tools and software libraries that facilitate the develop ment of robot systems and applications. With the increasing reliance on software, sensors, and actuators in the\nautomotive domain, it makes sense to view cars [3] and motorcycles as robots. Therefore, it also makes sense\nto use ROS2 in the simulation domain to solve the problems at hand.\nThis dissertation describes how ROS2, a well-known and accepted middleware for robotic applications, can\nalso play a role in these contexts acting as a universal interface between motorcycle simulators and external\nsubsystems and thereby significantly improving the system’s expansibility and those subsystems’ portability and reusability."
  },
  {
    "keywords": [
      "Deep Learning",
      "Storage Systems",
      "I/O",
      "TensorFlow",
      "PyTorch",
      "Prefetching",
      "Parallel I/O",
      "Aprendizagem Profunda",
      "Sistemas de Armazenamento",
      "E/S",
      "Pré-busca",
      "E/S Paralela",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "PRISMA: a prefetching storage middleware for accelerating deep learning frameworks",
    "autor": "Correia, Cláudia Sofia Mendonça de Sá",
    "data": "2021-03-05",
    "abstract": "Deep Learning (DL) is a widely used technique often applied to many domains, from computer vision to natural language processing. To avoid overfitting, DL applications have to access large amounts of data, which affects the training performance. Although significant hardware advances have already been made, current storage systems cannot keep up with the needs required by DL techniques. Considering this, multiple storage solutions have already been developed to improve the Input/Output (I/O) performance of DL training. Nevertheless, they are either specific to certain DL frameworks or present drawbacks, such as loss of accuracy. Most DL frameworks also contain internal I/O optimizations, however they cannot be easily decoupled and applied to other frameworks. Furthermore, most of these optimizations have to be manually configured or comprise greedy provisioning algorithms that waste computational resources. To address these issues, we propose PRISMA, a novel storage middleware that employs data prefetching and parallel I/O to improve DL training performance. PRISMA provides an autotuning mechanism to automatically select the optimal configuration. This mechanism was designed to achieve a good trade-off between performance and resource usage. PRISMA is framework-agnostic, meaning that it can be applied to any DL framework, and does not impact the accuracy of the training model. In addition to PRISMA, we provide a thorough study and evaluation of the TensorFlow Dataset Application Programming Interface (API), demonstrating that local DL can benefit from I/O optimization. PRISMA was integrated and evaluated with two popular DL frameworks, namely Tensor Flow and PyTorch, proving that it is successful under different I/O workloads. Experimental results demonstrate that PRISMA is the most efficient solution for the majority of the scenar ios that were studied, while for the other scenarios exhibits similar performance to built-in optimizations of TensorFlow and PyTorch."
  },
  {
    "keywords": [
      "Software engineering",
      "Formal methods",
      "Correct-by-construction",
      "Engenharia de software",
      "Métodos formais",
      "Correção-por-construção",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Back to programming from Galois connections",
    "autor": "Pereira, Paulo Ricardo Antunes",
    "data": "2024-01-15",
    "abstract": "It is clear that the trend towards higher levels of abstraction in programming methods, as well as the\neffort to make software design more of a scientific, engineering discipline, has led to the development of\nvarious programming paradigms and the use of rigorous proof methods to ensure the reliability and safety\nof critical software systems. However, the implementation of these formal methods can be challenging\ndue to their reliance on inductive proofs following the invent-and-verify method. Despite this, some in\nthe field continue to seek out and use these theoretical foundations in an attempt to produce high-quality\nsoftware. Therefore, this study presents the potential for the correct-by-construction method, using Galois\nConnections and theoretical concepts from computer science to develop a methodology for constructing\npractically applicable software systems whose correctness is guaranteed from the outset."
  },
  {
    "keywords": [
      "Aplicações por voz",
      "Assistentes digitais",
      "Dispositivos conectados",
      "Engenharia de Software",
      "Connected devices",
      "Digital assistants",
      "Software engineering",
      "Voice applications",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of a process for the creation of cross-platform voice applications for Amazon Alexa and Google Assistant",
    "autor": "Canavarro, Rita de Moura Machado Coelho",
    "data": "2019-12",
    "abstract": "Os dispositivos conectados pertencem à área de ”Ambient intelligence” (AmI) e são dispositivos inteligentes que podem fornecer diversos serviços ou através de comandos por voz ou de forma autónoma. Estes dispositivos conseguem ser autónomos, devido ao facto de conseguirem capturar informação do ambiente através dos seus sensores e depois processá-la, de modo a que consigam ativar a ação necessária (”Context-aware Computing”). Os assistentes digitais também pertencem à área de AmI e são programas de software baseados em ”Natural User Interfaces”, o que significa que estes funcionam com recurso a comandos por voz para efetuar uma determinada ação [46]. Os assistentes podem estar presentes em dispositivos conectados e foram desenvolvidos para ajudar as pessoas nas suas tarefas diárias. Devido ao aumento no uso de assistentes digitais, surgiu a necessidade de atender às exigências de uma gama mais ampla de utilizadores, dado que as funcionalidades básicas, para as quais os assistentes haviam sido programados, já não eram suficientes. Esta necessidade levou a uma nova abordagem em relação à expansão das funcionalidades dos assistentes digitais, que consistiu na criação de aplicações por voz. As aplicações por voz ainda são relativamente recentes e como tal ainda não existem muitas ferra mentas, padrões arquiteturais que tenham sido estabelecidos ou uma metodologia ”standard” que possa ser usada no processo de desenvolvimento. Este problema é ainda maior se abordarmos as aplicações por voz ”cross-platform”, dado que hoje em dia existe uma abundância de diferentes assistentes digitais integrados. A inexistência de uma metodologia ”standard” significa que os programadores irão acabar por usar a(s) metodologia(s) que lhes pareçam as mais adequadas tendo em conta o seu objetivo de obter um produto estável. A falta de standardização e de suporte ao desenvolvimento ”cross-platform” de aplicações por voz é a motivação desta dissertação de mestrado. O objetivo desta dissertação é o desenvolvimento de um processo de construção independente de plataforma, que irá promover a criação de aplicações por voz ”cross-platform” e a automatização do mesmo. Este processo vai estar disponível através de uma plataforma, com um editor visual incorporado, que irá permitir a criação de um template de modelo de linguagem que mais tarde irá ser usado para gerar modelos específicos a uma plataforma de modo a que se possa definir o ”frontend” e código ”boilerplate” para o desenvolvimento inicial da funcionalidade do ”backend”. Ao usar esta plataforma, os programadores irão ser capazes de criar e fazer o ”deploy” de aplicações por voz para a Amazon Alexa e para o Google Assistant a partir de uma única fonte de informação, apesar das diferenças que existem entre os seus modelos aplicacionais e, mais importante, recorrendo principalmente aos requisitos pretendidos e não somente aos aspectos tecnológicos."
  },
  {
    "keywords": [
      "Instituições de saúde",
      "Monitorização",
      "Plataforma web",
      "Sistemas de informação em saúde",
      "Tomada de decisões médicas",
      "Health information systems",
      "Health organizations",
      "Medical decision making",
      "Monitoring",
      "Web platform",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Plataforma colaborativa de monitorização, em tempo real, de análises clínicas, nas instituições de saúde",
    "autor": "Pinheiro, Quitéria Guimarães",
    "data": "2024-04-19",
    "abstract": "Na era atual, marcada pela rápida evolução tecnológica, testemunhamos transformações profundas que\npermeiam todos os setores da sociedade. A incessante inovação tecnológica tem redefinido a forma\ncomo vivemos, trabalhamos e interagimos, proporcionando uma revolução que molda a essência da vida\nmoderna.\nNo âmbito da saúde, essa revolução tecnológica não é apenas evidente, como também tem o potencial de redefinir padrões e expetativas. Os avanços tecnológicos na área da saúde oferecem benefícios\nsignificativos, desde diagnósticos mais precisos a tratamentos mais personalizados. A integração de tecnologias como inteligência artificial e big data, não apenas amplia o alcance dos serviços de saúde, como\ntambém melhora a eficiência e a qualidade do atendimento.\nOs softwares desempenham um papel central nesta transformação, proporcionando ferramentas poderosas para profissionais de saúde. A gestão eficiente de dados, análises avançadas e aprimoramento\nda comunicação entre equipas médicas são apenas alguns dos benefícios oferecidos por estas soluções.\nA relevância crescente destes softwares destaca-se na contribuição das operações clínicas e na melhoria\ndos processos de tomada de decisão, resultando numa prestação de cuidados mais eficaz e centrada no\npaciente.\nA elaboração da presente dissertação considerou o estudo minucioso da seguinte questão de investigação: ”Qual o impacto da utilização de uma aplicação de análise de dados no domínio da medicina e na\nmelhoria da qualidade de vida dos pacientes? ”Para responder a esta questão, foi desenvolvida a AnalyticCare, uma aplicação de monitorização de análises clínicas, com a finalidade de ampliar a capacidade de\ncomparação entre pacientes, proporcionando aos profissionais de saúde uma ferramenta valiosa. Este\nartefacto representa uma resposta às demandas crescentes por soluções que otimizem a análise comparativa de casos clínicos. Por fim, foi realizada uma Prova de Conceito - Análise SWOT, para avaliar a\neficiência da aplicação, destacando não apenas a sua viabilidade técnica, mas também a sua capacidade\nde promover avanços na eficácia dos cuidados de saúde, contribuindo para um ambiente clínico mais\nsofisticado e integrado."
  },
  {
    "keywords": [
      "Adaptive ansätze",
      "Quantum chemistry",
      "Quantum computing",
      "Variational quantum algorithms",
      "Variational quantum eigensolver",
      "Algoritmos variacionais quânticos",
      "Ansätze adaptativos",
      "Computação quântica",
      "Eigensolver variacional quântico",
      "Química quântica"
    ],
    "titulo": "Ansätze for noisy variational quantum Eigensolvers",
    "autor": "Alves, Mafalda Francisco Ramôa da Costa",
    "data": "2021-12-16",
    "abstract": "Simulating quantum mechanical systems is one of the main applications envisioned for quantum com puters. In contrast with the first algorithms created for this purpose, that were devised to be implemented\nin a fault-tolerant quantum computer, the Variational Quantum Eigensolver (VQE) aims to adjust to the con straints of Noisy Intermediate-Scale Quantum (NISQ) devices. There is hope that the class of Variational\nQuantum Algorithms (VQAs), to which VQE belongs, will be the first to achieve quantum advantage.\nThe choice of ansatz can dictate the success (or lack thereof) of a VQA: too deep ansätze can hinder\nnear-term viability, or lead to trainability issues that render the algorithm inefficient. In this context, this\ndissertation aimed to analyse different ansätze for quantum chemistry, examining their noise-resilience\nand viability in state-of-the-art quantum computers. In particular, dynamic ansätze were explored, and\ntheir performance compared against predetermined ansätze, with a focus on susceptibility to noise.\nMultiple variants of VQE, namely Unitary Coupled Cluster Singles and Doubles (UCCSD)-VQE (prede termined) and Adaptive Derivative-Assembled Pseudo-Trotter (ADAPT)-VQE (dynamic), were implemented\nboth in simulators and cloud quantum computers. Using noise models, the impact of several noise sources\non convergence was assessed. Additionally, the importance of the operator pool in ADAPT-VQE was anal ysed, and strategies to manipulate the ansatz beyond the ADAPT-VQE algorithm were explored.\nSeveral conclusions could be drawn from this work. Adapting the ansatz to the problem and system\nwas concluded to be fundamental in avoiding trainability issues, decreasing the circuit depth required\nfor a given accuracy, and improving noise-resilience (against circuit depth dependent and independent\nsources alike). Dynamic ansätze were shown to be capable of enduring significantly larger error rates than\npredetermined alternatives, and were thus proved to be better suited for NISQ devices. For 𝐻2, as far\nas ground state energy calculations are concerned, ADAPT-VQE was shown to tolerate a 20 times lower\nshot count, 150 times larger error rates in state preparations and measurements, and 850 times lower\ncoherence times than UCCSD-VQE. The difference is expected to increase with the size of the system.\nAdditionally, it was observed that there is still a margin for improving upon ADAPT-VQE. Further manip ulation of the ansatz was shown to be capable of producing yet shallower circuits for the same accuracy.\nUsing an idea previously proposed in the literature, a more conservative selection criterion was tested.\nAdditionally, removing operators on the fly based on available data was attempted as a new possibility.\nBoth approaches were shown to be capable of improving upon the ADAPT-VQE ansatz, resulting in an up\nto 35-fold decrease in the error for a similar circuit depth within the first 10 iterations of ADAPT-VQE."
  },
  {
    "keywords": [
      "681.3",
      "654.19"
    ],
    "titulo": "Semantic good morning : news collection, management and presentation",
    "autor": "Silva, José Pedro Vieira Costa e",
    "data": "2012-12-07",
    "abstract": "Televisions nowadays are shipped with more and more processing power. This allows the development of applications that run in the television. With the evolution of the different Smart TV development frameworks, application development for televisions will become more usual.\nThe aim of the project here reported is to study the viability of the available technologies for Smart TV development, research the means to gather and classify news articles, study techniques of similar document detection and finally to implement a system divided in two parts: the Back-end, where the news aggregation and management will occur; and the Front-end, a Smart TV application that will present to the user the news filtered according to the rating based on standard or customized criteria.\nAs a result of this project, a System for news collection, management, and presentation was implemented.\nThe Back-end collects, classifies, and detects similar news articles, and also obtains news related images. Then, and according to user preferences news are rated and served to the Front-end. The Front-end is a Samsung Smart TV application. Samsung Smart TV was chosen as the best suited Smart TV framework for the project. While news are presented on the Front-end, feedback about each news article is being sent to the Back-end, which will cause changes in the news\npresentation order."
  },
  {
    "keywords": [
      "Computer graphics",
      "Mesh shaders",
      "Performance",
      "Computação gráfica",
      "Mesh shaders",
      "Desempenho",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Exploring mesh shaders",
    "autor": "Carvalho, Miguel Ângelo Abreu de",
    "data": "2022-04-05",
    "abstract": "Every artist is somewhat limited by the mean by which they expose their art. This is also true for the field of\nComputer Graphics, where there are many limiting factors that developers must go out of their way to avoid. The\nmost limiting of these factors is the computing performance, which directly limits the complexity of what an artist\ncan fabricate in a piece of hardware.\nAs such, Computer Graphics’ investigators keep an eye out for the improvements made in the hardware\ndepartment that enables them to introduce more complexity to the scenes they create on their computers.\nThree years ago, a novel approach to compute the geometric complexity of three-dimensional (3D) scenes\nwas introduced: Mesh shaders. Mesh shaders pose as an alternative to the traditional geometric processing\nmethod and can be a more performant approach to handle specific geometric workloads. Notwithstanding, little\nattention has been given to these shaders.\nThus, this thesis presents an investigative effort to evaluate the value proposition of these shaders across\ndifferent scenarios. To do so, this thesis puts Mesh shaders against traditional implementations and measures\ntheir differences both in method and performance.\nBy the end of this thesis, the reader should have a concise understanding of Mesh shaders, but not a clear cut answer regarding their use. These shaders can provide performance benefits in specific scenarios over\nthe traditional approach, but not without considerable care by the developer. In fact, the flexibility provided by\nthe Mesh shaders’ approach gives the developer a significant responsibility regarding their final performance.\nWhen incorrectly set up, these shaders can result in mediocre performances compared to those of the traditional\npipeline.\nUltimately, these shaders should be used by experienced users intending to avoid specific bottlenecks of the\ntraditional approach. For others, the traditional pipeline offers a more streamlined approach, thoroughly optimised by default."
  },
  {
    "keywords": [
      "Arquitetura orientada a serviços",
      "SOA",
      "Web services",
      "REST",
      "API",
      "Services oriented architecture",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Arquitetura orientada a serviços para suporte a um sistema de agendamentos online",
    "autor": "Pereira, Nuno Miguel de Lima",
    "data": "2016",
    "abstract": "A necessidade das empresas evoluírem as suas aplicações por forma a disponibilizarem mais recursos aos seus\nutilizadores é uma realidade da atualidade. A disponibilização de informação em tempo real é cada vez mais necessária,\nmesmo que isso implique a interação entre sistemas distintos, o que exige que essa comunicação seja completamente\nagnóstica de tecnologias.\nSendo uma das premissas da Q-Better - empresa que permitiu o desenvolvimento desta dissertação em contexto\nempresarial - proporcionar aos seus clientes uma melhor experiência de utilização aliado ao acompanhamento da\nevolução tecnológica, tornou-se imperativo a conceção de uma arquitetura que fornecesse suporte ao desenvolvimento\nde novas aplicações e também às já desenvolvidas, ainda que para tal seja necessária uma reformulação das mesmas.\nInicialmente foi feito um estudo sobre a temática das arquiteturas orientadas a serviços, incluindo os vários tipos\nde web services existentes, e também uma passagem pela temática da sincronização de dados para proporcionar a\nsincronização entre as várias aplicações da Q-Better.\nA viabilidade da solução final - uma arquitetura orientada a serviços composta por um conjunto de web services\nREST - foi testada com a criação da aplicação Bloom Appointments cujo objetivo passa pela gestão de agendamentos\na partir de qualquer dispositivo que tenha ligação à internet ou à rede onde o sistema esteja instalado.\nFoi possível concluir que a escolha deste tipo de arquitetura se revelou acertada, uma vez que além de permitir a\ninteroperabilidade entre os vários sistemas existentes na Q-Better, permite uma maior expansão não só da aplicação\nusada como case study, mas também de todo o legacy software e de futuras aplicações."
  },
  {
    "keywords": [
      "Reinforcement learning",
      "Quantum computing",
      "Variational quantum circuits",
      "Neural networks",
      "Computação quântica",
      "Circuitos variacionais quânticos",
      "Redes neuronais",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Tradeoff between moving targets, gradient magnitude and performance in quantum variational Q-Learning",
    "autor": "Coelho, Rodrigo da Silva Gomes Peres",
    "data": "2023-10-20",
    "abstract": "Reinforcement Learning (RL) consists of designing agents that make intelligent decisions without human\nsupervision. When used alongside function approximators such as Neural Networks (NNs), RL is capable of\nsolving extremely complex problems. Deep Q-Learning, a RL algorithm that uses Deep NNs, even achieved\nsuper-human performance in some specific tasks. Nonetheless, it is also possible to use Variational\nQuantum Circuits (VQCs) as function approximators in RL algorithms. This work empirically studies the\nperformance and trainability of such VQC-based Deep Q-Learning models in OpenAI’s gym CartPole-v0\nand Acrobot-v1 environments. More specifically, we research how data re-uploading affects both these\nmetrics. We show that the magnitude and the variance of the gradients of these models remain substantial\nthroughout training due to the moving targets of Deep Q-Learning. Moreover, we show that increasing the\nnumber of qubits does not lead to a decrease in the magnitude and variance of the gradients, unlike what\nwas expected due to the Barren Plateau Phenomenon. This hints at the possibility of VQCs being specially\nadequate for being used as function approximators in such a context. We also use the Universal Quantum\nClassifier as a function approximator in VQC-based Deep Q-Learning and implement VQC-based models\ncapable of achieving considerable performance in the Acrobot-v1 environment, a previously untapped\nenvironment for VQCs."
  },
  {
    "keywords": [
      "681.3:61",
      "61:681.3"
    ],
    "titulo": "Monitorização e prevenção em plataformas de interoperabilidade hospitalar",
    "autor": "Marins, Fernando de Abreu",
    "data": "2013",
    "abstract": "A implementação da interoperabilidade nos Sistemas de Informação Hospitalar\n(SIH) é cada vez mais um requisito e não uma opção. A Agência\npara a Integração, Difusão e Arquivo de Informação Médica e Clínica (AIDA)\nconsiste numa plataforma de interoperabilidade hospitalar desenvolvida por\ninvestigadores da Universidade do Minho e que se encontra instalada no Centro\nHospitalar do Porto (CHP). A AIDA assegura a interoperabilidade entre\nos SIH e para alémdisto, assegura também a confidencialidade, integridade e\ndisponibilidade dos dados. A AIDA deve possuir um elevado nível de disponibilidade\ne um funcionamento eficiente 24 horas por dia. Um pequeno período\nde paragem poderá trazer graves consequências para a qualidade dos serviços\nprestados. Esta plataforma possui mecanismos de recuperação e tolerância\nde falhas, contudo devido à sua elevada importância, é preciso agir antes da\nocorrência das falhas, evitando sérios danos. Os processos de monitorização\ne prevenção de falhas devem ser implementados nos “órgãos vitais” da AIDA,\nque são as base de dados, máquinas e agentes inteligentes.\nUma vez que a prevenção de falhas em base de dados da AIDA já ter\nsido alvo de estudo, esta dissertação aborda a monitorização e prevenção de\nfalhas nas máquinas e agentes. Para prever as falhas, foram criados modelos\nbaseados no Modified Early Warning Score (MEWS). Este modelo através da\nrecolha frequente dos valores dos sinais vitais, calcula um conjunto de scores\npara determinar o nível de risco a que o paciente está submetido.\nForam desenvolvidos sistemas de monitorização de prevenção para as máquinas\ne agentes que permitem não só prevenir falhas, mas também observar\ne avaliar o comportamento destes componentes através de dashboards de monitorização.\nA prevenção de falhas nos agentes foi baseada na frequência com\nque estes registam as suas atividades nos seus ficheiros log, enquanto que para\nas máquinas a prevenção foi baseada em indicadores de desempenho como a\nmemória e o CPU. Apurou-se que os componentes, em geral, encontram-se\ncom os seus principais recursos bem balanceados e que os sistemas de prevenção\ndesenvolvidos detetaram situações críticas com sucesso, contribuindo\npara um aumento da integridade e disponibilidade da AIDA do CHP."
  },
  {
    "keywords": [
      "Linear programming",
      "Operations research",
      "Optimization",
      "Block-based languages",
      "Visual languages",
      "Blockly",
      "Programação linear",
      "Investigação operacional",
      "Linguagens de blocos",
      "Linguagens visuais",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "LPBlocks: a block-based language for linear programming",
    "autor": "Gião, Hugo Afonso da",
    "data": "2022",
    "abstract": "Linear programming is a mathematical optimization technique used in numerous fields including mathematics,\neconomics, and computer science, with numerous industrial contexts, including solving optimization\nproblems such as planning routes, allocating resources, and creating schedules. As a result of its wide\nbreadth of applications, a considerable amount of its user base lacks programming knowledge and experience\nand thus often resorts to using graphical software such as Microsoft Excel. However, despite its\npopularity amongst less technical users, the methodologies used by these tools are often ad-hoc and prone\nto errors.\nBlock-based languages have been successfully used to aid novice programmers and even children in\nprogramming. Thus, we created a block-based programming language termed LPBlocks that allows users\nto create linear programming models using data contained in spreadsheets. This language guides the users\nto write syntactically and semantically correct programs and thus aids them in a way that current languages\ndo not. We have also implemented a web application where users can define linear programming models,\nreactively see their mathematical representation and execute them to obtain the optimization values for\nthe variables defined by the users.\nTo assess the applicability of LPBlocks we used it to successfully express numerous and varied linear\nprogramming problems. Furthermore, we designed and ran a qualitative empirical study to understand the\nexperience our tool and language brings to users from various backgrounds. Although we see differences\namongst the users, most of them were able to model several problems using LPBlocks."
  },
  {
    "keywords": [
      "Biomarcador",
      "Deep learning",
      "Resistência a antibióticos",
      "Resistoma",
      "Antibiotic resistance",
      "Biomarker",
      "Deep learning",
      "Resistome",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Metagenomic wastewater and freshwater  core resistome analysis",
    "autor": "Cachetas, Diogo Macedo",
    "data": "2022-12-07",
    "abstract": "A escassez de água é atualmente uma grande preocupação. A reutilização de águas \nresiduais tratadas, seja por descarga em ambientes hídricos (por exemplo, rios) ou pela \nutilização em irrigação, é apontada como umas das principais soluções. No entanto, é \nimportante monitorizar o possível impacto desta reutilização, sobretudo ao nível da \ndisseminação de contaminantes emergentes como as bactérias resistentes a antibióticos \n(ARB) e os seus genes de resistência a antibióticos (ARGs). Este estudo teve como objetivo \ndeterminar e comparar os resistomas de diferentes amostras de água (afluente, lamas \nativadas, efluente e água doce), com base em metagenomas de diferentes geografias, de\nbases de dados públicas. O objetivo final foi identificar padrões e caraterísticas distintas entre \namostras. Estes permitirão identificar ARGs como possíveis biomarcadores para monitorizar a \ncontaminação de ambientes aquáticos com agentes biológicos de origem antropogénica.\nNo total, 139 metagenomas (30 afluente, 30 lamas ativadas, 21 efluente, 58 de água \ndoce) de 24 países foram analisados, usando métodos baseados em assembly e em reads. Os \nresultados mostraram que diferentes tipos de água partilham um grande número de ARGs. \nUma nova abordagem foi usada para combinar a anotação de duas das bases de dados de \nARGs mais abrangentes (CARD e ResFinder), superando a dificuldade que é lidar com \nanotações distintas provenientes de bases de dados diferentes. Esta abordagem permitiu \ndeterminar o resistoma core dos diferentes tipos de água, com o objetivo de obter genes \nbiomarcadores para rastrear a contaminação em termos de resistência a antibióticos \nprovocado pela descarga de águas residuais em ambientes recetores, como água doce. No \nfinal foram obtidos 60 possíveis biomarcadores, para os quais foram desenhadas sequências \nconsenso que poderão ser usadas, por exemplo, para o desenho de primers.\nAlém disso, 7 modelos de deep learning foram desenvolvidos para classificar a \ntransferibilidade de ARGs (genes adquiridos versus intrínsecos), dada a falta de informação \nsobre transferibilidade. Esta distinção é muito importante quer na monitorização quer na \npredição do risco, visto que os ARGs adquiridos são mais propensos à disseminação entre \nbactérias. O modelo de Redes Neurais Convolucionais superou os restantes com destaque\n(MCC de 0.881 e ROC-AUC de 0.906), o que é considerado um desempenho consistente."
  },
  {
    "keywords": [
      "Social networks",
      "Intelligent techniques",
      "Social influence",
      "Redes sociais",
      "Técnicas inteligentes",
      "Influência social",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "A data analysis approach to study events’ influence in social networks",
    "autor": "Carvalho, Nuno Miguel Vilela",
    "data": "2018",
    "abstract": "Nowadays, the assimilation of web content, by each individual, has a considerable impact\non our’ everyday life.\nWith the undeniable success of online social networks and microblogs, such as Facebook,\nInstagram and Twitter, the phenomenon of influence exerted by users of such platforms\non other users, and how it propagates in the network, has been attracting, for some years\ncomputer scientists, information technicians, and marketing specialists.\nIncreased connectivity, multi-model access and the rise of social media shortened the\ndistance between almost every person in the world, more and more content is generated.\nExtracting and analyzing a significant amount of data is not a trivial task, Big Data techniques\nare essential.\nThrough the analysis of this interaction, an exchange of information and feelings, it is\nentirely imaginable its usefulness in understanding complex human behaviours and so,\nhelp diverse organization’s decision-making. Influence maximization and viral marketing\nare among the possibilities.\nThis work is intended to study what is the impact and role that an event’s social influence\nhas and how does it propagate, particularly on its surrounding territory. This influence is\ninferred by analysis of the online platform’s data, by applying intelligent techniques, right\nafter its extraction. The final step is to validate the results with data from different sources.\nHelping businesses through actionable and valuable knowledge is the ultimate goal.\nThis document contemplates an introductory section where the study subject and its\nState of the Art are addressed. Next, the problem and what direction to take to solve it are\ndiscussed."
  },
  {
    "keywords": [
      "Database tuning",
      "Machine learning",
      "Query optimization",
      "Aprendizagem automática",
      "Otimização de queries",
      "Tuning de base de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Query optimizers based on machine learning techniques",
    "autor": "Souto, Rui Pedro Sousa Rodrigues do",
    "data": "2021-10-27",
    "abstract": "Query optimizers are considered one of the most relevant and sophisticated components\nin a database management system. However, despite currently producing nearly optimal\nresults, optimizers rely on statistical estimates and heuristics to reduce the search space\nof alternative execution plans for a single query. As a result, for more complex queries,\nerrors may grow exponentially, often translating into sub-optimal plans resulting in less\nthan ideal performance. Recent advances in machine learning techniques have opened\nnew opportunities for many of the existing problems related to system optimization.\nThis document proposes a solution built on top of PostgreSQL that learns to select\nthe most efficient set of optimizer strategy settings for a particular query. Instead of\ndepending entirely on the optimizer’s estimates to compare different plans under different\nconfigurations, it relies on a greedy selection algorithm that supports several types of\npredictive modeling techniques, from more traditional modeling techniques to a deep\nlearning approach.\nThe system is evaluated experimentally with the standard TPC-H and Join Order ing Benchmark workloads to measure the cost and benefits of adding machine learning\ncapabilities to traditional query optimizers."
  },
  {
    "keywords": [
      "Quantum simulation",
      "Magnetism",
      "Quantum entanglement",
      "Magnetic order",
      "Simulação quântica",
      "Magnetismo",
      "Entrelaçamento quântico",
      "Ordem magnética",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Quantum simulation of spin systems on quantum computers",
    "autor": "Reascos Valencia, Irving Leander",
    "data": "2023-12-15",
    "abstract": "Quantum simulation represents a formidable challenge for classical computers due to the intricate behavior of quantum systems. Digital quantum computers aim for precise approximations of a wide range of quantum systems. Within the realm of quantum simulation, the study of spin systems plays a pivotal role, providing insights into complex properties challenging to model through classical means. This work focuses on investigating chiral spin systems through the development of a dedicated quantum circuit for chirality measurement.\nIn this Masters Dissertation, we present an overview of the current state-of-the-art in quantum simulation of chiral spin systems and introduce our approach to addressing this challenge. Scalar spin chirality,\na three-body physical observable, holds a critical position both in classical magnetism, where it characterizes non-coplanar spin textures, and in quantum magnetism, serving as an order parameter for chiral spin liquids. In the context of quantum information, scalar spin chirality serves as a witness to genuine tripartite entanglement.\nIn this study, we delve into various methodologies to tackle the problem at hand, subjecting them to comparison. The objective is to identify the most suitable approach that demands fewer quantum resources. Our best proposed method introduces an indirect measurement scheme based on the Hadamard test, designed to estimate the scalar spin chirality for general quantum states. We apply this innovative approach to measure chirality in two specific types of quantum states: the generic one-magnon states of a ferromagnet and the ground state of a model characterized by competing symmetric and antisymmetric exchange interactions. Our research findings highlight the practicality of achieving a single-shot determination of scalar chirality for chirality eigenstates, leveraging the power of quantum phase estimation with a single auxiliary\nqutrit. This novel methodology extends beyond providing a solution to the chirality measurement problem; it also unifies the theory of chirality in both classical and quantum magnetism. The implications of our work offer valuable insights and pave the way for future quantum research endeavors in this domain."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Um sistema de controlo de acessos baseado no modelo cargo-organização",
    "autor": "Novais, José Pedro Vilaça",
    "data": "2011-11-20",
    "abstract": "O paradigma do controlo de acessos, em especial o controlo de acesso à informação, tem vindo a mudar nos últimos anos. Controlo este que inicialmente era efetuado pelas próprias aplicações de forma isolada e autónoma, sem a possibilidade de consultarem ou se integrarem com qualquer sistema centralizado. Todavia, com o crescente uso das tecnologias de informação nas organizações, novas soluções (tais como os serviços de diretoria LDAP) têm vindo a ser adotadas com o intuito de dar resposta à necessidade de uma política de acessos unificada e coesa, transversal aos diversos serviços e aplicações. Estas soluções representam uma mais-valia no desempenho das tarefas organizacionais.\n\nTendo em conta esta necessidade, este trabalho propõe uma nova solução para o controlo de acessos físicos e lógicos através da apresentação e implementação de um novo modelo de controlo de acessos baseado no par Cargo-Organização. É ainda apresentada e implementada neste projeto uma nova abordagem no controlo de acessos lógicos, sendo esta assim capaz de interagir e configurar aplicações que carecem do suporte de protocolos e mecanismos padrão para o controlo de acessos."
  },
  {
    "keywords": [
      "Humanidades digitais",
      "Ontologias OWL2",
      "Genealogia",
      "Inferência",
      "Prototipagem de aplicações web para ontologias",
      "Geração de formulários html",
      "RDFLib",
      "Processamento de linguagem natural",
      "Digital humanities",
      "OWL2 ontologies",
      "Genealogy",
      "Inference",
      "Web application prototyping for ontologies",
      "HTML form generation",
      "RDFLib",
      "Natural language processing",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Ancestors Note Book: uma aplicação para memorabilia e genealogia, baseada em ontologias",
    "autor": "Grenhas, João Manuel Pós de Mina",
    "data": "2022",
    "abstract": "O tema desta dissertação de mestrado desenrolou-se no âmbito das humanidades\ndigitais, pretendendo desenhar e construir um toolkit de apoio ao registo e processamento\nde histórias de família, micro-história, documentos, fotografias, elementos genealógicos e\nhistórias de instituição, com especial interesse na inferência de indivíduos. Foi desenvolvido\num protótipo de aplicação web de uso pessoal, sobre Flask/Python, configurável (e\nreutilizável, salvo ajustes), como prova de conceito a suportar uma ontologia OWL2 sobre o\ndomínio do problema.\nANB = Ancestors Note Book =\nOntologia AncestorsNB, para\nMemorabilia e Genealogia, Relações humanas e Ambiente\n+ Filosofia Wiki\n+ Inferência Interativa e Dinâmica\n+ SPARQL\n+ Geração de formulários\nO domínio do problema induziu naturalmente uma arquitetura de ontologia, à qual foi\ndada persistência num ficheiro de texto em sintaxe Turtle, carregada pela aplicação num\ngrafo RDFLib. A ontologia é expansível, mas já inclui à partida relacionamentos de genealogia\nde pessoas, com diversos graus de parentesco, e ainda relacionamento no âmbito social,\ninstitucional e geográfico, para pessoas, organizações e lugares. Ainda no âmbito da\nmemorabilia, prevê-se o registo e referência de eventos, fotografias, documentos,\nmultimédia, histórias de família, artigos sobre qualquer assunto.\nNa aplicação, acolhe-se uma filosofia Wiki, no sentido em que proporciona uma\napresentação gráfica (suportada em mardkdown e html), navegação por hiperligações\n(internas ou externas), e edição. Em virtualmente qualquer classe ontológica, a apresentação\nprettyprint de indivíduos e os formulários de ingestão permitem templating e alguma\nconfiguração. A apresentação prettyprint prevê documentos de variada morfologia, incluindo\ntexto, PDF, markdown e multimédia.\nPermite-se ao utilizador a ingestão de informação em lote (na sintaxe Turtle, por edição\ndireta ou via ficheiro), e CRUD interativo (via formulários ou SPARQL), num ambiente\noperacional rico, com pesquisas de utilizador combinadas com navegação, além de facilitar\na inferência sobre indivíduos, sejam pessoas ou não.\nA inferência é interativa (i.e., a pedido interativo do utilizador) e/ou dinâmica (por\netiquetagem no conteúdo de um campo configurado como elegível para este efeito),\njogando com classes, ID de indivíduo, nomes e datas. A inferência pode ter posicionamento\ngenealógico: basta que seja encontrado um grau de parentesco. Tanto a inferência como os\nmotores de pesquisa interativa se baseiam num tratamento rico de nomes e moradas em\nPortuguês, em que a normalização prevê grafias antigas, além de acentos e partículas de\nligação (de, da, do, etc.)."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Desenvolvimento de um Chatbot para apoio clínico",
    "autor": "Dias, André Portela de Lima",
    "data": "2018",
    "abstract": "Os avanços recentes das tecnologias de inteligência artificial e de processamento de dados mudaram radicalmente o paradigma do setor de saúde, dando origem a soluções digitais que prometem transformar os vários processos clínicos, permitindo um aumento da sua eficiência e qualidade enquanto capazes de reduzir os custos a eles associados.\nCom os profissionais de saúde a enfrentarem diariamente o problema de possuírem recursos limitados, fazendo com que não sejam capazes de monitorzar e apoiar diariamente todos os seus pacientes, cada vez mais se torna importante o desenvolvimento de alternativas válidas e fidignas, capazes de ajudar os vários pacientes, no mínimo tempo possível.\nUma das soluções mais adotadas de maneira a solucionar o problema referido, reside no desenvolvimento de sistemas conversacionais, comumente chamados de Chatbots, que se assumem como capazes de efetuar o esclarecimento de questões de âmbito clínico, incorporando uma função semelhante a um assistente virtual e preenchendo, desta forma, a lacuna existente na comunicação entre os vários pacientes e os profissionais de saúde.\nEsta dissertação tem como foco, a sugestão de uma arquitetura relativa a um sistema conversacional com o objetivo de efetuar aconselhamento psiquiátrico. O Chatbot proposto tem o nome de YEC, acrónimo em inglês para “Your Everyday Companion”, representativo de uma abordagem híbrida, pela combinação de técnicas de processamento de linguagem natural e de um modelo Deep Learning, para a geração da sua resposta. O sistema é desenhado para efetuar tratamento diferenciado por utilizador, permitindo desta forma a inferência do seu estado emocional, bem como do estabelecimento de um grau elevado de confiança e proximidade.\nDe forma a provar que o sistema apresentado na teoria, representa uma solução prática viável, procedeu-se ao desenvolvimento de uma fase primária do motor conversacional presente no sistema, expondo as diferentes abordagens realizadas de forma a que, pela análise dos seus resultados, fosse possível inferir sobre a sua melhor implementação.\nA realização desta dissertação permitiu assim concluír acerca do poder inerente à combinação de técnicas de DL e de NLP para modelação conversacional, aferindo assim da sua capacidade para ajudar a resolver os diferentes problemas clínicos observados nos dias de hoje, sendo no entanto necessário mais investigação de modo a enfatizar esta afirmação."
  },
  {
    "keywords": [
      "Telemonitorização",
      "eHealth",
      "Micro-serviço",
      "SmartAL",
      "Telemonitoring",
      "Microservices",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "e-Saúde - design e desenvolvimento de serviços web de telemonitorização",
    "autor": "Preto, Carlos João Teixeira",
    "data": "2023-11-27",
    "abstract": "A área da eHealth tem ganho uma enorme importância nos últimos anos. Tal advém não só da progressiva evolução tecnológica que se tem verificado ao longo dos tempos, mas também da necessidade de melhorar a qualidade de vida das pessoas que necessitam de assistência médica permanente ou ocasional. Mais especificamente, a telemonitorização permite aos pacientes estarem mais envolvidos no seu processo de recuperação e agirem mais rapidamente quando surgem novos problemas ou complicações. Neste tipo de serviços, existem geralmente três entidades, o paciente, o cuidador e os dispositivos tecnológicos, que funcionam como facilitadores entre ambos. Atualmente, existem cada vez mais serviços\nde telemonitorização, mas, na maioria dos casos, estes serviços seguem uma abordagem genérica, que pode ser demasiado complexa para alguns utilizadores. Tal deve-se ao facto de incluírem por vezes funcionalidades desnecessárias para certos doentes, o que não lhes permite a melhor experiência de navegação e utilização. No âmbito da dissertação, desenvolveu-se uma aplicação web que permite aos utentes registar as suas medições através de dispositivos Bluetooth, e responder a questionários providenciados pelos\nseus médicos. Quer as medições, quer as respostas aos questionários poderão ser consultadas por parte dos médicos dos pacientes, permitindo assim, tal como pretendido, uma melhor monitorização e controlo do estado de saúde dos pacientes. Esta aplicação pretende ser mais simples que as restantes, utilizando serviços modulares, para que os pacientes possam escolher o que querem utilizar, de acordo com o seu perfil, sempre e quando precisarem. Cada funcionalidade principal da aplicação pode ser vista como um micro-serviço independente. Adicionalmente, o projeto segue as mais recentes abordagens tecnológicas e arquiteturais. Em termos de interface e usabilidade, procurou-se desenvolver uma aplicação atrativa e intuitiva. Por fim, é importante mencionar que o presente projeto se enquadra no âmbito de desenvolvimento da solução de telemonitorização da Altice Labs, denominada SmartAL, que permite recolher dados do paciente a partir de dispositivos inteligentes e questionários."
  },
  {
    "keywords": [
      "Internet",
      "ISP",
      "Partilha de conteúdos",
      "P2P",
      "Rede overlay",
      "Content sharing",
      "Overlay network",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Uma rede overlay controlada pelo ISP para partilha de conteúdos",
    "autor": "Silva, Miguel Gil Pires da",
    "data": "2019",
    "abstract": "Atualmente, vivemos numa era extremamente tecnológica e exigente, traduzindo-se no\nincremento das necessidades dos utilizadores da Internet, face aos serviços que esta lhes\ndisponibiliza. Neste sentido, existe uma crescente preocupação em desenvolver novos sistemas\nde rede virtuais, como é o caso das redes overlay, muitas delas ligadas à distribuição\nde conteúdos. Porém, como consequência deste crescimento adoptivo das redes overlay, as\ninfraestruturas dos fornecedores de rede (Internet Service Provider (ISP)) são sobrecarregadas\ncom tráfego Peer-to-Peer (P2P), produzindo maiores dificuldades em relação à sua\nadministração. Assim, esta dissertação apresenta como objetivo primário conceber e promover\no uso de uma rede overlay controlada pelo ISP, que disponha de mecanismos de\ncontrolo de tráfego P2P, e que, em simultâneo, possibilite a oferta de um serviço de partilha\nde ficheiros aos seus clientes. Como tal, foi especificada uma arquitetura de rede\nbaseada no paradigma P2P, para suportar funcionalidades colaborativas com o ISP, sendo\ncomposta por quatro entidades principais: peer de acesso, peer de encaminhamento, coordenador\ne ISP. A partir desta, implementaram-se quatro aplicações correspondentes a cada\numa das entidades, tendo sido criadas interfaces gráficas direcionadas para o ISP e para\nos clientes (peers de acesso). Foram implementados mecanismos básicos de partilha de\nconteúdos com suporte de dois modos de transferência (cliente-servidor e P2P), além de\nmecanismos de proteção e priorização de links/routers críticos (com a possibilidade de planeamento\npor datas), limitação de circulação de conteúdos na rede overlay e suporte a duas\nestratégias distintas de encaminhamento aplicacional, como principais medidas de apoio\nao ISP. Posteriormente, a partir do emulador de rede Common Open Research Emulator\n(CORE), foram criados cenários distintos de teste, relativos aos mecanismos de colaboração\ncom o ISP implementados. Nestes, além de ser analisado o tráfego transmitido por router,\nde modo a verificar os impactos dos mecanismos colaborativos com o ISP, foi efetuada\na análise das rotas percorridas na rede física/aplicacional. Por fim, a partir dos cenários\nde teste efetuados, assume-se que os mecanismos de controlo de tráfego P2P, não afetam\nsignificativamente a qualidade do sistema da rede overlay, facilitando ao mesmo tempo as\ntarefas de administração do ISP."
  },
  {
    "keywords": [
      "Blockchain",
      "NFTs",
      "Hyperledger fabric",
      "Digital certificates",
      "Smart contract",
      "Certificados digitais",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Geração de NFTs para certificação de documentos",
    "autor": "Alves, Tiago Araújo",
    "data": "2023-11-27",
    "abstract": "Non-Fungible Tokens have emerged as a trend in recent times and have aroused interest among developers\nand companies due to their potential in a wide range of application areas. This dissertation arose from\nthe initiative of the company dstelecom, and its main objective is to study the use of NFTs in the context of\ndocument certification and the development of a software module that issues certificates based on NFTs,\nthus providing a viable alternative to the methods currently used to issue certificates.\nIn order to meet the objectives of the dissertation, it was divided into several stages, starting with the\nstudy of blockchain and NFT technologies, application examples and related work. Since creating and\nmanaging NFTs requires a blockchain base, one of the important decisions was choosing the blockchain\nplatform that would support the creation and maintenance of NFTs.\nOnce the technological decisions had been made, the next step was to implement a software module\nwhich, in the future, could be used as a tool for managing and issuing certificates for documents based\non NFTs."
  },
  {
    "keywords": [
      "Software system",
      "Logistics",
      "Systems integration",
      "Track&Trace",
      "Monitoring system",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of a software system for monitoring outbound logistics",
    "autor": "Loureiro, Beatriz Ribeiro Pires",
    "data": "2019",
    "abstract": "Currently, Information Technology (IT) is a driving factor in the process of globalization and\nin the innovative use of resources to promote new products and ideas, creating efficient and\neffective channels to exchange information.\nProducts based upon, or enhanced by, technology are used in nearly every aspect of life\nin contemporary industrial societies. Business productivity software ensures that organizations\nhave the tools to overcome the challenges of executing on strategy every day and\nprosper in an increasingly challenging era. IT has been the catalyst for global integration,\nthus it is hard to imagine a company that dispenses the support of these services.\nTherefore, IT allows the return of investment of all the logistics businesses, shipping\nscheduling, procurement of materials and suppliers. The use of IT tools is considered a\ncompetitive advantage, taking companies to invest in research and development in order to\nincrease profits, thus influencing their strategy and organizational model.\nLogistics is the process of planning, implementing, and control the efficient, cost effective\nflow and storage of raw materials, in-process inventory, finished goods and related\ninformation from point of origin to point of consumption, for the purpose of conforming\nto customer requirements. This project looks at the integration of logistics with IT, since\ninformation is a key element of logistics.\nThe dissertation proposal presented in this document has as main goal, the development\nof a continuous Finished Good (FG) monitoring system, also known as outbound logistics.\nGiven the responsibility and importance of such system different techniques and methodologies\nare carefully addressed. Techniques necessary to ensure the reliability and the correct\norganization of information present in the system, as well as a projection for expansion\nin the near future.\nAn intelligent environment is proposed that is able to track and provide real-time information\nrelated to FG in transit. The major focus is that the user should not be forced\nto actively search for deviations in the supply chain,but be able to pursue other tasks,and\nstill be notified of any change in the transportation process. To develop this work a prototype\nwas devised on which the behaviour and external system interactions were thoroughly\ntested and validated. The result of the project is a system capable of monitoring FG in transit,\nas well as provide updated information to the users in order to better predict or assess\nany deviations."
  },
  {
    "keywords": [
      "Artificial Intelligence",
      "Machine learning",
      "Explainable AI",
      "Fraud detection",
      "Human behaviour",
      "Inteligência Artificial",
      "Deteção de fraude",
      "Comportamento Humano",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Explainable AI in fraud detection based on human behaviour",
    "autor": "Barbosa, Pedro Miguel de Soveral Pacheco",
    "data": "2023-12-07",
    "abstract": "In recent years, Artificial Intelligence has prospered to such an extent that it is present in our daily lives\nthrough the different technologies we use. This fact is evident, for example, in fraud detection in online\npurchases or games, which has been increasingly precise and has undergone an exponential evolution\nas never seen before. However, the use of decision-making oriented by Artificial Intelligence and Machine\nLearning generates transparency concerns. Moreover, the need for explainability arises when Machine\nLearning models are manipulated in order to ensure fairness or to deliberate on decisions of greater\nimportance. This paper proposes an approach to the general problem of explaining the decisions made by Machine Learning models through the use of Explainable AI methods. The main goal is to develop a decision support solution applied to fraud detection based on human behaviour by exploring techniques that can help explain the results of Machine Learning models and also make them more transparent."
  },
  {
    "keywords": [
      "Raciocínio baseado em casos",
      "Redes neuronais",
      "Sistemas inteligentes",
      "Tarefas",
      "Equipas",
      "681.3"
    ],
    "titulo": "Gestão inteligente de tarefas: atribuição de tarefas numa equipa",
    "autor": "Silva, António Oliveira da",
    "data": "2014-12-05",
    "abstract": "O processo de tomada de decisão revela-se um fator cada vez mais diferenciador nas empresas atuais. Sejam elas empresas diretamente relacionadas com as Tecnologias de Informação, sejam empresas de outros sectores. Nesse aspeto, a recolha e tratamento de informação crítica a cada atividade, pode ser bastante útil para ser usado no processo de tomada de decisão, sendo sempre necessário avaliar até que ponto pode a informação ser válida e qual a sua qualidade e importância no processo em que é necessária.\nRecorrendo a um sistema de Case-based reasoning tentar-se-á criar-se um sistema que de forma autónoma aprenda a melhor maneira de atribuir tarefas dentro de uma equipa multifacetada, aprendendo a discernir quais os colabores que melhor se adaptam a cada tarefa e cujos resultados podem ser melhores para a empresa. Esse mesmo sistema também estará apto para lidar com equipas dinâmicas aproveitando os melhores recursos que tem em cada altura."
  },
  {
    "keywords": [
      "Software verification",
      "Model checking",
      "Safety",
      "Robotics",
      "Electrum",
      "ROS",
      "HAROS",
      "Verificação de software",
      "Robótica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Analysis of message passing software using electrum",
    "autor": "Carvalho, Bruno Renato Fernandes",
    "data": "2020-11-13",
    "abstract": "Automation developments are enabling industrial restructuring through the incorporation\nof more efficient and accurate processes with less associated cost. Consequently, robots are\nbeing increasingly used in the most various scenarios, including in Safety Critical domains.\nIn such cases, the use of suitable methods to attest both the system’s quality and their safety\nis absolutely essential.\nFollowing the current increase of complexity of cyber-physical systems, safety guards\nwhich used to be fully hardware dependent, are constantly migrating to software. Here upon, middleware software to abstract systems hardware are constantly evolving and are\nbeing increasingly adopted. The common feature of these systems is usually associated with\nits modular architectures based on message-passing communication patterns. A notorious\ncase is the ROS middleware, where highly configurable robots are usually built by composing\nthird-party modules. The verification of such systems is usually very hard, and its implemen tation in real industrial environments is, in most cases, impracticable. To promote adoption,\nthis work advocates the use of lightweight formal methods associated with semi-automatic\ntechniques that require minimal user input and provide valuable intuitive feedback.\nThis work explores and proposes a technique to automatically verify system-wide safety\nproperties of ROS-based applications in continuous integration environments. It is based\non the formalization of ROS architectural models and nodes behaviours in Electrum, a\nspecification language of first-order temporal logic supported by a model-finder over which,\nsystem-wide properties are subsequently model-checked. In order to automate the analysis,\nthe technique is deployed as an HAROS plug-in, a framework for quality assessment of ROS\nsoftware, specially aimed to its community.\nThe technique proposal and its implementation under the HAROS framework are eval uated with positive results on a real agricultural robot, AgRobV16, whose dimension and\ncomplexity are industrially representative."
  },
  {
    "keywords": [
      "Smart home",
      "SmartAL",
      "eHealth",
      "Assisted living",
      "Telemonitorização",
      "Telemonitoring",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Implementação de serviços Confort@Home",
    "autor": "Magalhães, Carlos Miguel Azevedo",
    "data": "2022-12-13",
    "abstract": "A área de eHealth/Assisted Living tem vindo a crescer com a necessidade de melhorar a qualidade de vida de pessoas que precisam de acompanhamento médico permanente (e.g., doença crónica) ou ocasional (e.g., pós-operatório, pandemia), de forma presencial ou à distância. No entanto, a tendência é manter as pessoas em casa o máximo tempo possível e não nas instituições, evitando o perigo de contaminação, precavendo a rutura dos serviços de saúde, e garantindo simultaneamente maior conforto e bem-estar ao utente. Contudo, estas pessoas precisam de ser acompanhadas e ter acesso a serviços de saúde de qualidade, tal como se estivessem nas instituições. Os serviços de telemedicina e telemonitorização servem este propósito, permitindo a cuidadores formais e/ou informais estar em contacto com os seus doentes, acompanhá-los remotamente em tempo real e prevenir situações de maior risco, agindo rapidamente em casos de urgência. No âmbito específico da telemonitorização, esta pode ser clínica e/ou não clínica, mais focada no controlo de sinais vitais ou apenas no acompanhamento da pessoa com base noutro tipo de informação pessoal (e.g., atividade, sono, localização), respetivamente. No caso particular desta dissertação, o objetivo é desenhar, especificar e implementar um serviço que ofereça conforto e segurança usando essencialmente informação não clínica e assegure o acompanhamento remoto de pessoas de idade avançada que vivam ou passem muito tempo sozinhas. A necessidade de criação de uma ferramenta deste tipo deve-se ao facto de, cada vez mais, se pretender apostar na prevenção e segurança das pessoas, sobretudo das pessoas de idade que se encontram em situações de alguma fragilidade (e.g., por doença, por isolamento), mas simultaneamente tentar passar um sentimento de companhia, mesmo à distância, procurando preservar ao máximo a sua autonomia. Neste contexto, o projeto pretende explorar cenários mistos de telemonitorização, através de informação proveniente de sensores ligados à casa edispositivos wearables, criando o conceito de Confort@Home, ou seja, um conceito de saúde e bem-estar centrado na pessoa, na sua casa e no ambiente circundante. A oferta de serviços a considerar incluirá in formação de localização dentro e fora de casa, dados adicionais provenientes de dispositivos a selecionar e notificações/alertas; na lógica da aplicação serão definidas as situações e condições que espelhem os possíveis riscos e falta de segurança do idoso, e enviados alertas para o seu cuidador/familiar, de forma a facilitar a vida de ambos no dia-a-dia. Como demonstração do conceito, será desenvolvida uma das vertentes do Confort@Home que se materializa numa aplicação Web fornecida em ambiente cloud que permita a cuidadores informais monitorizar os seus familiares/dependentes com base na sua localização."
  },
  {
    "keywords": [
      "681.3.06"
    ],
    "titulo": "Querying for model-driven spreadsheetsd",
    "autor": "Pereira, Rui Alexandre Afonso",
    "data": "2013-10-31",
    "abstract": "Spreadsheets are used for a diverse number of objectives, that range from simple applications\nto complete information systems. In all of these cases, they are frequently used as data\nrepositories that can grow tremendously in size, and as the amount of the data grows, the\nfrustration and challenge to withdraw information out of them also grows.\nThis Thesis project focuses on the problem of spreadsheet querying. Speci cally, the\nobjective is to meticulously and carefully study competing query languages, and proposing\nour very own expressive and composable query language to be used in spreadsheets,\nwhere intuitive queries can be de ned. This approach builds on a model-driven spreadsheet\ndevelopment environment, and queries are expressed referencing ClassSheet model entities\ninstead of the actual data. Furthermore, this language shall be integrated into the MDSheet\nframework, taking into account evolution mechanisms, auto-generation of models for query\nresults, and shall rely on Google's QUERY function for spreadsheets."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of a data integration pipeline for human metabolic models and databases",
    "autor": "Barbosa, Susana Raquel da Silva",
    "data": "2016",
    "abstract": "Systems Biology aims to integrate experimental and computational approaches with the purpose of explaining and predicting the organisms' behavior. The development of mathematical models in silico gives us a better in-depth knowledge of their biological mechanism. Bioinformatics tools enabled the integration of a large amount of complex biological data into computer models, but also capable to perform computational simulations with these models, that can predict the organisms' phenotypic behavior in different conditions.\nUp to date, genome-scale metabolic models (GSMMs) include several metabolic components of an organism. These are related to the metabolic capabilities encoded in the genome. In recent years, multiple GSMMs have been built by several research groups. With the increase in number, of these models, important issues regarding the standardization have arisen, a common problem is the different nomenclatures used by each of the research groups.\nIn this work, the major focus is to address these problems, specifically for the human GSSMs. Therefore, the two most recent human GSMMs were selected to go through a data integration process.\nIntegration strategies of these models most important entities (metabolites and reactions), were defined based on an exhaustive analysis of the models. The broad knowledge of their attributes enabled the creation of effective and efficient integration methods, supported by a core database developed in the local research group.\nThe final result of this work, is a unified repository of the human metabolism. It contains all the metabolites and reactions that were automatically integrated along with some manual curation."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Database replication in large scale systems",
    "autor": "Araújo, Miguel Gonçalves de",
    "data": "2011-07-29",
    "abstract": "There is nowadays an increasing need for database replication, as the construction of\nhigh performance, highly available, and large-scale applications depends on it to maintain\ndata synchronized across multiple servers and to achieve fault tolerance.\nA particularly popular approach, is the MySQL open source database management\nsystem and its built-in asynchronous replication mechanism. The limitations imposed\nby MySQL on replication topologies mean that data has to go through a number of hops\nor each server has to handle a large number of slaves. This is particularly worrisome\nwhen updates are accepted by multiple replicas and in large systems. Noting the most\ncommon topologies and taking into account the asynchrony referred, a problem arises,\nthe freshness of the data, i.e. the fact that the replicas do not have just the most recently\nwritten data. This problem contrasts with the state of the art in group communication.\nIn this context, the work presented in this Master’s thesis is the result of an evaluation\nof the models and mechanisms for group communication, as well as the practical advantages\nof group-based replication. The proposed solution extends the MySQL Proxy tool\nwith plugins combined with the Spread group communication system offering, transparently,\nactive and passive replication.\nFinally, to evaluate the proposed and implemented solution we used the reference\nworkload defined by the TPC-C benchmark, widely used to measure the performance\nof commercial databases. Under this specification, we have evaluated our proposal on\ndifferent scenarios and configurations"
  },
  {
    "keywords": [
      "Mobile development",
      "Terrain survey",
      "Geolocation",
      "Desenvolvimento de aplicações móveis",
      "Estudos de terreno",
      "Geolocalização",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Mobile app ”Telecom FTTx Survey”",
    "autor": "Oliveira, Pedro Manuel Gonçalves de",
    "data": "2023-11-27",
    "abstract": "Terrain surveys play an important role in the field of telecommunications. In order to expand a fiber\noptics network, it is fundamental to have knowledge of the area being considered for expansion, especially\nknowledge about existing structures, and possible limitations of that area. A terrain survey provides a way\nto collect this information, which can then be examined and acted upon. Survey results can often have\nissues depending on what tools and methods were used for the collection of data. The main goal of this\nwork is to develop a tool capable of efficiently, and correctly collecting georeferenced data, and outputing\nthat data in a way that is usable in the following stages of expansion of the fiber optics network."
  },
  {
    "keywords": [
      "Databases",
      "Streaming ingestion",
      "Stream processing",
      "AIS",
      "Maritime informatics",
      "Internet of things",
      "Continuous queries",
      "Apache Kafka",
      "Columnar database",
      "Bases de dados",
      "Ingestão streaming",
      "Processamento de streams",
      "Informática marítima",
      "Internet das coisas",
      "Queries contínuas",
      "Bases de dados Columnar",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Tagus: an IoT data ingestion pipeline for MonetDB",
    "autor": "Mota, Bernardo Braga Bastos",
    "data": "2021-10-27",
    "abstract": "In this project, we design and implement an IoT streaming data ingestion pipeline for\nMonetDB, using the distributed message queueing platform Apache Kafka. Our objective is\nto leverage MonetDB’s analytical power for IoT data, expanding its ingestion capabilities to\nimprove reliability and performance. The ingestion pipeline is put to the test with the real world maritime tracking system AIS. We also evaluate MonetDB’s current IoT processing\nengine and compare it to other state-of-the-art engines, to appraise its functionalities and\nidentify possible future improvements."
  },
  {
    "keywords": [
      "Bacteriophages",
      "Genome annotation",
      "Promoters",
      "Machine learning",
      "Models",
      "Bacteriófagos",
      "Anotação de genomas",
      "Promotores",
      "Aprendizagem máquina",
      "Modelos",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "PhagePromoter: phage promoters online analysis tool",
    "autor": "Sampaio, Marta Sofia Costa",
    "data": "2018",
    "abstract": "In the last decades, the emergence and evolution of the Next Generation Sequence technologies have revolutionised genomic research, leading to an exponential increase in the number of sequenced genomes. Many of the sequenced genomes belong to bacteriophages (phages), mostly due to their therapeutic potential against bacterial infections. This abundance of genomic data demands the creation of user-friendly bioinformatics tools for performing genome annotation. The most challenging step in phage genome annotation is the identification of regulatory elements, primarily promoters, to understand phage transcription regulation mechanisms.\nThus, in this work, PhagePromoter, a tool for promoter prediction in phage genomes, was developed, using machine learning methods. Several models were created using different datasets and machine learning algorithms, such as support vector machines (SVM), artificial neural networks (ANN) and Random Forests (RF). All models were tested using a 5-fold cross-validation process. The datasets were composed by known phage promoter sequences, mainly retrieved from the phiSITE database, and by a different number of negative cases. After optimization, the performance was similar for all models and two were selected to be integrated in the tool: the ANN model created with the dataset containing 1600 negative examples and the SVM model created with the dataset containing 2400 negatives. The ANN model presented 92% of accuracy, 89% of precision and 87% of recall, whereas the SVM model presented 93% of accuracy, 91% of precision and 80% of recall. Hence, the first model will predict more sequences as promoters and may lead to more false positives. The SVM model will return few positive results, but most of them will be correct classified while some real promoters may not be identified by the model.\nPhagePromoter was integrated in the widely used Galaxy framework, available at https://galaxy.bio.di.uminho.pt/?tool_id=get_proms&version=0.1.0&__identifer=4u05obc3o5w, which provides a graphical user interface. This tool returns better results when compared to other tools, such as BPROM, PromoterHunter and CNNpromoter_e."
  },
  {
    "keywords": [
      "IaaS",
      "Cloud privada",
      "Automação",
      "Scripting",
      "Otimização",
      "Private Cloud",
      "Automation",
      "Optimization",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desenho e implementação de processos  de automação de IaaS em ambientes Cloud",
    "autor": "Ferreira, João Pedro de Vasconcelos Cadavez",
    "data": "2022-12-28",
    "abstract": "Com o crescimento exponencial dos serviços hospedados em ambientes digitais \ne consecutivamente com o aumento da criticidade dos mesmos, tem-se verificado uma \nprocura constante por parte de desenvolvedores e gestores de projetos por plataformas\nque disponibilizem agilidade, flexibilidade e baixa complexidade para a disponibilização \nao público das suas soluções o mais rápido possível com o mínimo de esforço por parte \ndos mesmos. \nEste trabalho tem como objetivo precípuo o estudo e implementação de uma \nplataforma de IaaS em Cloud privada, com a finalidade de disponibilizar a várias \nentidades da área da saúde uma plataforma centralizada de gestão de ativos de \ncomputação e de rede, com o intuito de facilitar, perante o paradigma passado, a \nlogística associada à criação e coordenação dos ativos por parte das mesmas. Para tal o \npresente trabalho propõe um estudo de mercado, seguido de uma análise de formas e \nplataformas de automação de processos a serem implementados intrinsecamente e/ou \nextrinsecamente à plataforma de Cloud privada, de modo a trabalharem em simbiose. \nSão também apresentadas metodologias de desenho de scripting necessário para a \nrealização dos casos de uso propostos pela entidade SPMS, assim como o processo \nutilizado para a integração da solução com plataformas e serviços terceiros.\nCom este trabalho intenciona-se proporcionar à SPMS a otimização dos seus \nrecursos computacionais e de rede, bem como diminuir drasticamente as horas \nhumanas dedicadas a processos repetitivos e iterativos, canalizando-as para processos \nmais nobres."
  },
  {
    "keywords": [
      "Blockchain",
      "Hyperledger fabric",
      "Monitorização de rede",
      "Network monitoring",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aplicação de monitorização de rede baseada em Blockchain",
    "autor": "Amorim, João Manuel Silva de",
    "data": "2023-11-27",
    "abstract": "Na era atual dos avanços tecnológicos, a blockchain emergiu como uma grande inovação, ganhando reconhecimento generalizado pelas suas capacidades de manutenção de registos seguros e transparentes.\nAo mesmo tempo, a gestão e a comunicação eficazes de dados críticos da infraestrutura de tecnologias\nda informação (TI) continuam a ser uma necessidade premente. Esta dissertação aborda a intersecção\ndestes domínios, com o objetivo de aproveitar o potencial da tecnologia blockchain para otimizar e proteger os processos de comunicação de dados. Patrocinada pela dstelecom, um operador de rede de fibra\nótica neutra em Portugal, esta investigação investiga o papel transformador que a blockchain pode desempenhar na monitorização da rede. Com base numa análise abrangente da literatura, o estudo examina\na capacidade de aplicação e os desafios da blockchain, apresentando um caso de estudo prático, que\nutiliza o Hyperledger Fabric, para destacar o seu potencial na monitorização da rede."
  },
  {
    "keywords": [
      "Peer-to-peer",
      "Gossip",
      "Broadcasting",
      "Byzantine",
      "Disseminação",
      "Byzantino"
    ],
    "titulo": "Epidemic broadcast algorithms in a Byzantine environment",
    "autor": "Costa, Tomás Francisco Cruz",
    "data": "2022-12-19",
    "abstract": "Peer-to-peer broadcasting algorithms are a scalable and cheap way of disseminating information to a large\nnumber of participants. However, most of these algorithms do not consider the possibility of some members\nacting in an unintended way, with malicious or selfish motives. In order to be useful in a real world scenario,\nthese algorithms must be secure, robust and efficient, even in the presence of adversaries. This thesis presents an overview of the challenges that peer-to-peer broadcasting algorithms face, as well as some of the security mechanisms that can be employed to mitigate them. Thus, we present Bycast, a secure and efficient peer-topeer broadcasting algorithm that is able to tolerate up to 45% of malicious nodes in the system. In order to achieve a high level of security, Bycast relies on strong membership integrity guarantees that make it harder for attackers to successfully compromise other nodes. In order to force nodes to cooperate, and contribute to the good performance of the system, Bycast employs an innovative auditing scheme that is able to detect nodes that are not cooperating with their resources, and evict them from the system."
  },
  {
    "keywords": [
      "Machine learning",
      "Deep Learning",
      "Visualization",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Visualizing neural network architectures",
    "autor": "Tavares, Diogo de Oliveira Campos",
    "data": "2023-12-28",
    "abstract": "In the constantly evolving realms of Deep Learning and Machine Learning, the ascent of intricate and\npowerful neural network models has pushed the frontiers of computational intelligence to new heights.\nNevertheless, the conspicuous absence of an accessible and user-friendly visualization tool has presented\nan imposing hurdle in the endeavor to comprehend and dissect these intricate and multifaceted architectures. In direct response to this critical challenge, this dissertation unveils a network visualizer platform:\nNeural Network Explorer. this platform has been developed in an attempt to offer a comprehensive\nsolution to these complex problems.\nThrough an exhaustive and in-depth analysis of existing tools and APIs, the Neural Network Explorer platform provides a streamlined and intuitive methodology for visualizing a diverse and extensive\narray of neural networks. This innovative tool empowers users to seamlessly unravel and decode the intricacies of the various layers and structures that compose these intricate model architectures, thereby\nfostering a deeper and more insightful understanding of the fundamental mechanisms that govern their\nfunctioning.\nBy delving into the realm of cutting-edge network visualization techniques, the platform not only facilitates the seamless export of illustrative images but also grants access to a wealth of detailed and\nlayer-specific information. This abundant and comprehensive data resource acts as a dynamic catalyst\nfor researchers and developers, equipping them with the essential insights and understanding needed to\nadeptly navigate the landscape of deep learning models and harness the potential within.\nThrough its robust exploration and innovative methodologies, this work provides researchers and practitioners with an indispensable and user-friendly tool for unraveling and exploring the complex and architectures that underscore the foundation of advanced computational intelligence."
  },
  {
    "keywords": [
      "Dashboard",
      "Estatísticas desportivas",
      "Análise semântica",
      "Análise de dados",
      "Eventos desportivos",
      "Sports statistics",
      "Semantic analysis",
      "Data analysis",
      "Sports events",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Plataforma online para visualização de tendências e opiniões de eventos desportivos com base em redes sociais",
    "autor": "Pinto, Maria Inês Vieira",
    "data": "2022-04-14",
    "abstract": "A extração e o processamento de dados nas redes sociais são complementares e, por vezes, fundamentais no apoio à tomada de decisões em diversos negócios. Os eventos desportivos não são exceção, podendo dessa forma beneficiar dessas técnicas. Nesse contexto, os clubes desportivos podem aproveitar as informações obtidas nas redes sociais durante determinados eventos para dar suporte nas decisões, podendo dessa forma melhorar a experiência do espectador e, assim, atrair mais audiência. Foi necessário realizar uma análise ao tipo de dados a serem recolhidos das redes sociais, assim como a melhor forma para os exibir ao utilizador, dando ao mesmo uma boa experiência. Foi elaborado um protótipo como solução para o desafio, apresentando assim uma plataforma web, alinhada às melhores práticas de User Experiente e User Interface, que auxilia clubes desportivos e produtores de conteúdo televisivo a visualizar facilmente dados sociais, durante ou após um evento desportivo em específico. Foram apresentados os resultados obtidos com esta proposta, demonstrando que é possível obter informações valiosas e relevantes para os clubes desportivos, conseguindo captar e reunir certos conhecimentos em tempo real."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Monitoring attention and performance on critical situations",
    "autor": "Cardoso, Paulo Gonçalves",
    "data": "2017",
    "abstract": "In our current lifestyle we often face situations that push us a bit further, over our\nordinary limits. These situations can be considered as stress moments that make us react\ndifferently from a normal situation, both in a physical or psychological way. While a break\ncan be beneficial to our mind and body when we guarantee proper rest after experiencing\nthis types of moments, when, for instance, we do a physical activity, a chain of stressful\nmoments can have negative impact on human beings leading to serious health problems\non the long run if there has not been any preventive action. Whereas the physical aspect is\neasily understood when looking at, the psychological side is usually forgotten or\nundervalued because one cannot waste time and focuses on better achievements or simply\nlacks understanding of this matter. One of the reasons behind this inevitable fatigue is the\nhigh competitiveness in the markets, which forces employees to work harder or for longer\nperiods so as to accomplish the same results as in shorter periods of time. In theory\nworking harder can lead to more productivity thanks to challenging factors or it can have\nthe opposite effect when workers suffer from situations like stress or increase of fatigue. In\nthis dissertation we will examine the relation between performance and mental fatigue\nand will prove how this association works with the help of a simulation environment\ncreated for this purpose."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Machine learning approaches for predicting effects of drug combinations in cancer",
    "autor": "Baptista, Delora Soeiro",
    "data": "2016-07-27",
    "abstract": "Drug combination therapies are commonly used to overcome tumor drug\nresistance. Computational methods can be helpful tools in drug combination\ndiscovery, but there are currently no e stablished methods for the prediction of\ndrug combination effects.\nThis work, integrated in the AstraZeneca -Sanger Drug Combination\nPrediction challenge launched by the Dialogue for Reverse Engineering\nAssessments and Methods (DREAM) community, aimed to develop machine\nlearning methods to estimate the effects of drug combinations on cancer cell\nlines. The challenge was divided into three subchallenges (1A, 1B, and 2)\naddressing different clinical scenarios.\nA variety of machine learning models were devel oped and evaluated\nusing cross-validation. Tree-based ensembles, particularly GB, performed best\nfor this problem. Among the different the genomic datasets provided, the\nmonotherapy, mutation and CNV datasets were the most informative and were\nthe only ones used in the final models.\nThe best model, submitted to 1A, was an ensemble of gradient boosting\n(GB), random forest (RF), and partial least squares (PLS) regression models,\nhaving achieved an average weighted Pearson correlation of 0.30, and ranking\n24th among 76 submissions. The 1B model (average weighted Pearson\ncorrelation of 0.18; 47th/62 submissions) was also an ensemble of GB, RF,\nand PLS models. For subchallenge 2, a GB model was selected. It had a\nperformance score (based on a three-way analysis of variance (ANOVA) ) of 5.15\nand ranked 20th out of 39 submissions.\nThe strategies explored in this work and by the DREAM challenge\ncommunity will help to further the development of computational methods for\nthe rational design of effective drug combinations for cancer therapy."
  },
  {
    "keywords": [
      "Base de dados",
      "SIARD",
      "RODA",
      "Preservação digital",
      "681.3",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Preservação de bases de dados com SIARD",
    "autor": "Rocha, Fábio Alves",
    "data": "2014-12-19",
    "abstract": "Atualmente, com a expansão das tecnologias de informação, muito do conhecimento humano passou a estar registado em suportes digitais, o que nos remete à utilização de intermediários para a leitura dessa informação: hardware e software. Devido a esses intermediários estarem em constante evolução e havendo o perigo da sua descontinuidade, essa informação poderá ser perdida, não pelo desaparecimento do objeto digital, mas por ficar ilegível para os novos equipamentos e aplicações. Devido a estes problemas emergiu uma nova problemática no universo digital, a preservação digital.\n\nEste projeto estuda a problemática da preservação digital, mas foca-se numa única classe de objetos digitais: as bases de dados relacionais. As bases de dados relacionais são de extrema importância, particularmente para as organizações, pois é nas suas bases de dados que se encontra informação essencial às suas atividades. Por essa razão, é fundamental não comprometer a sua longevidade, integridade e autenticidade.\n\nO presente trabalho visa o desenvolvimento de um sistema de exploração de ontologias, que recebe ficheiros em formato SIARD, transformando-os em ontologias (OWL) e acrescentando-os ao repositório.\n\nComo última fase, foi criado um navegador Web que permite explorar a informação das ontologias armazenadas, onde é possível questionar as ontologias através de SPARQL e guardar essas mesmas interrogações para posterior uso.\n\nO repositório das ontologias foi desenvolvido segundo a norma OAIS, e tem por base uma aplicação Web com várias interfaces, para assim proporcionar a ingestão da informação e a sua administração, preservação e disseminação."
  },
  {
    "keywords": [
      "Blockchain",
      "Ethereum",
      "Smart contracts",
      "NFTs",
      "Off-chain",
      "Market",
      "Luxury objects",
      "Authenticity",
      "Solidity",
      "Contratos inteligentes",
      "Mercado",
      "Objetos de luxo",
      "Autenticidade",
      "Solidity",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Blockchain – NFTs for luxury objects authenticity",
    "autor": "Morais, André Loureiro",
    "data": "2023-10-19",
    "abstract": "Counterfeiting of luxury goods is becoming more of a problem, and manufacturers must battle it while also\naccepting the challenge of finding new ways to combat it. Companies have spent a lot of money over the\nyears to ensure that these products are authentic, and blockchain has given the luxury market fresh hope\nand potentially fuel a new process of authenticity. One of the technologies seeking to ensure the validity\nof these valuable products is blockchain technology, which has generated multiple start-ups, blockchain\nplatforms, and protocols.\nWith the explosive growth of blockchain technology, NFTs (Non-Fungible Tokens) are becoming increas ingly popular, even in sectors where there appears to be no benefit, such as in the postal service. Its use\nis already standardized (for example, through ERC-721, ERC-1155, ERC20).\nIn the context of this work, it was necessary to analyze and evaluate existing NFT platforms, with\nregard, among others, to costs, usability, ease of programming; design and implement a platform that is\nspecifically tailored to the authenticity of luxury items, such as jewelry, art, and luxury clothes; and finally\ntest all procedures and features that are appropriate for the authenticity of luxury products."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Data Warehouses suportados por Nuvens",
    "autor": "Rodrigues, Paulo Renato Dias",
    "data": "2013",
    "abstract": "O universo das Tecnologias de Informação está a assistir a grandes mudanças desde o surgimento do\nconceito de cloud computing. A cloud computing revela-se como um meio que possibilita a fácil\naquisição e liberação (elasticidade) de recursos computacionais, que disponibiliza infraestruturas\naltamente escaláveis e dispendiosas com o mínimo de configuração possível e, ainda, pelo facto de\nser um serviço com custos reduzidos comparativamente a uma solução in-house, pois tipicamente\nutiliza um modelo “pay-as-you-go”. Claro que com a delegação de toda a infraestrutura e dos dados\npara um provedor de clouds, questões como a segurança e privacidade dos dados começaram a ser\nequacionadas, apresentando-se assim como desvantagens para soluções em cloud. No entanto, além\nda cloud computing, outras variáveis, como o grande aumento do volume de dados nas empresas e\nos avanços tecnológicos alcançados nas redes de banda larga, têm “exigido” a adaptação das bases\nde dados para um ambiente em cloud, o que originou, pouco a pouco, o paradigma de Database as a\nService. Atualmente ainda existem dúvidas relativamente às bases de dados SQL, sobre se estas serão\nas mais indicadas para ambientes cloud. Este modelo de bases de dados tem dominado o mercado\nmas apresenta diversas limitações (por exemplo a nível de escalabilidade e garantia das propriedades\nACID) quando confrontadas com implementações num ambiente cloud. Por outro lado, o ecossistema\nde aplicações desenvolvidas com bases de dados SQL é demasiado grande para ser modificado para\noutro modelo. Apesar desta indefinição, a cloud parece ser um cenário ideal para data warehouses\npois são bases de dados que albergam usualmente enormes volumes de dados e são essencialmente\nde leitura. Com esta dissertação pretendeu-se estudar a viabilidade da implementação e migração de um sistema\nde data warehousing para um ambiente cloud e apresentar um protótipo que expusesse a utilidade do\nmesmo face a uma típica implementação in-house."
  },
  {
    "keywords": [
      "Quantum computing",
      "Qbricks",
      "Formal verification",
      "Quantum programming language",
      "Quantum fourier transform",
      "Grover’s algorithm",
      "Computação quântica",
      "Verificação formal",
      "Linguagem para programação quântica",
      "Transformada de fourier quântica",
      "Algoritmo de Grover",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "iQbricks: integration of a fully-featured quantum language in the framework Qbricks",
    "autor": "Carneiro, Tomás Barros",
    "data": "2023-06-07",
    "abstract": "Quantum Computing has noticeably grown over the last two decades, making it a revolu tionary field of investigation in the current era of technological research.\nSuch a growth has been leading to an increasing demand in research by several big enter prises such as IBM, Google and Microsoft, paving the way for a richer ecosystem and untold\nbenefits among the Quantum Computing community.\nVerification is a crucial aspect of software development, as it ensures that a program per forms as intended and reduces the risk of introducing errors. This is especially important\nin the field of Quantum Computing, where the complexity of programs is high and the\nbehavior of quantum systems is often counterintuitive. Verification of quantum programs\ncan help detect errors that may lead to incorrect results, which is of utmost importance when\ndealing with quantum algorithms and quantum simulations. As a result, having a formal\nverification framework for quantum programs can greatly benefit the development of reliable\nand accurate quantum software. Qbricks is a verification framework for building quantum\nprograms, and corresponds to the framework on which this project has been integrated.\nDuring the course of this thesis, iQbricks – an intuitive and user-friendly language to\nbuild and formally verify quantum programs – was developed, along with a framework to\ntranslate and generate verifiable Qbricks programs from iQbricks.\nThis project’s main achievements were: (1) the design and implementation of a high-level\nprogramming language for describing quantum circuits in an intuitive and user-friendly\nway and (2) the implementation of a translator, embedded in Qbricks’ framework, that\nconverts iQbricks programs to Qbricks ones.\nThe developed framework was evaluated against two different quantum algorithms: the\nQuantum Fourier Transform and Grover’s algorithm.\nThis project was accompanied by an internship at the Commissariat à l’énergie atomique et\naux énergies alternatives (CEA) - LSL, where this implementation was developed in direct\ninvolvement with Qbricks’ team of investigators."
  },
  {
    "keywords": [
      "Lesson Plans",
      "Teacher support tools",
      "Technology-enhanced learning",
      "Pedagogical tools integration",
      "Domain-specific languages",
      "Planos de Aula",
      "Ferramentas de suporte ao Professor",
      "Aprendizagem assistida por tecnologias",
      "Integração de ferramentas pedagógicas",
      "Linguagens de domínio específico",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Kiko, a lesson planner system to enhance classes",
    "autor": "Cruz, Tiago Miguel Alves",
    "data": "2020-01-09",
    "abstract": "This document presents and discusses the outcomes of a project in the area of Pedagogical\ntools to support teaching. This project is part of the work in the second year of the Master\ndegree in Software Engineering and was accomplished at Universidade do Minho in Braga,\nPortugal. The main outcome of this Master project is an application that allows a teacher to\nplan his classes by describing in a text file the topics and subtopics of the school program\nthat he has to teach in a lesson, linking to each concept the multimedia support materials\nand questions that he intends to do (orally, on paper, or on the computer). This application\nis based on a domain specific language (DSL) specially tailored to described easily lesson\nplans no matter the course or the teaching topic. The application contains all user-entered\nlesson plans, which are automatically processed to create a classroom presentation for each\nsubject, supporting and guiding a given class. Whenever intended by the user, the appli cation also processes all external resources, introducing hyperlinks to them in the various\nfields of the lesson plan. The application is publicly available at the Language-Processing\nResearch group server."
  },
  {
    "keywords": [
      "Configuração",
      "Estudo empírico",
      "Feature models",
      "Robótica",
      "ROS",
      "Variabilidade",
      "Configuration",
      "Empirical study",
      "Feature modules",
      "Robotics",
      "Variability",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Estudo empírico da variabilidade em sistemas ROS",
    "autor": "Melo, Sara Maria Barreira",
    "data": "2021-12-02",
    "abstract": "A utilização de sistemas robóticos tem vindo a crescer nos últimos anos e o software destes sistemas tem se tornado cada vez mais importante para o seu funcionamento. O Robot Operating System (ROS) é um\nmiddleware que simplifica a implementação destes sistemas, fornecendo várias primitivas que facilitam a\nescrita de software e a coordenação dos diversos componentes que os constituem. Os sistemas ROS são\ndistribuídos, com uma arquitetura organizada a partir de nós que comunicam entre si através da passagem\nde mensagens. Estes sistemas robóticos são fortemente configuráveis pois necessitam de se ajustar a\nambientes de trabalho cada vez mais diversificados e adversos. Em sistemas ROS existem ficheiros que\nincluem a configuração do sistema e é através destes que se pensa que é gerida a variabilidade.\nCom esta tese pretende-se estudar empiricamente o modo como, de facto, é gerida a variabilidade\ndestes sistemas visto que existe muito pouca informação sobre como é feita essa gestão. Em particular,\npretende-se estudar a viabilidade da extração automática de feature models (modelos gráficos que podem\najudar na quantificação da variabilidade) a partir dos ficheiros de configuração de um sistema ROS.\nDurante todo o processo de análise conseguiram-se identificar algumas técnicas de gestão de variabili dade. Foi também possível desenvolver uma ferramenta capaz de extrair feature models automaticamente,\napenas através da análise do código de sistemas ROS. Foram escolhidos cinco sistemas ROS para avaliar\na ferramenta desenvolvida, tendo sido possível obter resultados interessantes sobre a variabilidade dos\nmesmos."
  },
  {
    "keywords": [
      "Lince",
      "Hybrid programming",
      "Autonomous driving",
      "SageMath",
      "Programação híbrida",
      "Condução autónoma",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Simulation of hybrid systems regulated by newtonian mechanics",
    "autor": "Correia, Ricardo da Silva",
    "data": "2023-12-15",
    "abstract": "The evolution of software products that interact with the physical world has led to a greater need to simulate\ntheir behavior in order to verify their effectiveness and safety in different scenarios. This dissertation project\naims to enhance a simulation tool for hybrid programs called Lince, more specifically to provide more\npowerful simulation capabilities to hybrid programs regulated by Newtonian mechanics. These include the\naddition of new language constructs (such as the division operator and the trigonometric functions), the\nimplementation of non-linear expressions, grammar relaxation and organization, improved error detection,\nand the mitigation of existing tool-related issues.\nThroughout this dissertation, it is discussed how the implementation of these improvements benefits\nthe simulation of hybrid programs and are explained the key methods adopted for their conception. Finally,\nthis new version of Lince is put to the test by handling case studies related to autonomous driving (for\nexample, adaptive cruise control and a missile targeting a moving object) and other types of systems\nas well, such as purely physical systems and the so-called on-off systems. The results obtained in the\ntreatment of these case studies attest to the enhanced capabilities of this tool and the contribution of this\ndissertation to the scientific community, demonstrating its relevance in simulating integrated systems in\neveryday life."
  },
  {
    "keywords": [
      "Energy-aware",
      "Epidemic",
      "IoT",
      "Routing",
      "WSN",
      "Epidemico",
      "RSF",
      "Roteamento",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Energy-aware Gossip Protocol for Wireless Sensor Networks",
    "autor": "Ferreira, Bruno Chianca",
    "data": "2019-11-21",
    "abstract": "In Wireless Sensor Networks (WSNs), typically composed of nodes with resource constraints, leveraging efficient processes is crucial to enhance the network longevity and\nconsequently the sustainability in ultra-dense and heterogeneous environments, such as\nsmart cities. Epidemic algorithms are usually efficient in delivering packets to a sink or\nto all it’s peers but have poor energy efficiency due to the amount of packet redundancy.\nDirectional algorithms, such as Minimum Cost Forward Algorithm (MCFA) or Directed\nDiffusion, yield high energy efficiency but fail to handle mobile environments, and have\npoor network coverage.\nThis work proposes a new epidemic algorithm that uses the current energy state of the\nnetwork to create a topology that is cyclically updated, fault tolerant, whilst being able\nto handle the challenges of a static or mobile heterogeneous network. Depending on the\napplication, tuning in the protocol settings can be made to prioritise desired characteristics.\nThe proposed protocol has a small computational footprint and the required memory is\nproportional not to the size of the network, but to the number of neighbours of a node,\nenabling high scalability.\nThe proposed protocol was tested, using a ESP8266 as an energy model reference, in a\nsimulated environment with ad-hoc wireless nodes. It was implemented at the application\nlevel with UDP sockets, and resulted in a highly energy efficient protocol, capable of leveraging extended network longevity with different static or mobile topologies, with results\ncomparable to a static directional algorithm in delivery efficiency."
  },
  {
    "keywords": [
      "Computer vision",
      "Parallel computing",
      "Deep learning",
      "Inference",
      "Homogeneous programming",
      "Heterogeneous programming",
      "Optimization",
      "Autonomous driving",
      "Visão por computador",
      "Computação paralela",
      "Aprendizagem profunda",
      "Inferência",
      "Programação homogénea",
      "Programação heterogénea",
      "Otimização",
      "Condução autónoma",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Optimization of deep learning algorithms for an autonomous RC vehicle",
    "autor": "Pereira, André Filipe Amorim",
    "data": "2021-10-21",
    "abstract": "This dissertation aims to evaluate and improve the performance of deep learning (DL)\nalgorithms to autonomously drive a vehicle, using a Remo Car (an RC vehicle) as testbed.\nThe RC vehicle was built with a 1:10 scaled remote controlled car and fitted with an\nembedded system and a video camera to capture and process real-time image data. Two\ndifferent embedded systems were comparatively evaluated: an homogeneous system, a\nRaspberry Pi 4, and an heterogeneous system, a NVidia Jetson Nano. The Raspberry Pi 4 with\nan advanced 4-core ARM device supports multiprocessing, while the Jetson Nano, also with\na 4-core ARM device, has an integrated accelerator, a 128 CUDA-core NVidia GPU.\nThe captured video is processed with convolutional neural networks (CNNs), which\ninterpret image data of the vehicle’s surroundings and predict critical data, such as lane view\nand steering angle, to provide mechanisms to drive on its own, following a predefined path.\nTo improve the driving performance of the RC vehicle, this work analysed the programmed\nDL algorithms, namely different computer vision approaches for object detection and image\nclassification, aiming to explore DL techniques and improve their performance at the inference\nphase.\nThe work also analysed the computational efficiency of the control software, while running\nintense and complex deep learning tasks in the embedded devices, and fully explored the\nadvanced characteristics and instructions provided by the two embedded systems in the\nvehicle.\nDifferent machine learning (ML) libraries and frameworks were analysed and evaluated:\nTensorFlow, TensorFlow Lite, Arm NN, PyArmNN and TensorRT. They play a key role to\ndeploy the relevant algorithms and to fully engage the hardware capabilities.\nThe original algorithm was successfully optimized and both embedded systems could\nperfectly handle this workload. To understand the computational limits of both devices, an\nadditional and heavy DL algorithm was developed that aimed to detect traffic signs.\nThe homogeneous system, the Raspberry Pi 4, could not deliver feasible low-latency values,\nhence the detection of traffic signs was not possible in real-time. However, a great performance\nimprovement was achieved using the heterogeneous system, Jetson Nano, enabling their\nCUDA-cores to process the additional workload."
  },
  {
    "keywords": [
      "Anotação de imagens",
      "Continual learning",
      "Transfer learning",
      "Deteção de objetos",
      "Image annotation",
      "Object detection",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Automate the learning process of an item identification model in palletized packages",
    "autor": "Veloso, Gonçalo Jorge Vilas-Boas",
    "data": "2023-04-27",
    "abstract": "Numa determinada empresa, está a ser desenvolvido um sistema que visa identificar e classificar\ncertos objetos em embalagens paletizadas O sistema é composto por um conjunto de câmaras colocadas numa estação de carregamento que executa várias capturas de imagens. A isto segue-se um\nprocessamento que termina numa deteção e identificação de certos objetos que determinam as regras\nde empacotamento. Estas regras devem estar de acordo com as regras definidas pelo cliente. A solução\natual passa por anotar manualmente as imagens exemplo (processo lento e demorado) para serem utilizadas para o treino do algoritmo de deteção de objetos. Depois do treino estar terminado, o modelo de\ndeteção fica imutável, o que significa que este é incapaz de evoluir com os resultados de verificação que\nse podem seguir.\nO âmbito desta dissertação é o de estudar,propôr e desenvolver uma solução para automatizar partes do processo de treino de um sistema de visão por computador que executa deteção de objetos de\nitems paletizados através da implementação de geração automática de imagens e a automatização da\nsua anotação. Já foram desenvolvidos e cientificamente aprovados alguns métodos que produzem anotações automáticas recorrendo a processos de active learning, classificação e feature transferring, bem\ncomo softwares de anotação de imagens que são capazes de executar tal tarefa. A proposta apresentada\nintroduz uma estratégia para gerar imagens sintéticas e as anotações associadas para serem utilizadas\npara treinar modelos de deteção de objetos de modo a tornar mais ágil o processo de coleção e anotação de imagens. Os resultados mostram que, no geral, os modelos treinados com imagens sintéticas\ndemonstram melhores resultados que o modelo existente que foi treinado com imagens reais anotadas\npor um anotador. Demonstrando que é possivel criar modelos de deteção de objetos que obtenham bons\nniveis de desempenho, que foram treinados apenas com dados sintéticos."
  },
  {
    "keywords": [
      "scRNA-seq",
      "Multiple Sclerosis",
      "Choroid plexus",
      "Subventricular zone",
      "Oligoden-drocytes",
      "Esclerose Múltipla",
      "Zona subventricular",
      "Oligodendrócitos",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "From the choroid plexus to the (sub)ventricular cells for oligodendrocyte (re)generation",
    "autor": "Fernandes, Mónica da Silva",
    "data": "2023-01-12",
    "abstract": "Multiple sclerosis (MS) is an autoimmune demyelinating disorder that affects the central nervous\nsystem by damaging myelin and axons, but the exact cause of MS remains unclear. It is known\nthat the immune system destroys oligodendrocytes (OLs), the myelinating cells of the CNS, and\nregardless of the efforts, it remains challenging to induce replacement of the lost OLs. The\nchoroid plexus (CP) is fundamental for brain homeostasis as it secretes the cerebrospinal fluid.\nIt is also a key modulator of neurogenesis and constitutes a site of neuroinflammation. Since MS\nis an inflammatory disorder, studying the relation between the CP and ventricular-subventricular\nzone (V-SVZ) is of interest. In this dissertation we explored the role of the CP in the modulation\nof neural stem cells and OLs progenitor cells (OPCs), located in the SVZ, in order to assess its\npotential to induce OL (re)generation. For that we used single cell RNA sequencing (scRNA-seq)\ndata from both control and an MS model generated by our team.\nHere we explored the cell types, RNA velocities and cell-cell communication (CCC) inference.\nWe have characterized the cell populations present in the V-SVZ of controls and MS model, and\nfound differences in the clusters identified. RNA velocity revealed two central cores from where\nmost of progenitor cells seamed to arise from, being one located at the neuronal intermediate\nprogenitor cells cluster and the other one located in the astrocytes cluster. Further analysis\nhas to be performed in order to confirm these results. Concerning CCC inference, we found\nevidence of signalling pathways between endothelial cells and pericytes with OPCs, which\ncorroborates previous studies where this communication was reported. Together with literature,\nour data indicates that endothelial cells and pericytes regulate OPCs proliferation through Pdgf\nsignalling. When analyzing the results from CCC inference in the CP-SVZ integrated data we\nfound evidence of a communication from mesenchymal cells at the CP and ependymal cells\nthrough Wnt signalling. Another interaction was between OPCs and ependymal cells through\nTENASCIN signalling. Nevertheless, the existence of this interactions in vivo needs to be further\nvalidated."
  },
  {
    "keywords": [
      "681.3-7"
    ],
    "titulo": "Finite probability distributions in Coq",
    "autor": "Moreira, Diogo Araújo Carvalho Vilaça",
    "data": "2012-04-05",
    "abstract": "When building safety-critical systems, guaranteeing properties like correctness\nand security are one of the most important goals to achieve. Thus, from a scientific\npoint of view, one of the hardest problems in cryptography is to build\nsystems whose security properties can be formally demonstrated.\nIn the last few years we have assisted an exponential growth in the use of tools\nto formalize security proofs of primitives and cryptographic protocols, clearly\nshowing the strong connection between cryptography and formal methods. This\nnecessity comes from the great complexity and sometimes careless presentation\nof many security proofs, which often contain holes or rely on hidden assumptions\nthat may reveal unknown weaknesses. In this context, interactive theorem\nprovers appear as the perfect tool to aid in the formal certification of programs\ndue to their capability of producing proofs without glitches and providing additional\nevidence that the proof process is correct.\nHence, it is the purpose of this thesis to document the development of a framework\nfor reasoning over information theoretic concepts, which are particularly\nuseful to derive results on the security properties of cryptographic systems. For\nthis it is first necessary to understand, and formalize, the underlying probability\ntheoretic notions. The framework is implemented on top of the fintype and finfun\nmodules of SSREFLECT, which is a small scale reflection extension for the COQ\nproof assistant, in order to take advantage of the formalization of big operators\nand finite sets that are available."
  },
  {
    "keywords": [
      "Machine learning",
      "Chemistry",
      "Graphs",
      "Kernel methods",
      "Graph neural networks",
      "Química",
      "Grafos",
      "Redes neuronais de grafos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Graph kernels and neural networks for predicting yields of chemical reactions",
    "autor": "Braga, Diogo Filipe Ribeiro Ferreira",
    "data": "2022-04-01",
    "abstract": "Predicting chemical reaction yields is a widely investigated problem in drug discovery due to the natu ral appearance of diseases and viruses worldwide. With the evolution and further study of machine learn ing, this area of computer science has provided alternatives that help chemists search for more effective\nmolecule combinations. This dissertation presents two research hypotheses with different bases that seek\nto improve this prediction problem. The first research hypothesis is related to the support vector regression\nalgorithm, which uses graph kernels to measure similarity between molecules and then perform the pre diction. We propose the application of non-linearity in the Weisfeiler-Lehman graph kernel to improve the\nmeasure of comparison between molecules and thus enhance the complexity of the support vector regres sion models. The second research hypothesis is related to the class of neural networks. We propose a deep\nlearning base to solve this problem through graph neural networks, which use graph convolutional layers\nand global read-out operations to extract molecular features from graph-structure data. The main focus is\nto ensure that all models generalise well to obtain good results in experiments with unknown molecules.\nWe performed tests on chemical data for both methods and achieved improvements. The non-linearity in\ngraph kernels proved to be the most advantageous, having surpassed the state-of-the-art methods in one\nof the two global tests performed. The graph neural networks were not as effective, although they showed\ncompetitive results. Concerning neural networks, we highlight the creation of the deep learning base and\nthe in-depth analysis of the hyperparameters to enhance further research on the reaction yield prediction\nproblem, as this area shows immense potential in drug discovery."
  },
  {
    "keywords": [
      "XML",
      "XSL",
      "DTD",
      "XSD",
      "XSLT",
      "XPath",
      "IDE",
      "Web-platform",
      "Schema",
      "Documents",
      "Plataforma Web",
      "Documentos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Web XML IDE",
    "autor": "Ferreira, José Pedro Castro",
    "data": "2023-09-27",
    "abstract": "This document is the report for a Master Thesis, included in the second year of the Master’s\nDegree in Informatics Engineering at Universidade do Minho in Braga, Portugal.\nThe project consists on the design and development of a Web IDE for XML to support\nteaching annotation languages to students that deal with digital documents but are not\nrelated to computer programming. The developed tool must be easy to install and to\nuse, considering it is made to help users that are not experienced with programming or\nannotation languages nor IDEs.\nThe goal of this project is to get an user-friendly, easy to learn, Web-based platform to\nassist in: creating well-formed XML documents, DTDs, XML Schemas and XML Stylesheets;\nconverting DTDs to XML Schemas; validating XML documents according to given DTDs\nor Schemas; running XPath expressions on XML Documents; and automatically generating\nXML documents from a DTD or a Schema and some parameters.\nThe state of the art on XML IDEs was analyzed in order to prove the necessity for the\nproposed application. The different IDEs were analyzed regarding their features, UI/UX,\nease-of-use, price and availability.\nThis document also contemplates the proposed approach for meeting the project’s ob jectives, the plan for the development stage of this project, as well as the definition for\nthe desired features, and the report for the development process itself, describing how the\ndifferent features were implemented.\nThe developed application was compared to the analyzed state of the art IDEs and proved\nto meet the proposed objectives for the project.\nThe WebXMLIDE application is publicly available for free in the following link: https:\n//webxml.epl.di.uminho.pt/."
  },
  {
    "keywords": [
      "Ambient Intelligence",
      "Comfort",
      "Deep Learning",
      "Machine Learning",
      "Smart Devices",
      "Well-being",
      "Aprendizagem Máquina",
      "Aprendizagem Profunda",
      "Bem-estar",
      "Conforto",
      "Dispositivos Inteligentes",
      "Inteligência Ambiente",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Learning user well-being and comfort through smart devices",
    "autor": "Sousa, David José Teixeira de",
    "data": "2021-02-22",
    "abstract": "The growth of concepts such as Intelligent Environments and Internet of things allows us to understand the habits of users and consequently act to improve people’s daily lives. Through information gathering, it is thus possible to gather patterns about different kinds of human behavior and consequently build a learning model with predictive capabilities. In addition, there are increasing concerns from large companies about the influence, positive or negative, that aspects such as comfort and well-being have on the behavior and health of the population. In fact, as human beings, we are greatly influenced by the environment in which we are inserted. There are therefore conditions in a place that give us certain levels of comfort that will eventually interfere with our well-being. However, it is difficult to identify which of these factors are relevant and how they intervene in our daily lives. Also, the habits we adopt as a result of the routines we follow can contribute to improving or worsening any of these indicators With the help of the various types of sensors present, for example, in the smart devices (smartphones, smartwatches, wristbands), it is increasingly possible to collect information on these factors, easily and comprehensively. In this sense, firstly the main objective of this dissertation is thus to collect data on factors that may influence the user in order to create a user profile. These factors can be inferred through its interests, the visited locations, and its main activities. This objective involves a large-scale analysis, where there are no geographical restrictions. Furthermore, the study will be independent of the type of space (open or closed) that is explored. In that way, the perspective that will be used is from the user. Then there is an exploration of the data so that some intelligence can be inferred, and in this sense, build a mobile application capable of providing smart notifications based on user needs."
  },
  {
    "keywords": [
      "DM",
      "Chatbot",
      "AI",
      "Deep learning",
      "NLP",
      "HCD",
      "IA",
      "Aprendizagem profunda",
      "PLN",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Chatbot assistant for diabetic patients",
    "autor": "Cruz, Sandro Emanuel Machado",
    "data": "2021-12-02",
    "abstract": "Nowadays, with the existence of several chronic health conditions, Diabetes Mellitus (DM) being one of\nthe main ones, there is a great burden that patients must have in order to be able to take care of themselves.\nThus, in addition to seeking to resolve their needs by obtaining information from health professionals,\nthey increasingly seek information and advices in forums, communities and groups. The use of dialogue\nsystems has become essential in people’s lives. The development of conversational agents is still an\nunresolved research problem that poses many challenges in the Artificial Intelligence (AI) community.\nThis work aims to build an AI chatbot that is based on the principles and techniques of AI directed to\nNatural Language Processing (NLP) and Deep Learning to help people newly diagnosed with DM in the\nself management of the disease.\nA literature search of DM education and information for people newly diagnosed with DM was con ducted. Additionally, the main topics in which patients ask for support were retrieved from a search of\nseveral online support groups of DM, as well as questionnaires with 8 patients and interviews with 3 health\nprofessionals. The application were developed through the back-end side in Python and the front-end side\nin React Native and its communication was made through WebSockets. Furthermore, an interaction and\ninterface design are developed in this work using Human-Centered Design (HCD) methodology. For that\npurpose, iterative usability test sessions were conducted with 12 users using Think Aloud methods and\nthe System Usability Scale (SUS).\nThe chatbot developed is Information Retrieval (IR) type and answered questions asked by users in a\nhuman-like way. The result of the usability tests of the final version of the application was satisfactory (with\na System Usability Scale (SUS) score of 88) and users found the application quite intuitive and robust.\nFurther studies should concentrate on customizing the chatbot to each user by collecting information from\nprior interactions and verifying the impact of using this chatbot for newly diagnosed Portuguese users with\nDM."
  },
  {
    "keywords": [
      "Tolerância a faltas",
      "Consenso",
      "Raft",
      "Microsserviços",
      "Sistemas monolíticos",
      "Spring boot",
      "Middleware genérico",
      "Modularidade",
      "Fault tolerance",
      "Consensus",
      "Microservices",
      "Monolithic systems",
      "Generic middleware",
      "Modularity",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Acordo distribuído para arquiteturas de microsserviços",
    "autor": "Silva, João Pedro Oliveira da",
    "data": "2022-04-01",
    "abstract": "Alcançar o consenso distribuído é fundamental para que se consigam construir sistemas tolerantes a faltas, pois permite que uma coleção de processos opere como um grupo coerente que pode sobreviver a falhas de alguns dos seus membros. O algoritmo Raft resolve o problema de consenso e visa ser o mais compreensível possível, porém as atuais implementações deste algoritmo são dedicadas a casos de uso específicos. Consequentemente, quem desenvolve sistemas que tenham que contemplar componentes replicados vê-se obrigado a construir o seu próprio mecanismo de replicação, o que pode ser contraproducente e até ter certas implicações no código da aplicação. Paralelamente, as arquiteturas de microsserviços passaram a ser o novo normal, fazendo oposição à construção de sistemas monolíticos. Dada a sua natureza, este tipo de arquiteturas permite endereçar problemas que dizem respeito à resiliência e coerência dum dado serviço, existindo por isso uma oportunidade para cruzar algoritmos de consenso distribuído com microsserviços. Nesta dissertação propõe-se a construção de duas implementações de Raft num toolkit aplicacional típico de microsserviços, mais especificamente Spring Boot. Cada implementação deverá utilizar uma das diferentes stacks da framework, nomeadamente, a serviet stack ou a stack reativa. Ambas as implementações deverão ser modulares e genéricas o suficiente, para que possam ser simultaneamente configuráveis e aplicáveis a diferentes casos de uso. Para o efeito, começa-se por delinear as configurações em que o middleware poderá operar, assim como a arquitetura interna do mesmo, seguindo-se da fase de implementação, que detalha decisões tomadas ao longo da mesma. A fase de avaliação começa com a implementação, em ambas as stacks, de uma aplicação de armazenamento chave-valor que é configurada com diferentes parâmetros, para que finalmente possa ser comparada com o etcd que é um armazenamento chave-valor replicado. Desde logo, os resultados recolhidos fazem prever o desempenho de ambas as soluções de middleware construídas, que ficam aquém dos desempenhos alcançados por um cluster etcd, mas que dão garantias de viabilidade e extensibilidade, uma vez que as soluções são modulares para integrarem novas otimizações, e são genéricas e úteis para qualquer aplicação que necessite de assegurar garantias de coerência forte."
  },
  {
    "keywords": [
      "Computational thinking",
      "Game-based learning",
      "Game types",
      "Learning resource",
      "Ontology",
      "Student profile",
      "Aprendizagem baseada em jogos",
      "Ontologia",
      "Pensamento computacional",
      "Perfil do aluno",
      "Recurso de aprendizagem",
      "Tipos de jogo",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Adequa, a platform for choosing games suitable to students’ profile",
    "autor": "Teixeira, Maria de La Salete Dias",
    "data": "2021-04-06",
    "abstract": "Computational Thinking has been increasingly explored in the area of teaching. Many\nresearchers believe that the early introduction of this concept leads to a better understanding\nof multiple fields like Computer Science, Math and Engineering. However, the inclusion of\nComputational Thinking as part of the educational program needs to be carefully done. For\nthat, we need to choose the right Learning Resources.\nAs Game-Based Learning was proven to be effective by numerous researchers, in this\nproject is argued that games are proper Learning Resources to develop Computational\nThinking. With Game-Based Learning, this work aims to improve students’ motivation\nand learning experience on Computational Thinking by choosing the most suitable games\nfor each student. To find the relation between students and games, it is necessary to\nanalyze each of them. First, to differentiate types of games, OntoJogo, an ontology for game\nclassification, was built. The usability and coverage of OntoJogo were tested in an experiment\nconducted with five participants. Secondly, it was required to profile the students through\nthe analysis of sociodemographic, competencies, and psychological factors. For that, a profile\nquestionnaire was developed with the collaboration of two child psychologists. Lastly, a\ngame evaluation questionnaire was designed for the students to complete, making it possible\nto connect game classifications with students’ profiles.\nWith these tools, it was possible to develop a platform for games suggestion, fulfilling the\nprimary goal of this project. The platform Adequa supports the registration of games and\nstudents and the evaluation of games. Additionally, Adequa recommends the most suitable\ngames for each student. For the recommendation of games, it was designed an algorithm\nthat uses the data collected from the questionnaires and returns a list of suitable games. The\nalgorithm was developed from the results of an experiment conducted with twenty-four\nparticipants, where it was searched patterns between the participants and game types. From\nthe results, it was found that variables like gender, gaming habits, and emotional factors\ncan influence the motivation a student feels towards a game. This experiment was essential\nto prove the hypothesis that it is possible to relate students and games. Based on this\nconclusion, it is right to affirm that the future of education must pass through a personalized\nexperience, starting with the learning resources used."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Using software defined networking for flexible network measurements",
    "autor": "Silva, Catarina Isabel Pires da",
    "data": "2017",
    "abstract": "Network management evolved in a way where implementing complex, high level network\npolicies, implies dealing with some attributes that depend on low-level specific configuration.\nThis reflects on a difficulty of changing the underlying infrastructure. SDN (Software-\nDefined Networking) concept opens a road for new developments due to the centralized\nnon vendor-specific control of the network, most of it related with the separation of data\nand control planes. Since collecting actual data to create information is important at the\ntime of taking decisions, network operators need to understand the dynamic of their network\nthrough monitoring and sampling. An SDN approach offers different possibilities\nto solve network managing problems, raising new points of view on how networks can\noperate and, consequently, how they can be managed and monitored. This study is mainly\nfocused on exploring the SDN architecture, and its elements, for applying sampling techniques\nthrough flexible network measurements. To pursue this, SDN elements will be presented\nand explained, alongside with existing monitoring solutions. These solutions, after\nexplored and analysed, will lead to a new approach on applying and configuring flexible\nsampling techniques on SDN."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "PlaCoR: plataforma para a computação orientada ao recurso",
    "autor": "Ribeiro, Bruno Manuel Gonçalves",
    "data": "2019",
    "abstract": "A Plataforma para a Computação orientada ao Recurso (PlaCoR) foi desenhada como um\nambiente de programação e execução de aplicações baseadas no modelo da computação orientada\nao recurso (CoR), especificado em CoRes, integralmente escrito em C++ Moderno.\nA escolha do C++ trouxe enormes vantagens, no suporte à: i) programação orientada aos\nobjetos, através da herança múltipla (na construção dos recursos); ii) programação genérica\n(permitindo abstrair na API as diferentes classes de recursos); iii) programação concorrente\n(para tirar partido de fios de execução e estruturas de sincronização nativas ao C++).\nA plataforma possui facilidades para: i) comunicação inter-domínios, ii) passagem de mensagens\nentre recursos comunicantes, iii) memória partilhada distribuída (DSM), iv) ativação\nremota de fios de execução (RPC), v) criação e gestão de recursos e vi) gestão da consistência\nentre todas as réplicas de um recurso.\nAtualmente, o desenho de aplicações CoR assenta nos recursos domínio, grupo, clausura,\nagente, proto-agente, dado, barreira, guarda e guarda para leituras/escritas. Os domínios\nestabelecem o primeiro nível de concorrência/paralelismo, quer sejam criados no início da\naplicação ou lançados dinamicamente. Os agentes, pelo seu lado, estão associados ao grão\nfino de paralelismo e de comunicação por passagem de mensagens.\nO domínio, o grupo e a clausura são recursos estruturados que disponibilizam operações\nde adesão/saída de recursos; distingue-os o facto dos dois primeiros serem dinâmicos enquanto\na clausura é estática, na medida em que as operações de adesão/saída são coletivas\ne o número total de membros é fixado inicialmente - características necessárias para o arranque\nparalelo de aplicações do tipo SPMD e a passagem de mensagens intra-clausura.\nA guarda é usada para a criação de zonas de exclusão mútua distribuídas (leituras/escritas),\na barreira para a sincronização entre agentes, enquanto o dado contempla os mecanismos\nde memória partilhada distribuída, usado para disponibilizar os dados do utilizador num\nambiente de domínios distribuídos.\nA avaliação da plataforma tomou como exemplo de aplicação a leitura e processamento\nde eventos registados em TTree, recorrentemente usados na experiência ATLAS. As várias\nversões desenvolvidas justificaram a criação de um módulo específico, a unidade Pool, que\nrealiza o modelo fork-join.\nO experimento confirmou a viabilidade da orientação ao recurso como paradigma de programação\nhíbrido que integra múltiplos fios de execução e sincronização distribuída, com\nfacilidades de comunicação de grão fino para a passagem de mensagens e de comunicação\nem contextos seguros, o acesso remoto a memória e a ativação remota de agentes."
  },
  {
    "keywords": [
      "Home-Banking",
      "Desenvolvimento iOS",
      "Desenvolvimento móvel",
      "Segurança móvel",
      "CocoaTouch",
      "iOS Development",
      "Mobile development",
      "Mobile security",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "iOS development: increasing Home Banking reliability with integration of strong authentication mechanism",
    "autor": "Solans, Carlos Miguel Rebelo",
    "data": "2022-01-13",
    "abstract": "Neste momento, as aplicações móveis encontram-se cada vez mais presentes no quotidiano de cada individuo, permitindo desempenhar diferentes tarefas, tais como gerir contas bancárias e transação de fundos monetários. \nDevido à rápida adoção e desenvolvimento de IoT, é também importante garantir a proteção da segurança dos Utilizadores de ataques cibernauticos impedindo o acesso destas contas e execução de determinadas operações não autorizadas pelos respetivos títulares de contas via aplicações de Home-Banking.\nTendo isto em consideração, esta Dissertação tem como principal objetivo analisar e integrar uma camada de segurança para Autenticação e Validação de transações de fundos, designada por TrustFactor, numa aplicação existente de Home-Banking.\nVisto que a implementação será vocacionada para dispositivos móveis serão abordados temas relacionados com os Paradigmas de Desenvolvimento de Aplicações e tecnologias usadas no ecossistema Apple."
  },
  {
    "keywords": [
      "681.3.06"
    ],
    "titulo": "Pattern based user interface generation",
    "autor": "Barbosa, André Lopes",
    "data": "2012-12-12",
    "abstract": "Human Computer Interaction (HCI) is one of the most important aspects in software development. In order to produce valuable products, software companies are focusing more on the users and less on the technology behind their products.\nThis calls for new prospects for development cycles. Traditional methodologies are focused on the internals and there is little support to build a User Interface (UI) in a more iterative manner [12].\nModel Driven Development (MDD) [21] is a technique that has been used to increase software quality and boost development time. With MDD organizations are able to implement iterative development methodologies that start with high\nlevel models that are iteratively transformed into lower level models and ultimately source code in an automated way. High level models have several advantages because they are platform independent, easier to maintain, easier to reuse and ultimately they serve as documentation for the project.\nUnified Modelling Language (UML) is an industry standard language for modelling software. The problem with UML is that it’s not fit for UI models [4]. The UI requires a new modelling language that is able to represent UI aspects accurately. The HCI community came up with several solutions for this problems, ITS [28], WISDOM [16], Unified Modeling Language for Interactive Applications (UMLi) [4] and USer Interface eXtensible Mark-up Language (UsiXML) [23] are some examples on this matter.\nThis work proposes a method to reuse previous UI knowledge using patterns of high level models. The goal of this work is to improve the way developers build UI’s and maximize re-usability. Patterns are tested and robust solutions that have been used in other contexts and can even persist between different projects and teams. This work integrates in the Forward Engineering method (FEM) developed by the UsiXML community and uses the UsiXML User Interface Description Language (UIDL) to represent patterns of high level UI models.\nWe developed a pattern definition using a set of descriptive fields and UsiXML models. With the information provided by the pattern we are able to perform model transformations from the domain and task models to an Abstract User Interface (AUI) model. This gives developers the ability to reuse the structure of a UI developed in other context with a similar objective. This makes it easier for developers with little knowledge in HCI to develop good UI’s and also helps development teams to maintain consistency across an application."
  },
  {
    "keywords": [
      "Ergonomics",
      "Human-robot collaboration",
      "User interface",
      "Posture control",
      "Ergonomia",
      "Colaboração homem-robot",
      "Interface do utilizador",
      "Controlo de postura",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of a front-end application for a human-robot collaborative framework",
    "autor": "Martins, Guilherme da Silva Amorim",
    "data": "2023-12-15",
    "abstract": "ErgoAware aims at developing technology that reduce the worker’s exposure to ergonomic risk (e.g.\npoor postures) as there is a high incidence of Work-Related Musculoskeletal Disorders (WRMSDs), which\nrepresents an economic burden of 240 billion euros and is driven by the Industry 4.0 paradigm. Another\ngoal of ErgoAware is also to optimize Human-robot Collaborative processes to shape the robot’s assistance\naccording to the individual physiological requirements of each user.\nThis dissertation main goal is the development of a desktop graphical interface application to allow\nvisualization of the workers kinematic model, allow real-time monitoring of postural and fatigue metrics,\nand provide an abstraction layer for the ROS backend in order to allow the configuration of the integrated\nsensing technologies and develop control strategies. The objective will be for users to be able to manipu late ErgoAware’s adjustable parameters in an intuitive manner, with a user-friendly interface for easy tool\nusability."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Implementation and evaluation of tagged causal multicast as a rust library",
    "autor": "Pereira, Carlos Duarte Afonso",
    "data": "2020-06-16",
    "abstract": "Causal Consistency is gaining importance in modern geo-replicated distributed services: it\nis the strongest consistency model that does not sacrifice availability under high latency and\nnetwork partitions. However, traditional causal delivery middleware, while ensuring a de livery order consistent with causality, does not provide client applications with knowledge\nabout the end-to-end (as seen by each client process) happens-before relation. An end-to-end\nhappens-before is essential to modern applications, namely for the semantics of operation based CRDTs, but also for traditional applications, in which its absence may cause incorrect\nbehavior when using traditional causal delivery middleware. This thesis designs and im plements a Tagged Causal Multicast middleware service as a Rust library. Rust was chosen\nbecause it is a safe concurrent and fast programming language supporting both functional\nand imperative paradigms. This allows an efficient implementation where the use of com plex data structures does not decrease the performance as would be the case of using\nfunctional languages like Erlang. Finally, an empirical evaluation of the performance of\nthis middleware service is made, comparing the novel graph-based implementation against\na more traditional one based on vector clocks."
  },
  {
    "keywords": [
      "Augmented reality",
      "Computer vision",
      "Geographic Information System",
      "Maps",
      "Mapas",
      "Realidade aumentada",
      "Sistema de Informação Geográfica",
      "Visão por computador",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Augmented reality over maps",
    "autor": "Magalhães, Miguel Jorge da Lomba",
    "data": "2020-07-28",
    "abstract": "Maps and Geographic Information System (GIS) play a major role in modern society,\nparticularly on tourism, navigation and personal guidance. However, providing geographical\ninformation of interest related to individual queries remains a strenuous task. The main\nconstraints are (1) the several information scales available, (2) the large amount of information\navailable on each scale, and (3) difficulty in directly infer a meaningful geographical context\nfrom text, pictures, or diagrams that are used by most user-aiding systems. To that extent,\nand to overcome the aforementioned difficulties, we develop a solution which allows the\noverlap of visual information over the maps being queried — a method commonly referred\nto as Augmented Reality (AR).\nWith that in mind, the object of this dissertation is the research and implementation of a\nmethod for the delivery of visual cartographic information over physical (analogue) and\ndigital two-dimensional (2D) maps utilizing AR. We review existing state-of-art solutions and\noutline their limitations across different use cases. Afterwards, we provide a generic modular\nsolution for a multitude of real-life applications, to name a few: museums, fairs, expositions,\nand public street maps. During the development phase, we take into consideration the\ntrade-off between speed and accuracy in order to develop an accurate and real-time solution.\nFinally, we demonstrate the feasibility of our methods with an application on a real use case\nbased on a map of the city of Oporto, in Portugal."
  },
  {
    "keywords": [
      "Middleware",
      "Replicação",
      "Logs",
      "Logs distribuídos",
      "Bases de dados",
      "Replicação híbrida",
      "Replicação ativa",
      "Replicação passiva",
      "Tolerância a faltas",
      "Alta disponibilidade",
      "Replication",
      "Distributed logs",
      "Databases",
      "Hybrid replication",
      "Active replication",
      "Passive replication",
      "Fault tolerance",
      "High availability",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "High availability architecture for cloud based databases",
    "autor": "Abreu, Hugo Miguel Ferreira",
    "data": "2019-11-08",
    "abstract": "Com a constante expansão de sistemas informáticos nas diferentes áreas de aplicação, a\nquantidade de dados que exigem persistência aumenta exponencialmente. Assim, por\nforma a tolerar faltas e garantir a disponibilidade de dados, devem ser implementadas\ntécnicas de replicação.\nAtualmente existem várias abordagens e protocolos, tendo diferentes tipos de aplicações\nem vista. Existem duas grandes vertentes de protocolos de replicação, protocolos genéricos,\npara qualquer serviço, e protocolos específicos destinados a bases de dados. No que toca\na protocolos de replicação genéricos, as principais técnicas existentes, apesar de completa mente desenvolvidas e em utilização, têm algumas limitações, nomeadamente: problemas\nde performance relativamente a saturação da réplica primária na replicação passiva e o\ndeterminismo necessário associado à replicação ativa. Algumas destas desvantagens são\nmitigadas pelos protocolos específicos de base de dados (e.g., com recurso a multi-master)\nmas estes protocolos não permitem efetuar uma separação entre a lógica da replicação e\nos respetivos dados. Abordagens mais recentes tendem a basear-se em técnicas de repli cação com fundamentos em mecanismos distribuídos de logging. Tais mecanismos propor cionam alta disponibilidade de dados e tolerância a faltas, permitindo abordagens inovado ras baseadas puramente em logs.\nPor forma a atenuar as limitações encontradas não só no mecanismo de replicação ativa\ne passiva, mas também nas suas derivações, esta dissertação apresenta uma solução de\nreplicação híbrida baseada em middleware, o SQLware. A grande vantagem desta abor dagem baseia-se na divisão entre a camada de replicação e a camada de dados, utilizando\num log distribuído altamente escalável que oferece tolerância a faltas e alta disponibilidade.\nO protótipo desenvolvido foi validado com recurso à execução de testes de desempenho,\nsendo avaliado em duas infraestruturas diferentes, nomeadamente, um servidor privado\nde média gama e um grupo de servidores de computação de alto desempenho. Durante a\navaliação do protótipo, o standard da indústria TPC-C, tipicamente utilizado para avaliar\nsistemas de base de dados transacionais, foi utilizado. Os resultados obtidos demonstram\nque o SQLware oferece uma aumento de throughput de 150 vezes, comparativamente ao\nmecanismo de replicação nativo da base de dados considerada, o PostgreSQL."
  },
  {
    "keywords": [
      "Sampling",
      "Sampling techniques",
      "Security",
      "Measurement methods",
      "Active methods",
      "Passive methods",
      "Amostragem",
      "Técnicas de amostragem",
      "Segurança",
      "Métodos de medição",
      "Métodos ativos",
      "Métodos passivos"
    ],
    "titulo": "Sampling techniques applied to anomalous events detection",
    "autor": "Gama, Joel Filipe Esteves",
    "data": "2022-11-07",
    "abstract": "Nowadays, one of the major worries about a network is security. Since the network has become\nthe big platform it is, the number of attacks or attempts to steal information or just harm\nsomeone or something is getting bigger to handle or harder to find. Sampling techniques help\nto solve these problems as they are used to reduce the scope of the analysis, as well as the\nresources needed to perform it. By using sample techniques to search and find the attacks in\nthe network traffic it will become easier to detect attacks and keep the network secure. As\nwill be seen in the following sections, joining sampling and security is not an easy task to\ndo. Questions such as, what are the best techniques to be used, what are the best methods\nto be implemented, are inevitable when using sampling. However, sampling can bring more\nadvantages than disadvantages. Besides that, depending on the chosen measurement method,\nsampling technique or algorithm performed to analyse the samples, the results can change a lot\naccording to the target for the technique. To achieve results for evaluation, a Network-based\nIntrusion Detection System (NIDS) will be used to identify anomalous events present in the\nsamples."
  },
  {
    "keywords": [
      "Formal methods",
      "Hybrid systems",
      "Cyber-physical systems",
      "Theory of Programming Languages",
      "Functional programming",
      "Métodos formais",
      "Sistemas híbridos",
      "Sistemas ciber-físicos",
      "Teoria das Linguagens de Programação",
      "Programação funcional",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Adding uncertainty to real-time programming",
    "autor": "Carvalho, Rui Carlos Azevedo",
    "data": "2023-09-07",
    "abstract": "In this dissertation we cover the implementation in Haskell of an interpreter for a while-language capable\nof handling both hybrid and probabilistic effects. The interpreter is supported by both operational and\ndenotational semantics which were devised in this dissertation as well.\nWe started by studying a pre-existing syntax and operational semantics of a programming language\ncapable of performing wait calls and probabilistic choices through a random-number-generator. We then\nredefined this semantics to another one that is more suitable for statistical analysis in programming.\nNext we performed another iteration over these two semantics, more specifically we extended them to\nsupport full hybrid behaviour, traditionally used to encode interactions between digital devices and physical\nprocesses such as movement and time.\nWe also devised two denotational semantics corresponding to the operational semantics mentioned\nbefore, as a way of providing a mathematical abstraction, through the use of monads, to the programs of\nour language.\nNot only this, we also implemented a domain specific language embedded into Haskell, which thus\nprovides to the hybrid programmer all the expressive power that Haskell offers in addition to a palette of\ncombinators designed specifically for the hybrid domain. Such gives rise to an expressivity power much\ngreater than what the aforementioned while-language can provide.\nLastly, we presented and analysed several deterministic hybrid programs, such as cruise controllers,\nand added subtle probabilistic elements to them that reflect certain real-word scenarios. Such an addition\nlead from one possible execution to several possible executions; and most notably some of the latter\nrevealed safety issues introduced by the probabilistic elements.\nAll in all this dissertation has both theoretical and practical contributions that form a stepping stone\ntowards a rigorous engineering discipline of probabilistic hybrid systems."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "An evaluation of the GAMA/StarPU frameworks for heterogeneous platforms : the progressive photon mapping algorithm",
    "autor": "Palhas, Miguel Branco",
    "data": "2013",
    "abstract": "Recent evolution of high performance computing moved towards heterogeneous platforms:\nmultiple devices with different architectures, characteristics and programming models, share\napplication workloads. To aid the programmer to efficiently explore these heterogeneous\nplatforms several frameworks have been under development. These dynamically manage the\navailable computing resources through workload scheduling and data distribution, dealing\nwith the inherent difficulties of different programming models and memory accesses. Among\nother frameworks, these include GAMA and StarPU.\nThe GAMA framework aims to unify the multiple execution and memory models of\neach different device in a computer system, into a single, hardware agnostic model. It was\ndesigned to efficiently manage resources with both regular and irregular applications, and\ncurrently only supports conventional CPU devices and CUDA-enabled accelerators. StarPU\nhas similar goals and features with a wider user based community, but it lacks a single\nprogramming model.\nThe main goal of this dissertation was an in-depth evaluation of a heterogeneous framework\nusing a complex application as a case study. GAMA provided the starting vehicle\nfor training, while StarPU was the selected framework for a thorough evaluation. The progressive\nphoton mapping irregular algorithm was the selected case study. The evaluation\ngoal was to assert the StarPU effectiveness with a robust irregular application, and make a\nhigh-level comparison with the still under development GAMA, to provide some guidelines\nfor GAMA improvement.\nResults show that two main factors contribute to the performance of applications written\nwith StarPU: the consideration of data transfers in the performance model, and chosen\nscheduler. The study also allowed some caveats to be found within the StarPU API. Although\nthis have no effect on performance, they present a challenge for new coming developers.\nBoth these analysis resulted in a better understanding of the framework, and a comparative\nanalysis with GAMA could be made, pointing out the aspects where GAMA could be further\nimproved upon."
  },
  {
    "keywords": [
      "RGPD",
      "ElasticSearch",
      "Interoperabilidade",
      "Registo médico eletrónico",
      "GDPR",
      "ElasticSearch",
      "Interoperability",
      "Electronic health record",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Implementação de sistema de monitorização e controlo de interoperabilidade clínica",
    "autor": "Castanheira, António Manuel de Melo",
    "data": "2021",
    "abstract": "Atualmente tem-se verificado um aumento significativo da quantidade de informação\narmazenada digitalmente, devido a cada vez mais as infraestruturas de saúde recorrerem a\nsistemas digitais para armazenar a informação relativa aos intervenientes no seu universo.\nÉ, portanto, fulcral que essa informação seja regida por um conjunto de normas, de modo a\npermitir que seja compreendida sem se perderem dados importantes. Com o aumento do\nuso de ferramentas digitais para armazenamento e troca de informação também se torna\nimportante a monitorização dos dados trocados entre os sistemas, de modo a garantir a\nprivacidade e segurança dos intervenientes, bem como garantir o bom funcionamento do\nsistema.\nNesse sentido, o trabalho consistiu na implementação de uma ferramenta para a monitorização\nde mensagens trocadas na plataforma AIDA - Agência de Interoperação, Difusão e Arquivo\n- de maneira a contribuir de forma ativa para resolver as questões abordadas no RGPD. Esta\nferramenta é baseada em tecnologias como containers Docker, a base de dados ElasticSearch\ne a interface de monitorização Kibana."
  },
  {
    "keywords": [
      "Biotechnology",
      "Chemical compounds",
      "Time series",
      "Algorithms",
      "Preprocessing",
      "Biotecnologia",
      "Compostos químicos",
      "Series temporais",
      "Algoritmos",
      "Pré- processamento",
      "Engenharia e Tecnologia::Biotecnologia Industrial"
    ],
    "titulo": "Development of algorithms for the analysis and data mining of chemical compound prices",
    "autor": "Faria, Sofia Maria Alves",
    "data": "2020-05-25",
    "abstract": "Nowadays, the products deriving from the biotechnology industry have become quite valu able in the world market. Hence, it is highly advantageous to find out how the prices\nof the different chemical compounds needed for biotechnological processes behave in the\nbioeconomy. The SISBI project was developed to allow the retrieval and collection of different\nprices associated with certain chemical compounds through different available sources and\ndatabases. With access to this information, some behaviours and patterns can be detected\nin the price variations, indicating other relevant knowledge, such as the biotechnological\ninterest of this compound in the field. However, it is necessary to take into account that\nSISBI data, although relevant, have inconsistencies that do not support an efficient analysis\nof these data, which is the case for the existence of duplicates, different units and problems\nin the price integration. As a result, this study developed algorithms to identify and solve\nthese problems and to analyze the prices of compounds through time series. To effectively\nevaluate these data, a new database, bioanalysis, was built based on the data from the SISBI\nproject. Then, several preprocessing methods were applied, including the elimination of\nduplicates, conversion of units, removal of defective and inconsistent prices, which led to\nthe solution of the various complications encountered. Consequently, once the data was\nprepared for analysis, the prices pertaining to two specific metabolites, 4-aminopyridine and\nmethane, were examined. Thus, different price variations over time were compared between\ndifferent configurations (quantity + unit) of the same metabolite and between different\nmetabolites. These variations were divided by the different price providers to identify any\nspecific relationship or pattern depending on where the data originate. However, in this\nstudy, no particularly cheap provider was detected between 4-aminopyridine configurations\nor between the two metabolites. The only association found occurred only between certain\nmethane configurations. In addition, the price variations analyzed are mostly constant, and\nwhen they are not, they do not show any pattern or seasonality. These results revealed that,\nusing only the prices available to date, no correlation was determined by identifying the\nproviders associated with low prices when comparing different metabolites or configurations."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Design a market hub platform for utilities",
    "autor": "Rodrigues, Xavier Passos",
    "data": "2018-12-30",
    "abstract": "Software Engineering has been contributing, over the years, to a better and more \nefficient production of software. Through its methodologies and processes, it has been \nused to increase the assurance that the software produced is robust, of quality, easy to\nupdate, and above all that, conforms to the requirements identified by the stakeholders.\n With the growth of data sharing, collection and storage in the utilities sector, \nthere is also an urgent need to maintain and use the available information in a useful \nway. This requires the adoption of strategies and infrastructures that can help leveraging \nthis form of treatment to make it possible to improve the quality of certain utility sectors \n(gas, electricity, water, internet, communications). However, much of this process is still \nnowadays controlled solely by the production and distribution companies, preventing \nthe all other users to participate in the process.\nTo try to undo the supremacy of the production and distribution companies have on \nthe utilities panorama, and in support of the European Commission vision, the energy \nsector is trying to move towards a liberalized market and, with this, it aims to enable all \nentities such as consumers, retailers, producers, distributors, to contribute to the \nmanagement of the network and for new business models to emerge.\nTo sustain this ecosystem, where these entities can communicate and share data, it\nwould be advantageous to have a platform that would allow the communication of such \ndata between all users.\nIn this project, and through the application of SE techniques, we will develop, step \nby step, a model for a modular, scalable and integrated environment to enable demand \nresponse, data exploration, storage and fulfill all the European Union-wide Data \nProtection Regulation (GDPR)."
  },
  {
    "keywords": [
      "Customer",
      "Telecommunications",
      "Data mining",
      "Quality of service",
      "Artificial intelligence",
      "Cliente",
      "Telecomunicações",
      "Mineração de dados",
      "Qualidade de serviço",
      "Inteligência artificial",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Benchmarking deep learning for predicting telecommunications recurring problems",
    "autor": "Castro, Vitor José Ribeiro",
    "data": "2020-11-30",
    "abstract": "Nowadays, companies live in a scenario of strong competitiveness. The telecommunications\nmarket is not an exception and it is possible to offer a differentiation from competition through\nbetter service quality, differentiated support and even better value proposals. With the evolution of\ntechnologies, companies have more data about their customers and the usage profile of each one\nof them. With this information it is possible to establish a better relationship with the customer\nthrough a more efficient support service.\nThe evolution of artificial intelligence and computational power, combined with existing data, al lows for several comparisons between different machine learning algorithms. In this dissertation,\na prediction model capable of predicting recurrences of contacts with the customer service is pro posed. The aim is to predict whether a particular problem reported by the customer will repeat and\nrequire a new contact, so that it is possible to correct those problems in advance, making the user\nexperience more pleasant and fluid. In order to achieve the best possible model, different classical\nmachine learning approaches were tested, along with several deep neural network architectures.\nIn recent years, deep neural networks have shown interesting results in several non-tabular appli cations, therefore being interesting to test them in tabular applications like the one present in this\nwork. TabNet, developed by Google, is a deep neural network adjusted to perform the better in tabu lar datasets, and was also tested, as it has shown better performance than several neural networks\nor decision-tree bases algorithms.\nThe used data were collected by various internal systems, the most important of which being the\none related to customer support calls. The customer service, due to its size and complexity, has a\nsystem that monitors all calls and their motivations, as well as the parties involved (both operator\nand customer) and other additional data such as time spent and the call outcome. Data from other\nsystems is related to billing, service usage and customer profile, and is added to help to understand\nthe context of the call.\nThe model that shown the best results was CatBoost, a decision trees based algorithm, showing\nan AUC_ROC of 79%, with a Recall of 61% and a Precision of 62%, allowing the identification of about\n8,6% of the 3.9 million calls made to the support service as recurrences even before they occur,\nabout 340k cases. In an ideal scenario, all these calls would be avoided, allowing a substantial cost\nreduction for the company, as well as a consequent increase in customer satisfaction in relation to\nthe service.\nThe CatBoost model showed better training times and less memory needs, while achieving a better performance than the different architectures of deep neural networks proposed. Only TabNet\nwas able to achieve a similar performance, while maintaining a higher training time. However, in\nfutures uses, where the CatBoost model achieves a plateau and is not benefiting for the increasing\ndata, it could be useful to use TabNet as the model in production. TabNet has the advantage of being\na neural network and, for that reason, being more capable of breaking the plateau that classical\nmodels often achieve."
  },
  {
    "keywords": [
      "Waste management",
      "Object detection",
      "Instance segmentation",
      "Gestão de resíduos sólido-urbanos",
      "Deteção de objetos",
      "Segmentação de instância",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Prediction system for municipal waste containers",
    "autor": "Silva, Nuno Barbosa Leão Beça e",
    "data": "2023-12-28",
    "abstract": "Recycling stands as one of the most effective contemporary practices for pollution prevention.\nThrough the process of recycling, a reduction in our reliance on finite natural resources is\nachieved, concurrently leading to energy conservation, decreased carbon dioxide emissions,\nand economic savings. In the context of the European Union, it is noteworthy that Portugal\ncurrently registers one of the lowest recycling rates. Consequently, it becomes imperative\nfor the nation to commit towards accomplish the European objective of recycling all single use packaging materials. A significant strategy to boost these recycling rates involves\nthe widespread deployment of small, medium or large capacity waste containers, typically\nranging from 120 liters to 360 liters, across municipalities. However, the efficient management\nof these containers necessitates a consistent and meticulous approach by waste collection\nentities. Presently, the methodology employed in this regard is antiquated, characterized\nby waste collection teams manually inspecting each container within their designated areas\nto check their fill status. This labor-intensive process poses inherent inefficiencies and\nchallenges. The primary objective of this master’s project involves the development of\na system capable of detecting and classifying urban waste containers. This goal holds\npromising applications in the domain of waste management, potentially facilitating the\ngeneration of daily collection routes in the future. Images for this study were sourced from\nindividual contributors, from Street View feature in Google Maps and a project known as\nTidy City, which gathers various items, including containers, from a designated municipality.\nSubsequently, a model was constructed with the ability to discern and categorize a specific\ncontainer based on the type of waste it accommodates, the configuration of the container\n(e.g., 4 wheels, 2 wheels), and the condition of its lid (open, closed, or full). Additionally,\nthe model demonstrates proficiency in identifying and classifying waste materials in close\nproximity to the container."
  },
  {
    "keywords": [
      "Constraint-based model",
      "Kinetic model",
      "Hybrid model",
      "MEWpy",
      "Sampling",
      "Hybrid simulation",
      "FBA",
      "Succinate production",
      "GECKO",
      "Modelos com base em restrições",
      "Modelos cinéticos",
      "Modelos híbridos",
      "Amostragem",
      "Simulação hibrida",
      "Produção de succinato",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Integrating kinetic and constraint-based models of metabolism",
    "autor": "Pereira, Mariana Marques",
    "data": "2022-12-15",
    "abstract": "Mathematical models are fundamental tools for explaining biological behaviors. Dynamical and\nconstraint-based models are two different formulations that attempt to capture the phenotypic\ncapabilities of organisms.\nDynamic models are formulated as ordinary differential equations (ODEs) that simulate metabolic\nconcentration over time. These models, however, only depict changes in metabolic concentration and\nrely on mechanistic details and kinetic parameters that are not always available. Constraint-based\nmodels, on the other hand, have a better cellular perspective. By performing constraint-based\noptimizations, they simulate cell behavior under different genetic and environmental conditions.\nMetabolic models also have some drawbacks. In addition to providing no mechanical knowledge of any\nchemical reactions (beyond their stoichiometry) and no information regarding metabolic concentrations\nor reaction flux dynamics, they are based on a steady-state assumption that production and\nconsumption of metabolites are balanced within the cell. Constraint-based optimizations, Flux Balance\nAnalysis (FBA) methods, generally return an infinite set of solutions, requiring the imposition of additional\nassumptions to identify unique flux distributions.\nWhile individually, both modeling approaches have several advantages, one lacks the benefits\nprovided by the other. With this in mind, we implemented a tool in MEWpy capable of hybridizing kinetic\nand constraint-based models. With it, we were able to reduce the constraint-based model solution space\nby overlapping the kinetic solution space and sampling the kinetic model, analyze the impact of different\nstandard deviation values on the sampling, perform hybridization of enzymatic constrained models, and\nfurther compare distinct hybridization approaches. To demonstrate the potential of our tool and its\napplicability in strain optimization, we performed hybrid optimization of succinate production, where we\ndiscovered a set of genetic mutations that boosted its production."
  },
  {
    "keywords": [
      "Animation",
      "Lightning",
      "Physics",
      "Electric breakdown",
      "Patterns",
      "Rendering",
      "Real-time",
      "Computer graphics",
      "Physically plausible",
      "Space colonization",
      "Animação",
      "Trovoada",
      "Relampagos",
      "Física",
      "Quebra dielétrica",
      "Padrões",
      "Renderização",
      "Tempo real",
      "Computação gráfica",
      "Fisicamente plausível",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Space colonization for the procedural generation of lightning",
    "autor": "Reis, Nuno Filipe Maranhão dos",
    "data": "2022-07-21",
    "abstract": "The procedural generation of geometry within the space of computer graphics has been a topic of study for quite\nsome time, benefiting from a more unpredictable brand of randomness. Similarly, the exploration of lighting as a\nphenomenon within virtual space has been a field of study of comparable age.\nDespite its age and early adoption, there is a surprising lack of research in emulating the phenomenon of\nlighting past its interactions with the world. Most implementations of procedurally generated lightning within video\ngames are based on randomized data trees. When part of the skybox, 2D meshes or textures are randomly\nselected from a pre-made pool. There are, however, methods based entirely on the dielectric breakdown model,\nusing approximations to solve a Laplacian equation.\nThis dissertation aims to present an alternative approach to the randomized and procedural generation of\nlightning bolts based on the Space Colonization algorithm. While the algorithm was first conceived for use in\nbotanical applications, modeling the growth of biological structures, the similarities between the results produced\nby the dielectric breakdown model and botanic modeling algorithms coupled with the visual likeness of a lightning\nbolt and certain trees, made for solid groundwork upon which to establish this unique approach.\nAs such, this work largely aims to be a first step into this particular realm, showing Space Colonization\nas a suitable algorithm for this specific purpose. That being said, a large portion of time was spent iterating,\nmodifying and experimenting with ideas that were either discarded or adapted, an effort primarily dedicated\ntowards controlling and stifling the possible growth of branches in ways beyond the reduction of attractors.\nThe original algorithm was altered, focus put especially on the creation of a singular channel at a time, mixing\ndiscoveries from previous research with the work done on manipulating Space Colonization. Instead of the\nvenation patterns observed with the original work, the stifling of any growth means that each node has a chance,\nwhen created, of sprouting a branch and each branch is, in turn, a different, modified instance of the same\nunderlying concept providing an additional level of control. Effort was equally placed on showcasing different\nproperties inherent to a lightning strike, such as its iterative construction when descending from its origin.\nIn the rendering section, along with recreating the bloom and glow effect seen in previous works, effort was put\ninto recreating the strobing observed in capturing slow-motion footage of lightning bolts with special detail given\nto this. In addition, parameters were joined with a waypoint system to allow for a great degree of freedom when\ngenerating new bolts."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Um sistema de recomendação para o produto RAID",
    "autor": "Dias, Fábio Diogo Peixoto",
    "data": "2018",
    "abstract": "O ciclo de vida dos produtos informáticos é um processo contínuo, durante o qual vão surgindo de uma forma regular novas actualizações, dia após dia, que frequentemente visam a melhoria dos processos de interação do utilizador com o produto. Contudo, este processo é complicado, uma vez que responder de uma forma apropriada às necessidades de cada cliente não é uma tarefa fácil. De forma a combater esta tarefa, a WeDo Technologies decidiu implementar no seu principal produto de software, o RAID, um sistema capaz de monitorizar as diversas interações dos clientes com o produto e com base na informação recolhida apresentar-lhes cenários de interação avançados que de alguma forma reflitam as suas necessidades, por exemplo, em termos de introdução de dados ou configurações de serviços. Desta forma, é possível retirar algum trabalho aos utilizadores do software uma vez que o sistema adianta-se no seu fornecimento. Com este sistema, a WeDo pretende alargar um conjunto diverso de funcionalidades dos seus produtos de forma a facilitar os processos de interação com o utilizador, promovendo o desenvolvimento de um protótipo para a geração de recomendações adequadas a cada utilizador dos seus produtos de software, a partir de um conjunto base das suas preferências de utilização do sistema. Basicamente, com este trabalho de dissertação estudou-se, projetou-se e desenvolveu-se esse sistema de recomendação, materializando, de certa maneira algo que M. O'Brien referiu: “The Web is leaving the era of search and entering one of discovery. What’s the difference? Search is what you do when you’re looking for something. Discovery is when something wonderful that you didn’t know existed, or didn’t kown how to ask for, finds you.” (M. O'Brien, 2006)."
  },
  {
    "keywords": [
      "Computational biology",
      "Protein classification",
      "Machine learning",
      "Deep learning",
      "Biologia computacional",
      "Classificação de proteínas",
      "Aprendizagem de máquina",
      "Aprendizagem profunda",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Development of a deep learning-based computational framework for the classification of protein sequences",
    "autor": "Barros, Miguel Ângelo Pereira",
    "data": "2022-12-16",
    "abstract": "Proteins are one of the more important biological structures in living organisms, since they\nperform multiple biological functions. Each protein has different characteristics and properties,\nwhich can be employed in many industries, such as industrial biotechnology, clinical applications,\namong others, demonstrating a positive impact.\nModern high-throughput methods allow protein sequencing, which provides the protein\nsequence data. Machine learning methodologies are applied to characterize proteins using\ninformation of the protein sequence. However, a major problem associated with this method\nis how to properly encode the protein sequences without losing the biological relationship\nbetween the amino acid residues. The transformation of the protein sequence into a numeric\nrepresentation is done by encoder methods. In this sense, the main objective of this project is to\nstudy different encoders and identify the methods which yield the best biological representation\nof the protein sequences, when used in machine learning (ML) models to predict different labels\nrelated to their function.\nThe methods were analyzed in two study cases. The first is related to enzymes, since\nthey are a well-established case in the literature. The second used transporter sequences, a\nlesser studied case in the literature. In both cases, the data was collected from the curated\ndatabase Swiss-Prot. The encoders that were tested include: calculated protein descriptors;\nmatrix substitution methods; position-specific scoring matrices; and encoding by pre-trained\ntransformer methods. The use of state-of-the-art pretrained transformers to encode protein\nsequences proved to be a good biological representation for subsequent application in state-of-the-art ML methods. Namely, the ESM-1b transformer achieved a Mathews correlation coefficient\nabove 0.9 for any multiclassification task of the transporter classification system."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Geração de aplicações multi-plataforma a partir de modelos",
    "autor": "Mendes, Frederico Jorge Falcão Torres de Castro",
    "data": "2017",
    "abstract": "Na área de Engenharia de Software, a modelação de sistemas com recurso a diagramas,\npermite representar um sistema de forma padronizada, com o intuito de facilitar a compreensão da especificação, estrutura lógica, e documentação dos mesmos.\nHoje em dia, no mundo empresarial, a utilização de diagramas através de ferramentas\npróprias para o efeito tem como objetivo a comunicação entre equipas, inserindo-se na fase\nde modelação dos projetos. No entanto, a construção de aplicações com recurso a técnicas\nde low code, ou mesmo zero code, é uma realidade cada vez mais atual.\nA evolução natural deste conceito resultará na geração automática de código através de\numa linguagem visual, como os diagramas, facilitando, assim, a produção de código, e ao\nmesmo tempo, conseguir-se-á uma poupança de tempo aproveitando o trabalho realizado\nnuma fase mais precoce do projeto. Posto isto, a utilização de modelos, mais ou menos\nstandard, como forma de especificar e prototipar aplicações é e será, cada vez mais, uma\nrealidade bem fundada e com sucesso assinalável, permitindo também gerir de forma mais\neficaz questões de multi-plataforma, visto que a geração de código não é exclusiva a nenhum\nparadigma nem linguagem de programação específica.\nCom esta dissertação pretende-se, então, utilizar modelos UML como mecanismo único\nde especificação de aplicações, automatizando o processo de construção do respetivo código\ne os aspetos tecnológicos relativos ao seu deployment e instalação, disponibilizando uma\nferramenta que possibilite o processo de criação de aplicações web e android a partir de\ndiagramas UML.\nAssim, foi criada uma aplicação que, através da interação do utilizador, recebe diagramas\nde classe exportados em formato XML interpretando-os e gerando aplicações android e\naplicações web. Estas aplicações realizam as operações CRUD para cada entidade representada\nno diagrama de classe."
  },
  {
    "keywords": [
      "Business intelligence",
      "Data warehouse",
      "Data mining",
      "Processo de tomada de decisão",
      "Key performance indicators",
      "Dashboards",
      "Decision making process",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Soluções de business intelligence - gestão de oficina automóvel",
    "autor": "Alves, Carolina Gonçalves",
    "data": "2023-11-21",
    "abstract": "Diariamente existem mudanças no ambiente organizacional e as organizações \nprocuram adaptar-se e acompanhar as tendências do mercado. É necessário inovar nas \ntecnologias, haver flexibilidade e adaptabilidade por parte dos colaboradores e retirar o \nmelhor proveito dos dados.\nNesse sentido, o conceito de Business Intelligence surge apresentando um conjunto \nde métodos e ferramentas para disponibilizar informação e suportar a tomada de decisão. O \nobjetivo é otimizar e simplificar o processo de tomada de decisão, agilizar a extração de dados \ne sua divulgação como informação valiosa e possibilitar soluções de qualidade e confiáveis.\nNeste projeto apostou-se no desenvolvimento e implementação de uma solução de \nBusiness Intelligence capaz de suportar a gestão de oficina no ambiente organizacional em \nquestão.\nAo longo do desenvolvimento do projeto realizou-se a construção de um Data \nWarehouse, o levantamento de requisitos e métricas para implementação da solução, a \nconstrução de sistemas de processamento analítico e aplicações de front-end.\nO principal objetivo desta solução para a organização é a recolha e monitorização da \ninformação para observar valores passados e atuais, de modo a compreender as tendências, \nobservar pontos a melhorar e visualizar dados futuros.\nAtravés da solução desenvolvida, a organização sentiu melhorias nas decisões diárias, \numa maior capacidade de acesso ao detalhe e uma apresentação da informação de forma \nfidedigna e de qualidade."
  },
  {
    "keywords": [
      "Ambient intelligent system",
      "Decision support system",
      "Attention",
      "User behaviour",
      "Sistemas de ambientes inteligentes",
      "Sistema de apoio à decisão",
      "Atenção",
      "Comportamento do utilizador",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Learning frequent behaviours patterns in intelligent environments for attentiveness level",
    "autor": "Cardoso, Catarina",
    "data": "2017",
    "abstract": "Nowadays, when it comes to achieving goals in business environments or educational\nenvironments, the successful on a person performing a task has an important role.\nHowever, this performance can be affected by several factors. One of the most common\nis the lack of attention. The individual’s attention in performing a task can be\ndeterminant for the final quality or even at the task’s conclusion.\nIn this project is intended to design a solution that can help on the reduce or even\neliminate the lack of attention on performing a task. The idea consists on develop\na software that capture the user behaviour through the mouse and keyboard usage.\nFurthermore, the system will analyse how the devices are used. It will be quantified\nthe attention level and, after several captures for each user, it will be defined for each\nuser an user profile. Through standardization of user’s behaviour it will be possible to\ndetermine the learning style of each user."
  },
  {
    "keywords": [
      "Asynchronous communication",
      "Server push technologies",
      "Microservices",
      "Hexagonal pattern",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Arquiteturas assíncronas na comunicação entre serviços frontend e backend",
    "autor": "Petronilho, Ricardo André Gomes",
    "data": "2022",
    "abstract": "Hoje em dia a comunicação assíncrona entre serviços, independentemente da plataforma (desktop,\nmobile, smart tv, smartwatch, etc), é cada vez mais frequente.\nParte do software produzido pelas empresas de telecomunicações, neste caso, pela empresa\nCelfocus, consiste em realizar operações assíncronas e, por conseguinte, receber notificações sobre o\nestado dessas operações. A título de exemplo, quando um funcionário numa loja de telecomunicações\nrealiza uma operação (assíncrona) como alterar o tarifário de um cliente, é necessário feedback dessa\noperação (ou das várias operações espoletadas pela mesma), através de uma notificação com origem\nno servidor e destino para o browser do funcionário.\nPara conseguir implementar esta comunicação em tempo real, tecnologias denominadas por server\npush foram desenvolvidas. Cada uma delas tem um cenário ideal de uso, diferentes características,\nvantagens e desvantagens.\nA presente dissertação consiste em investigar as tecnologias existentes para a comunicação entre o\nfrontend e backend e, depois, desenvolver um sistema que permite enviar e receber notificações.\nPor fim, para comprovar que a solução conceptual proposta, efetivamente, funciona na prática, são\nrealizados testes funcionais."
  },
  {
    "keywords": [
      "Shallow water equations",
      "Fluid simulation",
      "Caustics",
      "Real time",
      "Heightfield",
      "Equações de águas pouco profundas",
      "Simulação de fluídos",
      "Cáusticas",
      "Tempo real",
      "Campo de alturas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Shallow waters simulation",
    "autor": "Castro, Carlos Peixoto Antunes de",
    "data": "2022",
    "abstract": "Realistic simulation and rendering of water in real-time is a challenge within the field of computer graphics, as it\nis very computationally demanding. A common simulation approach is to reduce the problem from 3D to 2D by\ntreating the water surface as a 2D heightfield. When simulating 2D fluids, the Shallow Water Equations (SWE)\nare often employed, which work under the assumption that the water’s horizontal scale is much greater than it’s\nvertical scale.\nThere are several methods that have been developed or adapted to model the SWE, each with its own advantages\nand disadvantages. A common solution is to use grid-based methods where there is the classic approach\nof solving the equations in a grid, but also the Lattice-Boltzmann Method (LBM) which originated from the field of\nstatistical physics. Particle based methods have also been used for modeling the SWE, namely as a variation of\nthe popular Smoothed-Particle Hydrodynamics (SPH) method.\nThis thesis presents an implementation for real-time simulation and rendering of a heightfield surface water\nvolume. The water’s behavior is modeled by a grid-based SWE scheme with an efficient single kernel compute\nshader implementation.\nWhen it comes to visualizing the water volume created by the simulation, there are a variety of effects that\ncan contribute to its realism and provide visual cues for its motion. In particular, When considering shallow water,\nthere are certain features that can be highlighted, such as the refraction of the ground below and corresponding\nlight attenuation, and the caustics patterns projected on it.\nUsing the state produced by the simulation, a water surface mesh is rendered, where set of visual effects are\nexplored. First, the water’s color is defined as a combination of reflected and transmitted light, while using a Cook-\nTorrance Bidirectional Reflectance Distribution Function (BRDF) to describe the Sun’s reflection. These results\nare then enhanced by data from a separate pass which provides caustics patterns and improved attenuation\ncomputations. Lastly, small-scale details are added to the surface by applying a normal map generated using\nnoise.\nAs part of the work, a thorough evaluation of the developed application is performed, providing a showcase of\nthe results, insight into some of the parameters and options, and performance benchmarks."
  },
  {
    "keywords": [
      "617.3:681.3",
      "681.3:617.3",
      "616-089.8"
    ],
    "titulo": "Planeamento cirúrgico ortopédico assistido por computador: uma abordagem 3D",
    "autor": "Ribeiro, João Pedro de Araújo",
    "data": "2012",
    "abstract": "O planeamento tem vindo a ganhar preponderncia entre a comunidade\nmédica de cirurgiões. A elaboração de um plano para a cirurgia é fundamental\npara que esta se desenrole da melhor forma possível, encurtando assim\nos tempos de recuperação do paciente. No caso da cirurgia ortopédica, o\nplaneamento tem uma importância ainda mais acentuada, devido à relação\nestreita entre os tempos de recuperação e fiabilidade a que o paciente fica\nsujeito e o sucesso da cirurgia. Assim, é importante que os cirurgiões disponham\nde ferramentas que os auxiliem nessa tarefa, por forma a torná-la menos\nmorosa e complexa. No entanto, isso não acontece. As soluções ao dispor\ndos cirurgiões revelam-se insuficientes, não possibilitando uma visão global\nda extensão da lesão e possíveis intervenções. Dessas soluções, apenas um\npequeno número permite a modelação tridimensional do estudo imagiológico\nde tomografia computorizada. Porém, não possibilitam que a análise da\nintervenção desenvolvida pelo cirurgião possa ser feita no mesmo universo\ngeométrico. Isto deve-se essencialmente à dificuldade de interoperação entre\ntipos de formato imagem diferentes, dado que o estudo imagiológico é do tipo\nmatricial e os templates representativos dos apoios físicos, vectorial. Posto\nisto, o presente trabalho pretende apresentar uma solução para este problema\nde interoperação, bem como a sua implementação. Através da solução\napresentada, o cirurgião tem a possibilidade de manipular uma isosuperfície\nrenderizada tridimensionalmente a partir do estudo imagiológico selecionado.\nDe seguida, é-lhe permitido adicionar as representações digitais dos apoios\nfísicos utilizados, por forma a avaliar a viabilidade da sua abordagem. Em\nconjunto, tem a possibilidade de gerar novas isosuperfícies de valores diferentes,\nbem como cortar o modelo final num plano previamente definido, o\nque permite uma análise, agora em duas dimensões, da intervenção planeada.\nPosto isto, é possível concluir que a solução apresentada auxilia o cirurgião\nno desenvolvimento de um planeamento mais adequado, podendo analisar\ntridimensionalmente o impacto da sua intervenção no paciente."
  },
  {
    "keywords": [
      "Ciências Médicas::Ciências da Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Segurança no acesso ao registo clínico eletrónico",
    "autor": "Rodrigues, Bruno Jorge de Sales Gomes",
    "data": "2015",
    "abstract": "A visualização de relatórios de Meios Complementares de Diagnóstico e\nTerapêutica (MCDTs) é importante para uma boa prática médica, no sentido\nem que estes fornecem indicações importantes sobre o estado de saúde do\npaciente.\nTorna-se, então, crucial o desenvolvimento de uma aplicação que permita\na médicos em Centros de Saúde aceder de uma forma segura à informação\nguardada numa base de dados Hospitalar. Essa necessidade foi encontrada\nno Centro Hospitalar do Porto, organização para a qual foi implementado\neste projeto.\nCom a presente dissertação, procurou-se, ainda, desenvolver uma análise\nde segurança ao sistema para que o trabalho  nal fosse projetado e elaborado\nde acordo com as reais necessidades do Hospital.\nO sistema é apresentado e analisado neste documento. Foram, também,\nfeitos testes à segurança do mesmo, sendo que não foi possível averiguar\nquanto à sua performance."
  },
  {
    "keywords": [
      "Supply chain analytics",
      "Big Data",
      "Apache",
      "Apache Spark",
      "Distributed data processing",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Otimização distribuída de indicadores logísticos com recurso a ferramentas Apache",
    "autor": "Oliveira, Miguel Pimentel de",
    "data": "2022-05-17",
    "abstract": "The success of a manufacturing company is linked to the efficiency of its supply chain. More specifically, procure ment, which refers to the scheduling and quantity planning of raw materials to order from suppliers, can have a\nbig impact on the company’s operating costs.\nTo be able to avoid stock shortages and assure the necessary stock levels for production, there are a few\nsafety measures such as: Safety Time and Safety Stock. In Bosch’s plant located in Braga, the assignment of\nthese values is done infrequently and the data taken into consideration is not very specific.\nThis dissertation proposes a dynamic solution that should be able to provide a set of optimal pair solutions of\nSafety Time and Safety Stock according to certain objective measures, for each of the raw materials currently\nbeing used. This calculation is based on logistic data, which of some are static (master data) and other is historic\ndata.\nThis solution was developed using distributed data processing frameworks, such as Apache, for execution\nspeedup. More specifically, HDFS (Hadoop Distributed File System), a technology from Apache Hadoop to store\ndata in a distributed manner and Apache Spark to load and process the data, by splitting processing tasks and\nassigning them to different servers inside a cluster."
  },
  {
    "keywords": [
      "Cancer",
      "Deep learning",
      "Machine learning",
      "Transcriptomics",
      "Cancro",
      "Aprendizagem máquina",
      "Transcriptómica",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Developing deep learning methods to predict cancer and its outcome from transcriptomics data",
    "autor": "Soares, Óscar Marques",
    "data": "2019-11-28",
    "abstract": "Cancer is one of the major causes of death in developed countries. It is not a single disease,\nbut a group of different types of diseases with specific symptoms, treatments and prognosis.\nEarly diagnosis and prognostic assessment are essential to select the best treatment for each\ncase.\nDeep learning is a branch of machine learning that became popular in recent years. Deep\nlearning methods have been employed in a broad range of areas including self-driving cars,\nnatural language processing, computer vision, health, among others.\nThe main goal of the thesis is to develop deep learning methods to predict cancer and its\noutcome from transcriptomics data. Reviewing literature, exploring datasets, developing\npipelines and validating the methods using a case study are some of the tasks needed to\nachieve the goals of the thesis.\nThe developed methods are implemented as a pipeline for creating models from gene\nexpression data. The framework is capable of reading and pre-processing these data, and\ntraining, optimizing and evaluating traditional machine learning and deep learning models.\nThe framework was showcased by using the METABRIC dataset as a case study, which\ncontains samples from breast cancer patients. The gene expression microarray data from the\ndataset was used to generate traditional, deep learning and multi-task models. The models\nwere used to predict the expression of Estrogen Receptor (ER), the subtype of breast cancer\nregarding ER, Human Epidermal Growth Factor (HER-2) and Progesterone Receptor (PR) and\nthe prognosis of breast cancer patients with Nottingham Prognostic Index (NPI), respectively.\nAnother dataset allowed the use of single-cell RNAseq data and confirmed the main trends\nof the results.\nOverall, the results were promising with classification tasks obtaining good results while\nregression models had a poorer performance. While the best results were obtained with\ntraditional machine learning models, deep learning models were near and could provide\nbetter results if the dataset contained a larger number of samples."
  },
  {
    "keywords": [
      "621.39"
    ],
    "titulo": "Avaliação de QoS-QoE no serviço de videochamada SkypeTM",
    "autor": "Lopes, André D.",
    "data": "2012",
    "abstract": "Web-based videotelephony services currently occupy a prominent place in the wide range\nof services and generated network traffic. With the growing use of such services comes also an\nincreasing need for the evaluation of user experience. Depending on network conditions, there\nare relevant QoS parameters (bandwidth, delay, loss ratio, jitter, etc.) that have an impact on the\nquality of experience of the service.\nThe objective of this thesis is to find out which network parameters affect videocalls quality\nof experience, which are the threshold values for which the quality of experience levels are\naffected, in what way is the quality of experience impacted and what is their relation.\nThe quality of experience was evaluated using objective methods for all video and audio\nsamples collected during the experimentation phase. Conclusions were made based on the results\nwhich are presented in this dissertation."
  },
  {
    "keywords": [
      "Hybrid applications",
      "Internet of things",
      "Smart homes",
      "Aplicações híbridas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aplicações híbridas para smart homes",
    "autor": "Gonçalves, Leonel da Cruz",
    "data": "2020-12-22",
    "abstract": "Internet of Things concept has achieved a huge popularity in the last years due to the quality of life its use is\nable to provide its users. Among the several areas where this concept is applied, Smart Homes are one of the most popular, which are increasingly a reality in our daily lives. In a smart home, in addition to the IoT devices and a Hub that connects them to each other, it’s indispensable the use of an application that allows the management of their equipment, regardless of the location around the house. In this context, whenever a company decides to invest in this business sector, among which several decisions are made in the initial phase of the project, stands out the choice of the type of application to be developed to control the different equipment. This choice may prove difficult due to the little information available regarding the behavior\nof application types changes in this context.  The goal of this dissertation is to study one of the existing types of applications, hybrid applications, and to bridge the analysis results in the development of one that allows the management of a smart home. This application will allow the analysis of the behavior of hybrid applications in the context of Smart Homes, providing study tools for the evolution of this business area."
  },
  {
    "keywords": [
      "Sistemas de data warehousing",
      "ETL",
      "Integração em tempo real",
      "Captura de dados novos ou alterados",
      "Data warehousing em tempo real",
      "Data warehousing systems",
      "Real time integration",
      "Change data capture",
      "Real time data warehousing",
      "681.3"
    ],
    "titulo": "Captura de dados em tempo real em sistemas de data warehousing",
    "autor": "Dias, Miguel Gonçalves",
    "data": "2013",
    "abstract": "massificação dos sistemas de informação tem contribuído significativamente para a\nforma como os utilizadores interagem com as empresas e seus sistemas. Esta nova relação\nentre cliente e fornecedor tem aumentado significativamente o volume de dados gerados\npelas organizações, criando novas necessidades de como manter e gerir toda esta\ninformação. Assim, as empresas têm investido cada vez mais em soluções que permitam\nmanter toda a informação tratada e consolidada num repositório único de dados. Estes\nsistemas são vulgarmente designados por sistemas de data warehousing. Tradicionalmente,\nestes sistemas são refrescados em modo offline, em períodos de tempo que podem ser\ndiários ou semanais. Contudo, o aumento da competitividade no mundo empresarial torna\neste tipo de refrescamentos desadequados, originando uma reação atrasada à ação que\ndespoletou essa informação. Na realidade, períodos longos de refrescamento tornam a\ninformação desatualizada, diminuído consequentemente a sua importância e valor para a\norganização em causa. Assim sendo, é cada vez mais necessário que a informação\narmazenada num sistema de data warehousing, seja a mais recente possível, evitando\ninterrupções na disponibilização da informação. A necessidade de obter a informação em\ntempo real, coloca alguns desafios, tais como manter os dados acessíveis 24 horas por dia,\n7 dias por semana, 365 dias por ano, reduzir o período de latência dos dados ou evitar\nestrangulamentos operacionais nos sistemas transacionais. Assim, é imperativo a utilização de técnicas de coleta de dados não intrusivas, que atuem no momento em que determinado\nevento ocorreu num sistema operacional e reflitam a sua informação de forma imediata (ou\nquase imediata) num sistema de data warehousing. Neste trabalho de dissertação pretendese\nestudar a problemática relacionada com a captura de dados em tempo real e conceber\num componente que capaz de suportar um sistema de extração de dados em tempo real\nuniversal, que capture as mudanças ocorridas nos sistemas transacionais, de forma não\nintrusiva, e as comunique na altura certa ao seu sistema de data warehousing."
  },
  {
    "keywords": [
      "Model-based testing",
      "Graphic user interfaces",
      "Task models",
      "Testes baseados em modelos",
      "Interfaces gráficas",
      "Modelos de tarefas",
      "681.3:65.015.11",
      "65.015.11:681.3",
      "681.326"
    ],
    "titulo": "Development of an environment for the generation, mutation and execution of test cases",
    "autor": "Cruz, Paulo Filipe Jesus",
    "data": "2013",
    "abstract": "Testing graphic user interfaces (GUI) involves, mainly, lengthy and expensive processes\ninvolving user testing. Finding simpler and easier alternatives to use than these processes\nbecomes an exciting proposal. This project presents an alternative to existing processes through\nthe use of Model-based Testing - MBT.\nThe MBT technique takes advantage of models that describe the correct operation of the\nsystem (for this project task models). The use of MBT may thus become a new approach to\ntesting GUI's, since the implemented GUI is tested against the model that specifies it the correct\nbehavior. All inconsistencies found during the tests will be treated as potential errors that must\nbe corrected.\nThis report describes the development of a prototype for an environment able to generate\nand execute test cases applying MBT to GUI's."
  },
  {
    "keywords": [
      "Breast cancer",
      "Breast biopsy",
      "Ultrasound",
      "OncoNavigator",
      "Augmented reality",
      "Cancro da mama",
      "Biópsia mamária",
      "Ultrassom",
      "Realidade aumentada",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Modular framework for a breast biopsy smart navigation system",
    "autor": "Costa, José Nuno Martins da",
    "data": "2022",
    "abstract": "Breast cancer is currently one of the most commonly diagnosed cancers and the fifth leading cause of\ncancer-related deaths. Its treatment has a higher survivorship rate when diagnosed in the disease’s early\nstages. The screening procedure uses medical imaging techniques, such as mammography or ultrasound,\nto discover possible lesions. When a physician finds a lesion that is likely to be malignant, a biopsy\nis performed to obtain a sample and determine its characteristics. Currently, real-time ultrasound is the\npreferred medical imaging modality to perform this procedure. The breast biopsy procedure is highly reliant\non the operator’s skill and experience, due to the difficulty in interpreting ultrasound images and correctly\naiming the needle. Robotic solutions, and the usage of automatic lesion segmentation in ultrasound\nimaging along with advanced visualization techniques, such as augmented reality, can potentially make\nthis process simpler, safer, and faster.\nThe OncoNavigator project, in which this dissertation integrates, aims to improve the precision of\nthe current breast cancer interventions. To accomplish this objective various medical training and robotic\nbiopsy aid were developed. An augmented reality ultrasound training solution was created and the device’s\ntracking capabilities were validated by comparing it with an electromagnetic tracking device. Another\nsolution for ultrasound-guided breast biopsy assisted with augmented reality was developed. This solution\ndisplays real-time ultrasound video, automatic lesion segmentation, and biopsy needle trajectory display\nin the user’s field of view. The validation of this solution was made by comparing its usability with the\ntraditional procedure. A modular software framework was also developed that focuses on the integration\nof a collaborative medical robot with real-time ultrasound imaging and automatic lesion segmentation.\nOverall, the developed solutions offered good results. The augmented reality glasses tracking capabilities\nproved to be as capable as the electromagnetic system, and the augmented reality assisted breast biopsy\nproved to make the procedure more accurate and precise than the traditional system."
  },
  {
    "keywords": [
      "Ciber-segurança",
      "SIEM",
      "SOC",
      "Eventos",
      "Logs",
      "Correlação",
      "Gestão de incidentes",
      "Alertas",
      "Cybersecurity",
      "Events",
      "Correlation",
      "Incident management",
      "Alerts",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Implementação de um SIEM",
    "autor": "Dantas, Joana Esteves",
    "data": "2024-05-20",
    "abstract": "Ao longo dos anos tem-se observado um aumento progressivo na frequência e sofisticação dos ataques informáticos. Esta tendência obriga ao melhoramento constante do software de apoio às equipas\nde segurança e administração de sistemas.\nOs sistemas de gestão e correlação de eventos de segurança (em inglês, Security Information and\nEvent Managing - SIEM) fornecem a análise em tempo real de alertas de segurança gerados por aplicações\ne hardware de rede.\nO princípio base de todos os sistemas SIEM é agregar dados relevantes (logs) provenientes de múltiplas fontes de forma centralizada, identificar desvios da norma e tomar as ações necessárias.\nEspera-se com este trabalho de dissertação implementar um SIEM eficaz, on-premises (i.e, que corra\nlocalmente usando os recursos computacionais da organização). A solução deverá permitir efetuar correlação de eventos, alertas e gestão de incidentes."
  },
  {
    "keywords": [
      "Transcription factors",
      "Protein functional domains",
      "Escherichia coli",
      "Regulatory function",
      "Two-component signal transduction systems",
      "Global TFs",
      "Factores de transcrição",
      "Domínios proteicos funcionais",
      "Escherichia coli",
      "Funcção regulatória",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Classification and structure-based inference of transcriptional regulatory proteins",
    "autor": "Barros, Diana Manuela Pinto",
    "data": "2016",
    "abstract": "Transcription factors (TFs) are proteins that mediate the cellular response to the changes of the\nsurrounding environment. Studying their functional domains and protein structure is fundamental\nin order to gain insight of the way they are triggered and how they shape genetic transcription.\nThe current work aimed for classifying both TFs and functional domains, understanding which\nfeatures can be related to the different functions of the TFs.\nBy using UniProtJAPI, a JAVA library that allows remote access to UniProt, the information\nof 200 Escherichia coli’s (E. coli) TFs has been retrieved. This data was manually curated, in\norder to remove domain duplicates and other excess information, and to add missing domains.\nThe obtained functional domains were classified according to their molecular function, while\nthe TFs were classified according to their regulatory function. TFs that exclusively induce gene\nexpression were classified as activators, while TFs that only perform gene repression were classified\nas repressors. On the other hand, TFs that perform both the activation and repression of\ntranscription were classified as duals. The information was then analysed altogether in order\nto understand what relationships between the TFs’ function and functional domains could exist.\nSeveral analysis were performed, which include statistical tests and clustering methods. Along\nwith the analysis of the full list of TFs, TFs that are part of two-component signal transduction\nsystems and global TFs were given special focus, due to their important role in cellular function.\nThe results showed that there is a relationship between the functional domains and the regulatory\nfunction of the different TFs. This may be related to the evolutionary relationships between\nrepressors and activators. It is also understandable that dual regulators are closely related to activators\nand repressors than what activators and repressors are to each other. Moreover, TFs of\ntwo-component signal transduction systems are similar to each other, given that they perform\nsimilar functions. Their domain architectures are also predictable and do not vary from what\nwas expected of these TFs. However, in global TFs the results are opposite of the ones obtained\nfor two-component system TFs: their structures are very different from each other and each TF\nis specific. The amount of different domains is high when comparing to the full sample of TFs,\nsince the number of domains exceeds the number of TFs. Domains of all classification types are\npresent in their structure and the domain architectures are varied, which reflects their different\nactivities within the cell."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Towards an efficient lattice basis reduction implementation",
    "autor": "Gonçalves, Hélder José Alves",
    "data": "2016-12-21",
    "abstract": "The security of most digital systems is under serious threats due to major technology breakthroughs\nwe are experienced in nowadays. Lattice-based cryptosystems are one of the most\npromising post-quantum types of cryptography, since it is believed to be secure against\nquantum computer attacks. Their security is based on the hardness of the Shortest Vector\nProblem and Closest Vector Problem.\nLattice basis reduction algorithms are used in several fields, such as lattice-based cryptography\nand signal processing. They aim to make the problem easier to solve by obtaining\nshorter and more orthogonal basis. Some case studies work with numbers with hundreds\nof digits to ensure harder problems, which require Multiple Precision (MP) arithmetic. This\ndissertation presents a novel integer representation for MP arithmetic and the algorithms\nfor the associated operations, MpIM. It also compares these implementations with other libraries,\nsuch as GNU Multiple Precision Arithmetic Library, where our experimental results\ndisplay a similar performance and for some operations better performances.\nThis dissertation also describes a novel lattice basis reduction module, LattBRed, which\nincluded a novel efficient implementation of the Qiao’s Jacobi method, a Lenstra-LenstraLovasz\n(LLL) algorithm and associated parallel implementations, a parallel variant of the ´\nBlock Korkine-Zolotarev (BKZ) algorithm and its implementation and MP versions of the\nthe Qiao’s Jacobi method, the LLL and BKZ algorithms.\nExperimental performances measurements with the set of implemented modifications of\nthe Qiao’s Jacobi method show some performance improvements and some degradations\nbut speedups greater than 100 in Ajtai-type bases."
  },
  {
    "keywords": [
      "Virtual Characters",
      "Tangible Interfaces",
      "Affective Interfaces",
      "Toy",
      "Children",
      "Personagens virtuais",
      "Interfaces tangíveis",
      "Interfaces \"Affective\"",
      "Brinquedo",
      "Criança",
      "681.3:778.5",
      "778.5:681.3"
    ],
    "titulo": "Voodoo : a system that allows children to create animated films with action figures as interface",
    "autor": "Ribeiro, Pedro do Rêgo",
    "data": "2011-12-19",
    "abstract": "Using any kind of dolls as a tangible interface, has the potential to provide a friendly and\neasy to learn interface that allows children to control virtual characters in a more intuitive\nway. The research effort in this domain has been motivated by the shortcomings of conventional\ninterfaces, typically mouse and keyboard, which in this context are neither compelling\nnor do promote immersion.\nThis dissertation focuses on the design and evaluation of a system which can interpret the\nbehaviors that children give to a doll in order to provide this behavioral information to the virtual\ncharacters. With this system, the user (children) gets the role of movie director, directing\nvirtual characters through this natural form of interaction. This dissertation aims to evaluate\nthe hypothesis that dolls behaviors recognition based on the context of a well-known story,\nmay enhance the ability of children in the creation of an animated film (virtual characters\nanimations). Unlike many approaches that use a direct mapping of the doll movements to\nthe virtual character, it is intended to test the mapping based on the crosses between, user\nbehavioral intention, and the context where the doll it is inserted (the role and the location\nof the character in the story). The results show that the concept of interaction proposed to\nempower the children with a way to create animated films is actually very intuitive and easy\nto use. However, due to the technology used, it was not possible to assess to what extent\nthis concept really empowers children to easily and joyfully create animated films."
  },
  {
    "keywords": [
      "Absentismo",
      "Aplicação web",
      "Business intelligence",
      "Processo de tomada de decisão",
      "Tecnologias de Informação",
      "Absenteeism",
      "Decision-making process",
      "Information and communication technology",
      "Web Application",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Desenvolvimento de uma plataforma de apoio à decisão clínica relativamente ao problema do absentismo numa empresa",
    "autor": "Silva, Sara Daniela Oliveira da",
    "data": "2020-01-03",
    "abstract": "O absentismo é um problema que tem vindo a crescer ao longo dos últimos anos e que afeta gravemente\na economia das empresas. Para além disso, esta questão está intrinsecamente relacionada com a saúde\ndos trabalhadores, dado que doenças e acidentes de trabalho são duas das maiores causas de absentismo\npor todo o mundo.\nDe modo a perceber melhor a razão por detrás da crescente taxa de absentismo, assim como a\nrelação entre o absentismo e a saúde e estilo de vida dos colaboradores de uma empresa portuguesa,\ntornou-se necessário recorrer às Tecnologias de Informação (TI), nomeadamente ao Business Intelligence\n(BI). Esta tecnologia permite uma rápida análise de dados e uma melhor compreensão da informação\nexistente.\nAssim sendo, esta dissertação tem como objetivo desenvolver uma aplicação Web, recorrendo a\nferramentas de BI, que permita o estudo dos dados de absentismo através de indicadores clínicos de\nforma a identificar as principais causas, bem como a implicação que o trabalho e o estilo de vida possa\nter na saúde dos colaboradores e, consequentemente, no absentismo. Por conseguinte, esta aplicação\nfornece apoio à tomada de decisão e prática clínica por parte dos profissionais de saúde e permite\ntambém, através da análise dos dados, que sejam encontradas soluções para a diminuição da taxa de\nabsentismo da empresa.\nEsta solução pretende ainda apresentar toda a informação relativa aos colaboradores e às suas\nausências de uma forma organizada e de fácil leitura, sendo, deste modo, menos suscetível a erros,\nassim como mais rápido e mais eficiente que o atual sistema utilizado para a análise do absentismo,\nfacilitando assim o trabalho dos profissionais."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Melhoria da qualidade de serviço e monitorização clínica da gravidez",
    "autor": "Peixoto, Catarina Sofia Alves",
    "data": "2018",
    "abstract": "O Boletim de Saúde da Grávida é um livro de registos para grávidas. Este permite o registo de informações alusivas à história clínica da grávida, bem como o registo diário do peso, da pressão arterial, dos movimentos fetais, e entre outros valores que serão relevantes para a sua monitorização e o seu acompanhamento por parte do médico. Desta forma, considerando a importância clínica associada, é prioritário proceder-se à melhoria desse sistema. Além disso, as utentes quando encorajadas a serem mais independentes no ato de registo, considerando-o mais relevante, contribuem para a minimização da perda de informação relevante. Tudo isto favorece o diagnóstico antecipado de potenciais riscos à saúde da grávida ou à condição do seu feto.\nO cálculo da data provável de parto e da semana gestacional da gravidez é um processo extremamente importante no acompanhamento da grávida. Contudo, os métodos comuns são demorados e, facilmente, induzem em erro. Assim, uma ferramenta capaz de calcular esses valores é vantajosa quer para o médico, quer para a paciente.\nEste trabalho visa o desenvolvimento de uma progressive web app capaz de proporcionar um suporte à gravida em termos de informação e monitorização, disponibilizando o Boletim de Saúde da Grávida em formato digital, sendo também uma ferramenta que facilitará o cálculo da data provável de parto e da idade gestacional.\nO desenvolvimento do projeto foi auxiliado pelo Centro Materno Infantil do Norte (CMIN), com o acompanhamento do Doutor Jorge Braga – Diretor Geral de Obstetrícia do CMIN."
  },
  {
    "keywords": [
      "Metabolic networks",
      "Flux analysis",
      "Synthetic biology",
      "Pathway optimization",
      "Network topological analysis",
      "Subgraph extraction",
      "Redes metabólicas",
      "Análise de fluxo",
      "Biologia sintética",
      "Optimização de vias metabólicas",
      "Análise topológica de redes",
      "Extração de sub-grafos",
      "681.3:577",
      "577:681.3"
    ],
    "titulo": "Computational tools for pathway optimization towards metabolic engineering applications",
    "autor": "Liu, Filipe Alexandre Wang",
    "data": "2013",
    "abstract": "Metabolic Engineering targets the microorganism's cellular metabolism to design\nnew strains with an industrial purpose. Applications of these metabolic manipulations\nin Biotechnological derive from the need of enhanced production of valuable\ncompounds. The development of in silico metabolic models proposes a quantifiable\napproach for the manipulation these microorganisms. In this context, constraint\nbased modelling is one of the major approaches to predict cellular behaviour. It\nallows to prune the feasible space of possibilities describing possible phenotype\noutcomes in terms of metabolic fluxes. Under these conditions, cellular metabolism\ncan be represented as an algebraic system constrained by the laws of mass\nbalance and thermodynamics.\nThese systems are prone to be represented as networks, taking advantage of different\ngraph-based paradigms, including bipartite graphs, hypergraphs and process\ngraphs. This thesis explores these representations and underlying algorithms for\nmetabolic network topological analysis. The main aim will be to identify potential\npathways towards the optimized biochemical production of selected compounds.\nRelated to this task, algorithms will also be designed aiming to complement networks\nof specific organisms, taking as input larger metabolic databases, inserting\nnew reactions making them able to produce a new compound of interest.\nTo address these problems, and also related tasks of data pre-processing and evaluation\nof the solutions, a complete computational framework was developed. It\nintegrates a number of previously proposed algorithms from distinct authors, together\nwith a number of improvements that were necessary to cope with large-scale\nmetabolic networks. These are the result of problems identi ed in the previous\nalgorithms regarding their scalability.\nA case study in synthetic metabolic engineering was selected from the literature to\nvalidate the algorithms and test the capabilities of the implemented framework. It\nallowed to compare the performance of the implemented algorithms and validate\nthe proposed improvements."
  },
  {
    "keywords": [
      "Systems biology",
      "Genome-scale metabolic models",
      "Metabolic networks",
      "Constraint-based modeling",
      "Merlin",
      "Streptococcus thermophilus",
      "Lactic acid bacteria metabolism",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Genome-Scale Metabolic Network Reconstruction of the dairy bacterium Streptococcus thermophilus",
    "autor": "Cruz, Fernando João Pereira da",
    "data": "2017",
    "abstract": "The dairy food industry is constantly changing as novel biotechnological techniques\nimprove the manufacturing process of dairy products. Widely used over the years in the yogurt\nand cheese manufacturing, Streptococcus thermophilus is now considered as an extremely\nvaluable lactic acid bacterium for the annual market of the dairy industry. A specific, but\nof easy-access knowledge regarding the thermophilic bacteria metabolism would be a plus for\nthe continuous growth of such industry.\nIn this work, we present the Genome-Scale Metabolic (GSM) model for the LMD-\n9 strain of S. thermophilus together with the detailed description of the species metabolic\ncapabilities at the cellular level. The reconstruction of the genome-scale metabolic model, was\nperformed using Metabolic Models Reconstruction Using Genome-Scale Information (merlin)\ntogether with COBRApy tool and OptFlux platform.\nS. thermophilus LMD-9 genome was functionally annotated and the encoded metabolic\ninformation was afterwards used to assemble a draft network. After extensive manual curation,\nthe metabolic network was converted to a comprehensive metabolic model. The assembled\nGSM model was then validated against experimental data.\nThe metabolism of this important stater for the dairy industry has been accessed in\ndetail through the reconstruction. The organism possesses a simple machinery for central carbon\nmetabolism and shows a narrow spectrum of carbohydrate utilization. The genome-scale\nmetabolic model additionally suggests the existence of several pyruvate dissipating pathways\nwhich end in the synthesis of various compounds of interest. In silico simulations demonstrated\nthe production of lactate and residual amounts of formate, acetolactate and acetaldehyde.\nRegarding the amino acid metabolism, the organism possesses complete pathways\nfor the biosynthesis of all amino acids, except for lysine, methionine and cysteine. Furthermore,\nthe GSM model can be used to simulate other relevant features of the S. thermophilus\nmetabolism, such as the aroma compounds and Exopolysaccharides (EPS) synthesis, oxygen\ntolerance, absence of complete citrate cycle and pentose phosphate pathway, urea metabolism\nor amino acid catabolism."
  },
  {
    "keywords": [
      "Software",
      "Database",
      "Monitoring",
      "Smart walker",
      "Rehabilitation sessions",
      "Base de dados",
      "Monitorização",
      "Andarilho inteligente",
      "Sessões de reabilitação",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "A software to manage rehabilitation sessions with a robotic walker",
    "autor": "Campos, Ana Margarida Reis Maia",
    "data": "2022",
    "abstract": "Cerebellar ataxia arises from damage or dysfunction that affects the cerebellum and its pathways. As a\nresult, the motor abilities of individuals with this condition become weakened. Robotics-assisted therapy is still an\nemerging area, but it has several advantages that could boost the rehabilitation of these individuals. Considering\nthis problematic, WALKit Smart Walker is being developed. Its main purpose is to improve the treatment of ataxic\npatients through intelligent and multidisciplinary rehabilitation sessions. Thus, it is equipped with several sensors\nthat provide monitoring capabilities through a continuous evaluation of the end-user gait and posture.\nA vast amount of data is acquired during each session by the walker sensors. For health professionals to\nanalyse this data and have feedback on the patient’s status throughout therapy, tools are needed to control,\nmanage, and monitor sessions in a clear, practical and intuitive way. Therefore, the main goal of this dissertation\nis centred on implementing an effective way to store the acquired data, along with the development of software\nthat satisfies these requirements.\nTo address these goals, a polyglot persistence database system, composed of a relational and a non-relational\ndatabase, was implemented to store the required data while maintaining efficiency. Furthermore, a web application\nwas developed to provide, not only to health professionals, but also to patients themselves, the management\nof the rehabilitation sessions with the walker. The application provides an individual and temporal analysis of\nthe sessions through interactive graphics adapted to each patient. Additionally, it allows the management of the\nseveral patients who are/were in treatment and the addition of clinical ratting scales, which are useful to assess\ntheir motor condition and adapt therapies as needed. In this way, professionals can have a better perception of\nthe patient’s condition, and can show patients their evolution, possibly contributing to increase their motivation in\ntherapy.\nMoreover, in the context of this dissertation, the embedded software of WALKit SmartW, which allows the\ntherapy configuration, was optimized. This software had no security mechanisms, thus the main goal was on the\nimplementation of techniques capable of making the software secure. Additionally, other functionalities such as\nfeedback alerts, were added to the existing application.\nThroughout the development of this project, it was possible to have continuous feedback from health professionals\nof the Hospital of Braga. Usability tests and questionnaires were also applied, and the results were very\npromising, enhancing the need for a system with these characteristics. Professionals claimed the system may\nhelp in analysing the patient clinical status in an intuitive form while keeping them motivated during treatments."
  },
  {
    "keywords": [],
    "titulo": "Towards procedural music-driven animation: exploring audio-visual complementarity",
    "autor": "Brito, Carlos Faria Aquino de",
    "data": "2017",
    "abstract": "This thesis intends to describe our approach towards developing a framework for the interactive\ncreation of music driven animations.\nWe aim to create an integrated environment where real-time musical information is easily\naccessible and is able to be flexibly used for manipulating different aspects of a reactive\nsimulation. Such modifications are specified through the use of a scripting language and\ninclude, for instance, geometrical transformations and geometry synthesis, gradual colour\nchanges as well as the application of arbitrary forces.\nOur framework thus represents a proof-of-concept for converting musical information\ninto arbitrary modifications to a dynamic simulation, producing a variety of animations.\nThis is possible due to a bargaining between control and automation, where control is\npresent by allowing the user to program these modifications with a scripting language\nand automation is present by using physics and interpolation to estimate the visual effects\nresulting from those modifications.\nThe particular test case for our system was the animation/simulation of a growing tree\nreacting to wind. In order to control or influence both the tree growth and wind field,\nas well as other visual parameters, the system accepts two different but complementary\nrepresentations of music: a MIDI event stream and raw audio data. Different musical\nfeatures are obtainable from each of these representations. On one hand, by using MIDI, we\nare able to discretely synchronise visual effects with the basic elements of music, such as the\nsounding of notes or chords. On the other, using audio, we are able to produce continuous\nchanges by obtaining numerical data from basic spectral analysis. Our framework provides\na common interface for the combined application of these different sources of musical\ninformation to the generation of visual imagery, under the form of procedural animations.\nWe will describe algorithms presented in multiple research papers, namely for tree generation,\nwind field generation and tree reaction to wind, briefly detailing our implementation\nand architecture. We also describe why each of these particular methods was chosen, how\nthey are organised in our platform and how their parameters may be modified from our\nscripting environment leading to what we regard as the procedural generation of animations.\nBy allowing the user to access musical information and give them control of what we have\ncome to refer to as animation primitives, such as wind and tree growth, we believe to have\ntaken a first step towards exploring a novel concept with a seemingly endless expressive\npotential."
  },
  {
    "keywords": [
      "681.3:618",
      "618:681.3",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Business intelligence: indicadores da interrupção voluntária da gravidez",
    "autor": "Brandão, Andreia Manuela Couto",
    "data": "2014",
    "abstract": "Nos últimos anos, tem surgido um grande interesse na aplicabilidade das\ntecnologias de Business Intelligence (BI) na área da saúde. A grande satisfação\nresultante da sua implementação em outras áreas fez com que os profissionais\nde saúde juntamente com os profissionais de Tecnologias de Informação\n(TI) cooperassem para a conceção e o desenvolvimento de plataformas de BI\nem ambiente clínico.\nA grande motivação para a sua implementação adveio da possibilidade\nde conceber um Sistema de Apoio à Decisão (SAD) médica, que suportasse\no processo de tomada de decisão e que permitisse que este fosse realizado de\nforma mais rápida e eficaz. Além disso, com a implementação do Processo\nClínico Eletrónico, surgiu a necessidade de dar usabilidade aos dados dos\nutentes armazenados nas bases de dados, tornando-se isso possível com a\nimplementação da plataforma de BI.\nNeste projeto, incidiu-se numa área médica específica dentro da especialidade\nde Ginecologia e Obstetrícia, relacionada com a Interrupção Voluntária\nda Gravidez (IVG), por ser uma unidade relativamente recente, onde não\nexiste nenhuma tecnologia semelhante implementada e, ainda, por ser um\nfoco de interesse para os profissionais de saúde. O local de estudo foi o Centro\nMaterno Infantil do Norte (CMIN), pertencente ao Centro Hospitalar\ndo Porto (CHP), onde se teve acesso às fontes de dados necessárias para o\ndesenvolvimento da plataforma de BI.\nAlém disso, foram também desenvolvidos modelos de Data Mining (DM),\nigualmente integrados na plataforma de BI, que permitem prever quais as\nutentes que recorrem à IVG que constituem um grupo de risco e quais as\nutentes que necessitam de acompanhamento da equipa de enfermagem durante o processo da IVG. Os resultados obtidos foram bastante satisfatórios,\numa vez que foram registados valores de 93% para a métrica da sensibilidade\nna questão relacionada com a probabilidade das utentes pertencerem\nao grupo de risco, e valores de 91% e de 87% para a sensibilidade e acuidade,\nrespetivamente, no problema relacionado com a previsão do local de\nrealização de uma das etapas do processo de IVG.\nNa seleção da tecnologia a utilizar para o desenvolvimento da plataforma\nde BI, optou-se pelo Pentaho BI Suite, depois de realizada uma pesquisa\naprofundada sobre ferramentas open source.\nApós a implementação da plataforma de BI, pode-se afirmar que o resultado\nfoi satisfatório, uma vez que todos os indicadores de desempenho\nrequisitados foram apresentados. Nestes indicadores, estão incluídos a distribuição\ndas utentes por idades, por profissão, por localidade, pela presença\nna última consulta de avaliação, entre outros. Além disso, a informação representada\né credível, pois esta foi submetida a um processo de validação por\nparte dos profissionais de saúde."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aprendizagem não supervisionada de padrões de interação homem-computador",
    "autor": "Quintas, Ana Sofia Martins Sá",
    "data": "2016-12-28",
    "abstract": "Durante longos períodos de atividade cognitiva é comum a sensação de cansaço e falta de energia, acompanhada\nde um decréscimo de desempenho. Este estado, geralmente denominado de fadiga mental, é considerado uma das\nprincipais causas de erro humano. Os efeitos da fadiga mental no desempenho de tarefas complexas e que requerem\naltos níveis de concentração devem ser estudados e antecipados de forma a minimizar erros. Exemplo disto é o caso\nda aviação, ou medicina onde pequenas distrações podem gerar graves acidentes.\nO impacto negativo da fadiga mental na performance, saúde e bem estar dos indivíduos, torna-se, desta forma,\num dos principais motivos que leva ao desenvolvimento de metodologias de deteção destes estados mentais, de forma\na preveni-los. Efetivamente, ao longo do tempo diversas metodologias de deteção de fadiga têm sido desenvolvidas,\ncontudo a maioria é pouco objetiva ou requer um grande investimento económico.\nA presente monografia apresenta um estudo de um sistema de aprendizagem não supervisionada de casos de\nfadiga mental, utilizando padrões de interação homem-computador, recolhidos através da sensorização de rato e teclado.\nEste estudo permitiu uma classificação correta de 83,3% de novos casos de fadiga mental."
  },
  {
    "keywords": [
      "Bloco operatório",
      "Intervenção cirúrgica",
      "Agendamento",
      "Planeamento",
      "Javascript",
      "Operating room",
      "Surgical intervention",
      "Scheduling",
      "Planning",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Agendamento inteligente de blocos operatórios",
    "autor": "Oliveira, Célia Valéria da Costa",
    "data": "2017",
    "abstract": "O mundo das aplicações web domina cada vez mais, a forma como nos comunicamos e transmitimos informações. O agendamento cirúrgico consiste em programar as cirurgias de forma a utilizar eficientemente os recursos e reduzir o risco das cirurgias canceladas podendo assim obter uma calendarização detalhada do início e fim das atividades de uma intervenção cirúrgica. Sendo assim as aplicações associadas à saúde diferenciam-se da generalidade, por se dedicarem exclusivamente à saúde, não só no seu conteúdo e informação, mas também nas suas funcionalidades.\nDevido à variação da duração de uma cirurgia e à chegada de cirurgias de urgência o agendamento cirúrgico é interrompido ao longo do dia e pode levar a uma mudança no horário previsto do início das cirurgias programadas. Estas alterações podem resultar em situações indesejáveis tanto para os pacientes como para os prestadores de saúde, daí a necessidade de um ajuste do horário. Deste modo, esta dissertação consiste no desenvolvimento do agendamento inteligente de blocos operatórios. A aplicação proposta permitirá aos prestadores de saúde efetuar marcações cirúrgicas, visualizar as marcações das intervenções cirúrgicas num calendário semanal assim como possibilita o acesso a uma lista para editar ou remover as marcações, melhorando a variação da duração das cirurgias. Com esta aplicação, é possível afirmar que se trata de uma plataforma de apoio para os utilizadores pelo tipo de partilha de informação que disponibiliza, permite o agendamento/planeamento das cirurgias efetuadas pelo bloco operatório e possui um conjunto de funcionalidades que a tornam numa aplicação útil, cómoda e de fácil acesso para o quotidiano dos utilizadores."
  },
  {
    "keywords": [
      "Green Computing",
      "Spectrum-based Fault Localization",
      "Energy Consumption",
      "Energy Leak",
      "Spectrum-based Energy Leak Localization",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Spectrum-based energy leak localization",
    "autor": "Carção, Tiago Alves",
    "data": "2014-12-19",
    "abstract": "For the past few years, we have begun to witness an exponential growth in the inform\nation and communication technologies (ICT) sector. While undoubtedly a milestone, all of\nthis occurs at the expense of high energy costs needed to supply servers, data centers, and\nany use of computers. Associated with these high energy costs is the emission of greenhouse\ngases. These two issues have become major problems in society. The ICT sector contributes\nto 7% of the overall energy consumption, with 50% of the energy costs of an organization\nbeing attributed to the information technology (IT) departments.\nMost of the measures taken to address the high level of energy consumption have been\non the hardware side. Although is the hardware that does consume energy, it is the software\nthat operates that hardware. As a consequence, the software is the main responsible for\nthe energy consumed by the hardware, very much like a driver that drives/operates a car\ninﬂuences drastically the fuel consumed by the car.\nThis dissertation proposes and implements a methodology to analyze the software energy\nconsumption. This methodology relates energy consumption to the source code of a soft\nware application, so that software developers are aware of the energy footprint that he/she\nis creating with his/her application. The proposed technique interprets abnormal energy\nconsumption as software faults, and adapts a well-known technique for locating faults on\nprograms’s source code, to locate “energy faults”, that we name as “energy leaks”.\nThis methodology has been fully implemented in a software framework that monitors\nthe energy consumed by a software program and identiﬁes its energy leaks, given its source\ncode. Moreover, a list of problematic parts of the code is produced, thus, helping software\ndevelopers identifying energy faults on their source code. We validate our ﬁndings by showing\nthat our methodology can automatically ﬁnd energy leaks in programs for which such leaks\nare known.\nWith this results, one intends to provide help to the development phase and to gener\nate more energy eﬃcient programs that will have less energy costs associated with, while\nsupporting practices that promote and contribute to sustainability."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistema de monitorização de vibrações baseado numa arquitetura REST para IoT",
    "autor": "Lima, João Francisco Miranda de Peixoto",
    "data": "2016-12-21",
    "abstract": "A Internet das Coisas é um fenómeno que, embora não seja recente, tem sentido um enorme\ncrescimento nos últimos anos. Atualmente existem cerca de 10 biliões de dispositivos ligados à Internet\ncom a expectativa de se alcançar entre 20 a 50 biliões de dispositivos ligados dentro dos próximos cinco\nanos. Através da introdução de serviços inovadores concebidos para diversas áreas, tais como a\nindústria, os cuidados de saúde, a domótica, os transportes, a agricultura, o retalho, a segurança, entre\nmuitas outras áreas do nosso quotidiano, a Internet das Coisas promete melhorar as nossas vidas, pois\ncom as capacidades de monitorização, processamento e comunicação dos dispositivos IoT é possível\ntornar as “coisas” do nosso dia-a-dia parte de algo maior.\nA monitorização, sendo uma parte integrante das soluções baseadas em IoT, pode ser utilizada para\nmedir vários parâmetros. Alguns dos mais comuns são a temperatura, humidade, pressão, som e a\nvibração. Embora a vibração possa ser vista de diferentes formas, na prática geotécnica a vibração\ncorresponde a uma resposta elástica do terreno (solos e/ou rochas) aquando da passagem de uma onda\nde tensão, tendo como origem um evento de génese natural (como por exemplo sismos ou o\ndeslizamento súbito de massas rochosas ao longo de falhas geológicas) ou artificial (explosões, cravação\nde estacas, trabalhos de construção, utilização de equipamentos diversos, linhas ferroviárias, tráfego\nrodoviário, entre outros). Esta vibração pode ser monitorizada recorrendo a diferentes tipos de sensores,\npelo que a proposta apresentada opta por recorrer aos acelerómetros MEMS, tirando partido do facto de\nestes serem extremamente pequenos, baratos e com uma baixa necessidade de consumo de energia.\nA proposta está dividida em três componentes principais: o Coletor, o Servidor e o Monitor. O Coletor é\no componente físico da proposta e tem como responsabilidade registar os eventos de natureza vibratória\nao longo do tempo. O Servidor é o componente central e é responsável por armazenar o histórico de\ntoda a informação recolhida pelo Coletor. Por fim, o Monitor é o componente que é responsável por\nfornecer uma interface capaz de aceder à informação recolhida pelo Coletor e gravada pelo Servidor.\nCom o trabalho desenvolvido foram executados alguns testes de forma a avaliar o funcionamento dos\ndiferentes componentes da proposta, em especial o desempenho do Coletor"
  },
  {
    "keywords": [
      "Observability",
      "Monitoring",
      "Cloud-native",
      "Microservices",
      "Opentelemetry",
      "Tracing",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "An observability approach for microservices architectures based on opentelemetry",
    "autor": "Moreira, André Cunha Azevedo",
    "data": "2023-12-15",
    "abstract": "The rapid adoption of microservices and cloud-native architectures has revolutionized the way modern\napplications are developed and deployed. However, this shift has introduced new challenges in terms\nof ensuring the reliability and performance of these distributed systems. In response, observability is\nproposed as a new methodology to address these challenges.\nObservability refers to the collection of telemetry data (including traces, metrics, and logs) from a\nsystem components in real time, allowing for a comprehensive understanding of its internal status and\nbehavior. This capability is essential for troubleshooting, performance optimization, and enhancing system\nreliability by facilitating the detection of errors and anomalies.\nThe main objective of this thesis is to implement an observability concept within a Python Flask based system. The system follows a cloud-native, microservices, and event-driven architecture. The main\nmotivation for this study is the recent, but important development of observability and the culture of\nDevelopment and Operations (DevOps).\nThe chosen method for implementation is OpenTelemetry, a neutral and open-source approach to\nobservability. This decision aims to avoid vendor lock-in, which can be a concern with vendor-specific\nagents.\nFurthermore, a study is carried out to make a choice among the vendors considered which are compat ible with OpenTelemetry, e.g. Jaeger, Zipkin, Prometheus, Elastic Search, New Relic, Datadog, Dynatrace,\nGrafana, Splunk, and AppDynamics. Each vendor offers different approaches to observability and visual ization of the telemetry data. In addition, a weighted decision matrix is used to aid in the decision along\nwith a decision criterion defined by the development team.\nThe results of this study not only highlight the vendor selection process for telemetry data visualization\nbut also emphasize that OpenTelemetry is a viable and standardized approach to observability, offering an\neffective means to prevent vendor lock-in."
  },
  {
    "keywords": [
      "Deteção de violência",
      "Machine learning",
      "Deep learning",
      "Transfer learning",
      "Reconhecimento da ação humana",
      "Áudio",
      "Violence detection",
      "Human action recognition",
      "Audio",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Análise de algoritmos de machine learning para deteção de violência em áudio",
    "autor": "Veloso, Bruno Cruz",
    "data": "2023-04-12",
    "abstract": "A violência tem sido parte integrante da humanidade. Existem diferentes tipos de violência, sendo a violência\nde cariz físico mais recorrente no nosso quotidiano, afetando cada vez mais a vida de muitas pessoas.\nO reconhecimento da ação humana tem sido crescentemente estudada nos últimos anos. O áudio (microfones)\ne vídeo (câmaras) são as formas mais utilizadas na captação de violência. O reconhecimento da ação humana\natravés do vídeo representa uma importante área na visão por computador. No entanto, a captação de vídeo requer\numa grande capacidade de processamento e de desempenho, tanto de hardware como software. O áudio surge\nassim como um fator capaz de colmatar estes problemas. No entanto, a deteção de áudio é altamente suscetível\na grandes flutuações de precisão, dependendo do ambiente acústico em que está inserido.\nNa presente dissertação, pretendeu-se comparar os diferentes algoritmos de Machine Learning com o intuito\nde averiguar qual o melhor algoritmo a utilizar para detetar violência em áudio.\nA revisão da literatura revelou que o áudio pode ser classificado usando algoritmos de Machine Learning, sendo\na sua conversão em imagens (mel spectrogram) a metodologia habitualmente utilizada, tendo sido a abordagem\ntomada. Além disto, estudaram-se os algoritmos frequentemente utilizados na classificação de áudio, tendo estes\nsido utilizados para posterior avaliação.\nOs resultados obtidos demonstram um bom desempenho das redes neuronais EfficientNet, sendo que as redes\nque obtiveram melhor precisão foram a EfficientNetB1 e EfficientNetB0, com 95.06% e 94.19%, respetivamente.\nAdicionalmente, verificou-se que a rede MobileNetV2 é a mais incapaz de classificar entradas de violência, com\numa classificação de 92.44%.\nA rede neuronal EfficientNetB1 apresentou uma melhor capacidade na classificação de violência em áudio."
  },
  {
    "keywords": [
      "Health Kiosk",
      "Self-Service Kiosk",
      "Emergency department",
      "Crowding",
      "Primary care",
      "Quiosque de saúde",
      "Self-Service quiosque",
      "Serviço de urgências",
      "Lotação hospitalar",
      "Cuidados primários",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Self-service kiosk-based anamnesis system for emergency departments",
    "autor": "Pacheco, Paulo Alexandre Gonçalves",
    "data": "2021-03-09",
    "abstract": "Emergency departments have a higher number of visits compared to other hospital de partments. Technology has played a crucial role in promoting improvements in hospital\nmanagement and clinical performance. The number of visits to emergency departments has\nincreased considerably, giving rise to crowding situations that cause several adverse effects.\nThis situation negatively affects the provision of emergency services, impairs the quality of\nhealth care and increases the time patients wait for medical check-up. One of the leading\ncauses contributing to the crowding is the high number of patients with low severity clinical\ncondition. These are referred to as non-urgent or inappropriate patients, whose clinical\nsituation should be taken care through self-care or primary health care.\nIt is the responsibility of the institutions to analyse and quantify the possible causes of\ncrowding to find the best solution to mitigate the adverse effects caused. It is believed that\nnon-urgent patients can use the time spent in the waiting room more productively, namely\nby using a self-service kiosk to which they can provide valuable information to facilitate and\naccelerate the clinical processing.\nThis work proposes a solution to be used in the waiting room of emergency departments,\nwhich aims to reduce the period of medical check-up. The solution uses a self-service kiosk\nfor the patient to provide relevant clinical data that would otherwise have to be collected\nby the physician during the clinical observation process. In particular, the kiosk will collect\nvital signs, past medical history, main complaint and usual medication. This data will\nbe processed and provided to the physician in a structured and uniform way before each\nmedical check-up. The primary purpose of this solution is to reduce the period of patients’\nmedical check-up and thus improve the response capacity of the emergency departments\nwith the same resources.\nDuring the Master’s work period, an Android application was implemented for patients\nto enter the clinical data mentioned above, and a Web application for physicians to access it.\nAdditionally, a data warehouse was implemented to store the data in a consolidated way\nto discover hidden relationships and patterns in the data. The first moment of evaluation,\nundertaken in a non-hospital facility, shows positive acceptability by participants, with a\nlarge majority considering the system user-friendly. Due to the pandemic, it was impossible\nto perform the second planned evaluation moment in a real emergency environment."
  },
  {
    "keywords": [
      "Information technology",
      "Monitoring systems",
      "Web application",
      "Healthcare",
      "Tecnologia da informação",
      "Sistema de monitorização",
      "Aplicação web",
      "Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Multiplatform application for monitoring services in a hospital environment",
    "autor": "Marques, Carolina Resende",
    "data": "2022-12-13",
    "abstract": "Many people believe that information technology has the potential to change the way the healthcare industry approaches its current challenges by improving healthcare quality, safety, and efficiency by bringing decision support to the point of care and enabling routine quality measurement. In the medical field, healthcare information technology refers to any information technology tool or software that is intended to increase hospital and administrative productivity, provide new information about medications and treatments, or improve overall quality of care. Infrastructures in hospitals must manage both information technology and specialized healthcare systems and protocols. It is important in this type of structure to ensure that all operations and information technology run smoothly and one of the ways to achieve this is by continuously and automatically monitor the hospital environment’s systems. The right monitoring and reporting tools can help keep medical staff efficient without worrying about failing systems, provide visibility into usage trends, equipment performance, downtime, and much more, saving time and resources. A monitoring application of this type is regarded as an important source of information. The main goal of this Dissertation within the scope of this project is to develop a monitoring web platform for healthcare information technology administrators that is based on a multi-site and multi-organization scheme. The proposed solution will monitor various hospital services and is expected to provide the current state of the system in a timely manner through a set of graphs and reports, allowing for appropriate operational decisions and ensuring that the system functions as expected. A web application for monitoring hospital services was developed, implemented, and evaluated during the dissertation work period. Using questionnaires, the platform was evaluated and validated in order to understand if this approach may improve information technology availability and, in the long run, alleviate some of the healthcare industry’s pains. A formal evaluation of the solution was also performed, which comprised a strengths, weaknesses, opportunities and threats analysis and a risk assessment report, both of which gave helpful insights into the system’s strengths and shortcomings, as well as potential improvement areas."
  },
  {
    "keywords": [
      "Protein classification",
      "Word embedding",
      "Natural language processing",
      "Machine learning",
      "Deep learning",
      "Classificação de proteínas",
      "Processamento de linguagem natural",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Development of language modelling techniques for protein sequence analysis",
    "autor": "Gomes, Ivan Alexandre Pereira",
    "data": "2022",
    "abstract": "Nowadays, the ability to predict protein functions directly from amino-acid sequences alone remains a major biological challenge. The understanding of protein properties and functions is extremely important and can have a wide range of biotechnological and medical applications.\nTechnological advances have led to an exponential growth of biological data challenging conventional\nanalysis strategies. High-level representations from the field of deep learning can\nprovide new alternatives to address these problems, particularly NLP methods, such as word\nembeddings, have shown particular success when applied for protein sequence analysis.\nHere, a module that eases the implementation of word embedding models toward protein\nrepresentation and classification is presented. Furthermore, this module was integrated in the\nProPythia framework, allowing to straightforwardly integrate WE representations with the training\nand testing of ML and DL models.\nThis module was validated using two protein classification problems namely, identification of\nplant ubiquitylation sites and lysine crotonylation site prediction. This module was further used\nto explore enzyme functional annotation. Several WE were tested and fed to different ML and\nDL networks. Overall, WE achieved good results being even competitive with state-of-the-art\nmodels, reinforcing the idea that language based methods can be applied with success to a\nwide range of protein classification problems.\nThis work presents a freely available tool to perform word embedding techniques for protein\nclassification. The case studies presented reinforce the usability and importance of using NLP\nand ML in protein classification problems."
  },
  {
    "keywords": [
      "Ciências Médicas::Ciências da Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Ferramenta de suporte à decisão e prática clínica em unidades de cuidados neonatais e pediátricos",
    "autor": "Guimarães, Tiago André Saraiva",
    "data": "2015",
    "abstract": "As crianças são uma população especialmente vulnerável, nomeadamente\nno que diz respeito à administração de medicamentos e necessidade de nutrição.\nEstima-se que os doentes neonatais e pediátricos são pelo menos três\nvezes mais vulneráveis a danos causados devido a eventos adversos e erros de\nmedicação do que a população adulta.\nO desenvolvimento de uma plataforma que suporte os médicos pediatras\nno exercício das suas funções diárias, de forma a reduzir o erro médico, é\no principal objetivo deste projeto. A sua necessidade foi identificada por\num médico pediatra em exercício de funções no Hospital de Santo António\nno Porto, de forma a que falhas existentes na ferramenta em uso fossem\ncolmatadas e ainda novas funcionalidades fossem desenvolvidas.\nCom a presente dissertação foi procurada ainda uma abordagem que permitisse\no desenvolvimento de um canal de passagem de informação entre os\nmédicos e a Farmácia Hospitalar, e que este sistema pudesse ser altamente\nescalável, sendo facilmente replicado em qualquer Instituição de Saúde.\nO desenvolvimento do sistema foi sempre acompanhado por um médico\npediatra, sendo este testado e refinado ao longo desse período. Por fim, uma\nversão para testes da aplicação é lançada assim como um questionário que\npretende avaliar a mesma."
  },
  {
    "keywords": [
      "Microsserviços",
      "Arquiteturas de software",
      "Ferramentas de reporting",
      "Padrões arquiteturais",
      "Microservices",
      "Software architectures",
      "Reporting tools",
      "Architectural patterns",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Primavera: microsserviço de reporting",
    "autor": "Santejo, Carolina Gil Afonso",
    "data": "2023-12-11",
    "abstract": "A Cegid Primavera tem vindo a desenvolver a sua cloud de microsserviços para facilitar a integração dos seus vários produtos e dos seus produtos com sistemas externos, com um foco particular no reaproveitamento de funcionalidades e de componentes semelhantes em produtos diferentes. Uma funcionalidade comum a muitos desses produtos é o reporting, que consiste na transformação de dados em informações úteis para o utilizador, sendo que neste caso o foco está centrado na impressão de documentos. Esta dissertação tem como objetivo isolar toda a lógica de reporting num único conjunto de microsserviços, para que os todos os produtos primavera tenham acesso ao mesmo. O sucesso na construção\ndeste microsserviço parte da escolha da melhor ferramenta de reporting para o contexto em questão e,\nprincipalmente, no desenho da melhor arquitetura de microsserviços."
  },
  {
    "keywords": [
      "Merlin",
      "Genome-Scale Metabolic Models",
      "Software",
      "Development",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Development and implementation of bioinformatics tools for the reconstruction of GiSMos",
    "autor": "Dias, António Carlos Fortuna Ribeiro",
    "data": "2017",
    "abstract": "The reconstruction of Genomic-Scale Metabolic Model (GiSMo)s is an increasingly\ngrowing methodology, which allows to develop models that can be used to perform in silico\npredictions on the phenotypical response of an organism to environmental changes and\ngenetic modifications. These predictions allow focusing in vivo experiments on methodologies\nthat will, theoretically, present better results, thus reducing the high costs on time\nand money spent in laboratorial experiments. GiSMos are a mathematical representation\nof the organism’s genome, in the form of metabolic networks. As complex as these can\nbe, because of the large number of compounds involved in many different reactions and\npathways, the treatment of all such data is not easily manually performed.\nSeveral bioinformatics software were developed with the aims of improving this procedure,\nby automating many operations in the reconstruction process. Metabolic Models\nReconstruction Using Genome-Scale Information (merlin) is one of such tools, following a\nphilosophy that thrives on providing an intuitive and powerful graphical environment, to\nannotate data on key metabolic components and building a complete genome-scale model.\nWhile already encompassing a wide range of tools, it is still a work in development.\nUpon analyzing its functioning, several improvement opportunities were identified, mainly\nin existing operations. Moreover, missing important features for the reconstruction of\nGiSMos were as well identified.\nThis work details the results of this analysis and the improvements performed to\nenrich merlin’s toolbox."
  },
  {
    "keywords": [
      "Comércio online",
      "Experiência aplicacional",
      "Machine learning",
      "Sistema de recomendação",
      "Machine learning",
      "Online shop",
      "Recommender system",
      "User experience",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistema de recomendação para uma loja online de livros",
    "autor": "Cardoso, Telmo André Moreira",
    "data": "2022-02-16",
    "abstract": "Nas últimas décadas, a tecnologia tem sofrido uma evolução exponencial, assumindo um papel fundamen tal no nosso quotidiano e quando aplicada nas mais diversas áreas proporciona-nos melhorias significati vas na nossa qualidade de vida. Por exemplo, a tecnologia permite-nos fazer compras sem sair de casa,\nque é um hábito que muitas pessoas têm e nos últimos tempos, devido ao panorama em que nos encon tramos, o seu crescimento tem sido ainda mais notável. Frequentemente, enquanto compramos online,\nsão nos apresentados itens que não têm qualquer interesse para nós. Diante este problema, surgiram os\nsistemas de recomendação, que se tornaram uma parte fundamental e um dos fatores diferenciadores a\nnível aplicacional. O principal objetivo de um sistema de recomendação é, recorrendo a algoritmos de Ma chine learning, produzir uma lista de itens ordenados de acordo com o grau de relevância esperado para\num determinado utilizador, permitindo, por exemplo, evitar o problema de recomendações indesejadas.\nMais especificamente, nesta dissertação desenvolveu-se e incorporou-se um sistema de recomendação\nnuma aplicação web de venda online de livros, onde as técnicas concebidas e implementadas permitiram\nmelhorar a qualidade da recomendação e consequentemente a experiência aplicacional, com sugestões\nque vão de encontro às preferências do utilizador."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Recuperação de transações em bases de dados NoSQL",
    "autor": "Morais, José Carlos Vieira",
    "data": "2018",
    "abstract": "Com o amadurecimento e ampla utilização das bases de dados NoSQL tem havido um\ninteresse crescente na adição de transações multi-linha, que proporcionem as propriedades\nACID sem comprometer o desempenho e capacidade de escala destes sistemas. Apesar\ndas propostas nesta área assentarem em técnicas bem conhecidas de bases de dados, como\na multi-versão e a recuperação, a sua aplicação está agora enquadrada em pressupostos\ndiferentes, não sendo claro que os compromissos tradicionais se mantenham.\nNeste contexto, este trabalho sistematiza os compromissos relacionados com a escolha de\num mecanismo de recuperação, que garante que as alterações efetuadas por uma transação\nconfirmada persistem atomicamente. Além de analisar qual o impacto na arquitetura do\nsistema da escolha do mecanismo de recuperação, comparamos experimentalmente as alternativas\nmais interessantes com diferentes cargas de trabalho."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Detecção inteligente de fugas de informação por analise comportamental",
    "autor": "Costeira, Ricardo Manuel Andrade",
    "data": "2016-06-16",
    "abstract": "The information that a company possesses is one of its most valuable assets. This information\nis nowadays digitally managed, which is the reason for the exponential increase in security\nbreaches, where information is defiled or even stolen. Seeking to solve this problem, Watchful\nSoftware developed a product, RightsWATCH, that allows for an organization to protect and\nwatch over its information.\nBy monitoring what happens to information, RightsWATCH provides, in case of an incident,\nthe means to undertake a very complete post-mortem analysis. Nevertheless, by the time this\nanalysis is complete, it might have been hours (or days) since the incident occurred. To make\nmatters worse, nowadays most threats actually come from the inside of the company. That\nbeing said, this dissertation defines as its main objective the need to understand if it is possible\nto detect data leaks in an intelligent way, through a real time analysis of the user’s behaviour\nwhile he handles the classified information. This possibility was indeed confirmed through an\ninvestigation comprising experiences with real world use cases and a variety of data preparation\nand data analysis techniques."
  },
  {
    "keywords": [
      "Automação de código",
      "Camada de serviço",
      "Diagrama de sequência UML",
      "Automatic programming",
      "Service layer",
      "UML sequence diagram",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Criação de uma camada de serviço especificada em diagramas de sequência UML",
    "autor": "Lima, Marcelo Alexandre Matos Fonseca",
    "data": "2019-12-23",
    "abstract": "Automatic code generation is an increasingly recurring theme these days, constantly manifesting itself in new tools that allow you to generate code from top-level languages that try to make the programmer’s job faster and easier. UML is a long-standing modeling language that is primarily used to model applications during the specification phase and is sometimes\nalso used for automatic code generation.\nIn this dissertation we introduce UMLayer, a middleware component that allows integration with certain types of applications through the provision of services. This layer accepts\nbehavior specifications through UML sequence diagrams, allowing applications to access\nservices that are specified from these diagrams.\nThe goal is thus to allow an application, still under development, to have immediate access\nto services that correspond to its use cases, having the user to provide only the sequence\ndiagrams that specify them. This, in the user’s view, allows his application’s service layer\nto be completely replaced by sequence diagrams and the application becomes immediately\nready to use. The code generated from these diagrams will be inaccessible and unalterable\non the part of the user, since it is only and exclusively through the diagrams that the user\nwill specify all the logic of his use cases. All this mechanism of generation and subsequent\naccess to the generated code becomes as transparent as possible to the user, having to only\nworry about the correct elaboration of his diagrams."
  },
  {
    "keywords": [
      "Datacenter",
      "Cloud",
      "SDN",
      "OpenFlow",
      "681.3"
    ],
    "titulo": "A new Framework to enable rapid innovation in Cloud Datacenter through a SDN approach",
    "autor": "Teixeira, José António Barros",
    "data": "2013",
    "abstract": "In the last years, the widespread of Cloud computing as the main paradigm to deliver a large\nplethora of virtualized services significantly increased the complexity of Datacenters management\nand raised new performance issues for the intra-Datacenter network. Providing heterogeneous\nservices and satisfying users’ experience is really challenging for Cloud service providers,\nsince system (IT resources) and network administration functions are definitely separated.\nAs the Software Defined Networking (SDN) approach seems to be a promising way to address\ninnovation in Datacenters, the thesis presents a new framework that allows to develop and\ntest new OpenFlow–based controllers for Cloud Datacenters. More specifically, the framework\nenhances both Mininet (a well–known SDN emulator) and POX (a Openflow controller written\nin python), with all the extensions necessary to experiment novel control and management\nstrategies of IT and network resources.\nFurther more, the framework was validated by implementing and testing well known policies.\nHybrid allocation policies (considering both network and servers) were also implemented and\nscalability tests were performed.\nThis work was developed under the ERASMUS student mobility program, in the Telecommunication\nNetworks Research Group, Dept. of Information Engineering, University of Pisa,\nand resulted in the paper Datacenter in a box: test your SDN cloud-datacenter controller at\nhome that was accepted into EWSDN2013."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "A fully configurable virtual laboratory of classical mechanics",
    "autor": "Silva, Alexandre Ventosa da",
    "data": "2018",
    "abstract": "Nowadays many mathematical applications allow the user to introduce its own equations\nin the system and also observe through different possibilities the desired results. Regarding\nphysics, an extended range of virtual laboratories allow the user to accomplish virtual\nphysics experiments. These virtual laboratories consist in predefined scenarios where the\nuser can change the value of the physics variables and then visualise the changes accomplished.\nOther virtual laboratories uses a physics engine allowing the user to create its\nown scenarios. However, the physical behaviour of the objects is hardcoded since it results\nstrictly on the physics equations used internally by the physics engine.\nThis dissertation pretends to investigate how far and with what degree of scientific rigor\nit is possible to associate the idea of the user introducing its own equations with the idea of\naccomplishing virtual experiments of physics. As a proof of concept, this dissertation focus\non a specific area of mechanics: the dynamic of rigid bodies. The result of this research is\na virtual laboratory completely different relatively the others.\nOur system has no knowledge about physics. Even the most general laws of physics\nsuch as the Newton’s second law are not known by the system. To the system, any equation\nintroduced is considered just as one more equation without any particular meaning\nassociated to it. The same happens for any physics entity. For example, if the gravitational\nacceleration is introduced by the user, to the system it is just another attribute of the world.\nTaking into account the dynamics of rigid bodies, an object can be identified as being, at\nany time, in one of three different states. These are: when a object is not in contact with\nany other, when an object collides with another object and they immediately separate, and\nwhen two objects remain in contact over time. The user must specify all the equations that\ndrive each of these three states. Using its geometrical knowledge, the engine determines at\nany time in which state an object is. Also, the system provides all the relevant geometrical\ninformation. For instance, in a collision between two objects, the point and the two normals\nvectors of the collision are provided.\nThe graphical simulations reflects strictly on the equations introduced. Therefore, if\nthe equations to solve a collision between two objects does not reflect the real underlying\nphysics of the situation, it is possible that the objects simply ends-up penetrating each\nother. All the relevant numerical information about an experience can be processed through\ndifferent forms. In fact, the user can request plots of variables, the graphical application of\nvectors on objects, and even the tracing of the variables at a specific event."
  },
  {
    "keywords": [
      "Usage control",
      "App",
      "Data",
      "Controlo de uso",
      "Aplicação",
      "Dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Usage policy control data app for data spaces",
    "autor": "Peixoto, Ana Rita Abreu",
    "data": "2023-12-28",
    "abstract": "Digitalization has taken over many businesses and industries, as technology is now widely used and\npreferred over manual work. Therefore, technical devices nowadays contain several sensitive pieces of\ninformation that are prone to be shared with others. Consequently, over the years there has been an\nincreasing need to protect our data and to exchange it in a secure and sovereign way, as data are one of\nthe most valuable assets of any industry and business. Therefore, the need to implement systems that\nprioritize such values emerged, in order to grant continuous monitoring and control over data usage, to\nkeep track of how data is used even after it has been exchanged with other entities.\nThe motivation to consider Data Spaces relies on the fact that it should follow an interoperable and\ncross-sector approach, offering a reference implementation suitable to every industry and business, to\nease the communication between different industries and to allow data sovereignty and usage control over\ndata inside the IDS Ecosystem.\nBuilding upon this groundwork, this study presents a data app designed to implement Usage Control,\nintegrated inside an IDS Connector. This proposal ensures that customised usage policies are included\nand enforced on the user side of a connector and emphasizes the protection of sensitive data. Moreover,\nthe Data App incorporates a cipher mechanism to safely exchange cryptographic keys used to protect data\nassets. By incorporating this data app, the IDS initiative is able to stay truthful to its principle of ensuring\ndata sovereignty and building a robust IDS Ecosystem."
  },
  {
    "keywords": [
      "614:681.324",
      "681.324:614"
    ],
    "titulo": "Novos paradigmas de interface de utilizador para aplicações na área da saúde",
    "autor": "Rodrigues, Marco André Ferreira",
    "data": "2012",
    "abstract": "As instituições de saúde vivem atualmente num ambiente de crescente densidade de\ninformação, sendo esta uma área de intensa transferência eletrónica de dados. Como\nresultado, tem-se recorrido cada vez mais às interfaces de utilizador como forma de auxilio no\nprocessamento de dados. Tratando-se de uma questão inovadora, o surgimento de novas\ntecnologias de informação e comunicação tem sido um motor para o desenvolvimento de\nnovos tipos de interfaces de utilizador.\nCom o aumento do volume de informação, tem surgido problemas no que diz respeito ao seu\nprocessamento. Cada vez mais são exigidos que os serviços sejam prestados com maior\neficiência, implicando maior rapidez na obtenção de dados. É neste contexto que surge a\nnecessidade de direcionar os novos avanços no campo da saúde, tendo como linha\norientadora a busca de um serviço mais eficiente e com maior qualidade. Neste sentido surge\na necessidade de optimização dos processos de acesso e utilização dessa informação,\nalterando a forma como a informação sobre os utentes é obtida. É, essencialmente, nestes\npontos que se foca esta dissertação.\nAdicionalmente, e do ponto de vista funcional, pode dizer-se que estes desenvolvimentos\napresentam características favoráveis à prevenção e ao controlo de infeções hospitalares,\nreduzindo a necessidade de contato direto entre os objetos, o que leva, à diminuição da\npropagação das ditas infeções.\nCom a concretização deste estudo procura-se avaliar as potencialidades do reconhecimento\ngestual aplicado às interfaces de utilizador, na sua implementação na área específica da\nsaúde.\nParalelamente desenvolveu-se um protótipo destinado aos utentes que frequentem as\ninstituições de saúde. Este protótipo teve como objetivo a validação das interfaces de utilizador\nanalisadas, considerando a utilização das tecnologias recentemente introduzidas no mercado.\nFoi dada especial atenção a interfaces de utilizador sem contato entre dispositivos e\nutilizadores (e.g. utilizando a tecnologia do Kinect da Microsoft).\nNo final é apresentado um estudo estatístico relativo à avaliação por parte dos utilizadores da\ninterface do protótipo desenvolvido, onde se conclui a funcionalidade da utilização de gestos\nse revelou intuitiva e de fácil execução."
  },
  {
    "keywords": [
      "Microservices",
      "Monolithic",
      "Software architectures",
      "Inter-service communication",
      "Performance evaluation",
      "Microsserviços",
      "Monolítico",
      "Arquiteturas de software",
      "Comunicação inter-serviços",
      "Avaliação de performance",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "The impact of microservices: an empirical analysis of the emerging software architecture",
    "autor": "Costa, Leandro José Abreu Dias",
    "data": "2021",
    "abstract": "The applications’ development paradigm has faced changes in recent years, with modern development being characterized by the need to continuously deliver new software iterations. With great affinity with those principles,\nmicroservices is a software architecture which features characteristics that potentially promote multiple quality\nattributes often required by modern, large-scale applications. Its recent growth in popularity and acceptance in\nthe industry made this architectural style often described as a form of modernizing applications that allegedly\nsolves all the traditional monolithic applications’ inconveniences. However, there are multiple worth mentioning costs associated with its adoption, which seem to be very vaguely described in existing empirical research, being often summarized as \"the complexity of a distributed system\". The adoption of microservices provides the\nagility to achieve its promised benefits, but to actually reach them, several key implementation principles have\nto be honored. Given that it is still a fairly recent approach to developing applications, the lack of established\nprinciples and knowledge from development teams results in the misjudgment of both costs and values of this\narchitectural style. The outcome is often implementations that conflict with its promised benefits. In order to\nimplement a microservices-based architecture that achieves its alleged benefits, there are multiple patterns and\nmethodologies involved that add a considerable amount of complexity. To evaluate its impact in a concrete and\nempirical way, one same e-commerce platform was developed from scratch following a monolithic architectural\nstyle and two architectural patterns based on microservices, featuring distinct inter-service communication and\ndata management mechanisms. The effort involved in dealing with eventual consistency, maintaining a communication infrastructure, and managing data in a distributed way portrayed significant overheads not existent in the\ndevelopment of traditional applications. Nonetheless, migrating from a monolithic architecture to a microservicesbased\nis currently accepted as the modern way of developing software and this ideology is not often contested, nor the involved technical challenges are appropriately emphasized. Sometimes considered over-engineering,\nother times necessary, this dissertation contributes with empirical data from insights that showcase the impact of the migration to microservices in several topics. From the trade-offs associated with the use of specific patterns, the development of the functionalities in a distributed way, and the processes to assure a variety of quality attributes, to performance benchmarks experiments and the use of observability techniques, the entire development process is described and constitutes the object of study of this dissertation."
  },
  {
    "keywords": [
      "Systems biology",
      "Genome-scale metabolic models",
      "Metabolic networks",
      "Constraint-based modelling",
      "Merlin",
      "Syntrophobacter fumaroxidans",
      "Methanospirllum hungatei",
      "Syntrophic community",
      "KBase",
      "Engenharia e Tecnologia::Biotecnologia Industrial"
    ],
    "titulo": "Modelling interspecies interactions of syntrophic communities of Syntrophobacter fumaroxidans and Methanospirillum hungatei",
    "autor": "Bastos, José Jorge Sampaio",
    "data": "2019",
    "abstract": "Microbial communities have gained particular interest and have been used for practical applications such as biorefineries, and bioremediation. However, studying these communities has proven to be difficult due to the absence of experimental protocols and computational tools like the ones available for single organisms.\nIn this work, we present Genome-Scale Metabolic models both for Methanospirillum hungatei strain JF1 and Syntrophobacter fumaroxidans strain MPOBT, together with a model that combines both into one community model. The genome-scale metabolic model reconstruction of S. fumaroxidans was performed in merlin whereas, the methane-producing archaeon M. hungatei was reconstructed in KBase’s environment and the model curation was performed in merlin. OptFlux and BioCoISO, a tool implemented over COBRApy developed specifically for debugging model pathways, were used for curating and validating both models.\nThe metabolism of each individual organism was assessed through its model reconstruction. In silico simulations demonstrated the production of various compounds of interest such as formate in M. hungatei and acetate in S. fumaroxidans. The meta-model representing the community composed by both organisms was assembled using FRAMED, and it was able to describe the metabolic exchanges between the formate scavenger M. hungatei and the syntrophic partner S. fumaroxidans.\nThe reconstructed models can be used to study further the metabolic interactions between these bacteria."
  },
  {
    "keywords": [
      "681.3:61",
      "61:681.3"
    ],
    "titulo": "Normas, nomenclaturas e uniformização do registo clínico",
    "autor": "Castro, Sara Catarina Oliveira",
    "data": "2013",
    "abstract": "A crescente utilização dos Sistemas de Informação (SI) nas unidades de\nsaúde tem um papel muito importante para garantir a qualidade das mesmas.\nCom as Tecnologias da Informação e da Comunicação (TIC), os dados\narmazenados estão estruturados e organizados de forma a possibilitar uma\nutilização rápida e e caz. O aumento de informações em formato eletrónico\nno processo de Registo Clínico (RC) apesar de diminuir em grande escala\nerros que resultavam da utilização de dados mal entendidos, trouxe um desa\n o aos técnicos de informática médica. Esse desa o passa por melhorar a\nqualidade da prestação de cuidados de saúde utilizando a informação armazenada.\nÉ neste âmbito que surgem as normas e sistemas de nomenclatura que\npossibilitam uma uniformização do RC de forma a evitar dados ambíguos\ne permitir a comunicação entre diferentes pro ssionais de saúde e serviços\nhospitalares. Estas normas são divididas conforme a sua  nalidade, havendo\nnormas de comunicação, imagem e representação.\nPretende-se, neste contexto, implementar o Systematized Nomenclature\nof Medicine (SNOMED) na Agência de Interoperação, Difusão e Arquivo\n(AIDA) no Centro Hospitalar do Alto Ave (CHAA) de forma a utilizar as suas\npotencialidades no processo de uniformização do Registo Clínico Eletrónico\n(RCE)."
  },
  {
    "keywords": [
      "Computação Cloud",
      "Internet of Things",
      "Tecnologias de Informação e Comunicação",
      "Advanced Driver Assistance Systems",
      "Sistema de Exploração de Dados Sistemática",
      "Cloud Computing",
      "Information and Comunication Tecnology",
      "Systematic Data Field Exploration",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Arquitetura lógica de software para sistemas de tratamento sistemático de dados: abordagem em ambiente Cloud no contexto da indústria automóvel",
    "autor": "Pinto, Eduardo Manuel Oliveira",
    "data": "2017",
    "abstract": "Com a evolução da Internet of Things, a presença de Tecnologias de Informação e Comunicação\nestá cada vez mais omnipresente e vários estudos realizados nesse âmbito têm\nproporcionado o desenvolvimento de novas arquiteturas baseadas no conceito de Cloud\nComputing.\nNeste contexto, foram desenvolvidos novos modelos de serviços e novas ferramentas que\nse tornam importantes para a gestão dos dados adquiridos e a descoberta de novos conhecimentos\nrelacionados com as realidades do nosso mundo e sobre os comportamentos,\nrelacionados com mobilidade automobilística.\nDevido à importância que os Advanced Driver Assistance Systems na indústria automóvel,\npois garantem uma maior segurança e conforto na condução. Com isto, surge uma oportunidade\nde ter os veículos conectados, a recolher dados durante a condução com a finalidade\nde gerar informação que seja útil para auxiliar e melhorar o processo de condução.\nUma arquitetura lógica para sistemas que visam realizar um Systematic Field Data Exploration,\nbaseada no conceito Cloud Computing, é proposta neste trabalho. A arquitetura visa\ndar uma resposta sistemas de visam realizar uma recolha de dados sistemática, considerando\na aquisição, armazenamento, processamento e partilha de dados."
  },
  {
    "keywords": [
      "Streaming de vídeo",
      "HTTP",
      "QUIC",
      "TCP",
      "Condições adversas de rede",
      "Video streaming",
      "Adverse network conditions",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Vídeo streaming em condições adversas de rede: avaliação dos protocolos HTTP/3 e QUIC",
    "autor": "Guimarães, Filipe Miguel Teixeira Freitas",
    "data": "2024-07-25",
    "abstract": "A Internet está em constante evolução, bem como as aplicações e serviços disponibilizados aos utiliza dores. Com o crescimento na adoção de plataformas como YouTube, Netflix, Disney\n+\n, HBO e Amazon\nPrime Video, é possível ter acesso a um amplo conteúdo em formato de vídeo. Estas mudanças na\nInternet obrigam a que se criem formas de melhorar a experiência de utilização nestes novos cenários.\nO QUIC é um novo protocolo de transporte considerado atualmente uma peça chave no suporte\nà nova norma HTTP/3. Este protocolo opera sobre User Datagram Protocol (UDP) e visa oferecer um\nserviço de transporte multistream, rápido, robusto e seguro, que permite contornar limitações conhecidas\ndo protocolo de transporte Transmission Control Protocol (TCP), a base para os atuais protocolos HTTP/1\ne HTTP/2.\nEste trabalho, propõe avaliar o protocolo de transporte QUIC usado como base no recém-imple mentado HTTP/3, em alternativa ao HTTP/2, assente em TCP. Como já existem alguns estudos que\ncomparam estes protocolos no acesso a páginas web, neste trabalho é analisado o protocolo HTTP/3 no\ncontexto de streaming de vídeo, em cenários que possam existir condições adversas de rede. Pretende se, neste trabalho, desenvolver uma plataforma experimental que consiga transmitir vídeo recorrendo a\nestes protocolos e analisar esta transmissão para perceber se o protocolo QUIC realmente será útil para\na web em constante evolução.\nÉ desenvolvido um software, com recurso à framework hls.js, com o objetivo de receber e reproduzir\no vídeo, recolhendo determinadas métricas relativas à reprodução. É, ainda, criada uma ferramenta para\nanalisar e gerar gráficos, para os dados adquiridos.\nSão apresentados alguns resultados, ainda que preliminares, que permitem observar diferenças entre\nos protocolos HTTP/2 e HTTP/3. Estas diferenças são mais notórias em condições adversas de rede,\nem que o HTTP/3 proporcionou uma transmissão de vídeo mais fluída, com menor latência, menor drift\ne maior carga de buffer, na generalidade dos cenários escolhidos."
  },
  {
    "keywords": [
      "Sistemas de data warehousing",
      "Processamento analítico de dados",
      "OLAP",
      "Sessões OLAP",
      "Cadeias de Markov",
      "Classes de equivalência",
      "Assinaturas OLAP",
      "Identificação e caracterização de perfis de exploração",
      "Data warehousing",
      "Data analytical processing",
      "OLAP sessions",
      "Markov chains",
      "Equivalence classes",
      "OLAP signatures",
      "Identification and characterization of exploration profiles",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Definição e caracterização de assinaturas OLAP",
    "autor": "Silva, Ricardo Manuel Arantes",
    "data": "2017",
    "abstract": "As assinaturas OLAP podem ser vistas como uma forma de caracterização de um dado perfil de exploração analítica. Porém, ao contrário de um perfil de exploração típico, uma assinatura OLAP não tem uma natureza estática. Uma assinatura OLAP congrega de uma forma única todos os elementos de informação recolhidos ao longo do tempo nas várias sessões de exploração OLAP desenvolvidas por um dado utilizador, caracterizando de uma forma bastante concreta esse utilizador ao longo do tempo. Num sistema OLAP as assinaturas podem ser utilizadas para traçar um perfil de exploração de dados de um dado utilizador, baseado nas queries que este coloca ao longo do tempo sobre um dado sistema de processamento analítico e dos seus hábitos e tendências de exploração. Através da análise das assinaturas OLAP podemos otimizar as estruturas multidimensionais – cubos - de um dado sistema analítico, de forma a reduzir o seu tamanho, guardando apenas informação relevante, e prever quais as operações que podem ser despoletadas a partir da ocorrência de uma dada querie. Desta forma é possível escolher a priori quais as partes do cubo que devem ser carregadas para memória ou aquelas que podem ser transferidas para a máquina do próprio utilizador. Tudo isto, para que seja possível minimizar a carga do servidor e reduzir o tráfego de dados no sistema de comunicação que suporta os processos de exploração analítica. Neste trabalho de dissertação exploraremos esta temática e definiremos um método sustentado para definição e manutenção de assinaturas OLAP."
  },
  {
    "keywords": [
      "Bayesian inference",
      "Monte Carlo integration",
      "Quantum metrology",
      "Inferência Bayesiana",
      "Integração de Monte Carlo",
      "Metrologia quântica"
    ],
    "titulo": "Learning the physics of open quantum systems from experiments",
    "autor": "Alves, Alexandra Francisco Ramôa da Costa",
    "data": "2021-12-06",
    "abstract": "The ability to efficiently determine the dynamics to which a quantum device conforms is vital for\nits reliable operation. Thus, as quantum machines evolve, the means for their characterization must\nevolve alongside them, especially as they reach the limits of classical tractability. Bayesian inference has\nbeen proposed as a solution, as it offers a flexible way of using experimental data to learn the dynamical\nparameters - or even models - governing the evolution of quantum systems. It gives rise to noise-resilient\nprotocols, which are capable of quantifying their own uncertainty, of learning from scarce information, and\nof real-time estimation. Importantly, and apart from the obvious applications in sensing devices, online\nprocessing enables adaptivity, a stepping stone for achieving fundamental limits of metrology via what is\ncalled quantum-enhanced estimation. Like so, the exploitation of quantum control as a resource within\nthe Bayesian paradigm opens the door to uncertainties scaling at the Heisenberg limit.\nMost work on the subject has relied on simple methods for portraying the Bayesian posterior, but they\nquickly become a bottleneck in realistic scenarios. The difficulty of the task grows with the complexity of\nthe devices to be verified, which adds to the fact that they are ultimately open systems and as such undergo\nuncontrollable interactions with their surroundings. While capturing unwanted processes like decoherence\nis by itself a valuable endeavor, this poses extra challenges, and requires careful computational treatment.\nIn general, when scaling up, the biggest difficulty of Bayesian learning is numerical integration. In this\ncontext, pairing it with advanced Monte Carlo methods makes for remarkably robust algorithms, which\ncan succeed under complex features and control a multitude of trade-offs.\nThis dissertation aims to overview both of these methodologies, and to apply them to the characterization of noisy quantum computers. Special emphasis is placed on state-of-the-art methods for statistical sim ulation - namely Hamiltonian and sequential Monte Carlo, variants thereof, and subsampling approaches.\nUsing these strategies to post-process 150 single-shot measurements on a Ramsey sequence, we learn an\noscillation frequency and coherence time to 0.4% and 33% uncertainty respectively, compared to 15% and\n333% using curve fits. With an echoed pulse set-up and 75 shots only, we then achieve an uncertainty of\n0.3% for the qubit’s precession frequency, whereas standard fitting methods either do worse or fail completely. Switching to online processing and further lowering the total shot count to 15, we use adaptive\nexperimental design to infer the frequency to 5% uncertainty, compared to 44% using offline processing\nunder otherwise identical conditions."
  },
  {
    "keywords": [
      "XML",
      "DTD",
      "XSD",
      "Schematron",
      "Graphs",
      "Grafos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "2D graph editor for XML with semantic validation",
    "autor": "Jácome, Bruno Pinto",
    "data": "2024-01-15",
    "abstract": "Nowadays, XML is one of the most used markup languages for storing and sharing data. As so, special\ncare must be taken when validating the content of these files, ensuring that they comply with the standards\ndefined for the data they represent. In order to ensure that these standards are met, XML validation languages such as DTD, XSD, or Schematron can be used. These languages allow the definition of structural\nor semantic rules, that need to be followed in order to guarantee that the XML documents contain valid\ndata.\nIn terms of XML visualizing and editing, although there are several XML editors available, many of\nwhich have a wide variety of features that make them easier to use, there is a problem that no editor\naddresses: as these documents grow in size, it becomes more difficult to identify the boundaries of each\nelement, which makes editing these files way harder than it should be.\nThis paper proposes and documents the development of a tool with two main components. The first\ncomponent is an XML validator that aims to offer greater freedom in defining validation constraints for these\nfiles, by allowing users to use a predefined programming language to write functions that will analyze the\ngraph in order to determine if it is both syntactically and semantically correct. The second component, an\nXML graphic editor, will allow an easier creation, editing, and visualization of these files, by representing\nthem in the form of 2D graphs."
  },
  {
    "keywords": [
      "Augmented reality",
      "Virtual reality",
      "Anxiety disorder",
      "Phobia",
      "WebAR",
      "Gradual exposure",
      "3D models",
      "Realidade aumentada",
      "Realidade virtual",
      "Transtorno de ansiedade",
      "Fobia",
      "Exposição gradual",
      "Modelos 3D",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "PhobiAR, an artefact of augmented reality to support the exposure therapy of specific phobias",
    "autor": "Vilas Boas, Raul",
    "data": "2020-12-22",
    "abstract": "Phobia is a type of anxiety disorder defined by a persistent and excessive fear of an\nobject or situation. Currently, exposure therapy is the most practiced method to treat\nphobias, although it comes with limitations. We can reduce these limitations by combining\nAugmented Reality techniques with exposure therapy. Its benefits are a decrease in costs,\nversatility of the process, and full control of the procedure by the therapist. As shown in\nmultiple research, Augmented Reality has obtained interesting results in the therapy of\npsychological disorders serving as a foundation for the development of this project. The\nrecent technological advances in the field also allowed for easier access to Augmented Reality\nwhich is accessible to use even in old smartphones. The goal of this Master’s dissertation\nwas to develop an artefact in conjunction with psychologists who treat phobic patients, to\ncreate a program to support the therapy of phobias with a gradual exposure system. Their\nhelp was essential to understand the most important features needed for the platform. The\nplatform was deployed in the informatics department servers, which could be accessed\nby everyone that had internet connection. Multiple psychologists were invited to test the\nplatform by following a user guide created and give their technical feedback in the end. The\nresults gathered were positive, which proves the viability of this system as an extension to\nthe current methods by providing comfort and efficiency."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Controlo de periféricos por voz",
    "autor": "Silva, Nelson Manuel Figueiredo da",
    "data": "2019-12-23",
    "abstract": "Nowadays, mouse and keyboard are crucial perpherals for interacting with a computer.\nThese peripherals allow computer users to navigate the windows, select items, type int\ntext, among others. However, the use of such peripherals, which is considered elementary\nfor most of us, is sometimes an obstacle to computer interaction, especially for users with\nphysical limitations or fine motor skill problems. This dissertation presents a system that\nwill allow the interaction with a computer using only the voice, without the need for a\nphysical/mechanical interaction. This system also allows the user to, besides controlling\nthe keyboard and the mouse cursor, carry out the most common tasks by using voice com mands, such as creating a task for a date or conduct a web search. Lastly, this dissertation\npresents the efficiency tests carried out and the system acceptance in a real context."
  },
  {
    "keywords": [
      "Ciências Médicas::Ciências da Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Arquitetura distribuída para análises multimodais de conectividade cerebral",
    "autor": "Fernandes, Filipe Bernardino",
    "data": "2015",
    "abstract": "O estudo da conectividade, estrutura, e integração das funções cerebrais\né actualmente uma das ferramentas de maior importância na compreensão\ndo cérebro humano. A realização destes estudos via aquisições de Ressonância\nMagnética exige no entanto acessibilidade e disponibilidade constante de\ninformação. A quantidade de procedimentos e técnicas de análise, associado\nà produção de grandes volumes de dados e multitude de soluções de software\nsão alguns dos principais entraves à organização, manutenção e partilha de\nestudos neuroimagiológicos.\nDesta forma, o objectivo principal deste trabalho consiste na concepção\nde um fluxo de processamento, que possa servir de padrão à conjugação\nde resultados de análises multimodais, através de uma estrutura de estudos\ndefinida e nomenclatura de ficheiros própria. Tendo por base este fluxo foi\ndesenvolvida uma aplicação, designada BrainArchive, para automatização\ndo processo de organização e partilha de estudos neuroimagiológicos. Esta\npermite por sua vez a disponibilização de grandes volumes de informação\nsem necessidade de caracterização manual de cada ficheiro, ao mesmo tempo\nque se revela uma ferramenta simples e intuitiva na aquisição de dados.\nO protótipo desenvolvido responde às necessidades do seu contexto de utilização\ne, por conseguinte, é espectável que potencie o processo de investigação,\natravés da simplificação e redução do tempo despendido na organização\ne partilha de informação."
  },
  {
    "keywords": [
      "Backend",
      "Estruturação textual",
      "Frontend",
      "Mapas mentais",
      "Planificação",
      "Mind Maps",
      "Planing",
      "Text structuring",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aplicação Web Fullstack para auxílio à redação de textos através de mapas mentais",
    "autor": "Martins, Marcos Alexandre Ferreira",
    "data": "2024-04-09",
    "abstract": "Esta dissertação propõe uma solução a um problema existente num publico que abrange desde o\nestudante comum até ao profissional do quotidiano. Qualquer indivíduo nesta audiência pode enfrentar\nobstáculos na estruturação de textos, como relatórios académicos ou profissionais, encontrando dificul dades em organizar as ideias para o formato textual desejado.\nA utilização de métodos de organização como os mapas mentais desempenha um papel crucial como\nprimeiro passo na escrita, pois auxiliam no planeamento da escrita, um processo essencial na construção\nde qualquer tipo de texto, facilitando a organização de ideias.\nA dissertação aborda o desenvolvimento de uma aplicação web, tanto na sua vertente de frontend\ncomo na de backend. A aplicação integra a edição do mapa mental e do texto no mesmo ambiente,\nagilizando a escrita, algo que segundo a pesquisa realizada, não existe.\nO desenvolvimento da aplicação incluiu o planeamento da marca, nomeadamente a criação de logó tipos e slogans. Seguindo-se a fase de design, onde foram realizados mockups das páginas. O NextJs\nfoi selecionado como framework para o frontend e backend com o Typescript. Para a Base de Dados\n(BD), foi utilizado o MongoDB com o Docker, para um deployment local, escalável e fácil de migrar. A\nimplementação do frontend foi estruturada em 3 camadas diferentes, como a camada de apresentação,\nde aplicação e a de negócios.\nNo desenvolvimento do frontend foram utilizadas várias bibliotecas de React para auxiliar no desenvol vimento dos mapas mentais e do editor de texto, onde foram impostas regras de funcionamento, como\npor exemplo, a criação máxima de 5 níveis diferentes no mapa mental e a proibição de promover ou\ndesprover nós que não cumpram com certos requisitos. O texto é estruturado ao longo da construção\ndo mapa mental, podendo apenas gerir o conteúdo de cada nível criado. Os dados são todos persistidos\nlocalmente no localStorage do browser para funcionamento offline.\nFinalmente, para avaliação da aplicação, foi realizada uma experiência com um grupo de estudantes.\nEstes responderam posteriormente a um questionário sobre o funcionamento da aplicação e sobre a\nexperiência realizada. Os resultados obtidos foram em média bastante positivos, tendo sido expostos\nalguns problemas em pequenas funcionalidades, que serão sujeitas a melhoramentos."
  },
  {
    "keywords": [
      "Traffic classification",
      "Encrypted traffic",
      "Machine Learning",
      "Traffic analysis",
      "Classificação de tráfego",
      "Tráfego criptografado",
      "Análise de tráfego",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Metodologias para classificação de tráfego de rede seguro",
    "autor": "Gonçalves, Matheus dos Santos",
    "data": "2022-02-19",
    "abstract": "Characterizing network traffic is a very important process for network planning,\nmanagement, and analysis. Despite having received attention over the years, there are\nstill many improvements to be developed, for example, how to accurately classify secure\nnetwork traffic. The research community has already presented numerous characterization\nmethodologies, and in this dissertation, one of the approaches for the characterization of\nsecure network traffic is investigated. First, the most common encrypted traffic protocols on\nthe Internet are presented. Its architecture and operating mode are shown to carry out data\ntraffic safely. Next, the methods for capture network traffic are examined and the most used\nand efficient methods of classification of network traffic are pointed out in the study of the\ncharacterization of secure traffic. The advantage of each method, the use of hybrid methods,\nthe accuracy of characterizing certain application protocols are discussed.\nAfter selecting the desired method of characterization of secure traffic from among the\nseveral that were presented, an analysis of the accuracy of this method was made with\nseveral datasets. In addition to the tests carried out with data capture in an experimental\nenvironment, where all generated traffic was controlled, tests with public datasets were also\naccomplished. Finally, the results obtained from the precision achieved in each environment\nare revealed and the results were synthesized with a brief explanation and highlighting their\ncharacteristics."
  },
  {
    "keywords": [
      "Indoor positioning system",
      "Wi-Fi",
      "CSI",
      "Tag",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Object tracking in industrial environments",
    "autor": "Rio, Diogo Miguel Pinto",
    "data": "2023-12-05",
    "abstract": "A sociedade atual está bastante dependente de sistemas de navegação por satélite. Estes sistemas\nutilizam satélites para fazer a geolocalização de um dispositivo. Um dos exemplos mais conhecidos de\num sistema deste tipo é o famoso Global Positioning System (GPS). Devido à atenuação de sinais causada por materiais de construção, um sistema de posicionamento por satélites está limitado a espaços\nexteriores. Um sistema de posicionamento interior tenta responder a este problema e usa um conjunto\nde dispositivos que permitem fazer o posicionamento de pessoas ou objetos em espaços interiores. Esta\nárea de estudo tem sido alvo de várias pesquisas nos últimos anos e, recentemente têm sido implementados em vários setores. Por exemplo, na monitorização de idosos que vivem sozinhos, na gestão de\nmaterial hospitalar, no seguimento de pessoas para fins de segurança e para uma melhor gestão de\nrecursos em grandes armazéns. Embora os sistemas de posicionamento interiores tenham evoluído significativamente nos últimos anos, existem poucos dispositivos móveis (tags) disponíveis para integração\ncom os sistemas. Além disto, as capacidades das tags que existem são limitadas, especialmente no que\ntoca à sua comunicação com sistemas não proprietários. Esta dissertação procura desenvolver e propor\numa tag que possa responder a estes problemas."
  },
  {
    "keywords": [
      "Wi-Fi fingerprinting",
      "Aplicação desktop",
      "MVVM",
      "Posicionamento indoor",
      "Integração com o Facebook",
      "Desktop application",
      "Indoor positioning",
      "Facebook integration",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Where@UM: aplicação de posicionamento colaborativo para PC baseada em Wi-Fi fingerprinting",
    "autor": "Mesquita, Ricardo Miguel Rego",
    "data": "2016-12-15",
    "abstract": "A utilização do posicionamento, no âmbito das aplicações fornecidas aos utilizadores, tem vindo\na aumentar exponencialmente. Os trabalhos desenvolvidos na área do posicionamento indoor têm vindo\na aumentar com o aumento da mobilidade dos utilizadores.\nAs possibilidades de uso destas tecnologias são imensas: aumentar a experiência do utilizador\ne a lealdade, aumentar as vendas através de marketing de proximidade, ajudar a movimentação de\nutilizadores em locais públicos, o uso de geofencing para encontrar pessoas, etc.\nDe forma a reaproveitar as infraestruturas já existentes nos edifícios, a tecnologia de Wi-Fi\nfingerprinting tem sido uma escolha frequente por parte das equipas de investigadores e programadores.\nO principal objetivo desta dissertação é desenvolver uma aplicação para computadores pessoais usando\num sistema de posicionamento baseado em Wi-Fi fingerprinting, integrando-a no sistema Where@UM.\nA solução apresentada detalhadamente na dissertação implementa funcionalidades já\ndisponibilizadas na aplicação Android. Foram também desenvolvidas novas respostas a problemas já\nexistentes e integrados novos módulos na arquitetura, como a integração com as redes sociais e o\nsuporte multiplataforma, tendo especial cuidado em manter alguma homogeneidade no ambiente\naplicacional Where@UM, através do uso de interfaces de utilizador similares."
  },
  {
    "keywords": [
      "SNMP",
      "Localização automática de veículos",
      "Informação em tempo real",
      "Transportes de passageiros",
      "Automatic vehicle localization",
      "Real time data",
      "Public transports",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistema de monitorização de transportes urbanos utilizando SNMP",
    "autor": "Zenha, Miguel Gomes",
    "data": "2019",
    "abstract": "As empresas do sector dos transportes públicos têm procurado introduzir novas tecnologias\ne aplicações com o objetivo de melhorar o serviço disponibilizado aos passageiros. Neste\nsentido, os operadores deste tipo de serviço têm investido no aumento da segurança e\nconforto do passageiro, na diversificação das funcionalidades disponibilizadas, no acrescento\nde novos destinos ou paragens e na eficácia do cumprimento de horários planeados, entre\noutros. Do lado das tecnologias de informação, uma das funcionalidades que mais tem\nrecebido atenção é o sistema de informação e monitorização acessível aos utilizadores.\nEnquanto uma parte substancial da informação mantida por estes sistemas é atualizada\npouco frequentemente, podendo ser, inclusive, denominada de informação de carácter fixo,\nos dados utilizados no sistema de monitorização devem ser atualizados o mais\nfrequentemente possível. Mas, o objetivo de aumentar a qualidade do serviço de informação\nprestado melhorando a qualidade da monitorização do sistema, implica, em geral, custos\nelevados. No planeamento e construção de sistemas e aplicações de informação para\nmonitorização de serviços de transporte de passageiros numa escala urbana ou regional, a\nescolha do tipo de infra-estrutura que torna possível obter, processar e utilizar os dados em\ntempo real, ou com frequência funcionalmente útil, é, assim, fulcral. Uma má escolha das\ntecnologias associadas e dos sistemas que integram o serviço, podem tornar os custos\ninerentes à sua implementação no terreno insustentáveis e podem comprometer seriamente a\nsustentabilidade das empresas do ramo.\nO presente trabalho começa com um estudo aprofundado das várias soluções e tecnologias já\nexistentes no mercado, analisando-as criticamente. Desse estudo resultou o desafio de\nconceber uma proposta dum sistema aplicativo integrado de gestão de informação que\ndisponibiliza ferramentas para a gestão, optimização e administração do serviço de\ntransportes públicos com base em tecnologias normalizadas, abertas e de sem custo de\nutilização.\nA solução desenhada, que se pretendeu modular e escalável, consiste num sistema integrado\nbaseado inteiramente no protocolo Simple Network Management Protocol (SNMP) e que combina aplicações informáticas com a capacidade de obter dados em tempo real dos\nveículos e também disponibilizar informação diferenciada aos diversos intervenientes do\nsistema (passageiros, gestores, motoristas).\nApós a implementação e teste dum sistema completo de aplicações protótipo, fica provada a\nefetividade funcional da arquitetura proposta. Sendo esta solução modular e baseada numa\ntecnologia normalizada e aberta e de utilização livre de direitos de autor, espera-se, em\nconsequência, que também seja eficaz no que concerne a eventuais custos de implementação\ne adoção no mundo real."
  },
  {
    "keywords": [
      "681.3.062"
    ],
    "titulo": "From natural language requirements to formal descriptions in Alloy through boilerplates",
    "autor": "Cadete, Daniel Nascimento",
    "data": "2012-04-05",
    "abstract": "Formal Methods are usually applied by specialists in the final phases of software development.\nThey aim to identify programming errors, and through that reduce the probability of a future failure.\nUsually, errors are more related with misinterpretation of requirements than with bad programming.\nMore than ever, requirements documents deal with complex terms, which programmers\naren’t familiar with, resulting in an increase of misinterpretation of requirements and increasing\nthe costs of the execution of a software project. The use of formal methods could reduce these\ncosts, if properly used to verify requirements and not source code. However, most companies\navoid using formal methods due to high costs associated with formal methods application. Programmers\nor requirements engineers can’t apply formal methods efficiently without previously\nhaving specific training, which implies hiring expensive specialists in formal methods.\nThis dissertation presents methods which aim to bring formal methods closer to requirements\ndescriptions. For such, formal modeling is used to verify and validate the descriptions of requirements,\nand not source code. Initially it’s presented a standard to create formal models, which\nmakes a direct correspondence between each requirement and its model. This standard is supported\nby a tool which, among other things, automatically generates graphics representations\nof requirements using its models. Afterwards it’s presented a connection between requirements\nboilerplates and Alloy models. This connection allows to generate formal models in an automatic\nfashion, without the need of a specialist. This drastically reduces the costs of using formal\nmethods in software projects. It’s also presented the beginning of an algebra which allows to\naggregate these templates. This aggregation allows one to write its requirements documents\nthrought boilerplates and at the end have the complete model of all requirements, for free.\nWhen one is modeling a requirements document in Alloy and at some point appears requirements\nwith explicit temporal restrictions, it’s necessary to recreate the whole model in a tool which\nallows that kind of specification (eg. Uppaal). This process is highly error prone, because it’s a\nmanual transformation and highly dependent on the interpretation of who is modeling. In this\ndissertation it’s presented a method which allows to automatically generate an Uppaal model\nfrom an Alloy model. This transformation allows that at any point in the requirements document,\nthe requirements engineer can generate the correspondent Uppaal model and there specify the\ntemporal properties."
  },
  {
    "keywords": [
      "681.3:614.2",
      "614.2:681.3"
    ],
    "titulo": "Suporte multidimensional para sistemas de business intelligence",
    "autor": "Gonçalves, Inês Cerqueira",
    "data": "2013",
    "abstract": "Os avanços tecnológicos e industriais, as constantes inovações e a necessidade\nde melhoria contínua ao longo dos últimos anos pelas instituições, têm proporcionado\num exponencial aumento na quantidade de informação gerada,\ne consequentemente armazenada por estas. As instituições de saúde, como\nqualquer outra organização, geram uma grande quantidade de dados relativos\naos seus processos, os quais, muitas das vezes, não são registados e geridos\nadequadamente, tornando muito difícil a sua posterior gestão e manipulação.\nPor outro lado, torna-se necessária a compreensão dos custos envolvidos\nna prestação dos diferentes cuidados de saúde por parte dos gestores hospitalares,\npara a melhoria da qualidade e e ciência dos diversos processos\ndiários neste tipo de instituições. Os Serviços de Informação para Gestão\n(SIG) hospitalares têm entre diversas responsabilidades, o registo de todos\nos procedimentos hospitalares relacionados com a área de gestão hospitalar,\noriginando um grande volume de dados e informação, a qual necessita de ser\nbem manipulada.\nEstes dados são de extrema importância em tomadas de decisão. Como\ntal, os mesmos precisam de sofrer um processo de modelação e organização\natravés da utilização de sistemas projetados especi camente para esta fun-\nção, como é o caso dos sistemas de Business Intelligence (BI). O conceito\nde BI emergiu nas instituições hospitalares como medida para solucionar o\nproblema existente no tratamento e processamento dos dados na área da\nsáude, transformando-os em informação e conhecimento útil para os pro ssionais.\nDe uma forma geral, os sistemas de BI representam um conjunto de\ntecnologias e aplicações, que atuam na recolha, análise e difusão dos dados\nexistentes, funcionando como suporte para tomadas de decisão e cientes.\nO principal objetivo deste projeto prende-se, essencialmente, com o desenvolvimento\nde uma aplicação de suporte multidimensional para sistemas\nde BI a ser implementada no Centro Hospitalar do Porto, para uso exclusivo\npor pro ssionais pertencentes aos SIG da mesma instituição. Esta aplicação\npermite a importação direta de folhas de Excel, que contém registos efetuados\npor estes pro ssionais, para uma Base de Dados, alimentando diretamente a Data Warehouse (DW) e as Data Marts (DMs) existentes e a existir para este\npropósito. Com isto, estes pro ssionais passam a ter total responsabilidade\nna manutenção e gestão de registos mantidos em folhas de Excel, o que não\nacontecia anteriormente, mantendo as componentes dimensional e factual da\nDW e DMs através de dados mantidos em folhas de cálculo.\nVeri cou-se que a aplicação desenvolvida, enquanto ferramenta de suporte\npara sistemas de BI, é inteiramente capaz de ser implementada e integrada\nnas operações diárias da organização hospitalar, facilitando a gestão destes\ndados e o trabalho dos pro ssionais, proporcionando um aumento da  uidez,\nrapidez, tratamento, recolha e análise da informação."
  },
  {
    "keywords": [
      "AdHoc",
      "DTN",
      "Mobilidade de nodos",
      "Tolerância a falhas",
      "Transferência de dados",
      "Data transfer",
      "Fault tolerance",
      "Node mobility",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Framework de rede tolerante a falhas",
    "autor": "Carvalho, João Pedro de Moura Pereira e Ferreira",
    "data": "2021",
    "abstract": "Uma Fault Tolerant Network é uma estrutura de Redes que tem como objetivo garantir a\ncomunicação entre Nodos de uma Rede mesmo que esta seja propícia à perda de Datagramas\ne à consequente perda de Informação. Estas perdas podem acontecer por vários motivos,\nmas este projeto tem como alvo analisar três casos, o da mobilidade de Nodos na Rede, o da\nconexão intermitente e o da conexão esporádica.\nTodos estes ambientes onde se pretende assegurar a troca de Informação entre Nodos apresentam\numa característica em comum, um possível volume elevado de perda de Datagramas\na qualquer instante que limita a quantidade de Dados que podem ser trocados bem como a\nQualidade de Serviço destas mesmas trocas de Dados. Esta é a principal característica que se\npretende atenuar com o desenvolvimento deste Projeto, porém existem outras relacionadas\ncomo a interrupção prolongada de uma Transmissão e a sua retoma que também foram\nanalisadas.\nComo todos os ambientes referidos anteriormente apresentam adversidades semelhantes\nou que podem ser tratadas como tal, optou-se pelo desenvolvimento de um Protocolo de\nTransferência de Dados adaptado a tais adversidades. Este encontra-se entre a Camada de\nTransporte e a Camada de Aplicações da Network Stack e pode ser utilizado como base para\no Desenvolvimento de Arquiteturas que possibilitem a Troca organizada de Informação\nentre Nodos.\nNeste Projeto foi Desenhado e Implementado um Protocolo de Transferência de Dados\nque possibilita a troca de informação nos ambientes anteriormente referidos, apresentando\nresiliência a Drops de Datagramas, grandes Delays na transmissão destes e movimentações de\nNodos na Rede.\nPara além deste Protocolo, foi Desenhada uma simples Arquitetura de Redes baseada em\nRedes Ad Hoc onde cada Nodo tem uma visualização da Rede centralizada nele próprio e os\nrestantes Nodos visíveis encontram-se organizados em Níveis de Vizinhança consoante a sua\ndistância ao Nodo central. Foi tamb´em definido o objetivo de desenho de uma Arquitetura\nde Redes baseada em Redes DTN, que fortemente influenciou o Protocolo de Transferência\nde dados devido às suas grandes exigências.\nPor fim foram realizados testes em determinados cenários reais pertinentes ao Protótipo\nImplementado de forma a provar que os objetivos delineados inicialmente foram atingidos."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Progressive web development: a case study in obstetrics",
    "autor": "Loreto, Patrícia Alexandra Soares",
    "data": "2018-12-13",
    "abstract": "In the past years, with the evolution of technology and the rise of the Internet,\nPersonal Health Records appeared. These records are maintained by patients\nand turn them into a more active stakeholder in their own health management.\nThey can be used to record medical parameters or give useful information to\nthe patient, among others.\nHowever, this was not the only result from this evolution. With the Internet,\nmore medical information became available to everyone, most of it not from\nreliable sources, which brought additional problems.\nThe obstetrics field was also impacted by these changes. This, combined with\nfact that a pregnancy is a delicate medical condition, brought the necessity to\nfind solutions for this case.\nThis dissertation aims to develop a Personal Health Record for pregnant women\nthat provides reliable information, and that has a set of features they find useful.\nWith this goal in mind, a literature review on the technologies and methodolo gies that are used nowadays and on the use of technology by pregnant women\nis made.\nThen, all the development process is presented, as well as the final result.\nThis process was supervised by a medical institution, which had the advantage\nof facilitating the process of having feedback from pregnant women and it also\nprovided all the medical information displayed in the application.\nThe final result is an application that has a wide range of useful features, pro vides trustworthy information, is available in all devices and that was developed\nusing modern technologies."
  },
  {
    "keywords": [
      "Home food delivery",
      "Take-away",
      "Restaurant",
      "Web development",
      "Angular",
      ".NET",
      "Entrega ao domicílio",
      "Comida para levar",
      "Restauração",
      "Desenvolvimento Web",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Wintouch cloud - delivery module",
    "autor": "Freitas, Filipe José da Silva",
    "data": "2022-12-13",
    "abstract": "This document constitutes the final report of a Master’s Thesis focused on developing a working soft ware product for the company Wintouch, with the purpose of managing the delivery of prepared/cooked\nmeals at restaurants.\nThe software product developed and here discussed (the working process and the final product)\nmanages the entire process of deliveries in restaurants, allowing clients to place orders online, via a\nphone call or in person, and keeping track of the subsequent tasks until the order is delivered to the\nclient. This management includes tasks such as deciding when to start preparing the order, sending the\ncouriers to clients’ homes, while managing their routes and maximizing the number of orders they take\nto a certain area. The software package and application developed and under discussion also ensures a\nproper interaction with Wintouch’s products, allowing restaurants to save information about clients, so as\nto increase the efficiency of future contacts with clients."
  },
  {
    "keywords": [
      "681.3-7"
    ],
    "titulo": "Identificação biométrica e comportamental de utilizadores em cenários de intrusão",
    "autor": "Martins, Henrique Fontão",
    "data": "2013",
    "abstract": "A usurpação de contas e o roubo de identidade são problemas muito frequentes nos atuais\nsistemas informáticos. A facilidade de acesso à internet e a exposição das pessoas a este meio,\ntorna muito frequente a utilização indevida e a usurpação de contas (tais como: e-mail, redes\nsociais, contas bancárias) por outras pessoas que não as suas legítimas proprietárias.\nAtualmente o método de autenticação dominante é o da combinação nome de utilizador e\npalavra-chave. No entanto, este método pode não ser fiável, pois estas credenciais podem ser\npartilhadas, roubadas ou até esquecidas. Por outro lado podem-se combinar várias técnicas para\nreforçar a segurança dos sistemas. Cartões de acesso (tokens), certificados digitais e biometrias são\nalgumas delas. Os cartões de acesso, por exemplo os das caixas multibanco, podem ser roubados\nou duplicados, como é frequentemente noticiado em fraudes bancárias. Os certificados seguem o\nmesmo caminho dos tokens uma vez que estes podem ser distribuídos por correio eletrónico ou em\ndispositivos USB. As biometrias físicas (impressão digital, íris, retina ou geometria da mão por\nexemplo), para além de serem um pouco intrusivas, requerem a aquisição de equipamento caro.\nUma possível solução para os problemas inumerados são as biometrias comportamentais.\nA forma como nos comportamos e agimos num computador pode ser usada como\ninformação biométrica. Esta informação pode ser utilizada à posteriori, geralmente complementada\ncom mais dados, para identificar, inequivocamente, (ou pelo menos com um determinado grau de\nconfiança) um indivíduo. A informação recolhida pode variar desde o tipo de escrita no teclado,\nhabilidade com o rato, hábitos, cliques, número de páginas abertas, origem do acesso, etc., que\ndepois será sujeita à utilização de algoritmos comportamentais para autenticar, de forma\ninequívoca, um utilizador.\nNeste trabalho pretende-se implementar como reforço aos atuais sistemas de autenticação\ne de deteção de intrusões, a verificação de perfis comportamentais do proprietário da conta. Este\nsistema não irá apresentar grandes custos, já que só serão usados equipamentos básicos, e será\ncompletamente invisível para o utilizador, ou seja este será continuamente autenticado de forma\nsilenciosa e não intrusiva."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Monitoring pregnant women lifestyle",
    "autor": "Fonseca, Francisca Maria Salgado",
    "data": "2018-12-13",
    "abstract": "Pregnancy is a period of change and uncertainties. There is a constant search for information and reassurance that all the changes going through pregnant women’s body are normal. Furthermore, during the last years the access to information has increased with the expansion of smartphones and their functionalities and of mobile apps. With this expansion, the concept of mHealth emerged as the use of mobile devices in medicine and public health. mHealth is centred on the patients’ care and aims to provide knowledge and empower patients to monitor their health.\nIn cooperation with Centro Materno Infantil do Norte a supporting platform for preg nant women was developed called ”The 10 moons: pregnancy, childbirth and post partum period of CMIN”. ”The 10 moons” goal is to provide pregnant women\nwith access to their pregnancy booklet Boletim de Saude da Gr ´ avida ´ , to personalised\ninformation and to a set of tools that will allow them to monitor their pregnancy\nas well as to record appointments and pregnancy-related documents. In order to\nmake it available to the widest range of users, it was tried to use an efficient devel opment approach. Hence, the software development methodology adopted for the\nimplementation of the platform was the one of the Progressive Web App.\nOverall, the development of the platform was divided into three main areas. This\ndissertation focus is the pregnancy monitoring and the provision of information in\norder to promote a healthy lifestyle and avoid complications during pregnancy."
  },
  {
    "keywords": [
      "Aplicação web",
      "Ontologia",
      "Relatório de informação",
      "Web semântica",
      "Web application",
      "Ontology",
      "Information report",
      "Semantic web",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "OntoReport: gerador de relatórios sobre ontologias",
    "autor": "Neves, Diogo Meira",
    "data": "2019-12-23",
    "abstract": "Uma ontologia pode ser definida como um modelo de dados, que representa uma descrição\nde conceitos num determinado domínio. Esta tem como principal objetivo, aumentar a compreensão partilhada num determinado domínio, eliminando as diferenças, sobreposições e\nincompatibilidades em conceitos, estruturas, entre outros, criando assim uma especificação\nformal legível por um computador, e explícita, no sentido em que as entidades da ontologia\nsão claramente definidas, distintas e inter-relacionadas entre si. \nTendo em conta o aumento exponencial de dados presentes na WEB, ontologias têm\nsido cada vez mais usadas como modelo de armazenamento de dados. Este aumento de\nusabilidade leva a que seja necessário o desenvolvimento de ferramentas que nos permitam criar/editar/analisar ontologias, ou seja, que nos permitam não só a interagir com elas, mas \ntambém realizar um tratamento dos dados consequentemente recolhidos. \nAssim, pretende-se nesta dissertação, desenvolver uma aplicação web capaz de gerar, \natravés de uma especificação, um relatório, referente a uma determinada ontologia acessível\natravés de um determinado endpoint. Por outras palavras, pretende-se a criação de uma \naplicação que permita ao utilizador obter, da ontologia, apenas os dados que pretende, em \nvez de uma quantidade enorme de dados sem qualquer tratamento prévio."
  },
  {
    "keywords": [
      "Inteligência artificial",
      "Machine learning",
      "Computer vision",
      "CNN",
      "Object detection",
      "Object tracking",
      "OpenCV",
      "Artificial Intelligence",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Framework para análise de comportamentos de objetos interativos em vídeo jogos",
    "autor": "Cruz, Catarina Freitas da",
    "data": "2022",
    "abstract": "Com um crescimento exponencial tanto na área da Inteligência Artificial como dos vídeo jogos, a\ncriação de plataformas que auxiliam os jogadores passou a ser fundamental. A criação de uma ferramenta\nanalítica que estude detalhadamente o comportamento humano, abre portas a jogos mais dinâmicos,\ncompetitivos e justos.\nA análise do ecrã de um jogador permite identificar, detetar e rastrear movimentos de determinados\nobjetos, em tempo real, podendo ter o intuito de o ajudar ou de o vigiar. Seja qual for o caso, é necessário,\nprimeiro, identificar e detetar os objetos visualizados, através de algoritmos de Object Detection. Depois,\njá identificado o objeto, é possível prever a sua próxima localização, bem como rastrear o seu movimento,\nutilizando algoritmos de Object Tracking.\nIntercalando o rastreamento com a deteção de objetos, quer quando este desaparece de vista, quer\npara obter confirmação que se está a seguir o objeto correto, é possível assim analisar o ecrã do jogador\npara o poder ajudar.\nEsta dissertação tem como objetivo desenvolver um modelo capaz de identificar o movimento de um\ndeterminado objeto, em tempo real, no ambiente de um jogo, utilizando para isso técnicas de Machine\nLearning e Computer Vision, mais especificamente métodos de Object Detection e Object Tracking.\nO ambiente prático foi desenvolvido utilizando a biblioteca OpenCV para Python, que tem ao dispor\num diverso leque de algoritmos de Computer Vision e ainda permite a utilização paralela de CPU e GPU\npara a otimização destes mesmos algoritmos."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Deeploy: a neural network computer vision tool (for the NVidia Tegra TX2 Embedded System)",
    "autor": "Fernandes, João Pedro Alves",
    "data": "2018-12-12",
    "abstract": "Machine Learning (ML) gives a computer system the ability to perform a certain task without being explicitly programmed to do it. Although ML is not a new topic in the field\nof computer science, these techniques have been gaining increasing popularity due to advances in hardware (especially GPUs). More powerful hardware supports more efficient\ntraining and a more responsive end-system, once deployed. These algorithms have proven\nto be particularly effective in image processing and feature detection, namely with deep\nneural networks.\nIn the context of a vehicle, autonomous or not, perceiving its external and internal environment enables the ability to detect and identify left behind objects, its misuse or other\npotentially dangerous situations. This captured data is relevant to trigger vehicle intelligent\nresponses. Bosch is currently developing a system that has these capabilities and plans to\nleverage deep learning approaches to implement it.\nThis work aimed to test and evaluate the suitability of a given embedded device for\nthe project. It also determined the best strategy to implement deep learning solutions in\nthe device. The supplied test bed was a NVidia Software Development Kit (SDK) system\nfor the embedded NVidia Jetson TX2 device with the System-on-Chip (SOC) Parker, an\nheterogeneous computing chip with 2 Denver-cores (a NVidia implementation of ARM-64\narchitecture), 4 CortexA57-cores (also ARM-64), 256 Pascal GPU-cores and support for up to\n6 video cameras. The SDK includes several software library packages, including for image\nprocessing and ML.\nWith the goal of fully exploiting the embedded device compute capabilities, this work\nstudied several inference frameworks, going as far as implementing an inference engine\nfrom scratch (named Deeploy) that produces inferences based on two libraries provided\nby NVidia: cuDNN and TensorRT. Deeploy was evaluated against well known and established frameworks, namely Tensorflow, PyTorch and Darknet, in terms of efficiency, resource\nmanagement and overall ease of use, maintainability and flexibility. This work also exploited key performance related features available on the device, such as power modes,\nhalf-precision floating point computation and the implemented shared memory architecture between the GPU-cores and the CPU-cores."
  },
  {
    "keywords": [
      "Data center",
      "Visualization",
      "Performance",
      "3D",
      "Flask",
      "Three.js",
      "Plugin",
      "Centro de dados",
      "Visualização",
      "Desempenho",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Data center visualization",
    "autor": "Capa, Pedro Miguel da Costa",
    "data": "2022-05-16",
    "abstract": "As more projects are financed by public institutions, demonstrating tasks and other information developed is increasingly required. Traditional monitoring tools like Prometheus are used in data centers to collect data from\nthe machines and supervise them as the devices can malfunction and make the service unavailable. Currently,\napplications like Prometheus that display the machines’ performance are limited to a restricted set of people: the\ndata center managers. These applications have limited data visualization methods since their focus is on retrieving the data from the machines. These applications are associated with specialized frameworks in showing the\nmachines’ performance. These frameworks present the data in several visualization methods, such as graphic\nlines and gauge graphics. However, the forms of exposing data are not attractive for people in general. So, other\nways to expose the data need to be developed.\nData visualization in three dimensions can expose data more attractively. Besides, 3D has some advantages\nover traditional ways. With a data friendlier exposition, catching the attention of people who do not manage the\ndata center is easier.\nThis project aims to build an application to expose the data center’s data in a 3D scenario. The data exposed\nare the machines’ tasks, components, and performance. By exposing the data center’s tasks and other information to the general public, the application can present to the viewer the usefulness of the data center. The\napplication must have its components flexible, so any data center can use it. Moreover, those data centers should\nexpose any visualization they desire through plugins.\nTo complete the goals, first, different techniques to explore and view the data are investigated. Several applications that expose data from a data center are analyzed to know the current status of these applications.\nFurthermore, different scenarios are constructed based on the research made. Using a tool capable of handling\nweb requests makes the application available to everyone. Besides, the application is flexible in some parts of the\narchitecture to be adaptable to any framework. So, any data center can use the application. Those parts are the\nserver that contains the machines’ performance data and the database management system. The system allows\nthe creation of a plugin to communicate with the machine’s performance server. Following a simple interface, a\nnew plugin can be developed with relative ease. Besides, the webserver is replicable, making it adaptable to the\ndata center’s needs. Moreover, the application allows the creation of arbitrary 3D scenarios. By following a set\nof steps a simple 3D scenario can be built, including the visualization and communication server stages. Such a\nscenario can be expanded freely, as long as the communication API is observed. The created scenario functions\nas a plugin that can be inserted into the application effortlessly. The application’s usefulness is validated through\nan experience with information from a real data center. Finally, the application’s performance is corroborated,\nsupporting a considerable amount of concurrent requests."
  },
  {
    "keywords": [
      "SAST",
      "Static Application Security Testing",
      "Type annotation",
      "C",
      "C++",
      "C#",
      "Unit testing",
      "Automation testing",
      "CxQL",
      "Teste de Segurança de Aplicativos Estáticos",
      "Anotação de tipos",
      "Teste unitário",
      "Teste de automação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Type annotation for SAST",
    "autor": "Pereira, Marco Avelino Teixeira",
    "data": "2023-12-05",
    "abstract": "This document describes the work done to understand the impact of implementing a type\nannotation algorithm plugged into the Static Application Security Testing (SAST) engine\nused by Checkmarx.\nThe main goal is to upgrade the Checkmarx’s SAST tool allowing the execution of\nvulnerability detection taking into account expression types. This means that every result of\nan expression needs to have a specific type assigned accordingly to the kind of operation\nand to the different operand types. Along with the assigned type, information on the size of\nthe variable and signedness should be available. As a case study, the C/C++ Programming\nLanguage was used. Then an extension to deal with expansion to the C# language was\ndeveloped and a study of the implications of expanding the system to multiple languages\nwas carried out.\nA proof of concept was implemented, using a language agnostic approach, that allows\nfor expressions type annotation previous to the analysis of code vulnerabilities. That tool\ndeveloped supports languages C, C++ and C# and the annotation of their base types. It\nwas possible to re implement some of the vulnerability analysis queries previously created\nand in use at Checkmax so that they use the implemented expression types. This led to a\nsignificant increase in readability of the queries, an increase in the amount of vulnerability\nresults found by those queries as well as a minimum efficiency increase."
  },
  {
    "keywords": [
      "RDMA",
      "Cache",
      "Analytical processing",
      "Columnar data",
      "Distributed systems",
      "Processamento analítico",
      "Dados colunares",
      "Sistemas distribuídos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "RDMA mechanisms for columnar data in analytical environments",
    "autor": "Silva, José Miguel Ribeiro da",
    "data": "2021-04-06",
    "abstract": "The amount of data in information systems is growing constantly and, as a consequence, the\ncomplexity of analytical processing is greater. There are several storage solutions to persist\nthis information, with different architectures targeting different use cases. For analytical\nprocessing, storage solutions with a column-oriented format are particularly relevant due\nto the convenient placement of the data in persistent storage and the closer mapping to\nin-memory processing.\nThe access to the database is typically remote and has overhead associated, mainly when\nit is necessary to obtain the same data multiple times. Thus, it is desirable to have a cache\non the processing side and there are solutions for this. The problem with the existing so lutions is the overhead introduced by network latency and memory-copy between logical\nlayers. Remote Direct Memory Access (RDMA) mechanisms have the potential to help min imize this overhead. Furthermore, this type of mechanism is indicated for large amounts of\ndata because zero-copy has more impact as the data volume increases. One of the problems\nassociated with RDMA mechanisms is the complexity of development. This complexity is\ninduced by its different development paradigm when compared to other network commu nication protocols, for example, TCP.\nAiming to improve the efficiency of analytical processing, this dissertation presents a dis tributed cache that takes advantage of RDMA mechanisms to improve analytical processing\nperformance. The cache abstracts the intricacies of RDMA mechanisms and is developed\nas a middleware making it transparent to take advantage of this technology. Moreover, this\ntechnique could be used in other contexts where a distributed cache makes sense, such as\na set of replicated web servers that access the same database."
  },
  {
    "keywords": [
      "COVID-19",
      "Pandemic",
      "Remote work",
      "Software engineering",
      "Survey",
      "Pandemia",
      "Teletrabalho",
      "Engenharia de software",
      "Inquérito",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Analysis of the impact of remote work on portuguese software professionals during the COVID-19 pandemic",
    "autor": "Almeida, Ana João Dias de",
    "data": "2022-07-19",
    "abstract": "COVID-19, a devastating virus that has been more and more controversial, fickle and problematic worldwide. This\npandemic brought multiple changes and restrictions to the way we live, rather like a historical buoyed period before\nand after Christ, translating, nowadays, into before and after COVID-19.\nThis pandemic forced the worldwide population to be subjected to some lockdown periods, and Portugal was not\nan exception, in which the population could only leave their home for exceptional and essential situations. From\nthose restrictions, a “new” way of work that has gained more and more popularity was born - the remote work\nor commonly known as work from home. Thus, it becomes pressing to investigate which are the impacts of this\nprofound change to remote work, in multiple domains (personal, professional,...).\nIn short, the primordial objective of this dissertation is to study the impact of the referred change to remote work,\ndue to the COVID-19 pandemic, on software professionals in Portugal.\nThroughout this dissertation, all the objectives and study questions, along with the relevant state of the art on the\ntheme are exposed. The construction and propagation of the chosen method (survey), the results arising from the\ncollection and treatment of data originated from this survey, along with their analysis, conclusions, limitations and\nadvantages are also highlighted.\nIn total, 176 valid answers were collected from software professionals from Portugal (mainly from Braga). After the\nperformed statistical analysis on the targeted population and focusing on the 10 elaborated research questions,\none can conclude with certainty two major findings: (i) having worked in a remote regimen before the pandemic\nperiod has a strong relation with a higher frequency use of teleconference tools (Microsoft Teams, Skype, Zoom,...)\nafter this period, and (ii) participants who do not feel safe about coming back to a fully on-site regimen are more\nlikely to prefer a fully remote regimen than the ones who feel safe and the latter group is more likely to prefer a\nhybrid regimen. Additionally, although not statistically significant and therefore without certainty, one can also imply\nthat, p.e, (i) having dependants and someone’s support in their care could possibly negatively affect participants’\nwork; (ii) having dependants could possibly show a relation to a preference for a mainly on-site hybrid regimen, and\n(iii) company employee dimension could show a relation to participants’ feel of support to maintain productivity.\nIn conclusion, this project has a strong pioneer investigation profile in Portugal, with an applicability in the software\nengineering area."
  },
  {
    "keywords": [
      "Machine learning",
      "Mobilidade elétrica",
      "Problemas de previsão",
      "Séries temporais",
      "Electric mobility",
      "Forecasting problems",
      "Time series",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Inteligência artificial aplicada à infraestrutura de carregamentos para veículos elétricos",
    "autor": "Silva, Rodolfo António Vieira da",
    "data": "2023-06-14",
    "abstract": "A capacidade de resolução de problemas pela Inteligência Artificial encontra-se em constante ex pansão, pelo que a otimização do seu funcionamento em sociedade está dependente da capacidade de\nextração das aplicabilidades da tecnologia. Uma das áreas da Inteligência Artificial em que tem sido visível\numa evolução significativa é a de Machine Learning, cujos algoritmos têm revelado um crescente nível de\nespecialização na resolução de diversos problemas.\nNa presente dissertação, pretendeu-se construir um modelo capaz de auxiliar na recomendação de\nconfigurações ideais para novas estações de carga para veículos elétricos, com a assistência de modelos\nde Machine Learning. A revisão da literatura revelou uma extensiva análise sobre problemas de previsão na\nárea de Machine Learning, pelo que algoritmos tradicionais de Machine Learning e algoritmos da subárea\nde Deep Learning se demonstram adequados para a resolução do problema proposto nesta dissertação.\nO dataset empregue neste projeto, com dados referentes a Portugal, foi construído com a assistência de\ndiversas API e, posteriormente ao seu tratamento, foram aplicados seis algoritmos de Machine Learning,\ncom o intuito de treinar um modelo que conseguisse prever a utilização futura de postos de carga.\nDe entre os algoritmos avaliados, o Random Forest Regressor e eXtreme Gradient Boosting são aque les que apresentam maior capacidade na resolução do problema em questão, com um MAE 6.6220 e\n6.6310, respetivamente. O modelo de Random Forest Regressor é aquele que melhor se adequou para\na previsão de utilização futura dos postos de carga, tendo sido utilizado para a construção do modelo de\nrecomendação."
  },
  {
    "keywords": [
      "Sustentabilidade",
      "Sensores",
      "Fusão de dados",
      "Fusão de informação",
      "Ambiente inteligente",
      "Inteligência ambiente",
      "Sustainability",
      "Sensors",
      "Data fusion",
      "Information fusion",
      "Ambient intelligence",
      "681.3:504",
      "504:681.3",
      "681.586",
      "621.39"
    ],
    "titulo": "Sensorização, fusão sensorial e dispositivos móveis : contribuições para a sustentabilidade de ambientes inteligentes",
    "autor": "Rosa, Luís Miguel Ferreira",
    "data": "2013",
    "abstract": "A sustentabilidade está dependente das decisões que o ser humano toma no ambiente em que se envolve. Por outro lado, é necessário ter consciência sobre o impacto das suas ações no meio ambiente. O crescimento da tecnologia e da área científica de sistemas inteligentes tem sido cada vez maior, tornando-se parceiras do ser humano, e o seu potencial para Ambientes Inteligentes e sustentabilidade tem sido evidenciado nos últimos tempos. Para enriquecer a resposta do Ambiente Inteligente aos seus utilizadores poder-se-á recorrer a sensores dispostos no ambiente e à fusão sensorial e de informação. As recomendações e previsões produzidas têm como objetivo a avaliação e o estudo da sustentabilidade do ambiente, nomeadamente, da utilização equilibrada da energia.\nDentro dos contextos de sensibilização, de prevenção das ações do utilizador e da sustentabilidade do ambiente em que o ser humano esteja inserido, existem outros objetivos a alcançar, nomeadamente, a fusão de informação como ferramenta na utilização em suporte tecnológicos. A necessidade de aliar este processamento de dados/informação ao desenvolvimento de um conjunto de plataformas que permitam ao utilizador perceber efeitos negativos ou positivos que as condições analisadas têm, passando por possíveis recomendações ao utilizador. Esta plataforma deliberativa e reativa apoiará, processos e práticas de consciencialização para a sustentabilidade, por forma a conseguir mudanças nos padrões de estilo de vida, de produção e consumo de energia.\nO presente trabalho incide sobre a integração das Tecnologias da Informação e Comunicação (TIC) no meio envolvente, com estratégias de sustentabilidade dentro da dimensão social, ambiental e económica. As TIC associadas a conceitos de inteligência ambiente e elementos físicos, como por exemplo, edifícios, permitem obter formas de melhorar aspectos como o consumo energético e o impacto ambiental, na medida em que podem gerir de forma eficiente o consumo de recursos e contribuir para a redução de desperdícios. Um exemplo de aplicação pode ser encontrado na plataforma de Agentes Inteligentes, denominada por PHESS, que com ligação a sensores que permite centralizar a recolha de dados e obter decisões através de processos deliberativos automaticamente. Esta dissertação foca-se na extensão da plataforma PHESS com processos de fusão sensorial e de informação e, ainda, na criação e da monitorização dos novos indicadores que permitem promover a sustentabilidade social, económica e ambiental proporcionando aos utilizadores novas possibilidades de acesso a serviços e de participação na comunidade. A utilização de sistemas inteligentes, auxiliam na ação sobre o meio e são, por isso, uma mais-valia para o conforto das pessoas e para a sustentabilidade do ambiente."
  },
  {
    "keywords": [
      "HTML5",
      "Frameworks JavaScript MVC",
      "681.3.062",
      "681.324",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Estudo e Implementação de Interfaces Web em HTML5",
    "autor": "Rodrigues, Samuel da Costa",
    "data": "2014-03-17",
    "abstract": "A web está em constante evolução. A evolução tecnológica fez com que as aplicações web estivessem cada vez mais presentes no mercado e surgissem novos padrões arquiteturais, novos dispositivos e novas experiências de utilização das aplicações. Tudo isto com o propósito de satisfazer as exigências que o mercado e os utilizadores finais impõem.\nToda esta evolução potenciou o aparecimento de uma nova versão do HTML, o HTML5, a qual está em constante progresso. Não se pode esperar que o HTML5 esteja totalmente concluído para implementar interfaces com esta tecnologia, podendo assim ser considerado um living standard, porque já existem funcionalidades suficientemente maduras e que inclusivamente já são utilizadas em produtos no mercado. Devido ao facto do HTML ser uma linguagem de marcação, torna-se indispensável associar o HTML ao JavaScript, de modo a poder disponibilizar dinamismo e funcionalidade às páginas web. Existiu por isso, tal como o HTML, uma grande evolução relativa à linguagem, mais propriamente às frameworks JavaScript de desenvolvimento web existentes.\nDevido ao progresso destas tecnologias, esta dissertação pretende analisar as várias funcionalidades disponibilizadas pela tecnologia HTML5 e as frameworks JavaScript existentes no mercado. Para tal, estas tecnologias serão usadas, de forma experimental, num projeto de desenvolvimento de software na empresa onde decorre este trabalho. Os resultados focam-se em identificar as funcionalidades HTML5 implementadas e apresentar uma comparação entre as frameworks JavaScript em estudo, segundo um conjunto específico de critérios. É do interesse da empresa onde este trabalho foi realizado aplicar as funcionalidades HTML5 e frameworks JavaScript, de modo a identificar as vantagens e desvantagens de cada tecnologia para, num futuro próximo, as aplicar em aplicações web.\nO objetivo deste trabalho é assim medir quantitativa e qualitativamente, segundo os critérios considerados para análise, o impacto da introdução do HTML5 e das frameworks JavaScript em produtos de software, caso estas substituam as que estão hoje em dia em utilização."
  },
  {
    "keywords": [
      "CLAV",
      "LC",
      "TS",
      "RADA",
      "PGD",
      "AE",
      "PCA",
      "DF",
      "Workflow e API de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "CLAV: autos de eliminação",
    "autor": "Teixeira,  Alexandre Nunes da Costa Morango",
    "data": "2020-11-13",
    "abstract": "Na Administração Pública portuguesa, existe uma preocupação recorrente em relação à in formação gerada e recebida pelas diferentes entidades produtoras, sendo o uso de papel um\nfator significativo na perda de informação.\nA CLAV é um projeto nacional, financiado pelo Simplex, que visa a classificação e avalia ção de todos os documentos da administração pública portuguesa, com base em normas e\norientações delineadas pela Direção Geral do Livro, dos Arquivos e das Bibliotecas.\nA aplicação e o modelo de desenvolvimento já suportavam a criação e a manutenção dos\ninstrumentos de classificação e avaliação, designados por Lista Consolidada e Tabelas de\nSeleção, onde já estavam definidos padrões de controlo, gestão e acesso à informação ou\ndocumentos arquivísticos.\nEsta dissertação implementou a ação de avaliação, especificamente, a gestão e controlo dos\nprocedimentos de eliminação. Assim, foi necessário definir um modelo para as instruções de\neliminação juntamente com todos os seus requisitos e invariantes. Na plataforma CLAV foi\ncrucial adicionar as interfaces que implementam os métodos e funcionalidades necessárias\npara criar ou importar as instruções de Eliminação e todas as funcionalidades relacionadas\nao seu processamento, nomeadamente, o registo da meta-informação num banco de dados\nestático, análise e consulta das declarações de eliminação, entre outros."
  },
  {
    "keywords": [
      "Whole shotgun metagenomic",
      "Next-generation sequencing",
      "Effluent-treatment plant",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "A metagenomic approach to identify and characterize wastewater populations",
    "autor": "Martins, Sara Patrícia Monteiro",
    "data": "2017",
    "abstract": "Water scarcity and pollution are two main ecological focus nowadays. Knowledge of wastewater composition, regarding microorganisms and pollutants, is of great importance to improve the capacities of the effluent treatment plants (ETP). Advances in Next-generation sequencing (NGS) methodologies allowed for faster, cheaper and more accurate study of microbial communities. Besides being an extremely powerful analysis resource, whole shotgun metagenomic analysis comprises many challenging aspects, regarding the processing and analysis.\nIn the present work a shotgun metagenomic bioinformatics analysis was performed comprising three samples from common ETPs (CETP) and four samples from a petrochemical complex ETPs (wastewaters with low and high salts collected in two distinct timepoints). The samples were sequenced with Illumina® HiSeq, generating paired-end reads with 2x150bp length. The main goals of this project were to evaluate currently available tools, establish a customized bioinformatics pipeline and to extract relevant biological information from the sequenced datasets.\nThere were generated simulated datasets representative of the target data, in order to evaluate the performance of the available bioinformatics tools. Datasets were generated with three coverage levels and were used to test pre-processing, assembly and taxonomic tools. The target datasets, both with and without coverage split, were then subjected to processing and analysis using the pre-defined pipeline. A preliminary functional study was also performed using MG-RAST and MGX.\nResults from the evaluation of the performance of the bioinformatics tools showed that different tools behave differently in distinct datasets. The pipeline was defined using BayesHammer and Fastq-mcf as pre-processing tools, SPAdes for assembly and MetaPhlAn v2.0 for the taxonomical analysis.\nThe assembly results for the target datasets showed a higher contiguity for high coverage levels and a lower contiguity for low coverage levels, highlighting the differences in microorganisms’ abundance and diversity and its impact during analysis.\nTaxonomical composition suggests the presence of putative pathogenic and opportunistic microorganisms on two of the CETP datasets (A2 and AKR12). It also suggests a more hostile environment in petrochemical complex ETPs datasets, which is concordant with a higher abundance of defence mechanisms on this datasets.\nThe present results must be accounted to the effluent treatment processes."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Tutor inteligente para primeiro e segundo ciclos",
    "autor": "Oliveira, Jorge Rafael Chaves",
    "data": "2017",
    "abstract": "Atualmente, existe um forte crescimento das aplicações destinadas à aprendizagem de\nconteúdos no âmbito escolar.\nA proposta de dissertação, no âmbito do Mestrado Integrado em Engenharia Informática,\npresente neste documento, foca-se na criação e implementação de um tutor inteligente\nde modo a criar recursos para crianças do primeiro e segundo ciclos com o objetivo de\nmonitorizar o progresso na aprendizagem desses mesmos conteúdos.\nO desenvolvimento desta aplicação visa ajudar os tutores e/ou encarregados de educação\na planear o estudo dos seus tutorados, de acordo com as dificuldades ou necessidades\napresentadas.\nO sistema desenvolvido acompanha não só o tutorado, mas também o seu tutor visto\nque são dadas sugestões e feitas análises de modo a agilizar o processo de criação de fichas,\ncom o objetivo de os tutorados superarem as suas dificuldades, enquanto fazem uso das\ntecnologias.\nEsta aplicação não só monitoriza a aprendizagem dos tutorados, como também tem presente\numa componente para introdução de novo conteúdo escolar por parte dos tutores\ne/ou encarregados de educação, com o objetivo de armazenar grandes quantidades de\ndados, de modo a diminuir a ocorrência de perguntas semelhantes.\nRelativamente ao sistema de monitorização, este apresenta dados estatísticos referentes\nàs áreas do programa escolar em que o tutorado apresenta mais dificuldades, bem como as\náreas em que este está mais à vontade, de modo a poder dinamizar o conteúdo das fichas\npara os seus intervenientes."
  },
  {
    "keywords": [
      "Root cause analysis",
      "Fault correlation",
      "Machine Learning",
      "Artificial Intelligence",
      "Network topologies",
      "Rules automation",
      "Alarm manager",
      "Correlação entre falhas",
      "Aprendizagem máquina",
      "Inteligência Artificial",
      "Topologias de rede",
      "Automatização de regras",
      "Gestão de alarmes"
    ],
    "titulo": "Machine Learning applied to fault correlation",
    "autor": "Lima, Pedro Jorge Rito",
    "data": "2021-12-21",
    "abstract": "Over the last years, one of the areas that have most evolved and extended its application to a multitude of possi bilities is Artificial Intelligence (AI). With the increasing complexity of the problems to be solved, human resolution\nbecomes impossible, as the amount of information and patterns that can be detected is limited, while AI thrives\non the dimension of the problem under analysis. Furthermore, as nowadays more and more traditional devices\nare computerized, an increasing number of elements are producing data that has many potential applications.\nConsequently, we find ourselves at the height of Big Data, where huge volumes of data are generated, being\nentirely unfeasible to process and analyze them manually.\nAdditionally, with the increasing complexity of network topologies, it is necessary to ensure the correct func tioning of all equipment, avoiding cascade failures among devices, which can lead to catastrophic consequences\ndepending on their use. Thus, Root Cause Analysis (RCA) tools become fundamental since these are developed\nto automatically, through rules established by its users, realize the underlying causes when some equipment mal functions. However, with the growing network complexity, the definition of rules becomes exponentially more\ncomplicated as the possible points of failure scale drastically.\nIn this context, framed by the Altice Labs RCA and network environment use case, the main objective of this\nresearch project is defined. The aim is to use Machine Learning (ML) techniques to extrapolate the relationship\nbetween different types of equipment alarms, gathered by the Alarm Manager tool, to have a better understanding\nof the impact of a failure on the entire system, thus easing and helping the process of manual implementation of\nRCA rules. As this tool manages millions of daily alarms, it becomes unfeasible to process them manually, making\nthe application of ML essential. Furthermore, ML algorithms have tremendous capabilities to detect patterns that\nhumans could not, ideally exposing which specific failure causes a series of malfunctions, thus allowing system\nadministrators to only focus their attention on the source problem instead of the multiple consequences.\nThe ML approach proposed in this project is based on the causality among alarms, instead of their features,\nand uses the cartesian product of a specific problem, the involved technology, and the manufacturer, to extrap olate the correlations among faults. The results achieved reveal the tremendous potential of this approach and\nopen the road to automatizing the definition of RCA rules, which represents a new vision on how to manage\nnetwork failures efficiently."
  },
  {
    "keywords": [
      "Gestão de projetos",
      "Método",
      "Metodologia",
      "Desenvolvimento de software",
      "Ágil",
      "PMBoK",
      "Scrum",
      "Software project management",
      "Method",
      "Methodology",
      "Software development",
      "Agile",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Como compatibilizar paradigmas estruturados da gestão de projetos com metodologias ágeis",
    "autor": "Rocha, Patrícia Sofia Duarte",
    "data": "2017",
    "abstract": "As organizações dispõem atualmente de standards e de guias de boas práticas de gestão de projetos bem estruturados e com ampla adoção. Tal é o caso, por exemplo, da ISO 21500 e do PMBoK 5. Não obstante, no contexto dos projetos de tecnologias e sistemas de informação realizados com recurso a metodologias ágeis, surgem novos desafios relacionados com a compatibilização das práticas ágeis com os paradigmas estruturados da gestão de projetos. A presente dissertação está focada em procurar respostas para a questão de investigação “Como compatibilizar paradigmas estruturados da gestão de projetos com metodologias ágeis”, através de uma abordagem metodológica baseada na Design Science Research (DSR). Como resultado do trabalho realizado, apresenta-se um novo método de gestão de projetos desenvolvido com base no PMBoK e no Scrum. Este método faz a conciliação das metodologias de gestão de projetos estruturadas com metodologias ágeis."
  },
  {
    "keywords": [
      "Forest fires",
      "Fire propagation modeling",
      "Planning of fire prevention and firefighting resources",
      "Optimization",
      "Incêndios florestais",
      "Modelação da propagação do fogo",
      "Gestão e planeamento dos recursos de prevenção e combate a incêndios",
      "Otimização",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Optimization under uncertainty for forest  fire containment",
    "autor": "Neto, David António Vieira dos Santos Moura",
    "data": "2023-10-09",
    "abstract": "Forest fires are a major problem that affects the entire world, causing tragic loss of life and \nserious injuries, which have been worsening due to global warming, making it essential to \nminimize the serious consequences of these phenomena. In this sense, this project addresses the \nproblem of positioning resources to combat forest fires. As uncertainty is an important aspect \nin fire propagation modeling, stochastic approaches are used, such as the Equivalent \nDeterministic Model and the Sample Average Approximation. The purpose of these approaches \nis to determine the best locations to deploy a limited number of combat assets, for example fire \ncrews. Another important point is to study how fire spreads in a forest given the region's \ntopography, wind and other factors to incorporate fire propagation modeling with the \nmanagement and planning of fire prevention and firefighting resources (optimization). \nAlthough there are several fire propagation simulation software, their integration with \noptimization problems is still very limited. In this work, this integration is achieved through the\nminimum travel time (MTT) principle that, when representing the forest by a network in which \nthe transmission times between adjacent homogeneous forest zones are known, states the fire \ntakes the quickest paths. This principle is used in mixed integer programming models to \noptimize the positioning of the available resources, both in a deterministic and in a stochastic \nsetting. Computational experiments are conducted to validate the approach."
  },
  {
    "keywords": [
      "Brain",
      "Deep learning",
      "MRI",
      "Neuroimaging",
      "Spatial normalization",
      "Cérebro",
      "Neuroimagem",
      "Normalização espacial",
      "Redes neurais artificiais",
      "Ressonância magnética",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Spatial normalization and analysis of brain MRI studies – a deep neural network based approach",
    "autor": "Jesus, Tiago Rafael Andrade",
    "data": "2019-11-19",
    "abstract": "Throughout the years, Deep Learning has proven to be an excellent technology to solve \nproblems that would otherwise be too complex. Furthermore, it has shown great success in the area \nof medical imaging, especially when applied to segmentation of brain tissues. As such, this \ndissertation explores a possible new approach, using Deep Artificial Neural Networks to perform \nspatial normalization on brain MRI studies as well as classify using Brain MRI studies regarding their \nstate of brain atrophy. \nSpatial normalization of Magnetic Resonance images by tools like the FSL, or SPM turned \nout to be inefficient for researches as they need too many resources to achieve good results. These\nresources include, for example, wasted human and computer time when executing the commands \nto normalize and waiting for the process to finish, this can take up to several hours just for one study. \nTherefore, a new approach was needed, a faster and easier way to normalize the MRI studies. To do \nso, Deep Artificial Neural Networks were used by creating a python program to deal with said studies\nin much less time. This program should free the researchers’ time for other more relevant tasks and \nhelp reach conclusions faster in their studies when trying to find patterns between the analysed \nbrains. Several architectures were tried, having better results with U-Net based architecture as well \nas GAN architecture.\nAt the end, the model couldn’t learn correctly all the brain features to be changed in any of \nthe approaches but showed great potential. Even though the final model did achieve the correct \nshape it could not yet achieve the final normalization.\nWith some more time invested in perfecting the models, these could, in the future, learn to \ncorrectly perform the final normalization and allow the researchers to perform it in less than 10 \nseconds per exam instead of hours.\nRegarding the Brain Atrophy models, the models showed some potential too as the \npredictions were partially correct. With more data, and less unbalanced, the model could probably \nlearn correctly and output the expected results for all classes."
  },
  {
    "keywords": [
      "Impulso glótico",
      "Estimação do impulso glótico",
      "Filtragem inversa",
      "Integração no domínio das frequências",
      "Estimação do impulso glótico no domínio das frequências",
      "Glottal pulse",
      "Estimation of the glottal pulse",
      "Filter",
      "Algorithm",
      "Frequency domain glottal source estimation",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Estimation of the glottal flow from the speech or singing voice",
    "autor": "Beleza, Hugo Miguel Ferreira",
    "data": "2016-03-03",
    "abstract": "O processo de produção humana de voz é, resumidamente, o resultado da convolução entre o sinal de\nexcitação, o impulso glótico, e a resposta impulsiva resultante da função de transferência do trato\nvocal. Este modelo de produção de voz é frequentemente referido na literatura como um modelo fontefiltro,\nem que a fonte representa o fluxo de ar que sai dos pulmões e passa pela glote (espaço entre as\npregas vocais), e o filtro retrata as ressonâncias do trato vocal e a radiação labial/nasal.\nEstimar a forma do impulso glótico a partir do sinal de voz é de importância significativa em diversas\náreas e aplicações, uma vez que as características de voz relacionadas, por exemplo, com a qualidade\nda voz, esforço vocal e distúrbios da voz, devem-se, principalmente, ao fluxo glotal. No entanto, este\nfluxo é um sinal difícil de determinar de forma direta e não invasiva.\nAo longo das últimas décadas foram desenvolvidos vários métodos para estimar o impulso glótico mas\nsem o desenvolvimento de um algoritmo eficiente e automático. A maioria dos métodos desenvolvidos\nbaseia-se num processo designado por filtragem inversa. A filtragem inversa representa a\ndesconvolução, ou seja, procura obter o sinal de entrada aplicando o inverso da função de\ntransferência do trato vocal ao sinal de saída. Apesar da simplicidade do conceito, o processo de\nfiltragem inversa não é simples uma vez que o sinal de saída pode incluir ruído e não é alcançável\nmodelar com precisão as características do filtro do trato vocal.\nNesta dissertação apresentamos um novo método de filtragem de um sinal de modo a melhorar um\nmétodo robusto de estimação da fonte glótica, no domínio das frequências, que usa uma característica\nde fase baseada nos Atrasos Relativos Normalizados (NRD) dos harmónicos. Este modelo é aplicado a\ndiversos sinais de voz (sintéticos e reais), e os resultados obtidos da estimação do impulso glótico são\ncomparados com os obtidos usando outros métodos analisados no estado da arte com e sem o\nreferido método de filtragem."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Proteção da privacidade em sistemas de dados",
    "autor": "Pereira, Ricardo Manuel Amaro",
    "data": "2015-03-02",
    "abstract": "Information is nowadays an extremely valuable asset and takes on certain occasions a central role in many organizations. This information however may ﬁnd itself legally protected (e.g. medical records) or contain data of real people as individual preferences or transactions whose privacy people do not want to see broken. On the other hand, there are sometimes obvious applications for this information as for example to generate statistics for medical research or for simply as a way to provide a better service. There is thus a conﬂict of interest between those who hold information and intend to value it somehow and those who do not want to see their privacy compromised. Two promising technologies arise in this context: techniques based on the notion of differential privacy and data anonymization algorithms. The goal of this dissertation is to explore the interaction between these two technologies and the possibility to utilize them together."
  },
  {
    "keywords": [
      "Start-up",
      "Braga",
      "Determinante",
      "Modelação da empresa",
      "Desenvolvimento do software",
      "Estabelecimento do mercado",
      "Internacionalização",
      "Metodologia lean startup",
      "Aprendizagem",
      "Determinant",
      "Shaping the company",
      "Developing the software",
      "Establishing the market",
      "Going international",
      "Lean startup methodology",
      "Learning",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Avaliação dos determinantes internos e externos no sucesso de start-ups de software: o caso do cluster de Braga",
    "autor": "Cruz, Isabel Maria Ferreira",
    "data": "2017",
    "abstract": "Nos últimos anos tem-se verificado um crescimento tecnológico, empreendedor e de inovação no cluster\nde Braga. Dadas estas circunstâncias, tem-se verificado um aumento na oferta e procura de produtos\ne serviços de software, o que incentiva o surgimento de novas start-ups. Contudo, devido à elevada\ncompetitividade do mercado tecnológico e à atual crise económica mundial, nem todas estas empresas\nsobrevivem nos seus primeiros anos de vida.\nNeste projeto de investigação procurou-se compreender como as start-ups entram no mercado e o\nque distingue as que vingam no mercado das restantes, através da avaliação dos seus determinantes\ninternos e externos. Deste modo, será possível contribuir para a compreensão das condições em que\nos empreendedores devem construir as suas start-ups, aumentando as possibilidades de sucesso na\nconstrução de produtos e serviços de software com viabilidade de mercado.\nNesta dissertação foi explorado o Early-life Decision Model, um modelo composto por diversos tipos\nde decisão que devem ser tomados pelos empreendedores para a sustentabilidade do negócio. Através\ndo modelo, procurou-se analisar e avaliar os determinantes internos e externos de diversas vertentes do\nnegócio das start-ups. A metodologia de investigação adotada consistiu na entrevista semi-estruturada,\npor apresentar características mais apropriadas para a finalidade deste estudo.\nForam preparadas e realizadas entrevistas em 15 empresas do cluster de Braga, foi efetuada uma análise\ne tratamento dos dados recolhidos e, posteriormente, foi feita uma análise dos resultados. Obtiveram-se\ndiversas conclusões como o impacto de cada determinante nos grupos de empresas identificados, bem\ncomo a quantidade de determinantes internos e externos identificados nas diversas vertentes do negócio.\nTambém foi possível verificar que a existência de determinantes que dificultam a entrada das empresas\nno mercado, influencia o rumo do negócio, e que a aprendizagem adquirida deles e da experiência vivida\ndurante a atividade da empresa, é essencial para a sustentabilidade do negócio."
  },
  {
    "keywords": [
      "Hierarchical temporal memory",
      "Machine intelligence",
      "Neocortex",
      "Hebbian learning",
      "Image classification",
      "Sound recognition",
      "Time series forecasting",
      "Artificial Intelligence",
      "Aprendizagem Hebbiana",
      "Classificação de imagem",
      "Reconhecimento de som",
      "Previsão de séries temporais",
      "Inteligência Artificial",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "HTM approach to image classification, sound recognition and time series forecasting",
    "autor": "Lima, Tiago Azevedo",
    "data": "2021-04-08",
    "abstract": "The introduction of Machine Learning (ML) on the orbit of the resolution of problems\ntypically associated within the human behaviour has brought great expectations to\nthe future. In fact, the possible development of machines capable of learning, in a\nsimilar way as of the humans, could bring grand perspectives to diverse areas like\nhealthcare, the banking sector, retail, and any other area in which we could avoid the\nconstant attention of a person dedicated to the solving of a problem; furthermore, there\nare those problems that are still not at the hands of humans to solve - these are now\nat the disposal of intelligent machines, bringing new possibilities to the humankind\ndevelopment.\nML algorithms, specifically Deep Learning (DL) methods, lack a bigger acceptance by\npart of the community, even though they are present in various systems in our daily\nbasis. This lack of confidence, mandatory to let systems make big, important decisions\nwith great impact in the everyday life is due to the difficulty on understanding the\nlearning mechanisms and previsions that result by the same - some algorithms represent\nthemselves as ”black boxes”, translating an input into an output, while not being totally\ntransparent to the outside. Another complication rises, when it is taken into account\nthat the same algorithms are trained to a specific task and in accordance to the training\ncases found on their development, being more susceptible to error in a real environment\n- one can argue that they do not constitute a true Artificial Intelligence (AI).\nFollowing this line of thought, this dissertation aims at studying a new theory,\nHierarchical Temporal Memory (HTM), that can be placed in the area of Machine\nIntelligence (MI), an area that studies the capacity of how the software systems can\nlearn, in an identical way to the learning of a human being. The HTM is still a fresh\ntheory, that lays on the present perception of the functioning of the human neocortex\nand assumes itself as under constant development; at the moment, the theory dictates\nthat the neocortex zones are organized in an hierarchical structure, being a memory\nsystem, capable of recognizing spatial and temporal patterns. In the course of this\nproject, an analysis was made to the functioning of the theory and its applicability\nto the various tasks typically solved with ML algorithms, like image classification, sound recognition and time series forecasting. At the end of this dissertation, after the\nevaluation of the different results obtained in various approaches, it was possible to\nconclude that even though these results were positive, the theory still needs to mature,\nnot only in its theoretical basis but also in the development of libraries and frameworks\nof software, to capture the attention of the AI community."
  },
  {
    "keywords": [
      "Koha",
      "Biblioteca",
      "Sistema de recomendação",
      "Library",
      "Recommendation systems",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistema de recomendação para bibliotecas",
    "autor": "Silva, Pedro Duarte Cunha e",
    "data": "2023-12-15",
    "abstract": "O Koha é um software com interface web para gestão de bibliotecas. Este software é utilizado em várias\nbibliotecas, incluindo as bibliotecas de várias universidades como a Universidade do Minho, e entidades\ngovernamentais como o Catálogo Coletivo das Bibliotecas nos Açores. Nesta dissertação pretende-se adi cionar um sistema de recomendação, onde após se fazer uma pesquisa e selecionar uma obra, mostrar\numa lista de recomendações similares de acordo com as várias informações da obra, podendo este sis tema eventualmente ser separado do Koha e complementar outros sistemas de software de gestão de\nbibliotecas. Para a criação deste sistema de recomendação foram estudados vários exemplos de outros\nsistemas de recomendação já existentes. Adicionalmente, também foram estudados os requisitos reque ridos por estes e por este projeto em particular. Estes estudos ajudaram a obter o melhor funcionamento\npossível do sistema de recomendação."
  },
  {
    "keywords": [
      "Gestão do produto de software",
      "Matriz de maturidade",
      "Análise e avaliação",
      "Área de foco",
      "Área de negócio",
      "Software product management",
      "Maturity matrix",
      "Analysis and evaluation",
      "Focus area",
      "Business functions",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Gestão do produto de software na região de Braga: análise e avaliação",
    "autor": "Carvalho, Frederico António de Sousa",
    "data": "2016",
    "abstract": "A Gestão do Produto de Software é um dos principais mecanismos a usar durante o desenvolvimento e a exploração de um produto de software. A forma como o produto é gerido durante o seu ciclo de vida, toma em conta várias áreas de negócio, que, infelizmente, nem sempre fazem parte da experiência do gestor de produto, devido também ao facto de ainda existir pouca formação para o exercício deste cargo. Como forma de avaliar as práticas de Gestão do Produto de Software na região de Braga, é apresentada a Matriz de Maturidade para a Gestão do Produto de Software, como instrumento de avaliação das práticas implementadas pelas empresas. Uma análise individual e coletiva das empresas é realizada, com a finalidade de analisar a maturidade das empresas em cada uma das áreas de foco da Matriz. Para tal, foram realizadas 13 entrevistas junto de colaboradores de empresas do ramo de software, que permitiram recolher dados essenciais na construção das matrizes de maturidade de cada uma destas empresas. A partir dos resultados obtidos destas matrizes, uma análise mais geral foi realizada, identificando pontos comuns de falha, bem como de áreas de foco nas quais as empresas direcionam mais os seus esforços. Foi possível concluir que apesar de existir uma grande diversidade de resultados dentro de cada uma das Áreas de Foco, bem como o facto de que por vezes o nível de maturidade atingido, não está diretamente relacionado com o número de competências implementadas em cada uma desta Áreas. Também se constatou que o uso de ferramentas e metodologias adotadas por estas empresas, facilitam a Gestão do Produto, o que se traduz no aumento do nível de maturidade para as empresas em determinadas áreas de negócio."
  },
  {
    "keywords": [
      "sistemas de monitorização",
      "indicadores",
      "otimização",
      "monitoring system",
      "indicators",
      "optimization",
      "dashboards"
    ],
    "titulo": "Monitorização e otimização da eficiência num laboratório de análises clínicas",
    "autor": "Teixeira, Bruno Filipe Martins",
    "data": "2017-11-20",
    "abstract": "O presente trabalho foi efetuado para obtenção do grau de Mestre em Engenharia de Redes e Serviços Telemáticos na Universidade do Minho e teve como primordial objetivo proceder ao desenvolvimento de um sistema de monitorização e otimização da eficiência num laboratório de análises clínicas.\nPara isso, começou-se pelo estudo do estado da arte com intuito de tomar conhecimento das áreas em que se vai trabalhar bem como saber se já existem soluções semelhantes existentes. Neste último ponto, não se encontrou nenhuma solução existente.\nPor consequência, foi necessário o desenvolvimento de um dispositivo autónomo, que, combinando hardware e software recolha a informação enviada pelos equipamentos da Siemens instalados no laboratório.\nParalelamente houve a necessidade de implementar um sistema que receba e armazene em base de dados estruturada a informação enviada pelo dispositivo desenvolvido.\nPor fim, procedeu-se ao desenvolvimento de um dashboard para apresentação do resultado do tratamento dos dados armazenados (informação de business intelligence), relevante para a gestão e otimização do processo analítico."
  },
  {
    "keywords": [
      "Ubíquo",
      "APEX",
      "OpenSimulator",
      "3D",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Modelação de ambientes ubíquos na plataforma APEX",
    "autor": "Abade, Tiago",
    "data": "2014-12-15",
    "abstract": "O teste e o desenvolvimento de ambientes ubíquos podem tornar-se uma tarefa bastante dispendiosa. A necessidade de adquirir ou de construir espaços físicos, assim como de todos os dispositivos que o compõem, como sensores e similares, levanta questões de cariz económico a ter em consideração. Em complemento, convém ter em consideração que nem todos os obstáculos são meramente monetários. Se ponderarmos uma fase mais avançada de um projeto de computação ubíqua, onde poder a ser necessária a recolha e analise de informação de equipamento presente em espaços delicados ou críticos, como o caso de hospitais ou aeroportos, verificamos que quer a nível politico, quer a nível de segurança publica, estamos deveras limitados. Deste modo, o desenvolvimento de protótipos de ambientes ubíquos utilizando mundos virtuais surge como uma potencial solução. Neste âmbito, este trabalho descreve a modelação de dois ambientes virtuais, uma Biblioteca com informação de lugares disponíveis e um Lar Residencial inteligente com componentes de prevenção e segurança implementados. A ferramenta utilizada neste trabalho, que permite esse desenvolvimento, e a APEX [14], esta plataforma permite modelar um ambiente a imagem do espaço físico pretendido, com o acréscimo de o comportamento dos objetos poder ser modelado. Assim torna-se possível replicar e explorar variados tipos de instalações tecnológicas de forma segura, bem como permitir a recolha generalizada da informação produzida pelos equipamentos. Os protótipos desenvolvidos neste trabalho permitiram avaliar uma solução para uma biblioteca universitária e recolher a opinião dos residentes de um lar, possibilitando desta forma, refinar os requisitos dos sistemas."
  },
  {
    "keywords": [
      "Chatbot",
      "Information processing",
      "Natural language processing",
      "COVID-19",
      "Processamento da informação",
      "Processamento de linguagem natural",
      "web crawling"
    ],
    "titulo": "Avoiding question-answering congestion on health services using chatbots",
    "autor": "Pereira, Henrique Manuel Palmeira",
    "data": "2022-05-18",
    "abstract": "The proliferation of social networks presents a significant amount of fake news and fake information every day and every second. The COVID-19 pandemic confirms this situation. The general ignorance of this disease causes the spreading of misleading information, harming people's lives and governments' actions to contain it. To fight this infodemic, the populations resorted to the health services' phone lines, congesting them with questions, most of them repeated among different individuals and locations. A chatbot for COVID-19- related questions would redirect this workload from the health services, mitigating such congestion. This chatbot should work for both the English and Portuguese languages. This work provides a background overview about web crawlers, information processing and chatbot development, which are the three components of the application. A systematic literature review was done to provide an analysis of the existing literature on the mentioned thematics. The application presented in this work consists of three main modules: a web crawler, using the ACHE crawler application, which downloads the web pages from the trustworthy sources; a text processor, that parses the web pages and indexes them according to their language to the respective ElasticSearch index; and a chatbot component, composed by a fine-tuned BERT model with the SQuAD 2.0 dataset and a web interface that queries the ElasticSearch indexes for the most relevant pages and extracts the answers to the given questions by the users. To comply with the English and Portuguese requirement, two sets of reliable sources were defined (one for each language) and a translated version of SQuAD 1.1 dataset was used to train the Portuguese BERT model. The chatbot queries the correct model using the web browser's defined language. Our system was evaluated using a set of COVID-19 QA pairs extracted from the United Nations website, and the obtained results are described in this work. These were far from the desirable outcomes, so some improvements were applied to the crawler and to the ElasticSearch indexes. However the results were still not satisfactory, requiring a set of future modifications that are presented in this work."
  },
  {
    "keywords": [
      "Web Application",
      "Web Development Architecture",
      "Queuing Theory and Simulations",
      "Production line",
      "Aplicação Web",
      "Arquiteturas de desenvolvimento Web",
      "Teoria de Filas e Simulações",
      "Linha de Produção",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Monitoring and real-time simulation of an industrial production pipeline",
    "autor": "Viana, José Diogo Lago",
    "data": "2021",
    "abstract": "There is a shortage of manufacturing management software solutions for businesses with various manual processes, and that offer a wide range of products. Existing solutions can become very expensive for small and medium-sized enterprises, and can discourage them to take the next step towards the 4th Industrial Revolution. This dissertation consists of joint work with Tipoprado, Artes Gráficas, to develop a package tracking and production performance analysis platform. The company has a notable number of different clients and offers different types of services. This way, different packages may go through different paths on the production pipeline. Given this, to offer a more close and customized service, Tipoprado, wants to develop a package tracking platform. This tracking is not geographical (delivery case), but about the package location over the production pipeline, giving clients the possibility to consult, in real-time, the actual state of their orders. Apart from this, implementing this platform produces a significant level of data about packages and clients. One of the main goals is to treat, process and analyze this data, to improve production efficiency and be able to help the its managers make crucial decisions about the referred pipeline. Production planning and predictions on delivery dates is the ultimate goal. This dissertation studies and implements the tracking method that best applies to Tipoprado production pipeline, together with data analysis, and prediction options. The given platform will transport the company to a tech production vision, and kick start its journey through the fourth industrial revolution. It is also expected to increase customer engagement levels, which correlate with a higher number of sales."
  },
  {
    "keywords": [
      "Action localization",
      "Action recognition",
      "Spatio-temporal",
      "Violence detection",
      "Localização de ações",
      "Reconhecimento de ação",
      "Espácio-temporal",
      "Deteção de violência",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Spatio-temporal action localization with Deep Learning",
    "autor": "Monteiro, Carlos Filipe Batista Cardoso",
    "data": "2022-06-08",
    "abstract": "The system that detects and identifies human activities are named human action recognition. \nOn the video approach, human activity is classified into four different categories, depending \non the complexity of the steps and the number of body parts involved in the action, namely \ngestures, actions, interactions, and activities, which is challenging for video Human action \nrecognition to capture valuable and discriminative features because of the human body’s \nvariations. So, deep learning techniques have provided practical applications in multiple fields \nof signal processing, usually surpassing traditional signal processing on a large scale. \nRecently, several applications, namely surveillance, human-computer interaction, and video \nrecovery based on its content, have studied violence’s detection and recognition. In recent \nyears there has been a rapid growth in the production and consumption of a wide variety of \nvideo data due to the popularization of high quality and relatively low-price video devices. \nSmartphones and digital cameras contributed a lot to this factor. At the same time, there are \nabout 300 hours of video data updates every minute on YouTube. Along with the growing \nproduction of video data, new technologies such as video captioning, answering video surveys, \nand video-based activity/event detection are emerging every day. From the video input data, \nthe detection of human activity indicates which activity is contained in the video and locates \nthe regions in the video where the activity occurs.\nThis dissertation has conducted an experiment to identify and detect violence with spatial action localization, adapting a public dataset for effect. The idea was used an annotated \ndataset of general action recognition and adapted only for violence detection."
  },
  {
    "keywords": [
      "E-commerce",
      "Web mining",
      "Web page classification",
      "Machine learning",
      "Crawler",
      "Scraper",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Prometheus: a generic e-commerce crawler for the study of business markets and other e-commerce problems",
    "autor": "Dias, João Tiago Pereira",
    "data": "2019",
    "abstract": "The continuous social and economic development has led over time to an increase in consumption,\nas well as greater demand from the consumer for better and cheaper products.\nHence, the selling price of a product assumes a fundamental role in the purchase decision\nby the consumer. In this context, online stores must carefully analyse and define the best\nprice for each product, based on several factors such as production/acquisition cost, positioning\nof the product (e.g. anchor product) and the competition companies strategy. The\nwork done by market analysts changed drastically over the last years.\nAs the number of Web sites increases exponentially, the number of E-commerce web\nsites also prosperous. Web page classification becomes more important in fields like Web\nmining and information retrieval. The traditional classifiers are usually hand-crafted and\nnon-adaptive, that makes them inappropriate to use in a broader context. We introduce an\nensemble of methods and the posterior study of its results to create a more generic and\nmodular crawler and scraper for detection and information extraction on E-commerce web\npages. The collected information may then be processed and used in the pricing decision.\nThis framework goes by the name Prometheus and has the goal of extracting knowledge\nfrom E-commerce Web sites.\nThe process requires crawling an online store and gathering product pages. This implies\nthat given a web page the framework must be able to determine if it is a product page.\nIn order to achieve this we classify the pages in three categories: catalogue, product and\n”spam”. The page classification stage was addressed based on the html text as well as on\nthe visual layout, featuring both traditional methods and Deep Learning approaches.\nOnce a set of product pages has been identified we proceed to the extraction of the pricing\ninformation. This is not a trivial task due to the disparity of approaches to create a web\npage. Furthermore, most product pages are dynamic in the sense that they are truly a page\nfor a family of related products. For instance, when visiting a shoe store, for a particular\nmodel there are probably a number of sizes and colours available. Such a model may be\ndisplayed in a single dynamic web page making it necessary for our framework to explore\nall the relevant combinations. This process is called scraping and is the last stage of the\nPrometheus framework."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Desenvolvimento de um sistema de localização baseado em tecnologia RFID",
    "autor": "Castro, Mauro Nuno Barbosa de",
    "data": "2012-11-22",
    "abstract": "Atualmente existem várias soluções comerciais que utilizam a tecnologia RFID para diversos fins aplicacionais. Na literatura estão também documentados várias abordagens e algoritmos para localização de dispositivos RFID. Partindo deste contexto, o objetivo principal da presente dissertação era modelar e conceber uma solução global de monitorização/localização baseada na tecnologia RFID (pontos de acesso e etiquetas), que fosse adequada para vários tipos de organismos/empresas. Ou seja, nunca foi intenção desta dissertação propor novos algoritmos de localização por RFID.\n\nSeguindo uma metodologia de desenvolvimento de software orientada aos modelos, e apoiada na tecnologia UML, o desenvolvimento do sistema percorreu as seguintes fases: (i) levantamento e documentação de requisitos, baseados na identificação das necessidades de três tipos de organismos/empresas e utilizando o modelo de Volere, (ii) identificação dos utilizadores do sistema, (iii) conceção do sistema, tarefa que incluiu a elaboração de casos de uso detalhados, diagramas de sequência ao nível do sistema, o modelo de dados persistentes do sistema, diagrama de classes e diagramas de sequência ao nível da implementação, (iv) implementação de um protótipo em Java e (iv) realização de testes com o protótipo.\n\nMuito mais do que um algoritmo, ou conjunto de algoritmos, de localização, o sistema desenvolvido é uma solução integrada de localização baseada na tecnologia RFID. A estratégia seguida no desenvolvimento do sistema assenta em 3 princípios: (1) uma dada empresa ou organização deseja monitorizar pessoas e bens, de forma impedir/permitir o acesso a determinadas zonas do espaço físico do edifício dessa empresa, (2) uma das tecnologias mais versátil para este atingir este fim é a RFID, (3) combinando a potência do sinal RF, enviado pela mesma etiqueta RFID, e recebido em pelo menos três pontos de acesso, é possível obter uma estimativa da localização do emissor desse sinal RF. Deste modo, o sistema desenvolvido pressupõe que existem três pontos de acesso (conjunto de APs) em cada local de acesso a zonas críticas do edifício, geometricamente localizados de forma a facilitar a trilateração de medições do sinal RF. Entre as funcionalidades do sistema de localização desenvolvido incluem-se: acesso ao sistema controlado, gestão de utilizadores do sistema, gestão de etiquetas RFID (atribuição, recolha, programação), definição da geometria de cada piso do edifício (usando os conceitos de ponto, segmento e zona), definição das zonas permitidas e interditas a cada tipo de utilizador, gestão dos pontos de acesso e dos conjuntos de pontos de acesso,\nlocalização de etiquetas RFID por trilateração e emissão de alertas, para uma ou mais entidades de segurança, nos casos em que ocorrer a “invasão” de uma zona interdita.\n\nEm termos de resultados, pode dizer-se que se concebeu um protótipo funcional que cumpre a maioria dos requisitos propostos, possui uma interface fácil de utilizar e é suficientemente genérico para poder ser usado por empresas de tipos diferentes. Dado que o código e os modelos estão perfeitamente sincronizados, alterar ou adicionar funcionalidades ao sistema é relativamente seguro e com um custo moderado. Em termos da qualidade das estimativas da localização, os resultados não são muito positivos. Duas alternativas para melhorar este aspeto passariam por (i) melhorar o algoritmo de localização e (ii) usar um tipo de hardware RFID mais sofisticado e vocacionado para ser aplicado num sistema de localização."
  },
  {
    "keywords": [
      "Recommender systems",
      "User-generated content",
      "Healthcare",
      "RESTful APIs",
      "Sistemas de recomendação",
      "Conteúdo gerado pelo utilizador",
      "Saúde",
      "APIs RESTful",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "“HealthAdvisor” – development of central services",
    "autor": "Barreira, João Miguel Pires",
    "data": "2021-12-17",
    "abstract": "Over the last years, there has been an increase in popularity of online platforms that enable users of the\nmost varied economic activities (e.g. restaurants, bars, shops, shopping centers, nightlife, etc.) to evaluate\nthe quality of the service received. Thus, based on the community’s aggregate of opinions, the user can\nthen make a more informed decision when opting between a wide variety of competing services.\nHowever, the field of medical services and health is one of the economic areas where this proce dure isn’t widely adopted. Consequently, the goal of this work is to develop an intelligent technological\nsolution centered on the user, that allows the registration, evaluation, and recommendation of healthcare\ninstitutions.\nIn this work, we present a platform called HealthAdvisor. This platform is comprised of a web User\nInterface, a Recommender System that generates recommendations for the user, and a backend service\nthat supports both of these components by providing and exposing multiple REST APIs (Representational\nState Transfer Application Programming Interfaces). By using this solution, it is possible for a user to search\nfor healthcare institutions, read the reviews of institutions made by other users, provide their own reviews,\nand receive recommendations based on their user profile as well as their past usage of the service."
  },
  {
    "keywords": [
      "Oracle retail",
      "Django",
      "Gerador de código",
      "Templates",
      "Gerador de código com templates",
      "Code generator",
      "Code generator with templates",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Geração automática de código Oracle Retail: uma solução baseada em templates e Django",
    "autor": "Veloso, José Pedro Fernandes",
    "data": "2023-11-27",
    "abstract": "O Oracle Retail oferece um conjunto de aplicações de software que ajuda os retalhistas a gerir os seus\nnegócios, incluindo ponto de venda, gestão de inventário, gestão de relações com clientes e gestão da\ncadeia de fornecimento.\nNo entanto há necessidade de adaptar a Solução Oracle Retail à realidade do cliente, isto é, fazer\nalgumas personalizações à Solução. Estas necessidades dos clientes são várias vezes idênticas, sendo o\nvolume de alterações por vezes elevado e repetitivo, mudando pequenas especificidades de cliente para\ncliente. Isto, por sua vez, resulta numa perda de tempo valioso que poderia ser alocado a tarefas mais\nprodutivas.\nCombinando esta realidade de trabalhos semelhantes com a pressão do mercado onde a disponibilidade e continuidade de recursos é cada vez mais complexa, as organizações devem procurar formas de\nautomatizar estes processos e beneficiar da redução do tempo gasto tanto no desenvolvimento como na\nresolução de problemas.\nÀ luz destes desafios, o desenvolvimento de uma aplicação destinada a gerar código Oracle Retail\npersonalizado é uma solução promissora para responder às necessidades dos clientes na adaptação da\nsolução Oracle Retail aos seus requisitos."
  },
  {
    "keywords": [
      "Apache Spark",
      "Cryptographic Schemes",
      "Databases",
      "Intel SGX",
      "Bases de Dados",
      "Esquemas Criptográficos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "SafeSpark: a secure data analytics platform using cryptographic techniques and trusted hardware",
    "autor": "Carvalho, Hugo Alves",
    "data": "2019-11-08",
    "abstract": "Nowadays, most companies resort to data analytics frameworks to extract value from the\nincreasing amounts of digital information. These systems give substantial competitive ad vantages to companies since they allow to support situations such as possible marketing\ndecisions or predict user behaviors.\nTherefore, organizations tend to leverage the cloud to store and perform analytics over\nthe data. Database services in the cloud present significant advantages as a high level\nof efficiency and flexibility, and the reduction of costs inherent to the maintenance and\nmanagement of private infrastructures. The problem is that these services are often a target\nfor malicious attacks, which means that sensitive and private personal information can be\ncompromised.\nThe current secure analytical processing solutions use a limited set of cryptographic\ntechniques or technologies, which makes it impossible to explore different trade-offs of\nperformance, security, and functionality requirements for different applications. Moreover,\nthese systems also do not explore the combination of multiple cryptographic techniques\nand trusted hardware to protect sensitive data.\nThe work presented here addresses this challenge, by using cryptographic schemes and\nthe Intel SGX technology to protect confidential information, ensuring a practical solution\nwhich can be adapted to applications with different requirements. In detail, this dissertation\nbegins by exposing a baseline study about cryptographic schemes and the Intel SGX tech nology, followed by the state-of-the-art revision about secure data analytics frameworks.\nA new solution based on the Apache Spark framework, called SafeSpark, is proposed. It\nprovides a modular and extensible architecture and prototype, which allows protecting in formation and processing analytical queries over encrypted data, using three cryptographic\nschemes and the SGX technology. We validated the prototype with an experimental evalu ation, where we analyze the performance costs of the solution and also its resource usage.\nFor this purpose, we use the TPC-DS benchmark to evaluate the proposed solution, and\nthe results show that it is possible to perform analytical processing on protected data with\na performance impact between 1.13x and 4.1x."
  },
  {
    "keywords": [
      "Computação gráfica",
      "Atmosphere scattering",
      "681.3:74",
      "74:681.3",
      "535.36",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática",
      "Ciências Naturais::Ciências Físicas"
    ],
    "titulo": "State of the art on atmospheric scattering",
    "autor": "Lopes, Diogo A. R.",
    "data": "2014-12-05",
    "abstract": "The colour of the sky has always fascinated people throughout the ages. The blue sky, the colour of the sun, the orange of the sunsets, the clouds, etc...\n\nFor centuries, physicists and mathematicians tried to explain with formulae what artists like Leonardo da Vinci reproduced in their paintings, like the colour of the sky, the bluish colour of distant mountains, fog, and several other nature effects.\n\nIn the area of computer graphics there is a great interest in recreating these natural effects, observable everyday, as real as possible. The reproduction of these effects is essential for certain applications such as flight simulators, video-games and other scientific applications, hence, the relevance of rendering these effects in real time.\n\nThis project aims to present a state of the art on atmospheric scattering and so consolidate knowledge on the subject.\n\nMuch work has been done with the purpose of recreating these effects digitally. In result many algorithms, implemented using different techniques, are now available.\n\nWhen evaluating atmospheric scattering there are several aspects that are considered such as the light source, the density and size of particles in the atmosphere, among others. \n\nIn this thesis, only daylight models will be considered and in these models, the sun is considered the only light source. The particles on the atmosphere can be divided in 2 main entities: air molecules and aerosol particles. \n\nThe phenomenon can be explained as the result of the interaction between the rays that come from the sun and the particles in the atmosphere. The most relevant interaction, regarding colour, is scattering.\n\nThe atmospheric scattering models present methods and techniques to evaluate and recreate this phenomenon. These models are very diverse but they base their techniques with the physics behind the phenomenon. \n\nTheir evolution showed a path that went from quality to efficiency. Part of this evolutionary trait came with the evolution on both hardware and software.\n\nThe early AS models had a greater concern in recreating with the most possible accuracy the atmospheric scattering phenomenon. That concern passed, over the years, to recreate this phenomenon the fastest way possible.\n\nWhen talking about rendering the colours of the sky, is natural to want to render the other natural effects such as clouds and light shafts. Therefore it is also valuable to present information of some algorithms that can recreate these effects."
  },
  {
    "keywords": [
      "User interface",
      "Prototype",
      "User-centred design",
      "Widget",
      "Protótipo",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Animating user interface prototypes with formal models",
    "autor": "Costa, Rafael Braga Gomes da",
    "data": "2020-12-10",
    "abstract": "The User Interface (UI) provides the first impression of an interactive system and should,\nthus, be intuitive, in order to guide users effectively and efficiently in performing their tasks.\nUser interface prototyping is a common activity in UI development, as it supports early\nexploration of the UI design by potential users.\nUI quality plays a crucial role in safety-critical contexts, where design errors can poten tially lead to catastrophic events. Model-based analysis approaches aim to detect usability\nand performance issues early in the design process by leveraging formal analysis. They\ncomplement prototyping, which supports user involvement, but not an exhaustive analysis\nof the designs.\nThe IVY Workbench emerges as a model-based analysis tool intended for non-expert\nusage. The tool was originally focused on supporting modelling and verification, but more\nrecently an effort began to combine the formal model capabilities with UI mock-ups, to\nproduce more interactive prototypes than traditional mock-up editors support.\nThis work addresses the enhancement of the prototyping features of the IVY Workbench.\nThe improvements of such features include the creation of a dynamic widget library that\ncan vastly improve the quality of prototypes. Such a library, however, should be compatible\nwith several mock-up editors to attract a broader design community.\nThe results of this work include an analysis of alternative prototyping tools, identifying\npotential features that can enhance the IVY Workbench, the creation of a dynamic widget\nlibrary that is compatible with several mock-up editors, and several improvements to IVY’s\nprototyping plugin, including the addition of code exporting functionalities. Usability tests\nwere conducted to validate the new features of the tool, with positive results. Two mobile\napplications were also created, allowing users to test prototypes in their mobile devices."
  },
  {
    "keywords": [
      "Quantum phase properties",
      "Dynamic quantum logic",
      "Paraconsistent Dynamic Quantum Logic",
      "PhLQP",
      "PhLQP◦",
      "Quantum Teleportation Protocol",
      "Quantum Leader Election Protocol",
      "Quantum Fourier Transform",
      "Deustch Gate",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Combining paraconsistent and dynamic logic for Qiskit",
    "autor": "Faria, Bernardo Almeida Leite",
    "data": "2021-10-21",
    "abstract": "This dissertation introduces a logic aimed at combining dynamic logic and paraconsistent\nlogic for application to the quantum domain, to reason about quantum phase properties:\nParaconsistent Phased Logic Of Quantum Programs (PhLQP◦\n).\nIn the design PhLQP◦\n, firstly the dynamic was built first, Phased Logic Of Quantum\nPrograms (PhLQP). PhLQP is itself a dynamic logic capable of dealing with quantum phase\nproperties, quantum measurements, unitary evolutions, and entanglements in compound\nsystems , since it is a redesign of the already existing Logic Of Quantum Programs (LQP), [14],\nover a representation of quantum states restricted to a space B equipped with only two\ncomputational basis, standard and Hadamard. As instances of applications of the logic\nPhLQP, there is a formal proof of the correctness of the Quantum Teleportation Protocol, of\nthe 2-party and 4-party of the Quantum Leader Election (QLE) protocol, and of the Quantum\nFourier Transform (QFT) operator for 1, 2 and 3 qubits .\nOn a second stage, PhLQP was extended with the connective ◦ known as the consistency\noperator, a typical connective of the paraconsistent logics Logics of Formal Inconsistency\n(LFIs), [8, 21, 22]. The definition of consistent quantum state and a set of proper para consistent axioms for the quantum domain, Fundamental Paraconsistent Quantum Axioms\n(FParQAxs), were provided.\nAn example of application of PhLQP◦\nis the possibility of express and prove correctness\nof the universal quantum gate, the Deustch gate."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistemas inovadores de segurança em bases de dados",
    "autor": "Marinho, André Alexandre Pinheiro",
    "data": "2019",
    "abstract": "Nos dias de hoje, tem-se notado um aumento do número e diversidade de dados digitais\nque circulam, são tratados, analisados e utilizados à escala global. Os números são significativos,\ne, por isso, as empresas começam a tomar partido de serviços de terceiros, para\nbeneficiar das vantagens de computação que estes proporcionam.\nPosto isto, serviços de nuvem disponibilizados pela Amazon, Google ou Microsoft, são utilizados\npor essas empresas, que procuram garantias não só de disponibilidade mas também\nde proteção dos seus dados. Como temos observado ao longo dos anos, os serviços de\nnuvem têm vindo a sofrer imensos ataques, onde falhas de segurança nos servidores de\narmazenamento acabam por ser responsáveis pela libertação de enormes quantidades de\ninformação confidencial.\nDe modo a resolver as preocupações existentes de aplicações que lidam com dados\nsensíveis e confiáveis foram propostas várias bases de dados capazes de armazenar e processar\ndados de forma segura na cloud. Contudo, o maior esforço de investigação encontra-se\nem desenvolver novos esquemas criptográficos que protegem os dados em texto cifrado de\ntal modo que as bases de dados consigam processar interrogações como se fosse texto simples.\nEsta abordagem apesar de eficiente acaba por libertar informação sensível que pode\nser utilizada para quebrar a segurança dos sistemas. Para além disso, a investigação existente\ntem dado prioridade às bases de dados SQL devido à sua grande aplicabilidade. Esta\ndissertação toma uma abordagem diferente e apresenta uma nova base de dados NoSQL\ncom processamento seguro, TrustNosQL, assente nas propriedades de segurança de hardware\nconfiável. Mais precisamente, este trabalho tem três contribuições principais. O primeiro é\numa análise compreensiva do estado da arte atual em base de dados com processamento\nseguro. Este estudo permite posicionar o sistema apresentado em relação às capacidades\ne propriedades de segurança dos sistemas existentes. A segunda contribuição é a\nbase de dados NoSQL com processamento seguro, TrustNoSQL, a primeira base de dados\nNoSQL que processa de forma segura as interrogações utilizando a tecnologia Intel SGX. A\núltima contribuição é uma extensa avaliação do sistema apresentado com uma plataforma\nde avaliação de base de dados reconhecida pela indústria."
  },
  {
    "keywords": [
      "Deep Learning",
      "Machine Learning",
      "Modelos",
      "Proteínas transportadoras",
      "Models",
      "Transport proteins",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Identification and classification of transporter proteins using deep learning models",
    "autor": "Silva, Andrea Ferreira Meireles",
    "data": "2019-11-28",
    "abstract": "Nos últimos anos a identificação e sequenciação de proteínas transportadoras tem crescido, uma \nvez que estas são de extrema importância no corpo humano e em todos os seres vivos, sendo \nresponsáveis pela absorção e movimentação de moléculas essenciais às células e ainda pela excreção \nde produtos do metabolismo celular. A identificação de genes que codificam proteínas transportadoras é \nmuito importante em várias áreas, como farmacocinética e reconstrução de modelos metabólicos em \nescala genómica que permitem perceber a relação entre genótipos-fenótipos.\nDe forma a tentar diferenciar proteínas transportadoras de não transportadoras duas abordagens \nforam realizadas, treinando e testando modelos de machine learning e de deep learning. Os dados \nutilizados provêm da base de dados TCDB, que contém proteínas transportadoras, e da base de dados \nSwiss-Prot, onde as proteínas foram filtradas para serem obtidas proteínas não transportadoras, obtendo \nno final um conjunto de dados equilibrado. De seguida, através desses dados foram obtidas \ncaracterísticas das proteínas através das suas sequências, sendo assim utilizado para treinar diferentes \nmodelos de machine learning e deep neural networks. Nesta abordagem os modelos apresentaram um \nbom desempenho global, atingindo 89% de acerto na identificação de proteínas transportadoras. Todos \nos modelos treinados apresentam um elevado número de falsos negativos em comparação com o \nnúmero de falsos positivos, indicando que a maior falha nos modelos prende-se na identificação de \nproteínas transportadoras como não transportadoras.\nO principal objetivo deste projeto prendia-se com a utilização de métodos de deep learning para \nidentificar proteínas transportadoras, apenas utilizando as suas sequências de aminoácidos como \nentrada, comparando assim as duas abordagens realizadas. Desta forma, utilizando apenas as \nsequencias das proteínas, diferentes redes neuronais foram treinadas e testadas, desde redes neuronais \nrecorrentes a convolucionais, obtendo um desempenho global muito semelhante ao da abordagem \nanterior, atingindo também um valor de 89% de acerto na identificação de proteínas transportadoras.\nAssim, foram alcançados modelos de desempenho preditivo semelhante sem a necessidade de calcular \ncaracterísticas."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "R-seqQI: RNA-Seq Quality Indicator",
    "autor": "Sousa, Abel Ernesto Fernandes de",
    "data": "2016-03-03",
    "abstract": "The current progress of sequencing systems facilitates the sequencing of the genomes and\ntranscriptomes of countless organisms on our planet. However, it is not simple to measure the quality\nof the processed data, mainly in the study of non-model organisms, for which there is little if any,\ninformation available. The Korf Lab developed a method for the evaluation of genomes integrity,\nthrough the identification of 248 core eukaryotic genes (CEGs) that are present in nearly all of the\neukaryotes. The main goal of this work is to evaluate the use of the CEGs in RNA-Seq of non-model\norganisms. For that two software’s were developed: seqQIrefmetrics to calculate a set of referencebased\nquality metrics, including identification, chimerism, accuracy and contiguity, based on the\nliterature, and three new metrics, comprising fragmentation(1,2,3,4,5+), coverage and non-match,\nincreasing the number of metrics available for transcriptome quality assessment; and\nseqQIidentifyCEGs to identify and report the number of CEGs present in each transcriptome assembly.\nTo carry out the main objective, RNA-Seq data from nine model organisms (Arabidopsis thaliana,\nAspergillus nidulans, Caenorhabditis elegans, Drosophila melanogaster, Homo sapiens, Mus musculus,\nOryza sativa, Saccharomyces cerevisiae and Xenopus tropicalis), processed with Trinity, were used to\nevaluate how CEG detection correlates with the quality of the transcriptomes. In order to identify CEGs,\nprotein sequences from assembled transcripts were predicted with TransDecoder. Metrics calculated\nby seqQIrefmetrics were associated with the number of CEGs identified by seqQIidentifyCEGs in each\nassembled transcriptome, through linear regressions. Among these metrics only contiguity and\ncoverage were used to create predictive models, achieving an R2 of 0.787 and 0.640; and a RMSE of\n5.86 and 6.90, respectively. These findings indicate that the CEGs can be used as a quality tool. In\nfact, the linear regressions enable to infer prospectively the quality of the assembled transcripts,\nwithout the necessity of additional information, such as a reference genome sequence or structural\nannotations. This approach is extremely important for RNA-Seq of non-model organisms, where there is\nno such information to evaluate the quality of the assembled transcripts in a reliable manner."
  },
  {
    "keywords": [
      "Análise de dados",
      "Monitorização de redes",
      "Redes sem fios",
      "Data analysis",
      "Network monitoring",
      "Wireless networks",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Monitorização da rede Eduroam no Departamento de Informática",
    "autor": "Silva, Mário Costa",
    "data": "2019-12-23",
    "abstract": "Perante os recorrentes problemas de qualidade na utilização da rede Eduroam no Departamento de Informática (DI) da Universidade do Minho, foi efetuada uma reformulação e\nreestruturação da rede sem fios. A sua monitorização permite tomar conhecimento de eventuais situações que possam causar o mau funcionamento de serviços, como outras redes\n(i.e., ad-hoc, não autorizadas, etc.) no mesmo espaço sem fios que afetam negativamente\no desempenho da rede Eduroam ou má utilização dessas redes com intenções maliciosas.\nPortanto, monitorizar a operação da Eduroam assume um papel importante em garantir\numa prestação adequada dos serviços de rede. Para que tal seja possível, irá ser necessário\nestudar a rede sem fios do DI, fazer uma recolha e análise de dados sem fios e disponibilizar\nos resultados da análise em tempo real. Neste contexto, foi desenvolvida uma aplicação web\ncom interface amigável de forma a monitorizar o ambiente sem fios em tempo real, identificando comportamentos incorretos e/ou degradação de desempenho. Foram utilizadas\nalgumas linguagens de programação como PHP e Javascript para processamento de dados\ne interação com o utilizador e ainda RRDTool como forma de armazenamento de dados.\nApós a análise dos resultados obtidos foi possível concluir que os 33 pontos de acesso do\nDI estão sobre uma taxa de utilização inferior a 30% sendo suficientes para a quantidade\nde dispositivos atual."
  },
  {
    "keywords": [
      "Clean code architecture",
      "Multi-tenant",
      "Phishing",
      "Relatórios",
      "Saas",
      "Reports",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Golang avaliação e melhorias na arquitetura e código",
    "autor": "Carvalho, Carlos Miguel Luzia de",
    "data": "2023-11-20",
    "abstract": "Com o sucessivo aumento da informatização e robotização de processos, existe uma crescente neces sidade de produção de código que tenha como objetivo a melhoria da escalabilidade, rapidez e eficiência\nde uma aplicação com a finalidade de garantir que a plataforma a trabalhar seja mais rápida, escalável e\nmais fácil de testar e modificar.\nAssim, a presente dissertação aborda a transição da arquitetura de uma aplicação da empresa EM VENCI, com o objetivo de melhorar a sua eficiência e desempenho. A motivação para este estudo surgiu\nda necessidade da empresa aplicar melhorias à arquitetura atual, visando aprimorar a compreensão do\ncódigo, identificar problemas com maior rapidez e obter ganhos significativos de desempenho.\nDesta forma, é realizada uma exposição da arquitetura inicial, identificando pontos críticos e anali sando potenciais melhorias, como repetidos acessos á base de dados, código pouco comentado e mistura\nde lógica de negócio com a camada de acesso a dados. Com base nessa análise, foi proposto um novo\ndesign arquitetural, que foi cuidadosamente planeado e fundamentado no uso de Clean Code Archi tecture. Apresentado posteriormente o processo de implementação deste design sempre exemplificado\ncom recurso a um caso de estudo que incorpora o projeto, nomeadamente um dos relatórios de Phishing.\nPor fim, foram conduzidos testes de funcionalidade e desempenho para garantir a saúde da aplica ção e realizar uma análise comparativa com a arquitetura anterior. Os resultados obtidos demonstraram\nclaramente o sucesso da nova arquitetura, com melhorias significativas no desempenho, na compreen são da estrutura e no tempo de resolução de problemas, destacando-se pela facilidade com que novos\nmembros da equipa, após uma breve formação na nova arquitetura, conseguem navegar e compreender\no código da plataforma em comparação com a arquitetura antiga. Os resultados obtidos evidenciam o\nimpacto positivo dessa transição no contexto da empresa, beneficiando tanto os colaboradores quanto os\nclientes. O trabalho futuro envolverá a continuidade da transição dos Use Cases para a nova arquitetura,\na fim de consolidar ainda mais os ganhos alcançados e manter a EMVENCI na vanguarda tecnológica\ndo seu segmento."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Efficient modelling of liquid surfaces on multi-core CPU and Xeon Phi devices",
    "autor": "Araújo, Bruno Tiago Abreu de",
    "data": "2015-12-18",
    "abstract": "The assembly of miniature electronic components requires an adequate scale of the size of the welding\nterminators in printed circuit boards to minimize the stresses due to deformation. An optimum\nterminator layout minimizes the surface tension of the liquid solder, but requires efficient simulation\nalgorithms to compute the results in an acceptable time slot. Current Surface Evolver is a software\ntool to study surfaces, shaped by surface tension and other energies, and its execution efficiency can be\nimproved to take advantage of shared memory systems based on multi-core and many-core computing\ndevices.\nThis dissertation aims to analyze the Surface Evolver, identifying the computational bottlenecks\nand working on solutions to improve the overall performance of the application. Parallel algorithms\nwere developed to explore the architectural features of current multi-core and many-core computing\ndevices namely the Xeon Phi, and including the growing vectorization features of newer processing\ndevices.\nAfter an analysis of the application and its profiling, the original data structure was identified as\nthe critical bottleneck for software performance: it is implemented with linked lists, which prevents\nthe use of the vectorization features of current devices and leads to inefficient parallel algorithms,\nboth key elements to improve the performance of the Surface Evolver. The modification of the data\nstructure was a key task in this dissertation.\nThe calculation force was identified as one of the most time consuming tasks of Surface Evolver and\nit was the target function of this work. This algorithm iterates over all vertices, edges and faces so is\na good example to conclude how vectorization and parallelism affects the performance of simulation\nsoftware used in the variety fields of science and engineering. In the end of this work it is possible\nto see that vectorization can greatly improve the performance of an application, bringing significant\nspeedups to Surface Evolver.\nThe measured execution times are presented and discussed, throughout the various development\nstages of the application, aiming to analyze the impact of the application of high performance techniques\non the Surface Evolver, suggesting yet further future improvements that were well identified in\nthe end of this work."
  },
  {
    "keywords": [
      "Buoyancy",
      "Performance",
      "Scalability",
      "Torque",
      "Flutuabilidade",
      "Escalabilidade",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Simulating buoyancy",
    "autor": "Cruz, Bruna Vieira",
    "data": "2023-07-13",
    "abstract": "The simulation of object buoyancy is a very interesting topic that involves a lot of research in the area of fluid\ndynamics.\nTo better understand how the simulation of floating objects is done, it is necessary to distill the articles found\nto make a simple simulation that still covers the main characteristics of the simulation.\nIn this sense, the research required to develop the algorithm is focused on the main dynamic characteristics\nof the object’s movement in the water, that is, the main forces that are applied.\nAlthough this type of simulation is widely used, as in video games, articles that address this topic in a simple\nway that can be easily adapted in a simulation are quite scarce in terms of the physics involved or focus on how\nto start and don’t develop much further.\nIn this sense, there are few techniques or methods of implementation, most of which are focused on minimizing\nthe costs of these same techniques and still providing a simulation as realistic as possible.\nFaced with this problem, the objective is to have a robust algorithm and be as realistic as possible, being able\nto demonstrate a good representation of the buoyancy of objects according to their mass and the forces that are\nexerted. To accomplish this objective, an algorithm was adapted from Kerner (2015) and Kerner (2016).\nEach added force contributes to a better representation of reality making the algorithm more realistic. With\nthis accomplished, it is implemented in shaders and tested with multiple objects simultaneously for performance\nanalysis, being possible to observe the capacity of the algorithm.\nThe simulation made in this dissertation was tested in terms of performance and time spent on GPU, by the\nnumber of boats and by the number of triangles in the mesh of the boat."
  },
  {
    "keywords": [
      "Internet tomography",
      "Network topology discovery",
      "TTL-limited probes",
      "Packet sandwich",
      "Link failure detection",
      "Tomografia de rede",
      "Descoberta da topologia de rede",
      "Detecção de falhas de link",
      "681.324"
    ],
    "titulo": "Internet tomography : network topology discovery and network performance evaluation",
    "autor": "Costa, Fábio Rafael Azevedo",
    "data": "2013",
    "abstract": "Due to the security threats and complexity of network services, such as video conferencing,\ninternet telephony or online gaming, which require high QoS guarantees,\nthe need for monitoring and evaluating network performance, in order to promptly\ndetect and face security threats and malfunctions, is crucial to the correct operation\nof networks and network-based services. As the internet evolves in size and\ndiversity, these tasks become difficult and demanding. Moreover, administrative\nlimitations can restrict the position and the scope of the links to be monitored,\nwhile legislation imposes limitations on the information that can be collected and\nexported for monitoring purposes and almost all organization can't monitor or\nhave knowledge or evaluate the performance of the entire network. They only can\ndo this to part of the network, which corresponds to their own network.\nIn this thesis, we propose the use of tomographic techniques for network topology\ndiscovery and performance evaluation. Network tomography studies the internal\ncharacteristics of the network using end-to-end probes, ie, it does not need the\ncooperation of the internal nodes of the network and can be successfully adopted\nin almost all scenarios. Thus, it is possible to have knowledge of the network\ncharacteristics out of the administrative borders.\nIn this thesis we propose a new approach to Probe Packet Sandwich, where we\nuse TTL-limited probes to infer the delay of a path hop-by-hop. We have shown\nthat this approach is more effective than existing ones.\nThis work was developed under the ERASMUS student mobility program, in the\nTelecommunication Networks Research Group, Dept. of Information Engineering,\nUniversity of Pisa."
  },
  {
    "keywords": [
      "Formal methods",
      "High Assurance ROS (HAROS)",
      "JavaScript",
      "NodeJS",
      "Robot Operating System (ROS)",
      "Software Product Line (SPL)",
      "Text-based Variability Language (TVL)",
      "Variability",
      "Linha de Produtos de Software (LPS)",
      "Métodos formais",
      "Variabilidade",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "An HAROS extension for variability aware ROS code analysis",
    "autor": "Pereira, Ricardo Ribeiro",
    "data": "2022-01-31",
    "abstract": "Human kind has proven how challenging and volatile the technological market can be,\ngrowing at an exponential rate. The benefits of such evolution are directly reflected in many\nways in our everyday life. Robots are a clear example of an advanced technology that may\nbe completely integrated in our societies in a near future, hopefully in such a way that their\nactions will be considered as trustable as human actions are. These machines are permanently\nrelying on software, which has a development process that many times cannot be considered\ntrustworthy. This may cause the final product to have multiple malfunctions, which in turn\nmay result in tremendous economic losses or even harm human lives.\nBearing this in mind, software industry and academia have been trying to establish new\nstandards and techniques that considerably lower the occurrence of the latter problems.\nThe solution is to apply certain formal methodologies and tools when developing software,\nnamely when developing critical software that controls machinery used, for example, in\nhealthcare sector, aeronautical industry, or in military operations.\nThe present dissertation aims to explore and improve techniques and tools to help devel opers in the process of building robotic systems, namely those developed with the Robot\nOperating System (ROS). The focus will be on a specific framework named HAROS, which\nperforms different types of analyses of ROS-based code. Although it has a solid set of useful\nfeatures, some need to be upgraded to enhance efficiency and also to promote a better experi ence to their users, in particular when the the software has many variants, as is often the case\nwith robotic applications.\nThe proposed extension offers ROS and HAROS users a practical methodology that, by\nmerging existing ROS and Software Product Line (SPL) development tools and concepts,\nconsiderably improves the understanding of the variability in a robotic application, without\nrequiring a steep learning curve."
  },
  {
    "keywords": [
      "ACI",
      "Cloud",
      "Kubernetes",
      "Microserviços",
      "SDN",
      "Microservices",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Mobilidade de micro serviços em datacenters suportada por Software Defined Networks (SDN)",
    "autor": "Valente, Daniel Jorge",
    "data": "2020-06-17",
    "abstract": "O termo microserviços não é propriamente recente, existem inúmeras referências ao longo da \núltima década sobre este conceito, no entanto não existe um verdadeiro consenso sobre quem foi o \nprimeiro a introduzir esta abordagem. Independentemente da indefinição sobre o autor, as vantagens e \nos desafios da sua utilização como base ao desenvolvimento de novas aplicações são hoje bem \nconhecidos. É também possível verificar que esta arquitetura de software, que inicialmente era mais \nutilizada em desenvolvimentos nativos para a Cloud, é cada vez mais utilizada em centros de dados \nlocais, o que lança novos desafios às infraestruturas de rede dos centros de dados.\nO simples facto dos microserviços serem independentes entre si, permite que sejam \ndesenvolvidos, distribuídos e atualizados individualmente, desta forma conseguimos atualizações mais \nrápidas e com maior frequência, endereçando a constante mudança de requisitos aplicacionais que se \nverifica em variadíssimas áreas de negócio.\nNo entanto a adoção de novas plataformas deve garantir que estes novos paradigmas \nintegram, e idealmente beneficiam de tecnologias ou soluções já existentes. Num ambiente altamente \ndistribuído, como é o caso de arquiteturas baseadas em microserviços, é evidente que a componente \nde comunicações tem um papel preponderante na qualidade do serviço, pelo que nos casos em que o \ncentro de dados onde se pretende utilizar a plataforma de orquestração utiliza redes baseadas em \nsoftware (SDN), o ideal é que as soluções integrem de forma bastante profunda. Esta integração é \nainda mais relevante se o referido centro de dados apresentar uma arquitetura híbrida, isto é, \ncomposto por capacidade de computação em múltiplos datacenter físicos, mas também em \nprovedores de Clouds públicas (Azure, AWS, Google Cloud, etc.).\nEste trabalho pretende enumerar os principais desafios à utilização de containers em centros \nde dados, bem como descrever a melhor forma de integrar a solução de gestão de rede de centros de \ndados do fabricante Cisco (ACI - Application Centric Infrastructure) com a solução de orquestração de \ncontainers mais utilizada atualmente (Kubernetes). É também âmbito deste trabalho apresentar uma \nproposta à integração do ambiente descrito anteriormente (ACI+Kubernetes) com soluções de \norquestração de containers alojados em Clouds públicas, nomeadamente na cloud da Microsoft \n(Azure)."
  },
  {
    "keywords": [
      "Esports",
      "Player",
      "Team",
      "Competition",
      "Biometrics",
      "Coach",
      "Monitoring",
      "Data analysis",
      "Recommendations system",
      "Prediction",
      "Machine learning (ML)",
      "Jogador",
      "Equipa",
      "Treinador",
      "Competição",
      "Biométricas",
      "Monitorização",
      "Análise de dados",
      "Sistema de recomendação",
      "Previsão",
      "ML",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Performance optimization and reporting platform for esports",
    "autor": "Duarte, Pedro Daniel Pinto",
    "data": "2018",
    "abstract": "The gaming industry has undergone some changes with the investments and professionalization\nof the sector, changing the way of playing video games from traditional leisure to\nbe like a sportsman job. There are currently organized multiplayer video games competitions\nwith professional players, know as Electronic Sports (Esports).\nThese professional video games players can be compared to athletes once they’re part\nof a team and with training, their performance can be improved as well as, given certain\nfactors, conditioned. The emergence of team coaches was naturally introduced, and he’s\nresponsible for optimizing team performance.\nDue to this fact, arises the need to develop tools with the aim of improving the performance\nof these professional players as well as increasing the duration of their careers by\ntaking care of their physical and mental health.\nIt was proposed for this study the development of a Performance Optimization and\nReporting Platform for Esports to help the coaches and players, continuously and automatically\ncollecting their behavioral states and reporting the obtained results in order to\nguide the training to improve individual and team performance.\nThis platform was tested in a real environment, with professional teams as a case study,\nwhere it was possible to analyze the impact of mental fatigue and behavioral biometric\nperformance on devices interaction in players’ game results."
  },
  {
    "keywords": [
      "CLAV",
      "FAIR",
      "API",
      "SWAGGER",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "CLAV: nova ontologia",
    "autor": "Cunha, José Ricardo Sousa Mendes da",
    "data": "2023-01-05",
    "abstract": "O Classificação e Avaliação de Processos da Administração Pública (CLAV) é um projeto nacio nal financiado pelo Simplex e que visa a classificação e a avaliação de toda a documentação\ncirculante na Administração Pública (AP) portuguesa. A ontologia atual que suporta o CLAV\nfoi desenvolvida de uma forma incremental (patchwork) e a certa altura não houve o cuidado\ne o formalismo necessário.\nEste projeto teve duas componentes. Uma teórica onde se especificou uma nova ontologia\nWeb Ontology Language (OWL) seguindo os padrões de nomenclatura Findability, Accessibility,\nInteroperability, and Reusability (FAIR), e em inglês.\nE uma prática, que consistiu em criar de raiz uma Application Programming Interface (API)\nde dados sobre esta ontologia, onde se implementou cerca de oitenta rotas que no seu\ntodo constituem um serviço autónomo (houve um reaproveitamento da API atual mas foi\nnecessário recodificar algumas rotas, criar novas e alterar o output de outras). Foi criada\ntambém a documentação da API em Swagger de modo a haver uma fácil compreensão da\nmesma.\nAinda no contexto deste projeto, foi desenvolvido um novo migrador que transfere\na informação das folhas de recolha de dados usadas atualmente para a nova ontologia\nespecificada."
  },
  {
    "keywords": [
      "681.3.02",
      "681.325.59",
      "519.612"
    ],
    "titulo": "Efficient computation of the matrix square root in heterogeneous platforms",
    "autor": "Costa, Pedro Filipe Araújo",
    "data": "2013",
    "abstract": "Matrix algorithms often deal with large amounts of data at a time, which impairs efficient\ncache memory usage. Recent collaborative work between the Numerical Algorithms\nGroup and the University of Minho led to a blocked approach to the matrix square root algorithm\nwith significant efficiency improvements, particularly in a multicore shared memory\nenvironment.\nDistributed memory architectures were left unexplored. In these systems data is distributed\nacross multiple memory spaces, including those associated with specialized accelerator\ndevices, such as GPUs. Systems with these devices are known as heterogeneous\nplatforms.\nThis dissertation focuses on studying the blocked matrix square root algorithm, first\nin a multicore environment, and then in heterogeneous platforms. Two types of hardware\naccelerators are explored: Intel Xeon Phi coprocessors and NVIDIA CUDA-enabled GPUs.\nThe initial implementation confirmed the advantages of the blocked method and showed\nexcellent scalability in a multicore environment. The same implementation was also used in\nthe Intel Xeon Phi, but the obtained performance results lagged behind the expected behaviour\nand the CPU-only alternative. Several optimizations techniques were applied to the\ncommon implementation, which managed to reduce the gap between the two environments.\nThe implementation for CUDA-enabled devices followed a different programming model\nand was not able to benefit from any of the previous solutions. It also required the implementation\nof BLAS and LAPACK routines, since no existing package fits the requirements of\nthis application. The measured performance also showed that the CPU-only implementation\nis still the fastest."
  },
  {
    "keywords": [
      "Gestor de passwords",
      "Criptografia",
      "KeePass",
      "Jasmin",
      "EasyCrypt",
      "Password managers",
      "Cryptography",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Implementação certificada da componente criptográfica do gestor de passwords KeePass",
    "autor": "Freitas, Pedro Miguel Marques",
    "data": "2022-04-26",
    "abstract": "Com a enorme quantidade de aplicações e sistemas web que nos são apresentados existe uma constante preocupação com a nossa segurança e privacidade como utilizadores dos mesmos. Todos nós temos o direito à privacidade dos nossos dados e quando fazemos um registo num novo produto de software queremos acreditar que estaremos protegidos de ataques alheios e que, a não ser que a nossa password seja descoberta, nenhuma informação nossa vai ser vazada. Para tal também nos é exigido, consumidores de tecnologia e aplicações, que tomemos uma atitude no sentido de nos protegermos. Uma dessas formas é usar passwords seguras e diferentes para cada conta criada. Como isto facilmente se toma impraticável devido à enorme quantidade de contas e, consequentemente passwords que é necessário decorar, surgiram os Gestores de Passwords. Estes servem para guardar as nossas passwords de forma segura e confiável para que sempre que precisemos de uma password a irmos buscar de forma simples e rápida. Assim este projecto visa re-implementar a componente criptográfica do gestor de passwords KeePass de forma a garantir os mais altos níveis de confiabilidade e segurança. Para isso, dever-se-á tirar partido das soluções tecnológicas mais recentes para assegurar os referidos níveis de confiabilidade e segurança, como sejam o uso de linguagens de domínio específico para codificação de técnicas criptográficas e sistemas de provas que possam assegurar a respectiva correcção. Para o efeito fazer-se-á uso da linguagem Jasmin e do sistema de provas Easycrypt."
  },
  {
    "keywords": [
      "API",
      "Software Engineering",
      "Software Modeling",
      "Requirements Engineering",
      "Portal de APIs",
      "Engenharia de Software",
      "Modelação de Software",
      "Engenharia de Requisitos",
      "API Portal",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Conceção e desenvolvimento de uma plataforma para a democratização de APIs",
    "autor": "Coutinho, André Rodrigues",
    "data": "2022-04-26",
    "abstract": "Numa era em que tudo está interligado, as organizações que mais antecipadamente adotam estraté-gias de conexão entre os seus produtos, dados e consumidores finais, são aquelas que frequentemente conseguem obter uma vantagem competitiva no mercado. A Sonae MC, como presença dominante no mercado português, adotou uma nova evolução de uma Arquitetura Orientada a Serviços, de modo a manter esse destaque. API-led Connectivity visa estabelecer a conexão acima referida, com recurso às APIs previamente existentes, cada uma com um único propósito, facilitando a sua reutilização e modularidade da arquite-tura. Este conceito é simbólico da transformação digital na Sonae MC e surge agora a oportunidade de estruturar, documentar e divulgar todas as APIs existentes no seu portefólio. Sendo assim, esta dissertação pretende expor um modo mais fácil e autónomo de implementação e manutenção, tanto de novos projetos, como daqueles já existentes dentro da empresa. Isto será atingível através de um reforço da governação e visibilidade dos ativos digitais da organização, que terão como face um novo Portal de APIs. A dissertação acompanha a criação deste portal e descreve como todos os conceitos envolvidos dão origem a uma nova e mais acessível forma de adoção de APIs pelas equipas que delas necessitam. Neste processo de criação, são abordadas diversas fases do desenvolvimento do software, nomeadamente o levantamento e modelação dos seus casos de uso e a sua análise, conceção e implementação. É provado que é de facto vantajoso para uma empresa fornecedora de APIs ter um portal para as apresentar e, por fim, são enumeradas formas de como comprovar e aumentar estas vantagens para o caso da Sonae MC."
  },
  {
    "keywords": [
      "Quantum computing",
      "Quantum simulation",
      "Feynman",
      "Computação quântica",
      "Simulação quântica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Feynman path-sum quantum computer simulator",
    "autor": "Ferreira, David Alves Campos",
    "data": "2023-07-11",
    "abstract": "Classical quantum simulators are essential tools for studying quantum systems and\nsimulating quantum algorithms. The hardware limitations of NISQ (Noisy Intermediate Scale Quantum) devices make being able to build, test and run a quantum circuit/algorithm\nmany times, and even test it under various noise scenarios, in a classical computer, extremely\nuseful. Currently, the most prominent technique for classically simulating quantum circuits\nis known as Schrodinger type simulation. The memory usage of simulations using this\ntechnique increases exponentially with the number of qubits in a circuit, reaching prohibitive\nmemory values relatively fast. This serves as motivation to investigate complementary\nclassical quantum simulation techniques. The present work offers an investigation on how\nto improve the runtime of the Feynman path-sum approach for classical simulation of\nquantum circuits, taking into account the computational basis input and output states given\nand the branching structure generated by branching gates in a quantum circuit. The main\ncontributions of this dissertation are two Feynman path-sum based simulation algorithms.\nThese algorithms were able to successfully simulate quantum circuits with a large number of\nqubits (> 30) using polynomial space and it was demonstrated that the time complexity of\nthese algorithms is more strongly influenced by the circuit structure rather than the circuit\nsize."
  },
  {
    "keywords": [
      "Poder disseminação",
      "Fake news",
      "Bots",
      "Redes sociais",
      "Modelos epidemiológicos",
      "Dissemination power",
      "Bot identification",
      "Social networks",
      "Epidemiological models",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Cálculo do poder de disseminação de um utilizador numa rede social online como fator de determinação de risco para a sua privacidade digital",
    "autor": "Miranda, João Lobarinhas Fernandes",
    "data": "2022",
    "abstract": "A presença ubíqua das redes sociais faz com que estas sejam plataformas ideais para a disseminação de notícias, mas quando o conteúdo destas notícias é falso estas podem pôr em risco a privacidade\ndigital de um utilizador. A disseminação de notícias nas redes sociais é principalmente feita com base\nem relações de confiança e confiabilidade entre os utilizadores, estas relações estão então diretamente\nligadas com o poder de disseminação de um utilizador. A partilha e compartilha das notícias nas redes\nsociais é atualmente feita por utilizadores e contas bot, a disseminação das notícias falsas por bots põem\na privacidade digital dos utilizadores das redes sociais em risco.\nDe forma a ter a recolher dados que pudessem ser livremente manipulados foi utilizado a ferramenta\n”FakeNewsNet”, esta permitiu que fosse criado um dataset com os dados recolhidos da rede social Twitter.\nDe forma a melhor compreender as redes de disseminação das notícias, os dados recolhidos foram\naplicados ao ”Community Health Assessment Model”, este modelo é baseado nos modelos epidemiológicos,\ne que permite obter dados sobre a disseminação das notícias nas redes sociais entre utilizadores e\ndentro de comunidade ou ”echo chambers”. Os dados contidos no dataset foram também aplicados ao\n”Botometer”, um modelo supervisionado de deteção de contas bot no Twitter, em que as contas dos disseminadores\nde notícias foram analisadas. Com os resultados obtidos dos modelos aplicados, é medido\no impacto das contas bot na disseminação das notícias. O impacto das contas bot para a disseminação\ndas notícias é entre 2,9% a 6,9% na disseminação de notícias dentro de comunidades e nas notícias falsas\ncontribuem entre 2,8% a 0,7% na disseminação entre utilizadores, sendo que estas contas correspondem\nentre 0,272% a 0,296% da população que dissemina as notícias. Utilizando os dados dos dataset foi também\nfeita uma análise de como as notícias são disseminadas. A análise foi feita em duas partes, uma\nparte foi dedicada às publicações e as relações que os utilizadores têm com contas bot e a outra parte\nfoi dedicada ao poder de disseminação dos utilizadores. A partir da análise foi identificado que as contas\nbot desempenham um papel maior na disseminação das notícias falsas.\nUtilizando esses dados são propostas medidas de modo a que os utilizadores consigam se proteger\nda influência das contas bot."
  },
  {
    "keywords": [
      "Call-Center",
      "Ciência de dados",
      "CRM",
      "Inteligência artificial",
      "Machine learning",
      "Artificial intelligence",
      "Data science",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Roteamento inteligente de chamadas",
    "autor": "Jorge, Sérgio Tiago Oliveira",
    "data": "2020-12-16",
    "abstract": "Nas empresas de telecomunicações, as centrais de chamadas são os elementos que têm maior\ninteração com os clientes, e o desempenho dos operadores é vital porque um excelente serviço\nsatisfaz o cliente e ajuda a um melhor funcionamento. Deste modo, tenta-se utilizar dados de\nclientes, dados de operadores de chamadas e dados históricos de serviço de forma a melhorar\no suporte. O emparelhamento de um cliente com um operador que se sinta confortável com\no problema a resolver ajuda as empresas a reduzir custos, melhora o atendimento ao cliente e\naumenta a produtividade dos colaboradores.\nNesta dissertação, propõe-se um modelo de previsão, baseado em machine learning e em otimização, que antevê o problema pelo qual o cliente está a ligar e encaminha a chamada e o cliente\npara o operador mais apropriado.\nApós a análise de um dataset não balanceado, com aproximadamente 2.9 milhões de entradas, e recorrendo à metodologia CRISP-DM para modelação, alguns algoritmos inovadores como o\nLightGBM, permitiram obter um micro-F1 de 0.41 e AUC-ROC de 0.84. Deste modo, pode-se aferir\nalguma capacidade na previsão da razão para o cliente estar a contactar a empresa.\nA alocação e otimização num cenário simulado, pós-previsão, indicou uma melhoria na ordem\ndos 50%, relativamente ao ganho de eficiência, eficácia e rapidez do call-center.\nOs resultados mostram que a utilização de grandes quantidades de dados comerciais para a\nprevisão pode melhorar o desempenho do suporte ao cliente. Além disso, as conclusões e as\nestratégias exploradas nesta tese podem levar à utilização deste sistema, e com isso ajudar a\nempresa a obter maiores lucros e podem ajudar futuros investigadores a tomar melhores decisões\nno estudo e no desenvolvimento de soluções parecidas."
  },
  {
    "keywords": [
      "Application security testing",
      "Code mutation",
      "Code refactoring",
      "Teste de segurança de aplicações",
      "Mutação de código",
      "Refatoração de código",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Vulnerabilities preservation using code mutation",
    "autor": "Cruz, Jorge Fernando Alves da",
    "data": "2022-12-19",
    "abstract": "The main goal of software security testing is to assess the security risks of an application so that\nprogrammers can eliminate all vulnerabilities, as early as possible, before they are exploited by attackers.\nThere are several tools on the market that allow to perform these tests during the software development\nlife cycle to ensure that there are no security flaws in the final product. However, like all tools, these can\nalso have imperfections, one of them being unable to detect weaknesses in vulnerable software.\nThe project of this dissertation aims to tackle this problem, so that it is possible to find and correct\nflaws in security tests in order to, consequently, increase the effectiveness of the tools that intend to\ncertify the security of applications. For this, the solution studied in this document is to apply syntactic\ntransformations in vulnerable code samples without interfering in the presence of the vulnerabilities that\nshould later be detected. This process is based on: 𝑖) code refactoring techniques that allow improving\nthe internal quality of the software; 𝑖𝑖) the mutation testing system used to evaluate the quality of software\ntesting.\nTo implement this idea, a tool called VSG was developed with the functionality of producing new\ncode samples with security flaws. This document describes the whole development process, from the\narchitecture to the implementation of the tool. In the end, there is an analysis with the results obtained\nwhen trying to detect the vulnerabilities present in the samples produced through the CxSAST application\nof the company Checkmarx, from which this dissertation emerged."
  },
  {
    "keywords": [
      "Text editor",
      "Online annotation tool",
      "Markup system",
      "Inappropriate social discourse",
      "Editor de texto",
      "Ferramenta de anotação online",
      "Sistema de marcação",
      "Discurso social impróprio",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "NetLangEd, an editor to support comment analysis",
    "autor": "Rodrigues, Rui Pedro Barbosa",
    "data": "2022-03-03",
    "abstract": "This document formally reports a M.Sc. Thesis project needed to obtain the Master’s degree\nin Informatics Engineering, focusing on the scientific areas of Digital Humanities, Social\nNetworks and Inappropriate Social Discourse. The Master’s work here presented was\naccomplished at Universidade do Minho in Braga.\nThe main objective of the referred Master’s project was the development of an online editor\nthat allows researchers to add their reflections and ideas to short sentences (usually called\n’comments’) that belong to a social dialogue triggered by a ’post’ on a social network or a\n’news’ on social media. Those comments, to be analyzed by linguists or social science experts,\nare provided online and are extracted from the corpus created under the international project\n– NetLang. NetLangEd, the editor developed and here reported, is mainly a tool to allow\nthe analysts to create their own notes to be associated in the right place of each comment\nwhile reading it. Basically, NetLangEd allows to highlight a multi-word term contained in\nthe comment, using a color chosen by the user, and associate to that term an ’annotation’.\nAn annotation is composed of two parts, a tag (also created and picked up at the user\nchoice) and a text explaining the user idea. To make this ’annotation’ process truly dynamic,\nNetLangEd provides, through a simple and user-friendly set of menus, three basic operations\nfor adding, editing and removing annotations. Additionally, the editor also provides easy\nto use mechanisms to manage the tags so far created, as well as to view and locate the\nannotations.\nThis Master’s dissertation also describes how NetLangEd was tested for usefulness and\nusability. For that purpose, an experiment was designed and conducted with end-users. The\nresults will be presented and discussed."
  },
  {
    "keywords": [
      "Quantum computing",
      "Quantum error correction",
      "Stabilizer codes",
      "IBMQ",
      "Computação quântica",
      "Correcção de erros quânticos",
      "Códigos estabilizadores",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Quantum error-correcting codes",
    "autor": "Pereira, Jose Eduardo Moreira Barros",
    "data": "2022-10-17",
    "abstract": "Quantum computing is a new and exciting field of research that, using the properties of\nquantum mechanics, has the potential to be a disruptive technology, being able to per form certain computations faster than any classical computer, such as Shor’s factorization\nalgorithm and Grover’s algorithm. Although there are several quantum computer with\ndifferent underlying technologies, one of the main challenges of quantum computation is\nthe occurence of errors, destroying the information and making computation impossible.\nErrors may have several different sources namely, thermal noise, faulty gates or incorrect\nmeasurements. The present dissertation aims to study and employ methods for reducing\nthe effects of errors during quantum computation and correct them using Stabilizer Codes,\nwhich are a very powerful tool to produce circuit encoding networks that can, in theory,\nprotect quantum systems from errors during transmission. A proof of concept algorithm\nwas implemented using Qiskit, a Python based program development language for the\nIBM Q machines, and tested on both simulators and real systems. The algorithm is capable\nof, given any stabilizer in standard form, generate the circuit encoding network. Due to\ntechnological limitations associated with current quantum computers the results obtained in\nibmq_guadalupe fail to show the efficacy of Stabilizer Codes."
  },
  {
    "keywords": [
      "Metabolomics",
      "Machine learning",
      "Univariate analysis",
      "Multivariate analysis",
      "681.3",
      "57"
    ],
    "titulo": "Development of an integrated computational platform for metabolomics data analysis and knowledge extraction",
    "autor": "Costa, Christopher Borges",
    "data": "2014-12-18",
    "abstract": "In the last few years, biological and biomedical research has been generating a large amount of quantitative data, given the surge of high-throughput techniques that are able to quantify different types of molecules in the cell. While transcriptomics and proteomics, which measure gene expression and amounts of proteins respectively, are the most mature, metabolomics, the quantification of small compounds, has been emerging in the last years as an advantageous alternative in many applications.\nAs it happens with other omics data, metabolomics brings important challenges regarding the capability of extracting relevant knowledge from typically large amounts of data. To respond to these challenges, an integrated computational platform for metabolomics data analysis and knowledge extraction was created to facilitate the use of several methods of visualization, data analysis and data mining.\nIn the first stage of the project, a state of the art analysis was conducted to assess the existing methods and computational tools in the field and what was missing or was difficult to use for a common user without computational expertise. This step helped to figure out which strategies to adopt and the main functionalities which were important to develop in the software. As a supporting framework, R was chosen given the easiness of creating and documenting data analysis scripts and the possibility of developing new packages adding new functions, while taking advantage of the numerous resources created by the vibrant R community.\nSo, the next step was to develop an R package with an integrated set of functions that would allow to conduct a metabolomics data analysis pipeline, with reduced effort, allowing to explore the data, apply different data analysis methods and visualize their results, in this way supporting the extraction of relevant knowledge from metabolomics data.\nRegarding data analysis, the package includes functions for data loading from different formats and pre-processing, as well as different methods for univariate and multivariate data analysis, including t-tests, analysis of variance, correlations, principal component analysis and clustering. Also, it includes a large set of methods for machine learning with distinct models for classification and regression, as well as feature selection methods. The package supports the analysis of metabolomics data from infrared, ultra violet visible and nuclear magnetic resonance spectroscopies.\nThe package has been validated on real examples, considering three case studies, including the analysis of data from natural products including bees propolis and cassava, as well as metabolomics data from cancer patients. \nEach of these data were analyzed using the developed package with different pipelines of analysis and HTML reports that include both analysis scripts and their results, were generated using the documentation features provided by the package."
  },
  {
    "keywords": [],
    "titulo": "Contributions for building a Corpora-Flow system",
    "autor": "Santos, André Fernandes",
    "data": "2011-12-09",
    "abstract": "Os corpora textuais são um recurso importante no processamento de linguagem natural e em áreas relacionadas, tais como a mineração de textos biomédicos, a linguística de corpus, aprendizagem máquina e recuperação de informação.\n\nA preparação de documentos para inclusão num corpus envolve vários passos distintos e uma rede complexa de dependências e condições, que resulta num fluxo difícil de gerir manualmente.\n\nEsta dissertação foca-se nos diversos desafios encontrados no processo de construção de corpora, e propõe métodos para ultrapassar essas questões.\n\nO primeiro problema abordado foi a limpeza de documentos de texto –remoção de resíduos estruturais, normalização de formatos e notações e deteção de delimitadores de secção– tornando os documentos passíveis de serem processados.\n\nOutra questão abordada foi a deteção de documentos duplicados e de pares de documentos candidatos a alinhamento, tendo sido introduzido e implementado um método para medição da similaridade entre documentos.\n\nPosteriormente, introduziu-se o conceito de sincronização de documentos, seguido da descrição de uma implementação baseada nos delimitadores de secção.\n\nDois casos de estudo reais foram utilizados para guiar a implementação das ferramentas desenvolvidas: alinhamento multi-língua de documentos para inclusão em corpora paralelos alinhados e a construção de corpora de textos biomédicos para mineração de texto.\n\nUm protótipo de um sistema de gestão da construção de corpora foi desenvolvido – um sistema de corpora-flow. Este sistema incorpora mecanismos que facilitam a implementação do fluxo necessário para a construção de um corpus.\n\nUma avaliação comparativa do conjunto de ferramentas desenvolvido foi realizada através do alinhamento de documentos com e sem a intervenção das ferramentas desenvolvidas. Um pequeno conjunto de ferramentas foi desenvolvido para avaliar os resultados de alinhamentos."
  },
  {
    "keywords": [
      "Instituições de saúde",
      "Tomada de decisões",
      "Sistemas de informação em saúde",
      "Agentes inteligentes",
      "Business intelligence",
      "Plataforma web",
      "Monitorização",
      "Inovações tecnológicas",
      "Interoperabilidade",
      "Health organizations",
      "Health information systems",
      "Intelligent agents",
      "Web platform",
      "Monitoring",
      "Technological advances",
      "Interoperability",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Monitorização inteligente da interoperabilidade em ambiente hospitalar",
    "autor": "Sousa, Ana Regina Coelho de",
    "data": "2019-09-30",
    "abstract": "Despite the advances made in recent decades, information technologies still have a lot\nto offer to the health sector. In the scientific community, the idea of technology as a\nvery important part of improving healthcare is already unanimous. Health institutions\nare increasingly willing to invest in technologies that support the daily activities life of\nhealth professionals, especially at the time of decision making so that it is as fast and\nas accurate as possible.\nThus, the number of hospital information systems has increased, especially the sys tems supporting and easing the diagnosis, treatment and follow-up of the patient.\nHowever, these systems are also necessary for decision-making on the part of the ins titutions computer processes, such as intelligent agents, which indirectly influence the\nquality of the services provided. Of all the choices that have been made within the\ndeveloped systems, the use of BI has proved quite effective in the presentation of infor mation as well as in the construction of decision support systems. As a consequence,\nthis dissertation project aims to develop a BI platform to continuously monitor the\nintelligent agents of the CHP as well as their activities.\nThe current digital revolution brings several challenges, the constant emergence of\ninnovative technologies often put at stake the work done due to the eventual obsoles cence that it can present in comparison with those that are built with these innovations.\nThus, health institutions, should always be alert and keep an eye on their information\nsystems, evaluating whether they have become archaic or even if there are new soluti ons that respond better to the problems they have daily at hand. In this way, they will\nbe updated at technological level and competitive at market level since, inevitably, the\nquality of the provided services improves significantly.\nWith all these innovations new problems arise, hospital units are increasingly com plex environments at the level of computer systems due to their heterogeneity. The\ninformation generated and stored in each system has characteristics and structures\nthat can be quite different, which causes the information to be individualized. In this\nway, the main issue addressed in this dissertation emerges, the interoperability. , the\ninteroperability. To answer all these challenges, AIDA was created, a system based on intelligent agents that aim to implement interoperability in health institutions. The in telligent agents have tasks of various types, but have the similarity of communicating\nwith heterogeneous systems in order to exchange information of great importance or\neven manage and store information in databases. Therefore, the need to monitor these\nagents as well as their activities arises in order to maintain the interoperability and\nquality of the services provided by the institution where they are implemented. Thus,\nthis dissertation aims to developa platform that monitors continuously and in real time\nthe agents of the CHP.\nThis project was based on the DSR methodology, that initially defines the problem\nfor which one intends to design a solution, outlining the objectives to be achieved. The\nremaining phases deal with the development and evaluation of the developed solution.\nAs proof of concept, the SWOT analysis and the technology acceptance study based\non TAM3 were chosen. The results of the proof of concept were quite positive and\nrevealed an excellent growth potential for the developed solution."
  },
  {
    "keywords": [
      "Traffic Analysis",
      "Internet Traffic",
      "Data Usage",
      "Youtube",
      "Análise de tráfego",
      "Tráfego Internet",
      "Utilização de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Caracterização de tráfego e desempenho em dispositivos móveis: Web e aplicacional",
    "autor": "Areal, Nuno Gabriel da Silva",
    "data": "2019-12-13",
    "abstract": "Mobile devices have evolved in a continuous and steady way both in terms of computing\ncapacity and communication capabilities. This evolution is also reflected in the level of\noperating systems and apps developed for the most diverse areas of activity and interest.\nHowever, the mobile data plan negotiated with network providers as well as the capacity of\nthe batteries continue to be limitations, implying an efficient management of these resour ces.\nAlthough monitoring of data usage and computational resources already exists in most\nmobile devices, it is not clear what are the differences between the available platforms, web\nand application, when these offer the same service. Thus, this study intends to develop\na process that details the differences of a service in both platforms, mainly focusing on\nassessing the data involved in its use and the necessary computational resources. This\ncomparative study contributes to assist the end user to elect the most convenient platform\nfor saving resources."
  },
  {
    "keywords": [
      "BDD",
      "Widget",
      "IVY Workbench",
      "MAL interactors",
      "CTL",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Análise de cenários para especificação de interfaces web",
    "autor": "Gonçalves, Rui Alexandre da Costa",
    "data": "2023-12-15",
    "abstract": "O Desenvolvimento Orientado pelo Comportamento (Behaviour-Driven-Development, BDD) é um paradigma de desenvolvimento de software que permite especificar as necessidades dos utilizadores e os seus critérios de aceitação. A especificação de um sistema é feita através da descrição de cenários de utilização que serão depois implementados. Rocha Silva (2022) propôs uma linguagem de especificação de cenários de utilização, para interfaces web, que utiliza os widgets da própria interface na especificação, permitindo não só especificar os requisitos do sistema, mas também referir como estes deverão ser implementados. No processo de BDD podem surgir na especificação cenários contraditórios que, passando despercebidos, podem levar a que a especificação tenha de ser revista na fase de implementação. Todo este processo acarreta custos, pelo que surgiu a necessidade de criar um método de determinar se uma interface está especificada sem quaisquer contradições (isto é, se a especificação é ou não consistente\nna descrição do sistema). O objetivo deste trabalho é então definir um método que permite, para uma interface web especificada na linguagem proposta por Rocha Silva (2022), determinar se a especificação apresenta ou não contradições (ou seja, é inconsistente) nos seus cenários. Para cumprir este objetivo será utilizada a ferramenta IVY workbench que permite analisar, de forma automática, modelos escritos em MAL interactors do comportamento de sistemas interativos (permitindo verificar propriedades sobre estes). O primeiro passo do projeto será, então, desenvolver uma ferramenta (o modelador) capaz de traduzir uma interface web\nespecificada na linguagem proposta por Rocha Silva (2022) para MAL interactors, de modo a que esta possa ser analisada na ferramenta IVY Workbench. Depois, serão ainda apresentadas as propriedades CTL que serão verificadas na ferramenta IVY Workbench para determinar se uma determinada especificação é consistente. Por fim, será apresentado um método de, utilizando a ferramenta IVY Workbench e o modelo em MAL interactors gerado pelo modelador com as devidas propriedades CTL, determinar não só se a especificação é inconsistente mas, caso o seja, os cenários que dão origem a essa inconsistência."
  },
  {
    "keywords": [
      "681.3:57",
      "57:681.3"
    ],
    "titulo": "Development of a computational platform for the visualization of metabolic models",
    "autor": "Noronha, Alberto Miguel Silva",
    "data": "2013-12-03",
    "abstract": "The recent sequencing techniques and omics approaches are generating huge amounts of data that can provide ways to extract meaningful knowledge, by resorting to appropriate computational tools. One important technique resorts to the use of genome scale model reconstructions. These models are widely used in Metabolic Engineering, attempting to optimize an organism's functions, genetically modifying it to produce compounds of industrial interest.\n\nAnother area that became widely important within the fields of Systems Biology and Bioinformatics was network analysis and visualization. Networks can provide a way to better understand the relationships between biological entities, by allowing their visual representation. However, biological networks usually comprise a large number of entities and interactions, that cannot be easily interpreted by the human eye. \n\nIntegrating visualization and analysis is, therefore, a goal of high interest in several scientific areas, and this has been tackled by several visualization tools available. However, regarding the integration of metabolic engineering techniques with metabolic network visualization, there are still few examples of success. Usually, it is necessary to use more than one tool and the agility of the methods is limited.\n\nIn this work, a metabolic network visualization framework is presented, with the goal of being a tool that will help researchers in metabolic engineering projects. This framework is divided in two layers: the first deals with the importation and exportation of networks in different formats, while the other layer provides all the visualization and edition features. \n\nA metabolic layout is based on the reactions contained in the metabolic model, and it can represent just a part of the metabolism of an organism. To have the possibility to use the same layout in different models, a strategy was defined to map the entities of the visualization with the entities of the model. \n\nThe layouts are displayed in a bipartite graph, with different node types and colors. It is possible to visualize additional information of the network by clicking the nodes. Some of the features include dragging, zooming and highlighting. On top of all this, it is also possible to apply filters and overlap information over these networks. The filters can change what is visible in the network, while the overlaps allow defining new labels, colors and shapes to the nodes, and new colors and thickness to the edges. Finally, the framework was also integrated within OptFlux, an open-source software to support metabolic engineering available at www.optflux.org, to provide a connection between visualization and metabolic simulation methods."
  },
  {
    "keywords": [
      "React native",
      "Node.js",
      "MHealth",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aplicação móvel inovadora de apoio ao planeamento e administração de medicação em lares",
    "autor": "Perneta, Cesário Miguel Pereira",
    "data": "2021-02-22",
    "abstract": "Nos dias de hoje, devido à acentuada evolução que a tecnologia tem vindo a sofrer, as aplicações informáticas tornaram-se indispensáveis no nosso dia-a-dia. Inúmeras áreas beneficiam, das mais variadas formas, das aplicações que tem ao seu dispor e a área da saúde não foge à regra. No que toca aos lares de idosos, tendo em conta que estes estão cada vez mais lotados, os auxiliares de saúde não têm mãos a medir em relação à saúde dos utentes. No entanto, ainda são usados métodos arcaicos, que passam por manter toda a informação relativa aos tratamentos farmacológicos em formato físico, o que conduz a uma probabilidade de erro humano bastante elevada. Para que seja possível tornar mais eficiente e reduzir a probabilidade de erro nos tratamentos farmacológicos dos utentes nos lares, é essencial que o processo de administração e planeamento destes tratamentos seja agilizado. Desta forma, pretende-se desenvolver uma aplicação que vise minimizar os problemas anteriormente mencionados, assegurando assim informação atualizada a todo o instante, históricos fidedignos, controlo sobre stocks de medicação, entre outros."
  },
  {
    "keywords": [
      "Benchmark",
      "BSM2",
      "ETAR",
      "Modelação",
      "Modeling",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Modelação de um problema de otimização em estações de tratamento de águas residuais usando modelos benchmark",
    "autor": "Gonçalves, Carina Filipa Araújo",
    "data": "2023-12-16",
    "abstract": "A procura incessante por água potável devido à sua escassez levou ao interesse crescente na \nrecuperação de recursos em estações de tratamento de águas residuais, o que ao desenvolvimento de \nestações de tratamento de águas residuais (ETAR) como instalações de reciclagem de recursos hídricos.\nA modelação matemática tem vindo a ser extremamente importante na implementação, operação e \notimização desses processos. A introdução de modelos padrão, incluindo várias subunidades, permitiu \numa avaliação objetiva do desempenho das estratégias de controle por simulação de maneira a \ncombater problemas operacionais que geram perdas económicas e ambientais devido ao desequilíbrio \nnas populações microbianas.\nAo longo do tempo foram propostas muitas estratégias de controlo sendo que têm sido usados softwares\ne benchmarks simulation models para testar e avaliar estratégias, melhorando a operação das ETAR.\nO modelo de simulação de referência número 2 inclui sedimentadores primários, espessadores de \nlamas, desaguamento e tratamento de lamas com digestão anaeróbia, ampliando o alcance em \ncomparação com os anteriores, que se focavam principalmente nas lamas ativadas e no sedimentador \nsecundário. Esses avanços são fundamentais para otimizar o tratamento de águas residuais e superar \ndesafios microbiológicos.\nEste projeto teve como objetivo principal criar um problema de otimização podendo mais tarde ser \nformulado também como um problema multiobjectivo.\nNeste estudo, a abordagem híbrida do algoritmo HGPSAL mostrou-se altamente eficaz ao lidar com um \nmodelo mais complexo, o BSM2, sem ultrapassar limites legais nas variáveis críticas. A inclusão de \nnovas variáveis, como fósforo, oxigénio dissolvido, sódio, SNH (composto de enxofre azotado), pH e SNO\n(composto de azoto nitrado), representou um avanço significativo no tratamento de águas residuais.\nEm suma, este estudo contribui significativamente para a aplicação de algoritmos de otimização em \nestações de tratamento de águas residuais, oferecendo soluções adaptáveis e eficazes. Os resultados \nobtidos representam um avanço importante em direção a soluções mais económicas e sustentáveis no \ntratamento de águas residuais, com potencial para impactar positivamente a gestão hídrica e ambiental, \ndestacando a importância da recuperação de recursos, a complexidade microbiológica e a necessidade \nde estratégias inovadoras de controle, bem como o uso de modelos de benchmark para avaliação de \ndesempenho."
  },
  {
    "keywords": [
      "Data science",
      "Data science professionals",
      "Empirical studies",
      "Interviews",
      "Survey",
      "Ciência de dados",
      "Entrevistas",
      "Estudos empíricos",
      "Inquérito",
      "Profissionais de ciência de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Characterizing data scientists in the real world",
    "autor": "Pereira, Paula Sofia da Cunha",
    "data": "2022",
    "abstract": "Every day, data is being collected from all different types of sources. According to the company Domo, data is being collected from ad clicks, likes on social media, shares, transactions, streaming content, and so much more. Their study, which focused on the\ndata generated on the most popular platforms in 2020, shows that, every minute of the day,\nusers sent 12M instant messages, shared 65K photos on Instagram, and conducted 5.7M of\nsearches on Google. Moreover, accordingly to Statista, by 2025, the volume of data created,\ncaptured, copied, and consumed worldwide will increase up to 180 zettabytes.\nThis enormous amount of data in itself may not be relevant. The real value of data lies in\nthe information it hides about individuals and the world. As a result, it is more crucial than\never for businesses of all sizes to focus on the data they collect from diverse sources and\nuse the insights they gain to become more competitive in their fields of expertise. In this\nscenario, companies rely on recruiting professionals to join data science teams capable of\ngleaning insights and extracting value from data.\nData science, as the name implies, can be seen as the science that studies data. It is a\nmultidisciplinary field where professionals, commonly known as data scientists, transform\ndata into insights and decisions. Several researchers have focused on data science, intending\nto explain it and demonstrate its value in several contexts. However, in this research study,\nwe shifted the focus to those who practice data science.\nThis work aims to take advantage of the information collected through interviews and a\npublic survey to fully understand who is doing data science, how they work, what skills\nthey hold and lack, and which tools they need. Based on the results, we argue that the\nacademic past of data science professionals has little impact on the way they work and that\nthe most difficult challenges they face are obtaining high-quality data and applying deep\nlearning techniques. We also discovered evidence of a gender gap in data science, which the\nscientific community should address in order to make data science accessible to everyone."
  },
  {
    "keywords": [
      "Healthcare",
      "Digital health",
      "Data Science",
      "Big Data",
      "Artificial Intelligence",
      "Data visualisation solutions",
      "Health Information Systems",
      "Data-driven solutions in healthcare",
      "Cuidados de saúde",
      "Saúde digital",
      "Ciência de dados",
      "Inteligência Artificial",
      "Soluções de visualização de dados",
      "Sistemas de informação de saúde",
      "Soluções baseadas em dados nos cuidados de saúde",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Health BlueBoard: clinical cockpit to support the healthcare professional",
    "autor": "Coelho, Mariana Carvalho",
    "data": "2024-09-19",
    "abstract": "In today's world, data is a ubiquitous concept, and the healthcare ecosystem is no exception, as healthcare organisations are inundated with data that is key to the quality of care. In recent years, the digitalisation of healthcare has changed the way this complex system operates, and it is possible to see the potential for new innovative solutions that apply new insights from Big Data, Data Science and Artificial Intelligence to revolutionise healthcare as a whole. The implementation of digital solutions in healthcare can provide new insights from data and be an extremely helpful tool to improve care and population health, leading to increased clinical efficiency and effectiveness, improved cost and resource containment for the healthcare system, and consequently improved professional satisfaction and patient experience. Thus, data science can be a powerful and impactful tool in the healthcare ecosystem, as efforts need to be made in data exploration and visualisation solutions that are appropriate for healthcare professionals to facilitate their decision-making processes. The main objective of this thesis is to use data modelling methods to create a user-friendly data visualisation dashboard that is suitable for specific end users, in this case doctors, to assist them in task and time management and thus healthcare decision making. It will also highlight the potential positive impact of data science in healthcare and the importance of developing data visualisation solutions that are appropriate for healthcare professionals and able to combine the data and information they need on a single viewing platform."
  },
  {
    "keywords": [
      "Spreadsheet",
      "Model-driven",
      "Error",
      "Automatic",
      "Evolution",
      "681.3.06",
      "519.863"
    ],
    "titulo": "Directed evolution of model-driven spreadsheets",
    "autor": "Silva, André António dos Santos da",
    "data": "2013",
    "abstract": "Spreadsheets are among the most used programming languages today. The easy to use\nand the intuitive nature of the visual interface makes them a preferred programming tool\nfor any kind of individual or organization. The flexibility they provide to organize data as\nusers need to is one of the reasons that makes them so popular. However, this flexibility\nalso makes them very error-prone.\nIn order to improve spreadsheet quality and reduce the number of errors, software engineering\npractices were introduced, namely object oriented and model-driven techniques.\nThese techniques enabled the specification of the spreadsheet business logic, which offers\nthe possibility to better structure data, while at the same time narrowing the range of types\nof errors made by user input. While these developments had a huge impact, spreadsheet\nevolution is still an inherently human process, which is in itself error-prone.\nIn many real world applications of spreadsheets, they are used to store and disseminate\ndata between different systems. Different systems can use different data formats, this leads\nto the need to change and adapt the data produced by a source system so that it complies\nto the data format consumed by a target system. Usually in these cases, both the initial\nand final data models are known in advance.\nThe objective of this thesis is to present techniques that enable data evolution to be made\nautomatically, using model-driven spreadsheets."
  },
  {
    "keywords": [
      "Urgência",
      "Saúde",
      "Afluência",
      "Doentes",
      "Agilizar",
      "Tempo de espera",
      "Tecnológica",
      "Protótipo",
      "Sugestões",
      "Recomendações",
      "Suporte à decisão",
      "Emergency",
      "Healthcare",
      "Attendance",
      "Patients",
      "Expedite",
      "Waiting time",
      "Technological",
      "Prototype",
      "Suggestions",
      "Recommendations",
      "Decision support",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Protótipo de um sistema de apoio à decisão clínica na urgência: caso de estudo exploratório",
    "autor": "Ferreira, Marco António Álvares",
    "data": "2023-12-28",
    "abstract": "O serviço de urgência é uma das áreas hospitalares com maior afluência, onde a procura e o grau\nde complexidade são elevados e imprevisíveis. Para além disso, o acesso é irrestrito e as exigências são\ncrescentes, assim como a necessidade de gestão de recursos para evitar o colapso das instituições e\ndiminuir os tempos de espera excessivos, que são das consequências mais preocupantes na área da\nsaúde.\nEste projeto surge da oportunidade de estágio e proposta de tema de dissertação/projeto académico,\nna empresa de consultoria tecnológica para a área da saúde, Glintt HealthCare Solutions, SA. O principal\nobjetivo, é o estudo teórico e desenvolvimento de um protótipo funcional de um sistema, tendo em vista\na realização de recomendações/sugestões aos profissionais de saúde a nível hospitalar, mais concreta mente nos Serviços de Urgência. Este sistema irá ter em conta, informação de utentes, como historial\nmédico e estado de saúde (p.e. doenças crónicas, medicação, informações recolhidas na triagem, alergias\nconhecidas, etc.).\nDe forma mais específica, a solução tecnológica proposta para dar resposta ao problema e contexto\nacima referido, contém várias etapas. Num primeiro momento, é realizada uma investigação relativa\naos softwares de urgência já existentes nos hospitais em diferentes contextos(nacional e internacional).\nSeguidamente, perceber, no panorama português, onde se encontram os dados relevantes em contexto\nde urgência, para que possam ser recolhidos e posteriormente utilizados. Numa etapa mais intermédia\ne após a definição dos requisitos essenciais, pretende-se a definição e conceção de uma arquitetura\ninteroperável, que englobe os mesmos. É nesta fase que se procede à análise das abordagens a integrar\nna arquitetura, bem como ao estudo do funcionamento do motor de inferência, responsável por gerar as\nrecomendações e sugestões destinadas a apoiar a tomada de decisão por parte do utilizador. A ideia,\nnuma fase final, é que a arquitetura definida, seja implementada e posteriormente sujeita a avaliação e\nvalidação, por parte dos profissionais ligados à empresa.\nDeste modo, este projeto visa otimizar a gestão de recursos na saúde, agilizando o atendimento e\nmelhorando a eficiência, beneficiando tanto pacientes, como profissionais de saúde."
  },
  {
    "keywords": [
      "Processamento na Nuvem",
      "Internet das Coisas",
      "Aprendizagem automática",
      "Software como um serviço",
      "Sondagem de Wifi",
      "Cloud",
      "Internet of things",
      "Machine Learning",
      "Software as a service",
      "WiFi Probing",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Cloud-based IoT as a service",
    "autor": "Martins, Bruno Manuel Chaves",
    "data": "2022-02-16",
    "abstract": "Internet of Things (IoT), está-se a tornar cada vez mais parte das nossas vidas e, quando aliada a\ncomputação na Cloud, torna-se uma ferramenta muito poderosa devido a remover stress computacional\nde pequenas placas ineficientes. Software como um serviço já representa uma grande parte do nosso\ndia-a-dia com empresas como a Netflix a aplicar o conceito de forma muito bem-sucedida. No contexto\nde IoT, este conceito ainda não está globalmente disseminado.\nA natureza heterogénea dos dispositivos representa um grande desafio para os conseguir integrar num\nsistema na Cloud. Os diferentes formatos e tipos de dados enviados para um middleware são difíceis de\nprocessar e, como consequência, leva a que exista uma grande pressão no programador para que todos\nos sensores sejam suportados.\nAo longo deste estudo são exploradas variadas arquiteturas de forma a ser possível desenhar um\nsistema eficiente e, como os diferentes protocolos de comunicação afetam a rede em termos de overhead\ne fiabilidade. O sistema concebido, baseado em toda o estudo realizado, consiste numa aplicação para\nsalas inteligentes que infere quantas pessoas estão lá dentro através de Probes de WiFi, disponibiliza essa\ninformação a utilizadores e é verificada a possbilidade de utilização de algoritmos de Machine Learning\ncomo forma de optimizar resultados. O sistema desenhado permite aos programadores adicionar outros\ndispositivos sem ter de se preocupar como as mensagens são recebidas, apenas necessitando de adicionar a lógica que extrai o conhecimento dos dados. No que toca à área de crowdsensing deste trabalho,\na precisão do sistema foi melhorada quando comparando com outros algoritmos estudados."
  },
  {
    "keywords": [
      "Cancro da mama",
      "Ultrassom",
      "Deep learning",
      "Multi-task learning",
      "Segmentação de tumores",
      "Classificação de tumores",
      "Computer-aided diagnosis",
      "Breast cancer",
      "Ultrasound",
      "Tumor segmentation",
      "Tumor classification",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Breast tumor segmentation and classification using deep learning methods",
    "autor": "Ferreira, Ana Margarida da Rocha",
    "data": "2023-04-12",
    "abstract": "O cancro da mama é o tipo de cancro mais comum e a principal causa de morte por cancro nas\nmulheres. A deteção atempada é crucial para o sucesso do tratamento e para a redução da taxa de\nmortalidade. Embora várias modalidades de imagem sejam utilizadas para detetar lesões mamárias, a\nultrassonografia tornou-se uma das mais frequentes, uma vez que é segura, portátil, de baixo custo e\npermite uma examinação em tempo real. Deste modo, a segmentação e classificação automáticas de\ntumores em imagens de ultrassom da mama podem auxiliar no seu diagnóstico, proporcionando uma\nsegunda opinião aos especialistas. No entanto, realizar uma segmentação e classificação de lesões\ncom precisão, através desta modalidade de imagem, é desafiante devido à má qualidade de imagem e\nelevada variabilidade das lesões. Recentemente, algoritmos de deep learning têm demonstrado grande\npotencial na área do processamento de imagem, nomeadamente para a segmentação e classificação em\nimagens de ultrassom da mama. Contudo, ainda há necessidade de investigação e melhoria para ser\npossível aplicar com confiança estas abordagens na prática clínica. Adicionalmente, o ultrassom pode\nser utilizado para orientar a agulha durante a biópsia mamária, um procedimento que exige elevado rigor.\nO projecto OncoNavigator, no âmbito do qual esta dissertação foi realizada, visa combinar imagem\nmédica em tempo real com um robô médico colaborativo para melhorar a precisão do rastreio do cancro\nda mama e na biópsia mamária guiada por ultrassom.\nConsiderando a necessidade de métodos automáticos para a segmentação e classificação para melhorar a prática clínica e de modo a aplicar inteligência artificial no robô médico colaborativo, nesta dissertação, foi desenvolvida uma nova multi-task learning network para a segmentação e classificação simultânea de tumores em imagens de ultrassom de mama. O método proposto foi avaliado em 810 imagens\nde ultrassom de dois conjuntos de dados, o BUSI e o UDIAT, tendo obtido um Dice de 80.72%, na seg mentação, e area under the curve de 94.34%, na classificação. Em suma, o método demonstrou ser bem\nsucedido no delineamento e categorização de lesões e revelou potencial para ser incorporado num robô\nmédico colaborativo para intervenções de cancro da mama e para auxiliar no seu diagnóstico."
  },
  {
    "keywords": [
      "Programming languages and grammars",
      "Static analysis",
      "Vulnerabilities",
      "SAST",
      "Language-based tools",
      "Linguagens de programação e gramáticas",
      "Análise estática",
      "Vulnerabilidades",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automatic generation of ASTs from a programming language grammar",
    "autor": "Silva, Pedro Miguel Mimoso Lopes Ferreira da",
    "data": "2023-01-31",
    "abstract": "Tools that detect security problems are very important nowadays and for the people doing\ncode reviews it is even more important to have tools that identify vulnerabilities in the code.\nIn this way companies that provide applications can be more confident that the code they\ndeploy is almost vulnerabilities free.\nA subset of these tools, known as Static Application Security Testing (SAST) tools, rely on\nthe analysis of the source code aiming at looking for patterns that correspond to vulnerabili ties. These analyzers use mainly language processors to help them extract from the source\ncode the information they need. Languages exist for many years but they never ceased to\nexist because they are in constant development.The description of languages is supported by\ngrammars. Grammars also evolve to sustain the referred languages evolution. They were\nprimarily used for compilers to analyse the structure of the language and parse it; nowadays\nthey help many other tools like SAST for example.\nHaving a tool that can detect vulnerabilities is very useful like was said, but to identify\nthose vulnerabilities it is necessary to find patterns in languages. By finding these abstract\npatterns the work is simplified since all concrete languages will present similar vulnerabilities.\nFor example, SQl Injection is a vulnerability that is shared across almost all languages; so,\nit is possible to define one general pattern to capture that common vulnerability in each\nlanguage.\nThose patterns are defined over Abstract Syntax Trees (AST). To build an AST while\nparsing a program, Checkmarx uses a set of functions called Visitors that are associated to the\nproductions of the programming language Grammar. In that context, the Language Factory\ntool developed by Checkmarx generates automatically some visitors and let programmers to\ngenerate the others by dragging and dropping rules. The main objective of Language Factory\ntool is to aid programmers understanding how to create visitors and, at the same time, to\ngenerate as many visitors as possible to be used directly. When Checkmarx is working on\na new Language to to include support for that language in the CxSAST tool, the use of\nLanguage Factory will help creating the appropriate Visitors making this process simpler\nand faster.\nThe Master’s project reported in this dissertation appears in that framework aiming at the\nimprovement of the Checkmarx Language Factory making it capable of infer more Visitors\nfrom the new language’s Grammar."
  },
  {
    "keywords": [
      "Dataframe",
      "Distributed and parallel computing",
      "Performance",
      "Energy consumption",
      "Benchmark",
      "Computação distribuída e paralela",
      "Consumo energético",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Analysis of trade-offs between performance and energy efficiency of scalable dataframes tools",
    "autor": "Martins, André Carvalho da Cunha",
    "data": "2023-11-27",
    "abstract": "Nowadays, we have the ability to trace everything, to extract valuable data from wherever we want, all to\nkeep us connected and to improve our lifestyle. This huge amount of information, produced every day,\nneeds to be treated, manipulated, and analysed, requiring convincing data structures to do so.\nDataframes, regularly used worldwide, are powerful data structures used to analyse and manipulate\ndata of any kind. A Dataframe organizes data into a 2-dimensional table of rows and columns, similar to\nSQL tables or CSV files. Furthermore, it can span alongside thousands of computers or servers, making\nit easier to work with huge amounts of data, called big data, using distributed systems and parallel computing.\nThis Dataframe’s distributed nature led to the rise of distinct scalable and parallel Dataframe tools. The\nmost used Dataframe tool, pandas, only performs on sequential execution and has some limitations when\nthere is the need to handle huge volumes of data, and some tools such as Modin, Polars, RAPIDS, and\nso forth, appeared in order to overcome those limitations. The vast offer of these scalable tools brought\nthe need to make an analysis and comparison between these frameworks and pandas, studying their\nbehaviour and results with different workflows. This comparison is not linear and there is a need to use\na benchmarking tool, in order to produce a homogeneous and reliable evaluation of the different frameworks.\nTo perform this analysis, we worked with several workflows, manipulating real and synthetically produced\ndata on distributed and parallel environments and on different hardware configurations.\nWe designed and developed a benchmarking tool that supports a set of Dataframe frameworks, is flexible\nto the addition of new frameworks, and is able to perform micro-benchmarking evaluation with the analysis\nof a group of individual and common operations used on data science, and macro-benchmarking evaluation with the analysis of workflows that represent a set of chained operations. Both of these evaluations\naggregate performance and energy consumption results for each framework."
  },
  {
    "keywords": [
      "579.25:681.3",
      "681.3:579.25"
    ],
    "titulo": "Taxonomic and functional analysis of metagenomes",
    "autor": "Barbosa, Pedro Santos",
    "data": "2013",
    "abstract": "Over the years, metagenomics has demonstrated to play an essential role on\nthe study of the microorganisms that live in microbial communities, particularly\nthose who inhabit the human body. Several bioinformatic tools and\npipelines have been developed, but usually they only address one question:\n\"Who is there?\" or \"What are they doing?\".\nThis work aimed to develop a computational framework to answer the\ntwo questions simultaneously, that is, perform a taxonomic and functional\nanalysis of microbial communities. Merlin, a previously developed software\ndesigned for the construction of genome-scale metabolic models for single\norganisms, was extended to deal with metagenomics data. It has an userfriendly\nand intuitive interface, not requiring command-line knowledge and\nfurther libraries dependencies or installation, as many other tools.\nThe extended version of Merlin can predict the taxonomic composition\nof an environmental sample based on the results of homology searches, where\nthe proportions of phyla and genera present are discriminated. Regarding\nthe metabolic analysis, it allows to identify which enzymes are present and\ncalculate their abundance, as well as to  nd out which metabolic pathways\nare e ectively present.\nThe performance of the tool was evaluated with samples from the Human\nMicrobiome Project, particularly from the saliva. The taxonomic membership\npredicted in Merlin was in agreement with other tools, despite some\ndi erences in the proportions. The functional characterization showed a conserved\npool of pathways through di erent samples, although Merlin sometimes\npresented less pathways than expected because the routine is highly\ndependent on the enzymes annotation. Overall, the results showed the same\npattern as reported before: while the pathways needed for microbial life remain\nrelatively stable, the community composition varies extensively among\nindividuals.\nIn the end, Merlin demonstrated to be a reliable standalone alternative\nto web services for those scientists that have concerns about sharing data."
  },
  {
    "keywords": [
      "Aprendizagem máquina",
      "Mineração de textos",
      "Processamento de Linguagem Natural (PLN)",
      "Sistemas de georreferenciação",
      "Livro das Propriedades",
      "Tombo da Mitra",
      "Machine Learning",
      "Text mining",
      "Natural Languague Processing (NLP)",
      "Geo-referencing Systems",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Georreferenciação de conteúdos de bases de dados documentais",
    "autor": "Gomes, João Pedro Carvalho",
    "data": "2021-10-27",
    "abstract": "A georreferenciação é o processo de localização geográfica de um determinado objeto espacial através da atribuição de coordenadas. Os sistemas de georreferenciação utilizam um\nprocessamento espacial automático executado por computador, por exemplo para colocar\numa entidade num mapa ou fornecer um recurso espacial. Quando este processo é aplicado\na coleções de documentos textuais, é descrito como uma combinação de reconhecimento de\nentidades nomeadas. O Livro das Propriedades, também designado como o Tombo da Mitra,\ncontém informação relativa aos tipos de terras, acidentes de terreno, nomes de ruas, proprietários e apontamentos biográficos e genealógicos das várias propriedades que a mesa\nArcebispal de Braga possuía no século XVII. Este trabalho de dissertação teve como objetivo\nconceber e implementar um sistema de georreferenciação textual para o conteúdo existente\nno Livro das Propriedades, com particular enfoque nos lugares que nele estão referidos, de\nforma a permitir aos estudiosos destes conteúdos possuírem informação acerca da localização geográfica desses elementos."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "On quantum bayesian networks",
    "autor": "Oliveira, Michael de",
    "data": "2019-12-03",
    "abstract": "As a compact representation of joint probability distributions over a dependence graph of\nrandom variables, and a tool for modeling and reasoning in the presence of uncertainty,\nBayesian networks are becoming increasingly relevant both for natural and social sciences,\nfor example, to combine domain knowledge, capture causal relationships, or learn from in complete datasets. Known as an NP- hard problem in a classical setting, Bayesian inference\npops up as a class of algorithms worth to explore in a quantum framework.\nThe present dissertation explores this research field and extends the previous algorithm\nby embedding them in decision-making processes. In this regard, several attempts were\nmade in order to find new and enhanced ways to deal with these processes. In a first at tempt, the quantum device was considered to run a subprocess of the decision-making pro cess, resulting in a quadratic speed-up for that subprocess. Afterward, “decision-networks”\nwere taken into account and allowed a fully quantum implementation of a decision-making\nprocess, benefiting from a quadratic speed-up during the whole process. Lastly, a solution\nwas found. It differs from the existing ones by the judicious use of the utility function in an\nentangled configuration. This algorithm explores the structure of input data to efficiently\ncompute a solution. In addition, for each one of the algorithms developed, their computa tional complexity was determined in order to provide the information necessary to choose\nthe most efficient one for a concrete decision problem.\nA prototype implementation in Qiskit (a Python-based program development language\nfor the IBM Q machines) was developed as a proof-of-concept. If Qiskit offered a simulation\nplatform for the algorithm considered in this dissertation, string diagrams provided the\nverification framework for algorithmic proprieties. Further, string diagrams were studied\nwith the intention to obtain formal proofs about the algorithms developed. This framework\nprovided relevant examples and the proof that two different implementations for the same\nalgorithm are equivalent."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia do Ambiente"
    ],
    "titulo": "Metagenomic analysis of a Nitritation - Anammox reactor: community members and processes",
    "autor": "Silva, Bruna Daniela Azevedo da",
    "data": "2017",
    "abstract": "Nitrogen is a fundamental element for all organisms. It is nevertheless predominantly\nfound in the atmosphere, in the form of unreactive nitrogen. In the last century, a manmade\nmethod for nitrogen fixation improved the crops yield, fuelling a populational growth.\nThe exponential increase of anthropogenic nitrogen in soils and water bodies has, however,\naffected the environment and deregulated the natural biogeochemical nitrogen cycle. Currently,\nthe costs of repairing the damage caused by the reactive nitrogen load from human\nactivities have overcome the profits of the agricultural improvement, derived from the application\nof fertilizers. Wastewater treatment plants remove the excessive amounts of nutrients\nsuch as carbon, nitrogen and phosphorus from wastewater to prevent environmental\nimpacts derived from excessive nitrogen in the biosphere, like eutrophication.\nThe current conventional wastewater treatment applied is nitrification coupled with denitrification.\nHowever, the requirement for an external carbon source and aeration render\nthis process costly. Furthermore, one of the intermediates of denitrification is nitrous oxide,\na greenhouse gas with an effect three hundred times worse than carbon dioxide and with\na lifespan of one hundred and twenty years in the ozone layer.\nThe Partial Nitrification/Anammox (PNA) process combines aerobic ammonium oxidation\nwith anaerobic ammonium oxidation while suppressing the activity of nitrite oxidizing\nbacteria. This efficient process of nitrogen removal from wastewater reduces the aeration\ncost and the need for external carbon with zero nitrous oxide emissions.\nUnknown microbial interactions may, on the other hand, impair this process, resulting in\nsuboptimal performance such as, excessive nitrate and nitrous oxide emissions. To better\nunderstand the microbial community and its interactions and to find the causes of the treatments\ninstability, metabolic analysis and genomic annotation was performed, using two\ncomplementary binning methods. The biological samples used in this study were retrieved\nfrom a high-rate PNA sequencing batch reactor, fed with carbon-free ammonium-rich synthetic\nwastewater.\nFifty-seven draft genomes making up about eighty percent of the total community metagenome\nwere recovered. In addition to the three genomes each from Nitrosomonas and Candidatus\nBrocadia, several genomes belonged to Proteobacteria, Chloroflexi, Planctomycetes, Bacteroidetes, Armatimonadetes, Ignavibacteriae, Acidobacteria, Chlorobi, Verrucomicrobia, Actinobacteria\nand Gemmatimonadetes phyla.\nIn this study, the heterotrophic organisms encoding partial denitrification could be divided\ninto niches accordingly to their role in this pathway, describing their interactions as\na community. The complexity of the community was also ascertained with the discovery of\nputative heterotrophic hydroxylamine oxidizing bacteria and putative heterotrophic nitrite\noxidizing bacteria.\nOverall, high quality genomes that constitute a high fraction of the metagenome were\nrecovered, allowing for a precise description of the PNA reactors community and the flow\nof nitrogen oxides. A complex community with high redundancy was uncovered basing\nthe main interactions on the partitioning of the nitrogen oxides respiratory pathway."
  },
  {
    "keywords": [
      "Computational thinking",
      "Visual impaired students education",
      "Teaching through music",
      "Pensamento computacional",
      "Educação de estudantes invisuais",
      "Ensino através da música",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Melodic, using music to train visually impaired kids in computational thinking",
    "autor": "Costa, Rui Diogo da Silva",
    "data": "2022-03-03",
    "abstract": "This document, in context of second year of Integrated Master of Informatics Engineering,\nreports the development of a project that intends to teach Computational Thinking to kids with\nspecial educational needs, in this case blindness. The aim of this research is to characterize\nboth subjects, Computational Thinking and Blindness, and identify what are the current most\nused and best practises to teach this different way of thinking to kids with special needs. To\nachieve this, Melodic was created. This is a system composed by a software and a hardware\nwhere the user must create sequences with the tactile blocks (the hardware) and then read\nthem with the mobile application (the software), that converts the sequence created into\nsound. With this, the user can easily hear the differences that the changes in the blocks\nsequence can make. This can be compared to the Computational Thinking teaching through\nthe use of robots, because in that case, users can see the result of their instructions in the\nrobot movement and with Melodic, the user can hear the result of their instruction with the\nmusical note sequence played by the app. In this document more technical aspects such as\nthe architecture of the application that is proposed to accomplish the goal of the present\nMaster’s project, will also be discussed. After this, the project development process that lead\nto the creation of Melodic is described as well as all the decisions taken. A description of\nall functionalities of this system can also be seen in this document. To prove the research\nhypothesis initially stated, some exercises were created and described. The referred exercises\nwere designed to access if Melodic actually develops Computational Thinking."
  },
  {
    "keywords": [
      "Hypatiamat",
      "Micro-services architecture",
      "Data API",
      "Office",
      "Documentation",
      "Interface",
      "Arquitetura micro-serviços",
      "API de dados",
      "Escritório",
      "Documentação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Hypatiamat: API de dados e escritório",
    "autor": "Martins, Luís Guilherme Gonçalves Macedo da Silva",
    "data": "2022-04-01",
    "abstract": "Mathematics learning in basic education is a much debated subject and the difficulties\nexperienced by most students in this area are worrisome. A teacher is concerned with\nquestions about how to attract the attention of students or try to provoke a greater taste\nin most students in this discipline. Therefore, the use of technologies, namely platforms\nthat capture the attention of students and help teachers in the teaching/learning process is\ncurrent and preponderant.\nThus, the Hypatiamat, which is a pedagogical web platform that helps basic mathematics\nteaching, it allows games and content applications depending on the school year and\nwith levels of difficulty that will increase over time, allowing students to evolve in the\ndiscipline. Furthermore, this platform allows teachers to observe the evolution of their\nstudents, resulting in a monitoring of their performance in different games and content\napplications.\nSo, this dissertation intends to evolve the current Hypatiamat architecture from a mo nolithic server to an architecture based on micro-services. Accordingly, a data API will\nbe specified and documented. In addition, an interface server, called the Office, will be\nbuilt, where new features related to project management will be added: more statistics\nabout games and content applications; user management (administrator, teacher, group,\nmunicipality and student); global statistics for an academic year, among other features."
  },
  {
    "keywords": [
      "User interface",
      "Prototype",
      "User-centred design",
      "Widget",
      "Protótipo",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Formal models based interactive prototypes",
    "autor": "Solino, Miguel André da Silva",
    "data": "2022-12-16",
    "abstract": "Since the UI is one of the first contacts that a user has with an interactive system being developed,\nit can include a user flow to be intuitive and effective. For this, prototypes are typically built in the early\nstages of the software development process so that both developers and users might have an early idea\nof what the final product will look like, smoothing development and testing in advance.\nOn critical interactive systems, the standard UI prototyping method is not enough. These systems\nrequire more advanced and detailed analysis methodologies to avoid failures that can lead to accidents.\nFor this, there are approaches based on models and formal analysis methods, complemented by the\nprototyping process, which enhance the process of looking for these usability flaws and support the analysis\nof the interaction made by users.\nThe IVY Workbench is a tool initially focused on modelling and verification, which recently accepted\nthe goal of combining formal model-based analysis with prototype interfaces. This objective is still under\ndevelopment. However, it is already possible to produce prototypes with some level of interaction combining\nmodels and UI mock-ups.\nThis work aims to develop improvements to this tool for these prototyping capabilities. The main\nobjective is to automatically produce a web application capable of offering the expected behaviour and\ninteraction as described in the model.\nThe results of this work will include an analysis of current prototyping methods, problems with these\nmethods and possible solutions; analysis of different prototyping tools where functionalities and characteristics that may be useful for the IVY Workbench will be identified; development of an automatic system\nfor generating web applications based on mock-ups and behaviour models."
  },
  {
    "keywords": [
      "Model-based user interface development",
      "Web development",
      "Web frameworks",
      "Prototyping tools",
      "Automatic code generation",
      "Desenvolvimento de interfaces de utilizador baseado em modelos",
      "Desenvolvimento web",
      "Ferramentas de prototipagem",
      "Frameworks web",
      "Geração automática de código",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Geração automática de interfaces de utilizador para aplicações web",
    "autor": "Machado, Catarina Araújo",
    "data": "2022-06-08",
    "abstract": "The main objective of this dissertation is to make a contribution in the automation of web applications' development, starting from prototypes of their graphical user interlaces. \nThe integration of model-based user interface development concepts with the more traditional user-centred development approach allows for a rethinking of GUI design development, independent of implementation details, and redefining models to realize these graphical interfaces. In the end, the intent is to increase the level of abstraction of the development process, promote better adaptation of applications to different devices and execution environments, and decrease the effort required to develop the graphical interlaces. \nDue to the exponential increase in the use of internet-based services and applications, there is an also increasing demand for Web designers and developers. At the same time, the proliferation of languages, frameworks and libraries illustrates the current state of immaturity of web development technologies. This state of affairs creates difficulties in the development and maintenance of Web applications. \nAn approach is presented that allows designers to use prototyping tools, in this case Adobe XD, to design graphical interfaces, and then automatically converts them to Vue.js + Bootstrap code, thus creating a first version of the implementation. This is done through the interpretation of the SVG file that Adobe XD exports. \nThe goal is not to produce the final version of the Ul. Instead, we aim to produce a first version of the code, which can then be refined by the developer. This enables us to place less requirements on the prototype, regarding the amount of information that it must contain. In the end, we get a skeleton of Vue.js code that is easy to maintain and reuse to further improve the project."
  },
  {
    "keywords": [
      "EEG",
      "ADL",
      "Falls",
      "Artificial Intelligence",
      "Slip-like perturbations",
      "Quedas",
      "Inteligência Artificial",
      "Perturbações do tipo escorregar",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Decoding activities of daily living and incipient falls using brain signals",
    "autor": "Manso, José Pedro Rodrigues",
    "data": "2024-01-08",
    "abstract": "Falls represent one of the biggest causes of deaths related to unintentional injuries. The increasing number\nof occurrences is associated a continuously expanding elderly population, along with its detrimental effects\non the survival and well-being of those aged 65 and above, has turned the issue of falls into a global\npublic health concern. It is estimated that 684,000 people worldwide lose their lives due to falls, which\nhappen approximately 37.3 million times annually. As a result, the financial expenses associated with\nhospitalisations are significant and present a complex challenge.\nThe Electroencephalogram (EEG) technique is widely utilised to assess brain electrical activity and\ndetect indicators of balance disruptions, such as Perturbation Evoked Potentials (PEPs), in brain signals.\nThis is possible because EEG data provides insights into motor planning and intention, making it a valuable\ntool for monitoring both falls and Activities of Daily Living (ADLs). Accordingly, this dissertation will establish\ntwo experimental protocols: one for simulating slip-like incidents and another for ADLs, with the aim of\ncollecting EEG data. The primary goal of this dissertation is to leverage Artificial Intelligence (AI)-based\nsystems to identify slip-like perturbations and various ADLs using the data from both protocols. The ultimate\nobjective is to integrate these algorithms into assistive robotic devices, e.g. exoskeletons.\nIn the context of the methods employed, the PEP components were identified within a time frame\nof 75–137 ms after the external perturbation onset. To analyse the pre-processed EEG data, four distinct\nartificial neural networks were evaluated, each with varying network architecture parameters. Among these\narchitectures, the Convolutional Neural Network (CNN)-Long Short-Term Memory (LSTM) model, trained\nto predict EEG perturbations, exhibited superior classification performance, achieving an accuracy rate of\n86% when using a short time window of 100 ms. In contrast, for classifying ADL, the best result obtained\nwas 53% accuracy, and this was also achieved using the CNN-LSTM architecture."
  },
  {
    "keywords": [
      "Ada",
      "Extração de Modelos",
      "Lógica Temporal",
      "Métodos Formais",
      "Sistemas Críticos",
      "SPIN",
      "Verificação de Programas",
      "Verificação Formal",
      "Verificação de Modelos",
      "Verificação de Modelos de Software",
      "Critical Systems",
      "Formal Methods",
      "Formal Verification",
      "Model Checking",
      "Model Extraction",
      "Software Verification",
      "Software Model Checking",
      "SPIN",
      "Temporal Logic",
      "681.3.06"
    ],
    "titulo": "Formal verification of Ada programs: an approach based on model checking",
    "autor": "Martins, João Pedro Marques da Silva",
    "data": "2011-12-14",
    "abstract": "O rápido crescimento da complexidade dos sistemas de software exige, agora mais do que nunca, uma validação rigorosa dos mesmos por forma a manter ou até mesmo aumentar a confiança nestes sistemas. Em particular nos sistemas críticos, onde as falhas podem ter consequências catastróficas podendo até incluir a perca de várias vidas humanas, é de externa importância o desenvolvimento de técnicas capazes de garantir altos níveis de confiança para estes sistemas.\nNesta tese é proposta a utilização de uma técnica formal para a verificação de programas Ada, que pretende aumentar a confiança em sistemas cuja implementação seja realizada nesta linguagem de programação. Mais precisamente, pretende-se a aplicação da técnica de verificação de modelos para a análise do código fonte de programas concorrentes Ada, com especial foco para o domínio dos sistemas críticos.\nA verificação de modelos é uma técnica bem-sucedida no que diz respeito à garantia de um aumento de fiabilidade destes sistemas. No entanto, a aplicação desta técnica a sistemas de software enfrenta ainda vários obstáculos, e as ferramentas e técnicas para ajudar a ultrapassar estes obstáculos estão ainda a ser desenvolvidas. A ferramenta desenvolvida no contexto desta tese (ATOS) visa responder a problemas como (i) a construção de modelos a partir de programas e (ii) a especificação de propriedades para estes modelos de acordo com as pretendidas para os programas.\nA construção manual de modelos que simulam o comportamento de programas é um processo complexo, temporalmente dispendioso, e sujeito a falhas devido à complexidade destes sistemas. De forma a ultrapassar este problema o ATOS propõe a extração automática de modelos a partir de programas Ada. Por outro lado, o mapeamento das propriedades desejadas dos programas em propriedades dos modelos pode ser urna tarefa com um grau de complexidade elevado, pois requer entre outros a utilização de um formalismo logico ao qual a maioria dos programadores não está acostumada. 0 ATOS ajuda no mapeamento destas propriedades, oferecendo vários mecanismos de suporte à sua especificação."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Exploração e desenvolvimento de soluções interoperáveis em ambiente hospitalar",
    "autor": "Miranda, Filipe Manuel Mota",
    "data": "2016",
    "abstract": "Em contexto hospitalar, elaborar soluções interoperáveis remete para o conceito de Sistemas\nde Informação (SI) e a forma como estes são capazes de cooperar na partilha de\ninformação. Aqueles que apresentam maior afinidade com a informática médica são naturalmente\nos Sistemas de Informação em Saúde (SIS), que constituem um dos pontos mais importantes\ne mais sensíveis no que à prestação de cuidados de saúde diz respeito. A evolução\ndesde o armazenamento de dados no formato de papel (paper-based) até à utilização de sistemas\ninformáticos apresenta inúmeras vantagens. A maior rapidez no acesso a dados, a\nredução de erros (consulta, processamento e apresentação) e o diminuição de perdas de\ninformação encabeçam sem dúvida uma lista bastante extensa de pontos positivos. Em\ncontraponto, na altura em que muitos dos sistemas existentes foram implementados, o conceito\nda partilha de informação não era equacionado, resultando na criação de “ilhas de\ninformação” sem qualquer comunicação com o exterior.\nCom a evolução da Internet e da capacidade de processamento dos sistemas informáticos,\na partilha de dados entre sistemas tornou-se uma constante, sendo vital a questão da interoperabilidade.\nExistem no entanto barreiras à implementação completa dessa ideologia motivado\npor divergências em formato de dados ou compatibilidade entre sistemas. Embora\nmuito já tenha sido estudado e implementado, existem ainda no Serviço Nacional de Saúde\n(SNS) lacunas que só soluções propostas de raiz em prol da interoperabilidade semântica\npodem corrigir.\nO setor da saúde nestes tempos de mudança deve aproveitar exemplos provenientes de\noutras áreas da economia, como por exemplo, a produção em larga escala onde uma nova\nrevolução está em marcha, a Revolução Industrial 4.0. Através da introdução do conceito\nde indústria 4.0 desenvolveram-se Cyber-physical System (CPS), onde dados relativos ao estado\nde uma máquina ou um processo são transmitidos pela rede com destino a unidades\ncentrais de processamento. Estas, através de processos de Extract, Load, Transform (ETL),\ntransformam dados em informação útil acerca do estado da máquina, de uma linha de\nprodução ou de toda ama unidade fabril, fazendo assim a ponte entre o mundo real e o virtual.\nCom esses dados recolhidos em tempo real, as ditas unidades centrais criam as bases\npara sistemas de monitorização, geralmente sob a forma de aplicações Web, permitem uma\ntomada de decisão mais informada por parte dos utilizadores envolvidos.\nOs dispositivos móveis são hoje em dia um filão de negócio a ser explorado pelas unidades\nde saúde numa tentativa de aproximação ao utente e ao mesmo tempo baixando custos\nde comunicação, fomentando assim uma relação de confiança entre ambos. No desenvolvimento de uma aplicação móvel é percetível a forma de como a interoperabilidade é vital\nem sistemas distribuídos. A interoperabilidade física é garantida com utilização de Internet\ne pedidos Hypertext Transfer Protocol (HTTP), a interoperabilidade sintática tira partido de\nestruturas como o JavaScript Object Notation (JSON) ou Extensible Markup Language (XML)\npara envio de dados na resposta aos diversos pedidos. Por último, a interoperabilidade\nsemântica faz com que diferentes layouts sejam apresentados pela aplicação consoante os\ndados recebidos.\nAo longo do presente projeto, desenvolvido no âmbito da dissertação de mestrado em\nEngenharia Biomédica, ramo de Informática Médica foi primeiro realizado um levantamento\nde requisitos com base no estado atual dos SI no Centro Hospitalar do Porto (CHP) e\nmediante os resultados, foram desenvolvidos casos de estudo de forma a perceber qual o\npossível impacto das soluções encontradas. Quatro possíveis áreas de intervenção foram\nidentificadas, a troca de informação seguindo a estrutura proposta pela Health Level Seven\nInternation na versão 2 relativa à troca de mensagens em contexto hospitalar, sistema de\nmonitorização de agentes implementados na unidade de saúde (HL7 e AIDA), formulário\nde codificação International Classification Diseases 9 Revision Clinical Modification (ICD-9-CM)\npara posterior integração num sistema Grupo de Diagnósticos Homogêneos (GDH) e por fim\na comunicação com o utente quando este está fora da unidade de saúde como forma de\nmelhorar a imagem da unidade de saúde.\nComo referido, utilizando as diretrizes Health Level Seven International para troca de mensagens\nna versão 2, foi desenvolvida uma interface do tipo interface engine na linguagem\nPython, que sustenta um “Multi-Agent System” (MAS) para envio e receção de mensagens.\nAproveitando as diretrizes da indústria 4.0, ´e fundamental que estes agentes inteligentes\ncomuniquem a cada instante o seu estado de forma a que os responsáveis conseguirem um\nfeedback do estado atual de todo o sistema.\nNo segundo caso de estudo foi desenvolvida uma plataforma de monitorização com vista\na rastrear o estado de agentes HL7 e AIDA j´a implementados no CHP e ainda daqueles\ndesenvolvidos no caso de estudo anterior. Primeiro foi implementado um processo de ETL\nque alimenta um DataWarehouse (DW) onde a aplicação Web faz queries para recolher dados.\nToda a plataforma foi desenvolvida, com base no conceito MVC com a framework Angularjs.\nO terceiro caso de estudo apresenta uma plataforma de codificação ICD-9-CM numa\nabordagem à interoperabilidade semântica relacionada com representação de diagnósticos\ne procedimentos clínicos. Utilizando também o conceito MVC em Angularjs, a plataforma\nestá inserida num sistema de GDH com objetivo de facilitar o trabalho do codificador,\naumentar a sua produtividade, servir de barómetro ao trabalho desenvolvido e ainda categorizar\nos resumo de altas hospitalares. Com o GDH pode num futuro ser definido todo o\nfinanciamento hospitalar ou até compras realizadas por cada unidade. No último caso de estudo, desenvolveu-se uma aplicação Android do enquadrada no\nconceito de Electronic Health (eHealth) do tipo appointment remInder. O objetivo é aproximar\no paciente da unidade de saúde e evitando que este falhe o seu agendamento poupando em\núltima instância dinheiro e recursos às unidades de saúde. Utilizou-se o design proposto\npela Google para o envio de notificações o serviço Firebase Cloud Message também ele proposto\npela Google. Mais que um lembrete a aplicação regista consultas passadas, futuras e\npermite fazer a confirmação da consulta. Este caso ainda se trata de um protótipo inicial\npara aceitação por parte da unidade de saúde.\nComo métodologia de desenvolvimento em todos os casos de estudo, utiliza-se o cicloDesign\nResearch. Primeiro é definido um requisito/problema que se pretende ver resolvido\ne todas as fases que se sucedem visam encontrar uma possível solução para tal. Como\nprova de conceito, escolheu-se a análise SWOT, situando-se esta, na fase de conclusão do\nciclo DS. Todos os casos de estudo foram submetidos a essa análise apresentado resultados\nconsiderados positivos.\nFica assim vincada a importância da interoperabilidade no desenvolvimento de sistemas\ninteroperáveis, sendo um conceito transversal a todas as soluções propostas, quer na\nfomentação direta da interoperabilidade ou no seu aproveitamento para a disseminação\nde informação. No primeiro caso, um sistema que favorece diretamente a partilha de\ninformação utilizando standards de comunicação. No segundo, o controlo de agentes que\ntrabalham diretamente na partilha de informação garantindo a sua segurança e rápida\natuação em caso de erro. No terceiro caso, uma solução diretamente ligada à interoperabilidade\nsemântica e por último uma aplicação que necessita de interoperabilidade para\nfuncionar na sua plenitude."
  },
  {
    "keywords": [
      "Data analytics",
      "Descriptive analytics",
      "AST",
      "Big Data",
      "Prometheus",
      "Visualizations",
      "Análise de dados",
      "Análise descritiva",
      "Visualizações",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Designing and developing an analytics tool using Prometheus in a company setting",
    "autor": "Cabo, João Paulo Mourão",
    "data": "2023-10-03",
    "abstract": "In the past few years, due to the exponential growth of technology of the world, there has\nbeen an increasing interest in the field of data analytics. This interest comes from how\nimportant data is to everyday life, since data can significantly benefit and influence a lot of\ndifferent areas, from security to health or even sports. Large amounts of data allows analysts\nto predict possible outcomes, evaluate behavioral patterns and study trends.\nThe main goal of this Master’s thesis is to show that a data analytics tool can also be very\nuseful in a company setting, since there is a considerable amount of data, generated by\nthe software tools of the company, that can be collected and used to improve the services\nprovided by the company.\nThe Master’s work here reported begun with a comprehensive study on different kind of\nanalytics tools and also an investigation of the company’s product. After this investigation\nprocess, the next step was to design and implement a custom analytics tool that uses\nPrometheus as a basis and takes into account the requirements of the company. The tool\nrequired the implementation of various processes starting with the collection of the generated\ndata, the transformation and cleaning of that data and finally a process that provides different\ntypes of data visualizations. These visualizations present relevant knowledge to customers\nand also to different departments inside the company.\nWhile the implemented analytics tool met all of the proposed requirements and passed\nthe tests made to verify it, there will be a need to further refine and improve the tool’s\ncapabilities, specifically regarding scalability and data storage.\nThis document provides a better understanding of the use of data analytics in a company\nsetting by showcasing the full process of designing and developing a custom analytics tool\nusing Prometheus. Through this document, it is possible to see that using product generated\ndata and providing data visualizations is beneficial for the company."
  },
  {
    "keywords": [
      "Cloud-computing",
      "Fornecedores de serviços de cloud",
      "Otimização de custos",
      "Cloud service providers",
      "Cost optimization",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Otimização dos custos de operação de aplicações Web em Cloud",
    "autor": "Machado, Diogo Alexandre Gonçalves",
    "data": "2019-12-23",
    "abstract": "O cloud computing tem sido amplamente adotado na área das tecnologias de informação na última década, devido às diversas vantagens que providencia, entre elas a possibilidade de redução de custos com infraestruturas. Embora a utilização da cloud possa minorar os custos de operação de aplicações Web, verifica-se que a definição dos preços praticados pelos fornecedores de serviços tem-se tornado cada vez mais complexa, ameaçando uma das principais razões que leva os utilizadores a migrar para a cloud: a redução de custos. Derivado deste aumento de complexidade, o surgimento de soluções de monitorização e otimização de custos de cloud tem vindo a aumentar por forma a combater este problema. Apesar de existirem algumas soluções capazes de auxiliar na otimização de custos, verifica-se que a visibilidade sobre os custos e dados de utilização é limitada, não sendo possível consultar a informação com a granularidade que os utilizadores pretendem. Por todos estes motivos, a equipa de Investigação e Desenvolvimento da Eurotux Informática, S.A. decidiu investir no desenvolvimento de uma solução que auxiliasse os seus colaboradores e clientes num problema que enfrentam no dia a dia. Após estudar as soluções existentes, identificou-se, junto dos principais intervenientes, os requisitos que a solução deveria cumprir. A criação de uma aplicação em Flask em conjunto com uma Elas& Stack constitui a base tecnológica da solução. A modularidade, escalabilidade e robustez da solução foi tida em conta em todo o processo de elaboração da solução. O resultado final é uma ferramenta totalmente funcional que permite satisfazer as necessidades impostas. A integração com os principais fornecedores de cloud estudados foi amplamente conseguida. A avaliação da mesma foi realizada tendo por base diversos casos de estudo de clientes reais da empresa."
  },
  {
    "keywords": [
      "Decentralized identity",
      "Self sovereign identity",
      "DLTs",
      "Decentralized networks",
      "Identidade descentralizada",
      "Self sovereign identity",
      "Redes descentralizadas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Self-sovereign identity decentralized identifiers, claims and credentials using non decentralized ledger technology",
    "autor": "Oliveira, Bruno Miguel Gomes",
    "data": "2021-11-12",
    "abstract": "Current identity management systems rely on centralized databases to store user’s personal data, which poses\na great risks for data security, as these infrastructure create a critical point of failure for the whole system. Beside\nthat service providers have to bear huge maintenance costs and comply with strict data protection regulations.\nSelf-sovereign identity (SSI) is a new identity management paradigm that tries to answer some of these\nproblems by providing a decentralized user-centric identity management system that gives users full control of\ntheir personal data. Some of its underlying concepts include Decentralized Identifiers (DIDs), Verifiable Claims\nand Credentials. This approach does not rely on any central authority to enforce trust as it often uses Blockchain\nor other Decentralized Ledger Technologies (DLT) as the trust anchor of the system, although other decentralized\nnetwork or databases could also be used for the same purpose.\nThis thesis focuses on finding alternative solutions to DLT, in the context of SSI. Despite being the most used\nsolution some DLTs are known to lack scalability and performance, and since a global identity management\nsystem heavily relies on these two requirements it might not be the best solution to the problem.\nThis document provides an overview of the state of the art and main standards of SSI, and then focuses on\na non-DLT approach to SSI, referencing non-DLT implementations and alternative decentralized infrastructures\nthat can be used to replace DLTs in SSI. It highlights some of the limitations associated with using DLTs for\nidentity management and presents a SSI framework based on decentralized names systems and networks. This\nframework couples all the main functionalities needed to create different SSI agents, which were showcased in\na proof of concept application."
  },
  {
    "keywords": [
      "Machine learning",
      "Meta-learning",
      "Metadata",
      "Machine learning algorithms selection",
      "Classification",
      "Data mining",
      "Metadados",
      "Seleção de algoritmos",
      "Problemas de classificação",
      "Análise de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "A meta-learning approach for selecting machine learning algorithms",
    "autor": "Monteiro, José Pedro Santos",
    "data": "2020-06-16",
    "abstract": "One of the major challenges in Machine Learning is to investigate the capabilities and lim itations of the existing algorithms to identify when one algorithm is more adequate than\nanother to solve particular problems. Traditional approaches to predicting the performance\nof algorithms often involve costly trial-and-error procedures or expert knowledge, which is\nnot always straightforward to acquire. Thus, the main goal of this dissertation is to support\nbeginners or even experienced data scientists by automatically indicating which classifica tion algorithm is most suitable for their datasets.\nThis dissertation proposes the use of Meta-Learning as a possible solution to the above mentioned problem. In this respect, we introduced a novel framework for the automatic\ngeneration of meta-datasets. Taking advantage of the developed framework, several clas sification datasets from public sources were used. The result is the meta-dataset for the\nexperiment of this research project.\nConcerning the goal of forecasting the best model for a classification dataset, two different\nsolutions are presented: the first toward binary classification and the second on multiclass\nclassification. A variety of Machine Learning algorithms are tested and compared through\ncross-validation.\nThe experiment confirms the feasibility of applying Meta-Learning to select the algorithm\nthat is expected to obtain the best performance for classification problems."
  },
  {
    "keywords": [
      "Arduino",
      "Safety-critical embedded systems",
      "Formal verification of software",
      "Communication protocols",
      "Frama-C",
      "Frama-Clan",
      "Verificação formal de software",
      "Protocolos de comunicação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Verificação funcional de controladores Arduino",
    "autor": "Barbosa, Rafael Alexandre Antunes",
    "data": "2018-12-12",
    "abstract": "In some contexts, namely in the development of autonomous vehicles in robotics, the exploitation of the Arduino platform for the development of embedded systems allows a simpler\nand faster implementation of the desired functionality than would be possible using the\ndevices known as Programmable Logic Controllers (PLCs), typically used. However, the soft ware certification is a much more complicated task than the PLCs certification, due mainly\nto the complexity of the software and its difficult analysis. Although the software testing\nis the most used technique to validate it, it can not be used to prove the absence of errors.\nOn the other hand, the formal verification of software is usually used to demonstrate the\ncorrectness of critical systems as, for example, aerospace industry and banking industry,\nwhere a failure means the loss of human lives or high monetary values. As the embedded\nsystems are increasingly used in robotic systems, whose criticality is increasingly high, it is\nimportant to ensure their correctness.\nWith this, the intervention area focused on the communication protocols because they\nhave an appropriate dimension given the scope of the dissertation. These are quite impor tant, particularly in the context of robotic, since embedded systems are in constant commu nication with several peripherals, being fundamental the correction of the communication\nprotocols. This dissertation aims at the application of formal verification tools to commu nication libraries of the Arduino platform. The adopted verification process comprises the\napplication of the Frama-C tool, together with the WP plug-in, to perform a static analysis\nof annotated ANSI-C code with ACSL properties. Furthermore, another objective is the\nexploration of the experimental Frama-Clang plug-in because of the widespread interest in\nachieving C++ code certification."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Extração automática de documentos médicos da web para análise textual",
    "autor": "Gomes, Inês Fraga",
    "data": "2020-01-03",
    "abstract": "A literatura científica na biomedicina é um elemento fundamental no processo de obtenção de \nconhecimento, uma vez que é a maior e mais confiável fonte de informação. Com os avanços \ntecnológicos e o aumento da competição profissional, o volume e diversidade de documentos médicos\ncientíficos tem vindo a aumentar consideravelmente, impedindo que os investigadores acompanhem o \ncrescimento da bibliografia. Para contornar esta situação e reduzir o tempo gasto pelos profissionais na \nextração dos dados e na revisão da literatura, surgiram os conceitos de Web Crawling, Web Scraping e \nProcessamento de Linguagem Natural, que permitem, respetivamente, a procura, extração e \nprocessamento automático de grandes quantidades de texto, abrangendo uma maior gama de \ndocumentos científicos do que os normalmente analisados de forma manual.\nO trabalho desenvolvido para a presente dissertação teve como foco principal o rastreamento e recolha\nde documentos científicos completos, do campo da biomedicina. Como a maioria dos repositórios da \nweb não disponibiliza, gratuitamente, a totalidade de um documento, mas sim apenas o resumo da \npublicação, foi importante a seleção de uma base de dados adequada. Por este motivo, as páginas web \nalvo de rastreamento foram restringidas ao domínio dos repositórios da editora BioMed Central, que \ndisponibilizam por completo, milhares de documentos científicos na área da biomedicina.\nA arquitetura do sistema desenvolvido divide-se em duas partes principais: fase online e a fase offline. A \nprimeira inclui a procura e extração dos URLs das páginas candidatas a serem extraídas, a recolha dos \ncampos de texto pretendidos e o seu armazenamento numa base de dados. A segunda fase consiste no \ntratamento e limpeza dos documentos recolhidos, deixando-os num formato estruturado e válido para \nser utilizado como entrada de qualquer sistema de análise de texto. Para a concretização da primeira\nparte, foram utilizadas a framework Scrapy, como base para a construção do scraper, e a base de dados \nde documentos MongoDB, para o armazenamento das publicações científicas recolhidas. Na segunda \netapa do processo, ou seja, na aplicação de técnicas de limpeza e padronização dos dados, foram \naproveitadas algumas das inúmeras bibliotecas e funcionalidades que a linguagem Python oferece.\nPara demonstrar o funcionamento do sistema de extração e tratamento de documentos da área médica, \nfoi estudado o caso prático de recolha de publicações científicas relacionadas com Transtornos Obsessivo \nCompulsivos. Como resultado de todo o procedimento, foi obtida uma base de dados com quatro \ncoleções de documentos com diferentes níveis de processamento."
  },
  {
    "keywords": [
      "EletronicID",
      "Aplicações moveis",
      "Documentos de identificão desmaterializados",
      "Segurança da informação",
      "Mobile driver’s license",
      "Mobile applications",
      "Dematerialized identification documents",
      "Information security",
      "Mobile driver’s license",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desmaterialização de documentos de identificação",
    "autor": "Araújo, Matias Nicolau",
    "data": "2019",
    "abstract": "Os telemóveis com os quais andamos hoje em dia já não são utilizados apenas para chamadas\ne mensagens. Em vez disso utilizamo-los também para ver vídeos, ler o jornal, ou até\nconsultar a nossa conta bancária. Dada esta grande variedade de utilizações, os dispositivos\nmóveis têm tido um enorme avanço tecnológico, o que leva a ainda mais possibilidades.\nPor isso, porque não utilizá-los também como uma carteira dos nossos documentos de\nidentificação?\nEsta dissertação tem como objetivo apresentar alguns dos projetos que estão a ser desenvolvidos\nna área da identificação pessoal, assim desenvolver uma alternativa possível aos\ndocumentos de identificação físicos com que estamos acostumados. Esta alternativa tem o\nnome de documentos de identificação desmaterializados, que são no fundo, versões digitais\ndos documentos de identificação que já utilizamos.\nDurante este trabalho, foi desenvolvido um protótipo de um sistema de documentos\nde identificação desmaterializados, de forma a mostrar que estes sistemas funcionam e\nsão seguros. Este tipo de sistema pode vir a ser utilizado por qualquer pessoa, para o\narmazenamento de todos os documentos de identificação em dispositivos móveis."
  },
  {
    "keywords": [
      "Comportamento humano",
      "Dispositivos móveis",
      "Machine learning",
      "Reconhecimento de atividades",
      "Sensorização",
      "Smartphones",
      "Activity recognition",
      "Human behavior",
      "Mobile devices",
      "Sensor data",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Reconhecimento de atividades em smartphones usando sensores não intrusivos",
    "autor": "Fernandes, Pedro Almeida",
    "data": "2023-12-18",
    "abstract": "O reconhecimento de atividades utilizando smartphones tem ganho uma atenção redobrada nos últimos anos devido à adoção generalizada destes dispositivos e consequentemente dos seus vários sensores. Estes sensores são capazes de fornecer dados bastante relevantes para este fim. Os sensores não intrusivos, em particular, oferecem a vantagem de recolher dados sem exigir ao utilizador a realização de quaisquer ações específicas ou o uso de dispositivos adicionais. Primeiramente são discutidos os diferentes tipos de sensores habitualmente utilizados em smartphones, incluindo acelerómetros, giroscópios, entre outros, e o tipo de informação que nos podem guarnecer. Depois serão apresentados vários algoritmos e técnicas de machine learning, incluindo aprendizagem supervisionada, aprendizagem não supervisionada, e ainda aprendizagem por reforço. Dentro destes são também apresentados alguns dos algoritmos mais utilizados. Para finalizar esta primeira parte, serão ainda apresentadas alguns trabalhos realizados por outros investigadores na área. O objetivo desta tese passou, então, pela criação de uma aplicação destinada ao reconhecimento de atividades recorrendo exclusivamente ao uso de sensores não intrusivos presentes em qualquer smartphone. Os dados colecionados por esses sensores forem submetidos a várias etapas de processamento e, após diversas iterações, obteve-se um conjunto de features altamente favoráveis ao treino dos modelos de machine learning criados. O melhor resultado foi obtido pelo modelo utilizando o algoritmo XGBoost, que alcançou uma impressionante taxa de accuracy de 0.979. Este resultado bastante sólido, permite verificar a alta eficácia do uso deste tipo de sensores para o reconhecimento de atividades."
  },
  {
    "keywords": [
      "Cancer",
      "Deep learning",
      "AutoML",
      "NAS",
      "Drug sensitivity",
      "Drug synergy",
      "Cancro",
      "Aprendizagem profunda",
      "Aprendizagem de máquina automática",
      "Pesquisa de arquiteturas neuronais",
      "Sensibilidade a fármacos",
      "Sinergia farmacológica",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Development of a software application based on deep learning to predict drug sensitivity of cancer cell lines",
    "autor": "Marreiros, Leonardo de Freitas",
    "data": "2023-12-21",
    "abstract": "With the increase in Deep Learning (DL) popularity, the need for efficient and effective model development and optimization has emerged. Neural Architecture Search (NAS) is a research area that aims to automate the time-consuming and iterative tasks involved in building and tuning DL models. This approach has already proven to be effective in various fields, such as computer vision and natural language processing, and holds potential in the field of cancer treatment. Cancer is a group of diseases characterized by the uncontrolled growth and spread of abnormal cells in the body. The high degree of diversity among tumors, along with drug resistance acquired during (or before) treatment, makes it challenging to find effective therapies for all types of cancer. High-throughput\nscreening (HTS) is a laboratory technique used to rapidly test large numbers of compounds against a biological target or assay to identify potential drug candidates. In cancer research, HTS is often used to identify compounds that have the potential to inhibit the growth of cancer cells or interfere with specific cancer-associated processes. Since the number of compounds with drug-like properties is much greater than the number of compounds that can be analyzed in HTS processes, DL has been used recently to predict drug response based on omics data. This work integrated a DL-based framework capable of handling single and multi-input datasets of various types into the company’s internal platform, where the work was carried out. Using Automated Machine Learning (AutoML) tools to reach this goal, namely tools with NAS capabilities, this new module automates the search for the best DL models that produce the most suitable results across a range of prediction tasks. The effectiveness of the developed work was validated in the context of drug response prediction by comparing the performance of the NAS-generated models with manually tuned models previously developed. The models found using this approach achieved comparable performance without the\nneed for human expertise, proving the potential of NAS in cancer treatment, but also as a useful tool for automating and optimizing the model selection process in Machine Learning."
  },
  {
    "keywords": [
      "Ab initio",
      "Bactéria",
      "Genoma",
      "Homologia",
      "Previsão de genes",
      "Sequenciação",
      "Genome",
      "Homology",
      "Gene prediction",
      "Sequencing",
      "681.3:57",
      "57:681.3"
    ],
    "titulo": "PGP: prokaryote gene prediction software",
    "autor": "Pacheco, José Carlos Ribeiro",
    "data": "2013",
    "abstract": "A correta previsão e anotação de genes bacterianos é essencial para a aplicação da informação contida no ADN em muitos tópicos de pesquisa (bio)médica, como microbiologia, imunologia e doenças infeciosas. Embora existam vários softwares de previsão de genes bacterianos como GenemarkHMM, Glimmer e Prodigal e pipelines completos como ISGA, xBASE, Maker e Consensus Prediction, a previsão de genes pode ser melhorada. O principal objetivo deste trabalho foi o desenvolvimento de um pipeline de previsão de genes bacterianos, o Prokaryote Gene Prediction (PGP), que combina métodos de ab initio e de homologia. Uma vez que o software ab initio Prodigal mostrou um melhor desempenho relativamente a outros softwares estudados, foi usado como o passo inicial para o PGP. Considerando as proteínas previstas pelo Prodigal, o PGP a) analisa os alinhamentos obtidos, b) determina a necessidade de encurtar ou estender genes, c) introduz as correções necessárias, d) faz a previsão de ARNr e ARNt utilizando os programas RNAmmer e tRNA-scan2 e e) determina a existência de eventuais genes não identificados nas regiões intergénicas, através de um BLASTx. Quando comparados os resultados do PGP com os dados produzidos pelo Prodigal utilizando 4 genomas com conteúdo G+C% moderado e 3 com conteúdo em G+C% extremo, o PGP apresentou melhorias de 1% tanto na taxa de erro como na especificidade, exibindo a mesma sensibilidade. Foi observado que para genomas com conteúdos G+C% extremos, o PGP tem mais impacto e portanto realiza mais correções. Os resultados do PGP ainda foram comparados com os pipelines ISGA, xBASE e Consensus Prediction. O PGP melhorou a previsão de genes corretos em 4,4%, comparativamente com ISGA e xBASE e ainda 3,1% em relação à previsão do Consensus Prediction, mantendo uma sensibilidade idêntica entre previsões. No que respeita à deteção de genes na região intergénica verificou-se um acréscimo na ordem de 9 falsos positivos em 12 genomas modelo, necessitando esta vertente de um melhor desenvolvimento. Concluiu-se que o PGP melhora a correta previsão de genes, especialmente em genomas bacterianos com conteúdos G+C% extremos, contribuindo para a anotação automática de genomas bacterianos de elevada qualidade."
  },
  {
    "keywords": [
      "Code snapshots",
      "Teaching programming",
      "Student performance monitoring",
      "Student evolution analysis",
      "Snapshots de código",
      "Ensino de programação",
      "Monitorização do desempenho de alunos",
      "Análise da evolução de alunos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Program corpus analysis to characterize the learning process of the programmers",
    "autor": "Lourenço, Gustavo Araújo",
    "data": "2024-04-17",
    "abstract": "This document is a report for the final project of the Master’s in Informatics Engineering\ndegree, accomplished at Universidade do Minho in Braga, Portugal.\nThe project consists, in a first phase, on the study of many snapshots of programs archived\nby Nuno Fonseca for his doctoral Thesis \"Contributos para a Monitorização do Desempenho de\nEstudantes de Programação\" at the Universidade de Coimbra.\nThis study’s main goal is to analyse the code snapshots and explore as much as possible the\nknowledge that can be extracted from that repository in order to understand the students’\nbehavior while solving problems by computer.\nThe research begins with the gathering and analysis of different types of tools whose purpose\nis to analyze code, both to assist students in learning and to help teachers in evaluation.\nAfter this research, the data provided by professor Nuno Fonseca is deeply analysed and\nadapted to fit this project’s requirements and fulfill the proposed goals.\nFinally, a web application that allows the teacher to conduct a comprehensive and systematic\nanalysis of the learning progression of their students was developed. All documentation\nassociated with this application was also produced.\nThe outcomes of this thesis are expected to contribute to the field of code teaching tools,\nwith the research made in this field and the resulting web application."
  },
  {
    "keywords": [
      "Cirurgia bariátrica",
      "Obesidade",
      "Sistemas de informação na saúde",
      "Registo de saúde eletrónico",
      "Interoperabilidade semântica",
      "OpenEHR",
      "Utentes",
      "Profissionais de saúde",
      "Bariatric surgery",
      "Obesity",
      "Health information systems",
      "Electronic health record",
      "Semantic interoperability",
      "Patients",
      "Health professionals",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Plataforma multidisciplinar de registo e monitorização de cirurgia bariátrica baseada em OpenEHR",
    "autor": "Afonso, Ana Beatriz Castro",
    "data": "2022-12-13",
    "abstract": "Desde o primeiro contacto da tecnologia com o universo da saúde, que esta agradável e harmoniosa\nrelação tem vindo a ser cada vez mais poderosa e majestosa. A utilização de sistemas de informação\nna saúde tem servido diversos propósitos, seja na gestão de profissionais de saúde, na descoberta de\nnovas curas, no apoio à tomada de decisão ou simplesmente no armazenamento de todas as informações\nrespeitantes a este ambiente.\nA prática clínica é uma ciência multidisciplinar, na qual os profissionais de saúde de diferentes especialidades colaboram em prol do bem comum do utente. Nesta linha de pensamento, emerge a cirurgia\nbariátrica, encarada como uma abordagem multidisciplinar no tratamento da obesidade. Esta cirurgia\nengloba o diagnóstico de essencialmente cinco especialidades distintas, já que a “saúde é o estado de\ncompleto bem-estar físico, mental e social e não somente a ausência de doença, segundo a Organização\nMundial de Saúde”, como tal é imprescindível a constante partilha de informação entre estas, de modo\na ser proporcionado um correto e eficaz tratamento ao utente.\nA quantidade extrema de informação clínica produzida diariamente e de forma constante, a diversidade de fontes de conteúdo e os diferentes formatos dos dados têm constituído grandes desafios de interoperabilidade entre sistemas de informação de saúde. Por conseguinte, a presente temática é pautada\npela integração da tecnologia openEHR na plataforma desenvolvida aliada ao registo e à monitorização\nde informações intrínsecas à cirurgia bariátrica.\nFace ao mencionado, o grande objetivo na conceção de um projeto desta dimensão concentra-se\nfundamentalmente em facilitar toda a especificidade do processo inerente à cirurgia bariátrica, quer para\nos profissionais de saúde (auxiliando-os nas suas tarefas diárias), quer para os utentes envolvidos (usufruírem de cuidados de saúde mais rápidos e orientados). Desta forma, construiu-se uma plataforma\nque produziu resultados únicos e inovadores, arrastando consigo a solução útil, necessária e interessante\npara o preenchimento de uma profunda lacuna nesta área da saúde."
  },
  {
    "keywords": [
      "Tráfego de rede",
      "Dispositivos móveis",
      "Caracterização de tráfego",
      "Serviços de internet",
      "Aplicações móveis",
      "Análise de dados",
      "Network traffic",
      "Mobile devices",
      "Traffic characterization",
      "Internet services",
      "Mobile applications",
      "Data analysis",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Caracterização de tráfego não solicitado em dispositivos móveis",
    "autor": "Silva, José Pedro Veiga da",
    "data": "2019-12-23",
    "abstract": "Smartphones increasingly play an important role in everyday life, whether to communicate\nor perform tasks that require more computing power. This has become an indispensable\nobject in the lives of many people so their use increases more and more. This growth is\nassociated with an increase in network traffic due to the existing applications and Internet\nservices.\nThis increase in traffic is also due to the development and growth of 3G and 4G mobile\nnetworks, which allow Internet access when out of Wi-Fi networks. Such access requires\na mobile data plan that is normally limited to a certain level. Expiring this plan, access to\nthe Internet from mobile networks is prohibited or limited to a certain rate which is often\ninadequate for the requirements of the applications used.\nThis disturbance is commonly associated with the consumption of bandwidth in order\nto reproduce contents downloaded from the network. These contents are often associated\nwith the normal operation of the applications, i.e., expected contents, however, there are\ncontents that were not requested by the user, for instance advertisements contributing to\ndata plan exhaustion.\nThis justifies the need for studying and understanding the traffic involved between the\nuser device and the network in order to assist the end user in identifying how much data\nhas been consumed and distinguishing by the type of traffic involved. To answer this need,\na systematic methodology is developed and proposed in this work considering as inputs\npopular user applications - YouTube, Facebook and Instagram - with potencial impact on\nthe user data plan. Therefore, the present dissertation is a contribution in the field of traffic\nanalysis and characterization, shedding light in the process of identifying and measuring\ntraffic not requested by the user."
  },
  {
    "keywords": [
      "Key-value store",
      "NVMM",
      "Persistent memory",
      "Hierarchical storage",
      "Armazenamento chave-valor",
      "Memória persistente",
      "Armazenamento hierárquico",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Co-designing log-structured merge key-value stores with a non-volatile storage hierarchy",
    "autor": "Adão, Rúben Daniel Almeida",
    "data": "2024-02-19",
    "abstract": "The trend of increasing size of datasets in storage-based applications has promoted the research of new\nmethods and technologies for efficiently storing, processing, and analyzing large amounts of data. As a\nresult, Log Structured Merge (LSM) Key-Value Stores (KVSs) have been highly adopted since their\ndesign allows high write throughput and enforces sequential disk access patterns. Additionally, with the\nadvent of Non-Volatile Main Memory (NVMM), new storage technologies have emerged that offer\nfaster access times compared to traditional block-based storage devices, thus accelerating KVSs.\nHowever, while NVMM devices offer faster access to data, they are typically limited in capacity and are\noften more expensive. To address this trade-off, contemporary storage solutions harness the capabilities of\nheterogeneous storage devices in two fundamental manners: caching and tiering. In this dissertation, we\nshow that, on one hand, read-dominated workloads benefit from a caching approach, but their performance\ndegrades under tiering. On the other hand, for write-dominated workloads, the tiering approach presents\nbetter performance, while storing the entire dataset on NVMM actually degrades performance.\nTo overcome these challenges, this dissertation proposes KEIGO, a novel storage middleware that al lows LSM-based KVS to efficiently use storage hierarchies composed of NVMM and block-based devices.\nKEIGO is aware of the different I/O operations done by the KVS (e.g., foreground requests, and background\nflushes and compactions) and the characteristics of the underlying devices (e.g., concurrency, read/write\nasymmetry). This knowledge serves as a pivotal factor in optimizing KEIGO’s performance in the face of\ndynamic and mixed production workloads such as those observed in Nutanix and Meta. Moreover, KEIGO\nrequires minimal code modifications to integrate into production-ready LSM KVSs.\nConducted experiments show that KEIGO significantly enhances the throughput of LSM KVS solu tions, including RocksDB, Speedb, and LevelDB, by as much as 12.4×. Furthermore, it substantially\nreduces tail latency by up to 21.3× over both general-purpose storage solutions and LSM KVSs built\nfrom the ground up for hierarchical storage."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Testes em aplicações web",
    "autor": "Brito, Tiago Filipe Andrade",
    "data": "2016",
    "abstract": "Com a evolução da tecnologia cada vez mais aplicações de software são desenvolvidas para\ncorrerem sob um browser de Internet, sendo normalmente designadas de aplicações Web.\nComo método de assegurar a qualidade destas aplicações, os testes atraem cada vez mais a\natenção das comunidades académica e empresarial.\nTer uma estratégia de testes bem definida desde o início do projeto, e executar os mesmos\ndurante a fase de desenvolvimento, além de assegurar a qualidade do software, reduz o\nrisco de surgirem problemas imprevistos numa fase posterior do projeto, que aumentam o\ncusto de implementação e consequentemente diminuem a rentabilidade do projeto para a\nempresa fornecedora, e ainda provocam atrasos que normalmente resultam na insatisfação\ndo cliente.\nPor outro lado, as constantes evoluções do software, que o mercado altamente competitivo\nde hoje obriga, aumentam o risco de destruir funcionalidades implementadas anteriormente\n(chamada Regressão). Por este motivo, é necessária que a estratégia de testes seja\nnão só definida e implementada durante o projeto, mas também após o seu fecho.\nA repetição dos testes que cobrem todo o software em cada momento de entrega, são um\ndesafio para as organizações, na medida em que o seu custo é elevado, e dado que existe a\npossibilidade de falha humana.\nPor todos estes motivos, os responsáveis pela equipa de desenvolvimento Web da Eurotux\napoiaram este trabalho de investigação e permitiram que o mesmo fosse aplicado em\nprojetos reais desenvolvidos ao longo dos últimos meses, com o objetivo de ser definida\numa estratégia de testes que permitisse tornar o processo de desenvolvimento Web mais\nrobusto, aumentando a qualidade das soluções desenvolvidas. Pretendia-se essencialmente\ndefinir uma estratégia de testes automáticos que diminuísse a possibilidade de erro humano\nna execução dos testes, e que permitisse aumentar a rentabilidade dos projetos gastando\nmenos horas na fase de verificação.\nForam assim estudadas várias metodologias, tendo sido os testes funcionais que asseguram\na concordância da parte funcional do software face às suas especificações, e os testes\nde regressão que garantem que o novo código não introduz erros, o foco da presente dissertação.\nForam ainda realizados alguns estudos comparativos de ferramentas, tendo sido\nescolhido o Selenium para definição de testes automáticos em conjunto com a ferramenta\nAlloy Analyser para avaliação da qualidade da especificação.\nApós a escolha das ferramentas que mais se adequaram aos objetivos propostos neste\nestudo, foi desenvolvida uma ferramenta denominadaWebTest que permite a programação automática de testes funcionais e produz relatórios que são enviados por email, permitindo\nà equipa de desenvolvimento Web acompanhar o nível de qualidade do seu software.\nA WebTest foi utilizada em projetos reais da empresa, e o feedback por parte da equipa\ne dos seus responsáveis foi bastante positiva. Com a WebTest foi possível reduzir o tempo\ngasto com os testes nos projetos e garantir a qualidade da solução apesar das alterações de\nrequisitos e incremento de funcionalidades no software."
  },
  {
    "keywords": [
      "519.86"
    ],
    "titulo": "Credit scoring as an asset for decision making in intelligent decision support systems",
    "autor": "Silva, Fábio",
    "data": "2011-11-17",
    "abstract": "Risk assessment is an important topic for financial institution nowadays, especially in the context of loan applications or loan requests and credit scoring. Some of these institutions have already implemented their own custom credit scoring systems to evaluate their clients’ risk supporting the loan application decision with this indicator. In fact, the information gathered by financial institutions constitutes a valuable source of data for the creation of information assets from which credit scoring mechanisms may be developed.\nHistorically, most financial institutions support their decision mechanisms on regression algorithms, however, these algorithms are no longer considered the state of the art on decision algorithms. This fact has led to the interest on the research of new types of learning algorithms from machine learning able to deal with the credit scoring problem.\nThe work presented in this dissertation has as an objective the evaluation of state of the art algorithms for credit decision proposing new optimization to improve their performance. In parallel, a suggestion system on credit scoring is also proposed in order to allow the perception of how algorithm produce decisions on clients’ loan applications, provide clients with a source of research on how to improve their chances of being granted with a loan and also develop client profiles that suit specific credit conditions and credit purposes.\nAt last, all the components studied and developed are combined on a platform able to deal with the problem of credit scoring through an experts system implemented upon a multi-agent system. The use of multi-agent systems to solve complex problems in today’s world is not a new approach. Nevertheless, there has been a growing interest in using its properties in conjunction with machine learning and data mining techniques in order to build efficient systems. The work presented aims to demonstrate the viability and utility of this type of systems for the credit scoring problem."
  },
  {
    "keywords": [
      "Machine Learning",
      "Meta-Learning",
      "Optimization",
      "Monitoring",
      "Algorithm recommendation",
      "Aprendizagem automática",
      "Meta aprendizagem",
      "Otimização",
      "Monitorização",
      "Recomendação de algoritmos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Monitoring and optimization of an autonomous learning system",
    "autor": "Palumbo, Guilherme Fraga",
    "data": "2022-11-28",
    "abstract": "In the last years, the number of Machine Learning algorithms and their parameters has increased significantly.\nThis allows for more accurate models to be found, but it also increases the complexity of the task of training a\nmodel, as the search space expands significantly.\nAs datasets keep growing in size, traditional approaches based on extensive search start to become costly\nin terms of computational resources and time, especially in data streaming scenarios. With this growth, new\nchallenges in Machine Learning started to appear. The speed at which data arrives and different ways of storing\ndata are forcing organizations to address and explore new ways of adapting fast enough so their ML models\ndon’t become obsolete.\nThis dissertation aims to develop an approach based on meta-learning that tackles two main challenges: predict ing the performance metrics of a future model and recommending the best algorithm/configuration for training\na model for a specific Machine Learning problem. Throughout this dissertation, all the study objectives and\nquestions, along with the relevant contextualization will be exposed.\nThe proposed solution, when compared to an AutoML approach is up to 130x faster and only 2% worse in terms\nof average model quality, showing it is a good solution for scenarios in which models need to be updated regularly,\nsuch as in streaming scenarios with Big Data, in which some accuracy can be traded for a much shorter model\ntraining time."
  },
  {
    "keywords": [
      "I/O optimization",
      "Storage tiering",
      "Deep learning",
      "Otimização de E/S",
      "Armazenamento por camadas",
      "Aprendizagem profunda",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Accelerating deep learning training on high-performance computing with storage tiering",
    "autor": "Dantas, Marco Filipe Leitão",
    "data": "2022-07-27",
    "abstract": "Deep Learning (DL) has become fundamental to the advancement of several areas, such as computer\nvision, natural language processing and expert systems. Utilizing DL techniques demands vast amounts\nof data and processing power, which raises challenges to the training performance of DL models. High Performance Computing (HPC) systems are becoming increasingly popular to support DL training, by\noffering extensive computing capabilities, however, due to convenience and usability, many DL jobs running\non these infrastructures resort to the shared Parallel File System (PFS) for storing and accessing training\ndata. Under such scenario, where multiple Input/Output (I/O)-intensive applications operate concurrently,\nthe PFS can quickly get saturated with simultaneous storage requests and become a critical performance\nbottleneck, leading to throughput variability and performance loss.\nTo solve these issues, this dissertation presents a storage middleware agnostic to any DL solution,\nMonarch, that deploys storage tiering to accelerate DL models’ training performance and decrease the I/O\npressure imposed over the PFS. It leverages from existing storage tiers of supercomputers (e.g., compute\nnode’s local storage, shared PFS), as well as the I/O patterns of DL solutions to improve data placement\nacross storage tiers. Furthermore, this middleware is non-intrusive and easily installed in HPC centers,\nthus enabling its wide adoption and applicability.\nThe performance and applicability of Monarch are validated with the TensorFlow and PyTorch DL\nframeworks. Results show that, when the training dataset can only be partially stored at the local storage\ntier, Monarch decreases TensorFlow’s and PyTorch’s training time by up to 28% and 37% for I/O-intensive\nmodels, respectively. Furthermore, Monarch can reduce the number of I/O operations submitted to the\nPFS by up to 56%."
  },
  {
    "keywords": [
      "Distributed databases",
      "CoherentPaaS",
      "Query monitoring",
      "Query analysis",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Monitoring and analysis of queries in distributed databases",
    "autor": "Guimarães, Pedro Miguel Pimentel",
    "data": "2015-01-07",
    "abstract": "Scalable database services combining multiple technologies, including SQL and NoSQL, are\nincreasingly in vogue. In this context, the CoherentPaaS research project aims at providing\nan integrated platform with multiple data management technologies, united by a common\nquery language and global transactional coherence.\nFor this integration to succeed, it must provide the same monitoring capabilities of tra\nditional relational databases, namely, for database administrators to optimise its operation.\nHowever, achieving this in a distributed and heterogeneous system is in itself a challenge.\nThis work proposes a solution to this problem with X-Ray, that allows monitoring code to\nbe added to a Java-based distributed system by manipulating its bytecode at runtime. The\nresulting information is collected in a NoSQL database and then processed and visualised\ngraphically. This system is evaluated experimentally by adding monitoring to Apache Derby\nand tested with the standard TPC-C benchmark workload."
  },
  {
    "keywords": [
      "Process Mining",
      "Ferramentas de análise de tráfego de rede",
      "Índice de qualidade",
      "Smart Cities",
      "Data warehouse",
      "Network monitoring and analysis tools",
      "Index quality",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Process Mining na análise de tráfego de uma rede de comunicações",
    "autor": "Gonçalves, Gil",
    "data": "2018-12-14",
    "abstract": "Nos primórdios a Internet era usada apenas por algumas pessoas. Nessa altura muitas das \ngrandes empresas de tecnologia ainda não tinham aparecido. Porém tudo mudou quando a \nInternet entrou nos circuitos comerciais, provocando o aparecimento de estruturas para fazer a \nligação das pessoas ao seu mundo. Desde aí que as grandes empresas têm adotado novas formas \nde encarar o mercado, aumentando gradualmente a sofisticação da forma de o fazerem. As \nempresas começaram a monitorizar a atividade dos seus clientes para que com isso melhorar a \noferta dos seus bens e serviços ao público em geral. Todavia o conhecimento acerca daquilo que o \ncliente gosta não é suficiente para o atrair. Uma empresa também precisa que a informação \napresentada ao cliente seja feita da maneira mais rápida possível. Por exemplo, se um cliente \nesperar mais do que “três” segundos para que o site seja carregado, o cliente irá abandoná-lo e, \nprovavelmente, procurar um outro site de uma empresa concorrente. A Google avalia a rapidez \ndos sites e com isso dá-lhes uma pontuação. Os sites com piores pontuações são apresentados em \núltimos, o que tem, como sabemos, um grande impacto na escolha dos clientes. Mas não é só com \nos clientes que as empresas se tem de preocupar. Internamente os serviços dos funcionários de \numa empresa podem ser afetados por uma Internet lenta, o que conduz a uma perda de \nperformance e ao aumento da frustração do próprio funcionário no local de trabalho. Por estas \nrazões é importante que as empresas estejam constantemente a monitorizar o tráfego passado \npelos seus servidores, para serem capazes de verificar se os motivos da lentidão dos seus serviços \nde rede são internos ou não. Neste trabalho de dissertação desenvolvemos um trabalho baseado\nem process mining, que através de uma ferramenta de monitorização de rede, wireshark, permite \navaliar a qualidade de serviço da rede através da observação e análise das logs produzidas por \nalguns dos seus equipamentos, em particular, dos seus routers. Como são geradas várias logs para \ncada um tipo de router foi necessário fazer a sua conciliação, para que, a partir daí, se pudesse \nobter o percurso que os vários pacotes realizaram na sua movimentação pela rede. Desta forma, é \npossível criar um modelo matemático capaz de determinar um índice de bem-estar relativo à \nqualidade de serviço da rede de uma empresa. Basicamente, este índice permitirá avaliar o\ndesempenho da rede e permitir aos seus gestores identificar, por exemplo, quais os pontos da \nrede que apresentam menor desempenho (ou estrangulamentos de serviço) e prevenir futuras \nquebras no serviço geral da rede em análise."
  },
  {
    "keywords": [
      "Ontology",
      "Graph",
      "Data",
      "Visualizations",
      "Ontologia",
      "Grafo",
      "Dados",
      "Visualizações",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Ulisses NextGen",
    "autor": "Cunha, Nuno Azevedo Alves da",
    "data": "2022-12-19",
    "abstract": "Nowadays data can have many different shapes and relations between itself, ontologies try\nto formalize the semantics subjacent to this data and make it understandable by humans\nand code alike.\nWhile code succeeds at parsing and interpreting this formalization traditional ontology\nformats can be tough for a human to understand without previously deepened knowledge\nof the ontologic paradigm and, even then, directly analyzing a format like RDF would be,\nat the very least, very tedious. This problem is not exclusive to ontologic data either as to\nmake sense of big datasets, even in famously human readable formats like JSON, humans\nneed visualizations and abstractions.\nThis dissertation is a study on graph visualization of ontologic data and how abstractions\ncan be used to convey information to the end user in meaningful ways\nThe information gathered is then used to implement an application called \"Ulisses\nNextGen\" that can generate an easily navigable graph visualizing application with a strong\nfocus to support ontological data but general enough to support any information that can\nbe abstracted as a graph. The application is served as a javascript package to be used in\nanywhere on the web where it can be used best to reach the end user."
  },
  {
    "keywords": [
      "681.3:658.0",
      "658.0:681.3"
    ],
    "titulo": "Selecção de hiper-cubos com base em padrões de exploração OLAP",
    "autor": "Rocha, Daniel",
    "data": "2011-11-22",
    "abstract": "Na literatura do domínio do processamento analítico de dados facilmente se podem encontrar métodos e soluções que respondem ao problema de seleção de vistas multidimensionais no processo de implementação de um cubo OLAP. Uma forma que se evidencia como sendo extremamente vantajosa, é a de fazer a seleção baseada em critérios que se apoiem essencialmente nos conteúdos que são consultados sobre o cubo de dados ao longo das sessões de consulta OLAP. As principais vantagens que advêm desta monitorização, está relacionada com a possibilidade de efetuar correspondências rigorosas com a informação em que os agentes de decisão mais se apoiam para efetuar as suas tomadas de decisão. Ao ser feita a identificação da informação que se evidencia como sendo a mais relevante, ou pelo menos a mais frequentemente consultada, várias ilações se podem retirar, como, por exemplo, a definição de perfis de utilização, a expressão de preferências, a identificação de metodologias de trabalho, ou então a definição de processos que procurem construir cubos iceberg com forte probabilidade de explorações futuras sobre o cubo. Este último aspeto constitui, basicamente, o trabalho desta dissertação. Ao se efetuar a materialização dos conteúdos mais pesquisados no servidor OLAP, obtém-se um melhor desempenho ao nível do servidor, uma vez que o preparamos antecipadamente com os dados que mais vezes são solicitados, reduzindo assim o número de vezes que seria necessário recorrer ao data warehouse para retornar os resultados pretendidos por uma dada query multidimensional. Em termos gerais, neste trabalho de dissertação, desenvolveu-se um estudo detalhado acerca das ideias e práticas que levam ao desenvolvimento de um dado método de seleção, que seja capaz de indicar de forma precisa as partes de um cubo que são mais utilizadas, sugerindo com base nessa informação uma nova estrutura para o cubo em questão que utilize menos recursos computacionais, nomeadamente espaço em disco e tempo de processamento."
  },
  {
    "keywords": [
      "Aplicação baseada na Web",
      "Mineração de literatura biomédica",
      "Recuperação de informação",
      "Biomedical Text Mining",
      "Information retrieval",
      "Web based application",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of a web-based platform for Biomedical Text Mining",
    "autor": "Fernandes, Emanuel Queiroga Amorim",
    "data": "2019-12-23",
    "abstract": "Biomedical Text Mining (BTM) seeks to derive high-quality information from literature in the biomedical domain, by creating tools/methodologies that can automate time-consuming tasks when searching for new information. This encompasses both Information Retrieval, the discovery and recovery of relevant documents, and Information Extraction, the capability to extract knowledge from text. In the last years, SilicoLife, with the collaboration\nof the University of Minho, has been developing @Note2, an open-source Java-based multiplatform BTM workbench, including libraries to perform the main BTM tasks, also provid ing user-friendly interfaces through a stand-alone application.\nThis work addressed the development of a web-based software platform that is able to\naddress some of the main tasks within BTM, supported by the existing core libraries from\nthe @Note project. This included the improvement of the available RESTful server, providing\nsome new methods and APIs, and improving others, while also developing a web-based\napplication through calls to the API provided by the server and providing a functional\nuser-friendly web-based interface.\nThis work focused on the development of tasks related with Information Retrieval, addressing the efficient search of relevant documents through an integrated interface. Also, at\nthis stage the aim was to have interfaces to visualize and explore the main entities involved\nin BTM: queries, documents, corpora, annotation processes entities and resources."
  },
  {
    "keywords": [
      "Machine learning",
      "Sensor fusion",
      "Impact detection",
      "Data fusion",
      "Feature fusion",
      "Fusão sensorial",
      "Deteção de impactos",
      "Fusão de data",
      "Fusão de características",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sensor fusion for impact detection in vehicles",
    "autor": "Parpot, José Gabriel Correia Neves",
    "data": "2023-07-03",
    "abstract": "With the advance of technology surrounding the automobile industry, we are starting to see a shift in\nthe need a personal vehicle, opting more often for other options like rental cars and car-sharing services.\nWith this shift these services face more problems and more specific damage to the vehicles in the\nfleet. In order to help these services keep track of their fleet state and to help detect impacts if they\nhappen, a multi-sensor fusion for impact detection in vehicles is proposed.\nThe main focus of this thesis is to implement a multi-sensor fusion approach to detect impacts in\nvehicles. A comparative study of the previously implemented solution is carried out to help develop and\nimplement the suggested approach. One of the sub-objectives of this work is to find which of the two\nimplemented fusion methods better improves the system performance.\nThe sensors that compose the detection strutcure, are a Inertial Measurement Unit (IMU) and a mi crophone, which are located inside the vehicle in different positions. Note that the structure used differs\nfor each dataset.\nThe fusion works by combining the information of all the accelerometers placed in the vehicle. Two\nsensor fusion methods applied to the two datasets in this thesis are as follows: a complementary filter\nwhich is part of the data fusion level and the second consists in a feature fusion approach by fusion features\nfrom vairous sensor combinations."
  },
  {
    "keywords": [
      "Alloy",
      "MER",
      "Multi-Paxos",
      "Paxos",
      "Vertical Paxos",
      "SMR",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Formalização da reconfiguração de protocolos de consenso usando Alloy",
    "autor": "Soares, Cecília da Conceição de Oliveira",
    "data": "2022",
    "abstract": "O protocolo de máquinas de estado replicadas (MER) é uma peça fundamental dos sistemas distribuídos. No\ncentro deste protocolo estão os algoritmos de consenso, como o Paxos, usados para manter a consistência das\nMER. Todavia, os sistemas modernos não podem depender estritamente das técnicas de MER, estes devem\ntambém implementar estratégias de reconfiguração. Estas estratégias consistem em alterar a configuração do\nsistema, adicionando, removendo ou substituindo os processos que o compõem. Dada a sua complexidade, a\nimplementação de protocolos de reconfiguração é muito suscetível a erros, daí que seja aconselhável a especificação,\nvalidação e verificação dos mesmos.\nNo presente trabalho apresentamos uma especificação em linguagem Alloy do protocolo de reconfiguração\nVertical Paxos e do protocolo de consenso Paxos. Além destes, modelamos o protocolo Multi-Paxos, o qual\nimplementa uma MER. Estes protocolos estão intrinsecamente relacionados e a compreensão do primeiro é\nfacilitada com o conhecimento dos demais. Atualmente, o Alloy é uma das linguagens de especificação mais\npopulares, mas pouco explorada na modelação de algoritmos distribuídos e, tanto quanto sabemos, não existe\nainda nenhuma especificação dos referidos protocolos em Alloy.\nO presente trabalho visa modelar e validar os referidos protocolos, bem como verificar as suas propriedades\nde safety, de modo a obtermos confiança nas especificações. Ademais, realizamos uma avaliação de desempenho\nde diferentes solvers e estratégias de decomposição nativas do Alloy, bem como uma breve análise\ncomparativa com o TLA+."
  },
  {
    "keywords": [
      "Computer Vision",
      "3D reconstruction",
      "Visão por computador",
      "Reconstrução 3D",
      "681.3"
    ],
    "titulo": "Computer vision component to environment scanning",
    "autor": "Soares, Pedro Emanuel Pereira",
    "data": "2012-11-19",
    "abstract": "Computer vision is usually used as the perception channel of robotic platforms. These\nplatforms must be able of visually scanning the environment to detect specific targets and\nobstacles. Part of detecting obstacles is knowing their relative distance to robot. In this\nwork different ways of detecting the distance of an object are analyzed and implemented.\nExtracting this depth perception from a scene involves three different steps: finding features\nin an image, finding those same features in another image and calculate the features’ distance.\nFor capturing the images two approaches were considered: single cameras, where we capture\nan image, move the camera and capture another, or stereo cameras, where images are taken\nfrom both cameras at the same time. Starting by SUSAN, then SIFT and SURF, these three\nfeature extraction algorithms will be presented as well as their matching procedure. An\nimportant part of computer vision systems is the camera. For that reason, the procedure\nof calibrating a camera will be explained. Epipolar geometry and the fundamental matrix\nare two important concepts regarding 3D reconstruction which will also be analyzed and\nexplained. In the final part of the work all concepts and ideas were implemented and, for each\napproach, tests were made and results analyzed. For controlled environments the relative\ndistance of the objects is correctly extracted but with more complex environment such results\nare harder to obtain."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Serviço safe-return-home baseado em análise de big data em tempo real",
    "autor": "Malheiro, Tarcísio",
    "data": "2016",
    "abstract": "Na nossa sociedade existem várias formas de crimes que ocorrem com bastante frequência.\nNo combate ao crime, vários sistemas safe-return estão a ser desenvolvidos para pessoas\nvulneráveis. Contudo estes sistemas focam-se principalmente na resposta a situações em\nque o perigo já aconteceu, não é feita nenhuma previsão de um possível crime. Estas\nprevisões de situações de perigo podem ser feitas com a informação recolhida, relativa às\nproximidades do utilizador, em tempo real.\nUm sistema de safe-return-home deve permitir que os utilizadores do serviço sejam notificados,\natempadamente, de possíveis situações perigosas que ocorram nas proximidades\nda localização do utilizador, em tempo real. Estas previsões são feitas através da análise\nde vários tipos de dados relativos ao que se encontra em redor do utilizador. A recolha\ne análise de dados deverá incluir informação relativa a notícias relacionadas com crimes,\nacidentes e desastres, incluindo também a localização do utilizador. A análise dos dados\npermite então fazer uma previsão de perigos, de forma que possam ser visualmente reconhecidos\nno smartphone do utilizador, sendo assim possível reagir de forma ativa a esses\nperigos, em tempo real."
  },
  {
    "keywords": [
      "Computer vision",
      "Image classification",
      "Image segmentation",
      "Machine learning",
      "Coins",
      "Visão por computador",
      "Classificação de imagens",
      "Segmentação de imagens",
      "Moedas"
    ],
    "titulo": "Medieval coin automatic recognition by computer vision",
    "autor": "Salgado, Luís",
    "data": "2016",
    "abstract": "The use of computer vision for identification and recognition of coins is well studied and of renowned interest. However the focus of research has consistently been on modern coins and the used algorithms present quite disappointing results when applied to ancient coins. This discrepancy is explained by the nature of ancient coins that are manually minted, having plenty variances, failures, ripples and centuries of degradation which further deform the characteristic patterns, making their identification a hard task even for humans. Another noteworthy factor in almost all similar studies is the controlled environments and uniform illumination of all images of the datasets. Though it makes sense to focus on the more problematic variables, this is an impossible premise to find outside the researchers’ laboratory, therefore a problematic that must be approached.\nThis dissertation focuses on medieval and ancient coin recognition in uncontrolled “real world” images, thus trying to pave way to the use of vast repositories of coin images all over the internet that could be used to make our algorithms more robust.\nThe first part of the dissertation proposes a fast and automatic method to segment ancient coins over complex backgrounds using a Histogram Backprojection approach combined with edge detection methods. Results are compared against an automation of GrabCut algorithm. The proposed method achieves a Good or Acceptable rate on 76% of the images, taking an average of 0.29s per image, against 49% in 19.58s for GrabCut. Although this work is oriented to ancient coin segmentation, the method can also be used in other contexts presenting thin objects with uniform colors.\nIn the second part, several state of the art machine learning algorithms are compared in the search for the most promising approach to classify these challenging coins. The best results are achieved using dense SIFT descriptors organized into Bags of Visual Words, and using Support Vector Machine or Naïve Bayes as machine learning strategies."
  },
  {
    "keywords": [
      "Intrusion detection system",
      "Machine learning",
      "Controller area network",
      "Connected vehicles",
      "Sistema de deteção de intrusão",
      "Aprendizagem máquina",
      "Controller area network",
      "Veículos conectados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automated and intelligent hacking detection system",
    "autor": "Carvalho, Bruno Alves Martins",
    "data": "2022-12-13",
    "abstract": "The Controller Area Network (CAN) is the backbone of automotive networking, connecting many Electronic ControlUnits (ECUs) that control virtually every vehicle function from fuel injection to parking sensors. It possesses,however, no security functionality such as message encryption or authentication by default. Attackers can easily inject or modify packets in the network, causing vehicle malfunction and endangering the driver and passengers. There is an increasing number of ECUs in modern vehicles, primarily driven by the consumer’s expectation of more features and comfort in their vehicles as well as ever-stricter government regulations on efficiency and emissions. Combined with vehicle connectivity to the exterior via Bluetooth, Wi-Fi, or cellular, this raises the risk of attacks. Traditional networks, such as Internet Protocol (IP), typically have an Intrusion Detection System (IDS) analysing traffic and signalling when an attack occurs. The system here proposed is an adaptation of the traditional IDS into the CAN bus using a One Class Support Vector Machine (OCSVM) trained with live, attack-free traffic. The system is capable of reliably detecting a variety of attacks, both known and unknown, without needing to understand payload syntax, which is largely proprietary and vehicle/model dependent. This allows it to be installed in any vehicle in a plug-and-play fashion while maintaining a large degree of accuracy with very few false positives."
  },
  {
    "keywords": [
      "Acelerómetros",
      "Cidades Inteligentes",
      "Estado do pavimento",
      "Monitorização de trânsito",
      "Accelerometers sensors",
      "Road conditions",
      "Smart Cities",
      "Traffic monitoring",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Cidades Inteligentes - Sistema de Monitorização de Pavimento e Tráfego",
    "autor": "Rodrigues, Vítor Hugo de Castro",
    "data": "2022-04-05",
    "abstract": "Na atual conjuntura em que vivemos é crucial que seja realizado um estudo aprofundado\nsobre a utilização dos recursos da cidade, especialmente em zonas urbanas. Além disso,\numa rede de estradas bem conservada deveria ser uma prioridade para o desenvolvimento\neconómico e bem-estar dos habitantes de qualquer país.\nCom o desenvolvimento da tecnologia, em particular na área das Cidades Inteligentes\n(“Smart cities”), é importante implementar sistemas de apoio, de modo a centralizar toda a\ninformação existente sobre um aspeto de interesse, de maneira a “virtualizar” as cidades.\nNeste trabalho pretende-se desenvolver esse suporte em torno de uma rede de transportes\npúblicos, compilando toda a informação que essa rede nos poderá fornecer para que possa\nser útil a várias entidades.\nNeste contexto, a presente dissertação pretende estudar duas vertentes no âmbito das\nCidades Inteligentes. Na primeira será realizado um estudo do estado do pavimento através\nde sensores de aceleração (acelerómetros), sendo estes sensores colocados numa rede urbana\nde transportes públicos já existentes, assim a rota será conhecida pela entidade responsável,\ncomo por exemplo a Câmara Municipal do distrito em questão. Assim, espera-se que\ncom esta informação seja possível uma tomada de decisão mais adequada, face ao estado\ndo pavimento, de maneira a que possam ser realizados os trabalhos necessários para a\nreconstrução do mesmo. Na segunda vertente será realizado um estudo da monitorização\ndo trânsito em áreas em que o fluxo é elevado, com base na mesma rede de transportes\npúblicos, sendo a informação obtida através dos mesmos sensores."
  },
  {
    "keywords": [
      "Backend",
      "Desenvolvimento web",
      "Frontend",
      "Loja online",
      "Programação modular",
      "Web development",
      "Online store",
      "Modular programming",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Ferramenta de criação de lojas online para programadores baseada em NextJS",
    "autor": "Ferreira, José Carlos Peixoto",
    "data": "2024-07-02",
    "abstract": "A presente dissertação baseia-se na expectativa de diminuir o trabalho e tempo necessário para o desenvolvimento de uma loja online por programadores. Com a evolução do comércio, começaram a surgir cada vez mais negócios com lojas online, sendo por isso cada vez maiores as exigências e condições para que estas se tornassem um sucesso. Todas as lojas online costumam ter aspetos em comum como o carrinho de compras, a autenticação, entre outros componentes básicos sobre os quais se poderia salvar tempo de desenvolvimento se não se tivesse de reescrever sempre o mesmo código. Esta dissertação teve início com a investigação dos parâmetros necessários para que uma loja on-line funcionasse, pelo que em seguida foi desenvolvido uma loja online de venda de jogos e produtos relacionados. Após a investigação foi possível identificar vários aspetos e parâmetros necessários para o\ndesenvolvimento web tanto no frontend como no backend, conseguindo-se assim fazer a distinção entre componentes e as suas relações.\nDe seguida foi abordado o principal objetivo da dissertação que consistia em desenvolver uma ferramenta de criação de lojas online automática seguindo as configurações de diferentes programadores. Esta ferramenta pretende diminuir o tempo despendido pelos programadores em cada novo projeto, ao lhes dar a oportunidade de obter o código de lojas online com diferentes layouts e diferentes parâmetros definidos por eles. Após a criação deste projeto template usando a configuração dos programadores, o objetivo é que eles o adaptem aos pedidos dos seus clientes de uma forma simples, tendo sido usada uma programação modular ao longo deste projeto para facilitar a sua utilização. O projeto desenvolvido nesta dissertação focou-se na autenticação, algumas páginas relacionadas com o produto, na organização do código, na programação modular e na capacidade de os inputs dos programadores alterarem o resultado da ferramenta. No final deste trabalho, foi possível obter uma framework funcional com os pontos anteriormente mencionados, pelo que se atingiu o principal objetivo de desenvolver uma ferramenta que simplificasse o trabalho dos programadores."
  },
  {
    "keywords": [
      "Software architecture",
      "HPC",
      "Algoritmic skeletons",
      "Arquitectura de software",
      "HPC",
      "Esqueletos algorítmicos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Padrões arquitecturais e de desenho  para aplicações paralelas",
    "autor": "Leitão, Vasco Luzio",
    "data": "2021-12-22",
    "abstract": "Parallel programming is becoming increasingly common, given the evolution of hardware\ninto multicore and manycore architectures. To take advantage of these architectures it is\nnecessary to develop parallel code, since the automatic generation of parallel code from a\nsequential base code is not a viable option.\nParallel programming tends to be complex and therefore the developed code tends to be\ndifficult to maintain or even difficult to reuse.\nThis dissertation aims to explore the use of algorithmic skeletons in the development\nof parallel applications in order to allow faster development as well as the production of\nhigher quality final software. A skeleton framework was developed. The skeletons were\ndeveloped through a technique called ”mixin layers”, which makes use of generic classes of\nC++. A great emphasis wass given to the use of generic classes and other alternatives in\nthe construction of the skeletons.\nFinally, the performance of these solutions were evaluated in comparison with solutions\nalready consolidated within parallel computing. For that, the codes of a set of applications\nwere restructured in order to make use of the skeletons provided by the library"
  },
  {
    "keywords": [
      "API",
      "REST",
      "RESTful",
      "Web services",
      "Web security",
      "Serviços web",
      "Segurança web",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Concepção e desenvolvimento de uma API REST com incorporação de mecanismos de segurança aplicacional",
    "autor": "Gonçalves, André da Silva",
    "data": "2022-04-05",
    "abstract": "The constant technological evolution of the last decades makes more and more companies to focus on providing more resources to their customers, showing new perspectives for the development of solutions with high\nlevels of performance, availability, scalability and flexibility.\nOne of the biggest contributions in this regard was the appearance of Application Programming Interfaces\n(APIs), increasingly crucial as integration, automation and efficiency become more important. With the abrupt\nemergence of APIs, API security has become a significant topic in the tech world. If an API does not have\nadequate security, it can be vulnerable to attacks that can compromise a company’s data or system. Security\nshould be considered from the beginning of any API development project and built into each step of the process\nto ensure that the API is adequately protected.\nIn this dissertation we intended to investigate the functioning of APIs, with special focus on the Representational State Transfer (REST) architecture and their security, allowing us to verify that, despite several techniques\nand tools for the creation of solid and robust REST APIs have already been studied in detail and applied to a\nwide variety of domains, REST services still need practical approaches specialized in the design and security of\ntheir APIs. It is proposed to fill this gap with the definition of a set of metrics capable of helping in the creation of\na REST API with good design principles and absent of any vulnerabilities.\nIn the context of UN1Qnx as a company that develops authenticity solutions, an IT infrastructure capable of\nhandling multiple customers and systems is essential for its business.\nBearing this need in mind, the opportunity arose to implement in practice the result of all the research carried\nout throughout the dissertation through the development of an Application Programming Interface (API) that\nfollows the principles of architectural style based on REST in order to allow managing the data flow of the\nUN1Qnx system together with the definition of mechanisms to integrate the entire UN1Qnx service with third-party applications and services in order to automate procedures for creating and changing data."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Delay tolerant networks with traffic differentiation capabilities",
    "autor": "Mendes, Nuno Filipe",
    "data": "2016",
    "abstract": "In the last few decades, an increasing growth of Internet usage was witnessed worldwide.\nHowever, infrastructures do not always allow the existence of Internet connectivity everywhere.\nTherefore, to address this issue, the concept of Delay Tolerant Networks (DTNs) was\ndeveloped. DTNs purpose is to provide a different level of intermittent connectivity, dissimulating\nconnection problems that arise in complex connectivity scenarios. Examples of such\nscenarios are, for instance, cities, where cars exchange information about their location; in\nunderdeveloped countries, where Internet is inexistent; in freeways, where is not viable to\nprovide infrastructures for a continuous connectivity, but cars, tolls, and services need to be\naware of each other. Thus, DTNs constitute a possible solution for all the aforementioned\ncommunication environments.\nHowever, DTNs still faces some obstacles in terms of delivering a service with quality as it\nlacks specific mechanisms, such as traffic differentiation. Traffic differentiation is essential to\nprovide different levels of service quality regarding delivering of messages. Current proposals\nto improve service delivery through traffic differentiation on DTNs are still under development\nor lack the proper testing and simulation. The main focus of these proposals is on buffer\nmanagement mechanisms at each DTN node, instead of message prioritisation mechanisms.\nMessage prioritisation allows some messages to be prioritised over others, improving the\ndelivery rate and, therefore, increasing the probability of a message being correctly delivered.\nThe present thesis implements traffic differentiation in DTNs based on prioritisation strategies,\nassuming a clear alternative to other buffer management proposals and message prioritisation.\nUsing The One simulation tool, three popular DTNs routing protocols (Epidemic,\nSpray & Wait, and PRoPHET) are adapted to comply with traffic differentiation. The DTNs\ntraffic prioritisation objective is achieved by designing, implementing and testing four distinct\nalgorithms that classify and order messages according to their priority levels. These\nalgorithms are based and extend some traditional traffic differentiation mechanisms, namely\nthe well-known Priority Queuing and Weighted Round Robin strategies.\nResults from the simulation tests corroborate that the delivery rate of the messages is\naffected according to their priorities. Specifically, the simulation shows an increase in the\ndelivery rate of high priority messages, with low impact on the total number of messages delivered,\ncomparatively to the same scenario without differentiation capabilities. To conclude,\nDTNs can effectively benefit from traffic differentiation based on message prioritisation techniques,\nbeing a promising approach to improve service quality levels in such scenarios."
  },
  {
    "keywords": [
      "Mineração de Dados",
      "Processos de certificação de Empresas",
      "Classificação e Segmentação de Dados",
      "Descoberta de Padrões de Certificação",
      "Data Mining",
      "Process of Certification of Companies",
      "Data Classification and Data Clustering",
      "Discovery of Patterns of Certification",
      "658.56",
      "681.3:658.56",
      "658.56:681.3"
    ],
    "titulo": "Descoberta de padrões num sistema de certificação de empresas",
    "autor": "Carvalho, Mariana Reimão Queiroga Valério de",
    "data": "2011-12-22",
    "abstract": "A mineração de dados tem como principal objetivo descobrir padrões escondidos num conjunto de dados, isto é, informação útil que ajude a tomar decisões sobre um determinado tema. Nesta dissertação usa-se a mineração numa base de dados que descreve os certificados adquiridos pelas empresas portuguesas no período 2008-2010. A certificação é um processo voluntário, que, apesar de ser bastante demorado, custoso e envolver demasiada burocracia, pode ser crucial para a sobrevivência das empresas. Os certificados demonstram o compromisso da empresa com a qualidade dos produtos e dos serviços, e com o meio ambiente, a saúde e a segurança dos trabalhadores, entre outros. Quais os certificados que uma empresa deve adquirir é uma das principais questões que se colocam a uma nova empresa no mercado. A resposta a esta pergunta pode variar com vários fatores, como a região onde a empresa se encontra localizada ou o seu sector de atividade. É necessária uma análise prévia que forneça um conhecimento do estado do mercado para tomar as melhores decisões. A aplicação de técnicas de mineração de dados permite obter uma descrição do estado do mercado e uma previsão de quais os certificados a adquirir consoante as características de uma empresa. A informação extraída facilita a tomada de decisão relativamente ao conjunto de certificados que melhor se adapta às características da empresa. Este conjunto de certificados varia com a competitividade resultante do número de empresas na região e do número de empresas no sector de atividade em que a nova empresa se encontra inserida. Os resultados apresentados nesta dissertação fornecem às novas empresas uma orientação inicial no mercado competitivo."
  },
  {
    "keywords": [
      "DevOps",
      "Continuous delivery",
      "Automation",
      "Entrega contínua",
      "Automação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "DevOps: the new development process",
    "autor": "Rodrigues, Guilherme dos Santos",
    "data": "2016-10-21",
    "abstract": "As result of the disruption caused by the appearance of digital companies with an agile\ndelivery model, there is a need to review the delivery model in organisations. The DevOps\nphilosophy aims to provide an answer to this problem, bringing all of the individuals\nresponsible for the delivery to work collaboratively, with the support of a set of tools to automate\nand streamline all of the processes. The objective of this dissertation is to re-evaluate\nand, consequently, improve the delivery model within Deloitte, proposing an automated\nprocess for code analysis, functional tests and deployment of software packages. To this\nend, a set of cutting-edge tools were analysed and a case study in the context of a real\nproject was built, in order to put into practice the automation of the processes developed."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Developing deep learning computational tools for cancer using omics data",
    "autor": "Peixoto, Luís Miguel da Cunha",
    "data": "2018",
    "abstract": "There has been an increasing investment in cancer research that generated an enormous amount of biological and clinical data, especially after the advent of the next-generation sequencing technologies. To analyze the large datasets provided by omics data of cancer samples, scientists have successfully been recurring to machine learning algorithms, identifying\npatterns and developing models by using statistical techniques to make accurate\npredictions.\nDeep learning is a branch of machine learning, best known by its applications in artificial\nintelligence (computer vision, speech recognition, natural language processing and\nrobotics). In general, deep learning models differ from machine learning “shallow” methods\n(single hidden layer) because they recur to multiple layers of abstraction. In this way, it\nis possible to learn high level features and complex relations in the given data.\nGiven the context specified above, the main target of this work is the development and\nevaluation of deep learning methods for the analysis of cancer omics datasets, covering both\nunsupervised methods for feature generation from different types of data, and supervised\nmethods to address cancer diagnostics and prognostic predictions.\nWe worked with a Neuroblastoma (NB) dataset from two different platforms (RNA-Seq\nand microarrays) and developed both supervised (Deep Neural Networks (DNN), Multi-Task\nDeep Neural Network (MT-DNN)) and unsupervised (Stacked Denoising Autoencoders (SDA))\ndeep architectures, and compared them with shallow traditional algorithms.\nOverall we achieved promising results with deep learning on both platforms, meaning\nthat it is possible to retrieve the advantages of deep learning models on cancer omics data.\nAt the same time we faced some difficulties related to the complexity and computational\npower requirements, as well as the lack of samples to truly benefit from the deep architectures.\nThere was generated code that can be applied to other datasets, wich is available in a\ngithub repository https://github.com/lmpeixoto/deepl_learning [49]."
  },
  {
    "keywords": [
      "Storage",
      "Remote",
      "Modular",
      "Flexible",
      "Extensible",
      "Armazenamento",
      "Remoto",
      "Flexível",
      "Extensível",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "RSafeFS: sistema de ficheiros modular para armazenamento remoto",
    "autor": "Leitão, Diogo Lúzio",
    "data": "2021-02-22",
    "abstract": "File systems are widely used for storing digital information, as they offer abstractions that\nallow data to be intuitively separated and organized through files and directories, according\nto the requirements of applications and users. The continuous growth of data volume and\ncomplexity leads to the constant evolution of these systems. However, the complexity of\nintegration of new features and lack of continuous support, leads to many file systems not\nbeing adopted in practice.\nIn this sense, stackable file systems have emerged, which allow the development of complex\nfile systems, providing existing systems with new functionalities through independent\nprocessing layers. Despite this, the development of these systems presents some challenges,\nnamely in terms of speed of implementation, portability, and resilience, since they are\ndeveloped in kernel. In this way, later solutions emerged that allowed the development of\nfile systems in user space, thus mitigating some of the problems identified in the development\nof this type of file systems. However, these solutions have not been properly explored in the\ndevelopment of remote file systems.\nTherefore, this dissertation presents RSafeFS, a platform that extends the SafeFS system to\nallow developing modular, flexible and extensible remote file systems in user space. The\nproposed solution enables extensible remote file system implementations that adjust to the\nrequirements of different types of applications and storage workloads. It was then necessary\nto develop a layer that would allow an RSafeFS instance to operate as a system server, and\na communication layer, based on remote procedure calls (RPCs), to allow interoperability\nbetween client and server instances. To demonstrate the ease of integration of new features,\ntaking advantage of the modularity and flexibility of RSafeFS, the developed prototype was\nequipped with two layers of caching, namely data and metadata, which aim to improve\nsystem peformance. The results obtained with this prototype reveal that the file systems\ndeveloped through RSafeFS obtain performances comparable to remote storage solutions\nbased on FUSE. Furthermore, with the processing layers developed it is possible to adjust\nthe system to different types of workloads, allowing, for example, to improve system\nperformance by 1.5× in certain workloads."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "In-vehicle object detection with YOLO algorithm",
    "autor": "Farinha, João Simões",
    "data": "2018",
    "abstract": "With the growing computational power that we have at our disposal and the ever-increasing amount of\ndata available the field of machine learning has given rise to deep learning, a subset of machine learning\nalgorithms that have shown extraordinary results in a variety of applications from natural language\nprocessing to computer vision. In the field of computer vision, these algorithms have greatly improved\nthe state-of-the-art accuracy in tasks associated with object recognition such as detection. This thesis\nmakes use of one of these algorithms, specifically the YOLO algorithm, as a basis in the development\nof a system capable of detecting objects laying inside a car cockpit. To this end a dataset is collected\nfor the purpose of training the YOLO algorithm on this task.\nA comparative analysis of the detection performance of the YOLOv2 and YOLOv3 architectures\nis performed.Several experiments are performed by modifying the YOLOv3 architecture to attempt\nto improve its accuracy. Specifically tests are performed in regards to network size, and the multiple\noutputs present in this network. Explorative experiments are done in order to test the effect that parallel\nnetwork might have on detection performance. Lastly tests are done to try to find an optimal learning\nrate and batch size for our dataset on the new architectures."
  },
  {
    "keywords": [
      "Distributed systems",
      "Satisfiability modulo theories",
      "Software testing",
      "Offline monitoring",
      "Dynamic race detection",
      "Sistemas distribuídos",
      "Teste de software",
      "Monitorização offline",
      "Deteção dinâmica de races",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Scalable trace analysis of distributed systems: finding data races",
    "autor": "Pereira, João Carlos Mendes",
    "data": "2020-07-14",
    "abstract": "Distributed Systems and Protocols are widely employed in the infrastructure that supports\nthe Internet and the services available online such as streaming services and social networks.\nAt the same time, they are well known for usually being hard to implement correctly, even\nwhen this task is left to experienced programmers. Consequently, Distributed Systems are\nprone to suffer from distributed concurrency bugs, which are a frequent source of significant\nservice outages. Thus, it is of the utmost importance to ensure that widely-used distributed\nsystems are reliable and do not suffer from this kind of bugs.\nFormal Verification looks like a promising way to achieve this. However, we argue that\nthe currently available techniques require too much of an investment in order to verify\ncorrectness of implementations of complex distributed systems. Instead, we defend the\nusage of clever testing techniques and tools for all but the most critical of contexts. In this\ndissertation, we present one such tool – Spider – designed to automatically detect data races\nfrom traced executions of distributed systems. Data races originate when two memory\naccesses to the same memory location occur concurrently and they have been shown to be a\nmajor source of concurrency bugs in distributed systems. Unfortunately, data races are often\ntriggered by non-deterministic event orderings that are hard to detect when testing complex\ndistributed systems.\nSpider encodes the causal relations between the events in the trace as a symbolic constraint\nmodel, which is then fed into an SMT solver to check for the presence of conflicting\nconcurrent accesses. To reduce the constraint solving time, Spider employs a pruning\ntechnique aimed at removing redundant portions of the trace. Our experiments with\nmultiple benchmarks show that Spider is effective in detecting data races in distributed\nexecutions in a practical amount of time, providing evidence of its usefulness as a testing\ntool."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Teste baseado em modelos de aplicações Android",
    "autor": "Vale, Pedro Miguel Braga do",
    "data": "2018-12-14",
    "abstract": "With the evolution of smartphones and the growing number of its users, mobile applicati ons are regularly used by more and more people. Because of this growing use of mobile\napplications, it is critical to ensure their quality. The graphical user interface (GUI) is a very\nrelevant component in these applications, since it enables the user to interact with the avai lable resources. A malfunctioning of it may make it impossible for the application to work\nproperly and, consequently, for the software system to be invalidated. One way to ensure a\nsmooth operation of the system is by performing software tests. Model-based testing (MBT)\nis a black-box technique that checks whether software has the expected behavior. The MBT\nfocuses on generating and running tests from a system under test (SUT) model.\nThis dissertation continues the development of an MBT tool called TOM. After validating\nthe Web components of said tool, we have now developed a component of generation and\nexecution of test cases for Android applications. In the course of the dissertation we show case the various decisions and changes made in the TOM tool during the implementation\nof this new component, presenting at the end two case studies to prove the operation of the\nAndroid component."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Mobile patient relationship mangement: a case study",
    "autor": "Moreira, António Pedro da Rocha",
    "data": "2018",
    "abstract": "Há cerca de duas décadas atrás, grande parte dos hospitais portugueses começaram a\nutilizar um sistema de notificação de pacientes, para os alertar dos seus eventos médicos\nnos hospitais, como consultas, cirurgias, exames, tratamentos, entre outros, através de mensagens\nde texto. Este sistema de notificação é ainda usado nos dias de hoje, mas enfrenta\num grande problema: a quantidade de dinheiro avultada que é gasta com as empresas\nde telecomunicações. Ainda que cada mensagem custe uma fração de cêntimos, poderá\nfacilmente ser representada num gasto superior a 50,000 euros anuais para um hospital.\nUma vez que a tecnologia e o uso de smartphones tem evoluído de forma tão rápida, é estimado\nque na próxima década, quase toda a população portuguesa possuirá ou poderá ter\nacesso indireto a um smartphone. Por essas razões, o objetivo principal deste projeto de dissertação,\né desenhar e desenvolver uma aplicação móvel de forma a substituir o serviço de\nnotificação atual, por uma aplicação móvel que notificará o paciente através de notificações\npush, cuja informação poderá ser salva no calendário do próprio smartphone, traduzindo-se\nnuma maior comodidade para o paciente, assim como a erradicação de custos no envio de\nnotificações por parte do hospital. A principal motivação é, portanto, suprimir esses custos\npara o hospital, aproximar os pacientes do hospital, integrar outros sistemas na aplicação e\ntornar o sistema de notificação mais eficiente. De forma a gerir os utilizadores da aplicação\nconsiderou-se necessário o desenvolvimento de uma aplicação web.\nNeste contexto, a presente dissertação apresenta então uma aplicação web, desenvolvida\nem AngularJS, orientada para a gestão de utilizadores da aplicação móvel e apresenta também\na aplicação móvel com a funcionalidade principal de notificar o paciente dos seus\neventos médicos, oferecendo também funcionalidades como a consulta do seu histórico de\neventos médicos passados, consulta de eventos médicos futuros, bem como a verificação\ndos eventos médicos próximos da data."
  },
  {
    "keywords": [
      "577.2:681.3",
      "681.3:577.2",
      "61:681.3",
      "681.3:61"
    ],
    "titulo": "Desenvolvimento de um sistema integrado para o tratamento de dados de sequenciação de próxima geração",
    "autor": "Reis, Marco André Ferreira",
    "data": "2013",
    "abstract": "A sequenciação de próxima geração veio permitir a sequenciação em paralelo de milhões de\npares de bases de DNA / RNA, tendo tido desde o início um grande impacto, ao ponto de se\ntornar o método escolhido em projetos de grande escala, em detrimento do método de Sanger.\nEntre as principais aplicações desta tecnologia encontram-se a análise em larga escala\nda metilação de DNA, o Chip-Seq para análise da interação entre proteínas e DNA ou RNA, e\no mapeamento de rearranjos estruturais. Destacam-se, especialmente, a sequenciação de\nnovos organismos ou indivíduos, o estudo de polimorfismos de nucleótido único (DNA-Seq) e\na análise de expressão genética (RNA-Seq).\nNeste trabalho, foi desenvolvido um sistema onde foram integradas ferramentas necessárias\npara estudos de DNA-Seq e RNA-Seq. Inicialmente, foi efetuado um estudo das aplicações\nexistentes, tendo de seguida sido selecionadas as que se destacaram em parâmetros\ncomo a facilidade de utilização, documentação e possibilidade de integração com as restantes\nferramentas do sistema. O sistema foi desenvolvido utilizando-se as linguagens de programação\nRuby, Java e R, sendo as principais funcionalidades o estudo de polimorfismos, a\nassemblagem de novo e a análise de expressão genética a partir de dados de RNA-Seq. Este\npermite uma utilização simplificada e semiautomática dos vários programas, sendo acessível\na utilizadores com poucos conhecimentos informáticos.\nO sistema foi testado em três casos de estudo: caracterização de duas estirpes de\nMycobacterium Tuberculosis, assemblagem de novo da Pseudomonas str. M1 e o estudo da\nexpressão genética em amostras de Saccharomyces cerevisiae."
  },
  {
    "keywords": [
      "Algoritmos evolucionários",
      "Otimização",
      "Machine learning",
      "Network function virtualization",
      "Segment routing",
      "Evolutionary algorithms",
      "Optimization",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Otimização de recursos de rede para encadeamento de serviços em Segment Routing",
    "autor": "Moreira, Gonçalo Dias Camaz",
    "data": "2021",
    "abstract": "O Segment Routing (SR) é uma implementação de Software Defined Networking (SDN)\npara encaminhamento de tráfego. Baseado num paradigma de source routing, cada nodo\nna fronteira da rede adiciona um conjunto de etiquetas ao cabeçalho dos pacotes e define\nexplicitamente o caminho que cada fluxo de tráfego deverá percorrer até ao destino. Cada\netiqueta é um segmento que identifica uma instrução topológica, um serviço ou mesmo\numa instrução de processamento. Embora o SR seja muito parecido com o Multi Protocol\nLabel Switching (MPLS) no que respeita ao encaminhamento simples de pacotes, a sua\nutilização simplifica os processos de gestão da rede. Por outro lado, o SR também proporciona\nsoluções para problemas de escalabilidade existente em implementações SDN como\no OpenFlow. Esta tecnologia está rapidamente a tornar-se um standard e é já suportada por\nempresas importantes, como a Cisco e a Huawei, desempenhando um papel importante na\narquitetura das futuras redes 5G. Face ao desenvolvimento desta nova tecnologia, surgiram\nnovos tipos de serviço que tiram partido das capacidades da mesma. Devido ao crescente\nnúmero de dispositivos 5G, surgiu a necessidade de disponibilizar os serviços em diversos\npontos da rede, fornecendo assim resposta às necessidades requeridas. Como tal, as operadoras\nde telecomunicações desenvolveram o conceito de Network Function Virtualization\n(NFV).\nTodavia existem questões importantes que necessitam ser tratadas, nomeadamente, como\ndistribuir os serviços pela topologia de forma a reduzir o nível de utilização das ligações\ne nodos da mesma. Este trabalho tem como objetivo a exploração de soluções baseadas\nem SR e em NFV, recorrendo a técnicas de otimização como algoritmos evolucionários e\nmachine learning."
  },
  {
    "keywords": [
      "Microservices",
      "Resilience",
      "Patterns",
      "Service degradation",
      "Distributed systems",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Improving the resilience of microservices-based applications",
    "autor": "Silva,  Marco António Rodrigues Oliveira",
    "data": "2021-02-19",
    "abstract": "Atualmente, escalabilidade, manutenibilidade e disponibilidade são algumas das medidas mais utilizadas na avaliação qualitativa de software. Com uma presença cada vez maior de produtos de software no nosso dia a dia, há consequentemente a necessidade de torná-los melhores aos olhos do utilizador, surgindo novos desafios a serem explorados e superados na hora de projetar e desenvolver produtos de software. Mais focado neste tema de dissertação de mestrado, a resiliência é de facto um ponto chave para o sucesso de um qualquer produto de software. Cada vez mais as pessoas se encontram diretamente ligadas a produtos de software no seu dia a dia, o que torna o bom funcionamento destes essencial. Assim sendo, o estudo de metodologias que permitam aumentar a resiliência e consequentemente a disponibilidade destes serviços ganha relevância. O principal objetivo desta dissertação é desenvolver uma metodologia para aumentar a resiliência de soluções orientadas aos microsserviços. Assim, é fundamental primeiro entender quais soluções já desenvolvidas para esse fim. Após reunir um conjunto de técnicas para aumentar a resiliência, analisamos um caso de estudo procurando possíveis problemas de resiliência. Para além desta procura de vulnerabilidades, foram apresentadas propostas para a sua resolução, tendo em conta o conjunto de soluções já levantado. Por fim, e avançando para a construção da metodologia alvo da dissertação, procedeu-se à análise de todas as propostas apresentadas, bem como a caracterização das interações problemáticas. Desta forma, foi possível extrair a informação necessária do estudo para a construção da metodologia. Como resultado deste estudo, também foi possível identificar uma nova proposta para aumentar a resiliência diante das necessidades do estudo de caso e da recorrência em que esta se tornou útil."
  },
  {
    "keywords": [
      "Software defined networking",
      "Data centers",
      "Engenharia de tráfego",
      "Balanceameto de carga",
      "Mininet",
      "Traffic engineering",
      "Load Balancing",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desenvolvimento de mecanismos de engenharia de tráfego em data centers através de SDN",
    "autor": "Leal, Anthony Jonathan da Silva",
    "data": "2016-05-25",
    "abstract": "O crescente uso de aplicações que geram altos volumes de tráfego motivou o\ndesenvolvimento de novas abordagens de Engenharia de Tráfego que pudessem melhorar o\ndesempenho e eficiência das infraestruturas de comunicação, e.g. redes dos ISPs (Internet\nService Providers), Data Centers, etc. Neste contexto, a área denominada por Software Defined\nNetworking (SDN) poderá ser útil para a definição de alguns mecanismos inovadores nestes\ncenários. Este paradigma, que tem sido recentemente explorado, oferece novas tecnologias e\nprotocolos proporcionando novas oportunidades para uma gestão mais expedita e eficiente das\ninfraestruturas de rede.\nEste trabalho propõe-se contribuir para o desenvolvimento de mecanismos de Engenharia\nde Tráfego na área das SDN. Os mecanismos a estudar estarão orientados para tarefas de\nbalanceamento de carga em redes de Data Centers e implementados com a ferramenta de\nemulação Mininet. Para tal, será feito inicialmente um estudo das diversas arquiteturas de redes\nde Data Centers, dos conceitos que englobam o paradigma SDN e uma análise das estratégias\nde balanceamento de carga já existentes. De seguida será desenvolvida uma bancada de testes\ne implementados alguns mecanismos de balanceamento de carga. Posteriormente, serão\nefetuados testes de desempenho aos mecanismos desenvolvidos"
  },
  {
    "keywords": [
      "Mineração de opiniões",
      "Mineração de sentimentos",
      "Índices de satisfação",
      "Profiling",
      "Análise de opiniões",
      "Análise de sentimentos",
      "Categorização de opiniões",
      "Categorização de sentimentos",
      "Feedback do consumidor",
      "Mineração de críticas",
      "Métricas de avaliação de performance",
      "Opinion mining",
      "Sentiment mining",
      "Satisfaction indexes",
      "Profiling",
      "Opinion analysis",
      "Sentiment analysis",
      "Opinion categorization",
      "Sentiment categorization",
      "Consumer’s feedback",
      "Reviews mining",
      "Performance evaluation metrics",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Mineração de opiniões e de sentimentos para o estabelecimento de índices de satisfação",
    "autor": "Fernandes, Hugo Alexandre Machado",
    "data": "2018",
    "abstract": "Nunca como hoje as opiniões e os sentimentos de cada ser humano\ndesempenharam um papel tão fundamental no quotidiano da sociedade.\nA mineração de opiniões e de sentimentos providencia um grau de\nconhecimento bastante bom sobre o nível de satisfação que um dado\nnegócio, um acontecimento, ou uma decisão estratégica pode gerar. Estar\nna posse de um índice de satisfação sobre um determinado bem, serviço,\nou acontecimento é dispor de condições que nos permitem alcançar\nsituações de sucesso que podem ser usadas para melhorar o planeamento\nou a estruturação de uma decisão ou mesmo de um negócio, face ao\nfeedback que usualmente provocam.\nO conhecimento recolhido por este tipo de processo de mineração e,\nconsequentemente, acumulado sobre a experiência que um certo cliente\ndetém num determinado negócio, ou sobre a opinião (ou sentimento) que\num certo indivíduo possui relativamente a um acontecimento de grande\nrelevância político ou social permite definir novas estratégias de abordagem\nem determinados mercados. Neste trabalho de dissertação realizámos a\nimplementação de um sistema de mineração de opiniões e de sentimentos\ntendo como finalidade de definir e estabelecer um dado índice de satisfação,\nde forma a que seja possível angariar conhecimento útil sobre aspetos que potenciem a geração de opiniões e de sentimentos, potenciando novas\noportunidades de negócio"
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Plataforma de apoio à prática de cuidados de enfermagem em contexto hospitalar",
    "autor": "Oliveira, Daniela Sofia Rijo",
    "data": "2017",
    "abstract": "Nas instituições de saúde, um dos serviços mais exigentes em termos de prestação\nde cuidados de saúde contínuos e atenção por parte dos profissionais, é o serviço\nde internamento hospitalar. Aqui existe uma constante necessidade de informações\natualizadas sobre o registo clínico eletrónico do paciente, através de sistemas de apoio\nao ato clínico facilmente utilizáveis.\nNeste contexto, a presente dissertação apresenta uma nova plataforma web para a\nmonitorização diária de pacientes, projetada para ser usada por profissionais de saúde,\nespecialmente enfermeiros. A aplicação é baseada em React, uma Library Open Source\nde JavaScript, para criar User Interfaces (UI).\nA nova plataforma é constituída por duas principais componentes: Um quadro de\nenfermagem que contém informação de todos os pacientes internados, numa unidade\nde saúde, num determinado momento. Este quadro baseia-se em indicadores, com\no principal objetivo de alertar ações futuras (Exames, Análises, Medicação, Dietas, Jejum\ne Cirurgias), relativamente a cada paciente. A outra componente designa-se por\nregisto clínico de internamento e contém todas as informações sobre sobre as ações\nsupramenciadas. De notar que cada unidade de saúde deverá adaptar a plataforma às\nsuas principais necessidades."
  },
  {
    "keywords": [
      "Metagenómica",
      "Stress ambiental",
      "Ecossistemas",
      "Rios",
      "Microrganismos",
      "Metagenomics",
      "Stressors",
      "Ecosystems",
      "Rivers",
      "Microorganisms",
      "Ciências Naturais::Ciências Biológicas"
    ],
    "titulo": "Metagenomics reveal taxonomic diversity of microbial communities along gradients of environmental stress in river basins of North Portugal",
    "autor": "Gonçalves, Pedro Alexandre Cunha Senra Bogas",
    "data": "2024-07-09",
    "abstract": "Uma das consequências do problema emergente das alterações climáticas, devido ao aumento do\nstress antropogénico, é a falta de informação sobre o impacto específico destas alterações nos ecossis temas e as potenciais medidas para as atenuar. Este facto cria uma necessidade premente de melhores\nferramentas de biomonitorização para avaliar o estado de diversos ecossistemas. Especificamente, os\necossistemas de água doce que desempenham um papel crucial na prestação de bens e serviços essen ciais à humanidade. No entanto, os esforços de investigação e biomonitorização concentram-se frequen temente nos invertebrados e algas, deixando uma lacuna na monitorização dos microrganismos que são\nchave para o funcionamento dos ecossistemas aquáticos. Muitos microrganismos que habitam nestes\nambientes não são cultiváveis em meios tradicionais, o que constitui um desafio para a compreensão das\nsuas respostas a vários gradientes de stress ambiental. Para melhorar a compreensão e facilitar a tomada\nde decisão para a proteção destes ecossistemas, torna-se imperativo dispor de dados mais precisos so bre os microrganismos presentes nestes ecossistemas. Este trabalho teve como objetivo colmatar essa\nlacuna, usando a metagenómica para ajudar a compreender como a comunidade de fungos e bactérias\né afetada pelos diferentes fatores de stress ambiental. As amostras biológicas foram recolhidas em 50\nlocais localizados em 4 bacias hidrográficas do Norte de Portugal, dos rios Ave, Cávado, Lima e Minho. A\namostragem consistiu em recolher folhada naturalmente acumulada no leito dos rios. As amostras foram\nrecolhidas durante o verão de 2020. Para além do material biológico, foram também recolhidos dados de\nvariáveis ambientais nos mesmos locais de amostragem. O material genético foi extraído e foi feita uma\nsequenciação completa do genoma das comunidades presentes. A identificação taxonómica foi feita com\nrecurso ao servidor web Kaiju. Alguns taxa destacaram-se pela sua dominância nas amostras em relação\naos restantes. A riqueza de taxa de fungos apresentou uma resposta negativa à presença combinada\ndos fatores de stress ambiental, à intensificação do uso do solo, e ao aumento da condutividade e da\nconcentração de azoto inorgânico dissolvido. Por outro lado, a riqueza taxonómica dos fungos aumentou\ncom o aumento da altitude. A intensificação do uso do solo foi o fator de stress que melhor explicou\na riqueza em taxa. A diversidade taxonómica das bactérias apresentou uma resposta à velocidade da\ncorrente de acordo com o modelo de Gauss. Isto significa que um distúrbio moderado pode favorecer a\nbiodiversidade, o poderá ser explicado pela hipótese do distúrbio intermédio."
  },
  {
    "keywords": [
      "616-07:681.324",
      "681.324:616-07",
      "Ciências Médicas::Medicina Clínica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Meios complementares de diagnóstico e terapêutica no Processo Clínico Electrónico via \"m-Health\"",
    "autor": "Pereira, Ana Sofia Vieira Tavares de Amorim",
    "data": "2014",
    "abstract": "As aplicações Mobile Health (M-Health) desenvolvidas para dispositivos\nmóveis têm sido alvo de muitas investigações, sendo a área da Saúde um\ndos principais focos de desenvolvimento, em que estas aplicações pretendem\nauxiliar nas práticas médicas. Um dos principais desa os da área da saúde\né melhorar a acessibilidade e a disponibilidade da informação clínica dos\npacientes. Com este desa o em mente, foi desenvolvida a Agência para a\nIntegração, Difusão e Arquivo de Informação Médica (AIDA) de forma a garantir\na interoperabilidade entre os vários Sistema de Informação Hospitalar\n(SIH), estando esta implementada num dos maiores centros hospitalares de\nPortugal, o CHP.\nO desenvolvimento de uma aplicação para dispositivos móveis que permita\na visualização e consulta da informação clínica dos pacientes, providenciando\num acesso rápido e fácil a esta informação aos pro ssionais de saúde, é o principal\nobjetivo do presente projeto. A aplicação pode ser considerada como\numa ferramenta adicional do Processo Clínico Eletrónico da AIDA (AIDAPCE)\ne é para uso dos pro ssionais de saúde pertencentes ao CHP. Este\nprojeto expõe assim uma nova metodologia que se baseia nos princípios da\ncomputação calma para a apresentação dos relatórios dos Meios Complementares\nde Diagnóstico e Terapêutica (MCDT) dos pacientes e pretende\nmelhorar a qualidade e a e ciência na prestação dos cuidados de saúde.\nUm dos objetivos deste projeto incide na realização de uma prova de conceito\nà metodologia abordada, de forma a avaliar o impacto da implementação\nda aplicação desenvolvida no CHP. Desta forma, foi realizada uma análise\nStrengths Weaknesses Opportunities and Threats (SWOT), com o intuito de\nidenti car as suas forças e fragilidades, e foram realizados testes de e ciência\ne usabilidade, em que para este último recorreu-se ao método de avaliação de\nusabilidade do tipo inquérito. A realização destes estudos permitiu averiguar\nos resultados que teria a implementação da aplicação, veri cando que esta\nseria bem aceite e utilizada pelos pro ssionais de saúde."
  },
  {
    "keywords": [
      "Brain connectivity",
      "Data visualization",
      "Machine learning",
      "Medical imaging informatics",
      "Pattern recognition",
      "Conectividade cerebral",
      "Descoberta de padrões",
      "Informática de imagem médica",
      "Visualização de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "On pattern recognition of brain connectivity in resting-state functional MRI",
    "autor": "Cardoso, Ana Catarina Salgueiro",
    "data": "2020-01-03",
    "abstract": "The human urge and pursuit for information have led to the development of increasingly complex\ntechnologies, and new means to study and understand the most advanced and intricate biological\nsystem: the human brain. Large-scale neuronal communication within the brain, and how it relates to\nhuman behaviour can be inferred by delving into the brain network, and searching for patterns in\nconnectivity. Functional connectivity is a steady characteristic of the brain, and it has been proved to be\nvery useful for examining how mental disorders affect connections within the brain. The detection of\nabnormal behaviour in brain networks is performed by experts, such as physicians, who limit the process\nwith human subjectivity, and unwittingly introduce errors in the interpretation. The continuous search for\nalternatives to obtain faster and robuster results have put Machine Learning and Deep Learning in the\nleading position of computer vision, as they enable the extraction of meaningful patterns, some beyond\nhuman perception.\nThe aim of this dissertation is to design and develop an experiment setup to analyse functional\nconnectivity at a voxel level, in order to find functional patterns. For the purpose, a pipeline was outlined\nto include steps from data download to data analysis, resulting in four methods: Data Download, Data\nPreprocessing, Dimensionality Reduction, and Analysis. The proposed experiment setup was modeled\nusing as materials resting state fMRI data from two sources: Life and Health Sciences Research Institute\n(Portugal), and Human Connectome Project (USA). To evaluate its performance, a case study was\nperformed using the In-House data for concerning a smaller number of subjects to study. The pipeline\nwas successful at delivering results, although limitations concerning the memory of the machine used\nrestricted some aspects of this experiment setup’s testing.\nWith appropriate resources, this experiment setup may support the process of analysing and extracting\npatterns from any resting state functional connectivity data, and aid in the detection of mental disorders."
  },
  {
    "keywords": [
      "Cloud",
      "Monitoring",
      "Control",
      "Nuvem",
      "Monitorização",
      "Controlo",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Monitorização e controlo de aplicações na nuvem",
    "autor": "Silva, Daniel António Landeira da",
    "data": "2021-03-05",
    "abstract": "Cloud orchestration systems, as Kubernetes, allow us to dynamically manage aspects\nsuch as location of components. This makes traditional resource-oriented monitoring\nsystems inadequate. They also make it desirable that control mechanisms act directly on the\norchestrator and not on individual components.\nThis project aims to design, develop and test an application for monitoring and control\ndistributed database systems, solving the challenges posed by this new environment. This\ndissertation motivated by project H2020 CloudDB Appliance."
  },
  {
    "keywords": [
      "Preservação de bases de dados",
      "Database preservation",
      "Digital preservation",
      "Preservação digital",
      "DBML",
      "SIARD",
      "CHRONOS",
      "Bases de dados",
      "681.3",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Database preservation toolkit",
    "autor": "Coutada, Miguel",
    "data": "2014-12-15",
    "abstract": "A preservação de sistemas de informação é um dos maiores desafios da\npreservação digital. Entre esses sistemas, encontram-se as bases de dados.\nEstas servem de sustento à maior parte dos sistemas de gestão de\ninformação, apresentando, assim, um grande interesse no que diz respeito\nà sua preservação.\nSe por um lado existe a necessidade de migrar as bases de dados para\noutras mais atuais, que vão aparecendo com o evoluir da tecnologia, por\noutro, existe também a necessidade de preservar a informação nelas contida\ndurante um longo período de tempo, quer seja por questões legais,\nquer seja por questões de arquivo. Desta forma, essa informação deverá\nestar disponível independentemente do sistema de gestão da base de dados.\nNesta área, os produtos que existem para preservação de base de dados\nrelacionais ainda são poucos - CHRONOS e SIARD são os principais. O\nprimeiro é muitas das vezes inacessível devido ao preço que apresenta.\nO segundo apenas suporta funcionalidades básicas.\nAssim, existe a oportunidade de explorar as principais qualidades e fraquezas\ndas ferramentas existentes, de forma a melhorar a ferramenta de\npreservação de base de dados db-preservation-toolkit, componente extraída\ndo projeto RODA.\nDeste modo, esta ferramenta foi melhorada, quer pela adição de funcionalidades\nde forma a ser capaz de suportar um maior número de sistemas\nde gestão de bases de dados, quer pela adição do suporte ao formato\nSIARD, mas também pelo desenvolvimento de uma interface que\npossibilita a visualização e pesquisa de informação numa base de dados\narquivada."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Análise de sentimentos em conteúdos textuais",
    "autor": "Lopes, João Miguel Ferreira",
    "data": "2018",
    "abstract": "A grande quantidade de dados que é gerada diariamente em empresas ou por pessoas em\ntermos individuais despertou a atenção de algumas entidades que viram o grande interesse e\npotencial da exploração dessa informação. O desenvolvimento de soluções orientadas para esse\ntipo de exploração começou, assim, a ser incentivado de forma muito dinâmica. Na maioria dos\ncasos, essa exploração tem como objetivo alimentar sistemas de profiling, que posteriormente\ntentam estabelecer algum tipo de padrão comportamental através da utilização de uma ou\nmais técnicas de análise de dados.\nA análise de sentimentos presentes em textos é uma das áreas de análise de dados que também\ntem despertado muito interesse nos últimos anos, tendo sido gradualmente aplicada sobre\numa gama de problemas muito diversificada para determinar, por exemplo, como é que um\ndado produto está a ser aceite pelas pessoas. Contudo, embora existam já vários modelos\ndesenvolvidos para este tipo de análise, a sua precisão ainda é muito questionada, em parte\ndevido às dificuldades que existem na realização deste tipo de análise, na qual, de certa forma,\né necessário que a linguagem escrita seja compreendida de forma natural por um dado conjunto\nde algoritmos.\nNeste trabalho de dissertação explorámos esta vertente de análise de dados, com particular\nênfase na análise de sentimentos em conteúdos textuais. Foi aplicado um conjunto de transformações\nresponsáveis pelo pré-processamento e transformação dos dados para um formato\napropriado para serem utilizados pelos modelos. Ao longo da construção do pré-processamento\nfoi, ainda, demonstrada a importância desta fase, para qualquer problema de análise de dados,\nque sem ela não é possível compreender o problema de análise o que frequentemente leva a\nque os resultados obtidos não sejam os melhores possíveis. Após o pré-processamento dos\ndados, foram desenvolvidos três modelos de análise de sentimentos em textos: modelo supervisionado\nde aprendizagem automática, modelo baseado em dicionários de sentimentos e modelo\nhíbrido. Qualquer um dos modelos faz uso de técnicas de análise de textos de modo a serem\nreconhecidos sentimentos e respetivas polaridades, aspetos a que os sentimentos se referem, entre outros. Dos três modelos desenvolvidos, o modelo híbrido foi o que obteve melhores\nresultados, com uma percentagem de classificações incorretas aproximadamente igual a 6% do\ntotal dos dados de teste."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Geração automática dum mapa do coberto do solo português",
    "autor": "Valente, Nuno Afonso Gonçalves Solha Moreira",
    "data": "2022-04-26",
    "abstract": "A aplicação de técnicas orientadas a objectos e de aprendizagem automática sobre imagens de satélite tem sido alvo de interesse nos últimos anos. O aumento da qualidade e quantidade de imagens, disponibilizadas por programas de observação da Terra como por exemplo o Programa Cope rnicus, levou à geração de uma grande quantidade de dados. De entre as várias aplicações destes dados destaca-se a criação de mapas do coberto do solo. Com a presente dissertação pretendia-se criar modelos de aprendizagem automática capazes de segmentar e classificar com precisão imagens de satélite, gerando automaticamente um mapa do coberto do território Português. Durante a dissertação foram realizadas várias experiências com as bandas espetrais do satélite Sentinel-2, com índices espetrais e com diversos conjuntos de classes do coberto. Foram testadas três arquiteturas nos modelos de aprendizagem automática treinados, que adotam duas técnicas diferentes para classificação das imagens. Numa das técnicas a classificação é orientada ao objeto, e neste caso a arquitetura adotada nos modelos foi uma rede neuronal artificial U-Net. Na outra técnica, a classificação é orientada ao pixel e os modelos de aprendizagem automática testados foram a floresta aleatória e a máquina de vetores de suporte. A acurácia global dos resultados obtidos variou entre 82.32% e 94.75%, dependendo fortemente do número de classes em que se classifica o coberto. O resultado de 94.75% foi obtido quando se classifica o coberto em apenas 5 dasses. Contudo conseguiu-se uma acurácia bem interessante de 92.37%, no modelo treinado para classificar 8 classes."
  },
  {
    "keywords": [
      "681.3:614",
      "614:681.3",
      "658.56"
    ],
    "titulo": "Avaliação da qualidade do registo clínico eletrónico",
    "autor": "Leal, Maria Filipa Rodrigues",
    "data": "2013",
    "abstract": "Os registos clínicos têm como função facilitar a continuação da prestação\nde cuidados, a documentação dos seus processos e a comunicação entre os\npro ssionais de saúde. Têm sido desenvolvidos protocolos para registos de\ndados clínicos, no entanto, existe pouco conhecimento sobre o que realmente\né documentado. O Registo Clínico Electrónico (RCE) encontra-se em ampla\nexpansão, recorrendo-se, cada vez mais, à sua implementação em unidades\nhospitalares, o que proporciona uma maior agilidade no tratamento dos processos\ne uma consequente melhoria na qualidade da abordagem ao historial\ndo paciente.\nA crescente focalização na avaliação do desenvolvimento de produtos, baseada\nem testes de garantia e no design do produto, está na origem do termo\nusabilidade, que ganha uma importância vertiginosa, à medida que aumenta\no número de pessoas que dependem de dispositivos técnicos para realizar tarefas.\nA usabilidade de um sistema informático pode ser de nida como uma\nqualidade inerente ao sistema, que possibilita que os utilizadores o utilizem\ncom satisfação, e cácia e e ciência. De facto, a adoção do RCE e a satisfa-\nção do utilizador estão intimamente associados à usabilidade do sistema. Ao\ncontrário de muitas indústrias, onde a usabilidade é a norma em design de\nproduto, a prática de usabilidade no RCE tem sido esporádica, não sistemá-\ntica, casual e super cial, em parte devido à falta de estruturas especí cas de\nRCE e de aplicação de métodos que avaliem a usabilidade.\nAssim, da conjunção da problemática de avaliação do RCE com o desa o\nde tirar o melhor partido das potencialidades dos sistemas RCE já implementados,\nsurge o presente projeto. Pretende-se apresentar uma abordagem\nglobal dos vários aspetos relacionados com a avaliação da qualidade do RCE nas unidades de saúde. Pretende-se ainda realizar essa avaliação ao RCE\nimplementado no CHAA, para se proceder à elaboração de um protótipo que\ncontemple as alterações mais emergentes ao sistema.\nDesta forma, foi avaliada a usabilidade dos sistemas SAM, SONHO, SAPE e\nAIDA, através de questionários, distribuídos a 38 participantes. Constataramse\nque as maiores di culdades sentidas com os utilizadores dos sistemas\nrelacionam-se com o elevado tempo de desempenho do sistema e com a falta\nde motivação para introduçao de registos informatizados. Contudo, os dados\nrecolhidos realçam as enormes vantagens desta desmaterialização hospitalar,\ncomo sejam a diminuição dos erros e do tempo de leitura e transmição dos\nregistos clínicos."
  },
  {
    "keywords": [
      "Spark",
      "Scheduling",
      "Energy Efficiency",
      "Agendamento",
      "Eficiência energética",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Energy efficiency aware job scheduling for scalable data processing tools",
    "autor": "Azevedo, Renato André Araújo",
    "data": "2023-12-15",
    "abstract": "Massive data processing tools for distributed environments such as Spark or Dask allow programmers\nto process massive amounts of data in data centers. A large portion of the operation costs of these\ninfrastructures corresponds to the energy consumption resulting in performing these operations.\nCurrent tools use simple algorithms for efficient scheduling of data processing jobs in distributed\ncomputing, relying on heuristics without considering the workload characteristics. Recent work explores\nefficient scheduling of data processing jobs in distributed computing, especially in heterogeneous environ ments, despite these infrastructures being typically homogeneous.\nThis dissertation makes an analysis of job executions in Spark and proposes EASAHUM a new al gorithm for job scheduling in massive data processing tools with energy efficiency concerns using the\nconclusions drawn. The implementation and evaluation in a simulator using real and synthetic execution\ntraces in Spark demonstrate that the algorithm can reduce energy consumption by up to 16% and reduce\njob execution time by up to 12.25% without significant impact on the scheduling time."
  },
  {
    "keywords": [
      "681.324:349.2",
      "349.2:681.324"
    ],
    "titulo": "Resolução de conflitos em linha : OntoLab uma aplicação ao direito laboral",
    "autor": "Ribeiro, João Pedro Pinto Vasques",
    "data": "2011-11-17",
    "abstract": "A Resolução Alternativa de Conflitos, visa, por um lado, promover o acesso à justiça,\nsendo um meio alternativo à resolução de conflitos judiciais, neste caso recorre-se aos tribunais\narbitrais e julgados de paz. Por outro lado, visa, também, apoiar a criação e o funcionamento de\nmeios extrajudiciais para resolução alternativa de conflitos, incluindo a mediação, negociação e\narbitragem.\nNo de Direito, especificamente na área do direito do trabalho, a utilização de métodos de\nresolução alternativa de conflitos é muito vantajosa. É conhecido o estado actual da justiça em\nPortugal, grande parte dos casos em litígio arrastam-se ao longo de anos nos tribunais, sem\nexistir previsão de resolução. O recurso à resolução de conflitos em linha, visa retirar dos\ntribunais vários destes casos, tornando assim a resolução do litígio mais célere, bem como\nmenos dispendiosa para ambas as partes.\nEstes métodos de resolução online de conflitos são uma abordagem bastante recente, que\nse serve da Internet e de ferramentas de suporte à decisão. Neste tipo de ambientes as partes\ninteragem e expõe os seus pontos de vista, em qualquer momento e em qualquer local, uma vez\nque estes sistemas estão disponíveis em linha.\nNesta dissertação procurou-se definir qual a melhor metodologia para o desenvolvimento\nde uma ontologia base, nesta área do conhecimento. Nesse sentido desenvolveu-se uma\nontologia e um motor de inferência de conhecimento que actua sobre ela de forma a\ndisponibilizar o conhecimento obtido ao sistema.\nCom a utilização das ontologias aliadas aos sistemas de resolução de conflitos em linha\nexistiram enormes ganhos para a justiça, na medida em que estes processos tendem a ser mais\ntransparentes, mais rápidos e mais justos."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of a database and web tool for the in silico characterization of plasmid data",
    "autor": "Santos, Catarina Freitas de Sousa",
    "data": "2017",
    "abstract": "Bacterial plasmids are mobile genetic structures capable of conferring selective advantages\nto their hosts, such as resistance to antibiotics, virulence genes and tolerance to pollutants.\nBy associating with other genetic elements, like integrons and transposons, plasmids provide\na platform for genetic recombination and for gene transfer between different bacterial\nspecies, allowing them to colonize multiple environments and guaranteeing their persistence.\nAlthough there are over 4000 complete plasmid sequences available in GenBank, most\nhave absent or non-standardized (disorganized) information regarding their isolation source,\nenvironment and year and country of isolation. Furthermore, a prediction about their mobility\nand incompability group is also lacking.\nThe goals of this thesis are, besides completing the missing information about plasmid\ndata, the development of a repository of fully sequenced plasmids and the development\nof easy-to-use web tools for the characterization of plasmid data regardless of their source\nenvironment and bacterial host. For the development of these tools, Shiny was used, which\nis a package from the R scientific computing environment.\nThe present work is organized as follows: the core concepts related to plasmids are\ndescribed, their background is characterized and a critical analysis of the available web tools\nfor plasmid classification is carried out. Then, the adopted approach and the development\n(implementation, outcomes) of the database and web tool are explained. Lastly, the main\nconclusions are highlighted."
  },
  {
    "keywords": [
      "Deep learning",
      "Machine learning",
      "Personalized medicine",
      "Transcriptomics",
      "Aprendizagem máquina",
      "Aprendizagem profunda",
      "Medicina personalizada",
      "Transcriptómica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Developing deep learnig methods to predict phenotypes and clinical outcomes from transcriptomics data",
    "autor": "Vieira, Maria Fernanda Silva",
    "data": "2022-03-24",
    "abstract": "Personalized medicine is a constantly growing area. Important goals of this field are early diagnosis and the discovery of new personalized treatments. Gene expression data play a key role at this level, as variations in these data can often offer explanations for some phenotypes. To this end, Machine Learning (ML) models capable of predicting biologically relevant information, have been widely used. \nDeep Learning (DL) is a branch of ML that has become popular over the past few years. The increasing amounts of data that have been generated, and the growing use of this type of models in biomedical areas, have been accelerating the analysis of biological processes associated with cancer and other complex diseases.\nIn this work, we focused on developing a framework that allows to create and evaluate distinct work-flows for the application of a variety of machine and deep learning models, working over gene expression data, including different options regarding data preprocessing pipelines, distinct ML and DL models, including traditional ML models, Dense Neural Networks, Convolutional Neural Networks and Variational Autoencoders. \nThe framework has been validated using different case studies, where the data sources were two of the main repositories of gene expression data (TCGA and GTEx). The goal of each case study was to predict important variables for clinical application. A variety of models were developed and evaluated for each case study, generally with competitive performance. \nFor the first case study, the task was to predict the type of cancer from TCGA data, and the best performing DL model was a dense neural network, being outperformed by a logistic regression model. In the second case, where the task was to predict the hypoxia score, the best DL model was a two dimensional convolutional neural network (2D CNN), being outperformed by the LightGBM model. As for the third case study, where the objective was to predict the aneuploidy score, the best model was an one dimensional convolutional neural network (1D CNN). For the fourth case, where the task was to predict body mass index, the best model was again a 1D CNN. Finally, in the fifth case study, where the main goal was to predict gene expression for a set of genes based on landmark genes, the best DL model was found by an 1D CNN, still slightly outperformed by linear regression. \nSome of the DL models developed in this work show promising results. However, these need to be improved in the future as they are not clinically applicable at this time. This framework can be reused for new problems and can be easily expanded."
  },
  {
    "keywords": [
      "Digital image correlation",
      "Scanning electron microscope",
      "3D reconstruction",
      "Correlação de imagens digitais",
      "Microscópio eletrônico de varrimento",
      "Reconstrução 3D",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Improving digital image correlation in the TopoSEM Software Package",
    "autor": "Ferreira, José Filipe de Sousa Matos",
    "data": "2023-03-06",
    "abstract": "TopoSEM is a software package with the aim of reconstructing a 3D surface topography of a microscopic sample\nfrom a set of 2D Scanning Electron Microscopy (SEM) images. TopoSEM is also able to produce a stability report\non the calibration of the SEM hardware based solely on output images.\nOne of the key steps in both of these workflows is the use of a Digital Image Correlation (DIC) algorithm, a\nno-contact imaging technique, to measure full-field displacements of an input image. A novel DIC implementation\nfine-tuned for 3D reconstructions was originally developed in MATLAB to satisfy the feature requirement of this\nproject. However, near real-time usability of the TopoSEM is paramount for its users, and the main barrier towards\nthis goal is the under-performing DIC implementation.\nThis dissertation work ported the original MATLAB implementation of TopoSEM to sequential C++ and its\nperformance was further optimised: (i) to improve memory accesses, (ii) to explore the available vector exten sions in each core of current multiprocessor chips processors to perform computationally intensive operations\non vectors and matrices of single and double-precision floating point values, and (iii) to additionally improve the\nexecution performance through parallelization on multi-core devices, by using multiple threads with a front wave\npropagation scheduler.\nThe initial MATLAB implementation took 3279.4 seconds to compute the full-field displacement of a 2576\npixels by 2086 pixels image on a quad-core laptop. With all added improvements, the new parallel C++ version\non the same laptop lowered the execution time to 1.52 seconds, achieving an overall speedup of 2158."
  },
  {
    "keywords": [
      "Fermentation",
      "NGS",
      "Saccharomyces cerevisiae",
      "Torulaspora delbrueckii",
      "Winemaking",
      "Fermentação",
      "Produção de  vinho"
    ],
    "titulo": "Exploitation and annotation of Torulaspora  delbrueckii genomes: comparison with  biotechnological data",
    "autor": "Torre, Carolina Santiago Garrido Dias da",
    "data": "2021-03-15",
    "abstract": "Nowadays, the most widely used yeast in wine, beer, and bread fermentations is Saccharomyces \ncerevisiae. However, in the past years, Torulaspora delbrueckii attracted interest due to its properties, \nfrom flavor and aroma-enhanced wine to the ability to be preserved longer in frozen dough. The main \nobjective of this thesis was to explore T. delbrueckii genomes publicly available and the ones belonging \nto our project’s collection, exploring their genomic information and establishing its relationship with their \norigins and biotechnological applications.\nIn the first phase, publicly available genomes of T. delbrueckii were explored, and their annotation was \nimproved. EggNOG-mapper was used to perform functional annotation of the deduced T. delbrueckii\ncoding genes, offering insights into its biological significance, and revealing 24 clusters of orthologous \ngroups (COG), gathered in three main functional categories: information storage and processing (28% of \nthe proteins), cellular processing and signaling (27%) and metabolism (23%). Small intra-species variability \nwas found when considering functional annotation of the four T. delbrueckii available genomes. A \ncomparative study was also conducted between T. delbrueckii genome and those from 386 fungal \nspecies, revealing high homology with species of Zygotorulaspora and Zygosaccharomyces genera, but \nalso with Lachancea and S. cerevisiae. Lastly, the phylogenetic placement of T. delbrueckii was assessed \nusing the core homologues found across 204 common protein sequences of 386 fungal species and \nstrains.\nIn a second phase, the genome of fifty-four T. delbrueckii strains were sequenced and data was explored.\nThe alignment, SNP statistics, annotation, among other steps, were attempted, for the first time, for those \nstrains. PCA analysis was performed with those strains and the ones publicly available, to better\nunderstand the connection between the strains’ technological groups. \nThe present work represents a successful effort to increase and improve the annotation of T. delbrueckii’s \ngenome. Overall, this work provides a starting point to unravel the diversity of potential biotechnological \napplications of T. delbrueckii."
  },
  {
    "keywords": [
      "Planos de preservação digital",
      "CLAV",
      "Digitalização",
      "Classificação",
      "Avaliação",
      "Digital preservation plans",
      "Digitization",
      "Classification",
      "Evaluation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "CLAV: planos de preservação digital",
    "autor": "Freitas, José Duarte Santos",
    "data": "2022-05-30",
    "abstract": "Não podendo deixar de aproveitar os benefícios da era tecnológica que vivemos a Administração Pública Portuguesa caminha a passos largos para a digitalização de todo o seu\nprocesso organizacional. Tal se explica através de diversos fatores que, apesar de díspares,\nse complementam e interligam entre si. Assim, primeiramente se pensarmos no fator da\nproteção ambiental, a digitalização vai permitir uma redução da utilização de papel e simultaneamente uma redução de custos com o mesmo. Por outro lado, esta garante uma maior\nagilização e otimização dos processos administrativos, assegurando, ainda, uma maior longevidade aos documentos. De forma a atingir estes objetivos nasceu a plataforma Classificação\ne avaliação da informação pública (CLAV), plataforma essa que tem vindo a crescer ao longo\ndos últimos anos, que conta com vários colaboradores do Departamento de informática da\nuniversidade do Minho, sendo financiado pelo Simplex, visando a classificação e avaliação\nde toda a documentação presente na Administração Pública Portuguesa. Como referido\nesta plataforma já está bem madura e como tal já conta com diversas funcionalidades para\ncriação e manutenção dos instrumentos de classificação e avaliação, esta dissertação pretende\nacrescentar uma nova componente ao CLAV que permita não só a criar como também gerir\nos planos de preservação digital. Para isso, foi necessário definir um modelo a seguir, tendo\nem conta todos os seus requisitos e invariantes, e adicionar as interfaces necessárias ao\nCLAV, com todas as funcionalidades e métodos necessários para a criação, importação e\nmanutenção dos planos de preservação digital."
  },
  {
    "keywords": [
      "Public lighting",
      "Object detection",
      "Smart cities",
      "Multimodal machine learning",
      "Fog and Edge computing",
      "Multi-agent systems",
      "YOLO model",
      "Sound classification",
      "Data fusion",
      "Iluminação pública",
      "Deteção de objetos",
      "Cidades inteligentes",
      "Aprendizagem automática multimodal",
      "Computação Fog e Edge",
      "Sistemas multi-agente",
      "Modelo YOLO",
      "Classificação de som",
      "Fusão de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Edge-enabled multi-agent system architecture for multimodal machine learning",
    "autor": "Coelho, Eduardo Benjamim Lopes",
    "data": "2023-12-05",
    "abstract": "Public lighting, despite being one of the most important services a city must provide, it is \nalso one of the main sources of energy consumption and consequently running costs. One \nway to drastically improve the efficiency of these systems is by using object detection in \norder to make each of the public lighting poles aware of their surroundings, giving them the \nability to adapt to the environment. This is one of the main goals of an industry leader \ncompany's project in the area of public lighting, where this study is inserted. In this project, \neach public lighting pole is upgraded with a set of sensors: radar, microphone and camera\n(what we refer to as fog computing). By developing a multimodal machine learning model, \nthe goal is to leverage the data from the different sensors to improve the object detection \ncapabilities of traditional unimodal machine learning model. Additionally, the developed \nmodel will be deployed into an edge device that is also installed in the public lighting pole, \ndue to data privacy concerns and network latency problems that would otherwise occur with \ntraditional server-side approaches. This constrain raises the main question that this study \nwill try to answer, which is how to develop a complex multimodal machine learning model \nfor low-power efficient edge devices. In this study, a multi-agent architecture will be \nproposed, that authors can adapt to their own multimodal machine learning problems with \nedge devices. To prove the efficient of the proposed system, a proof of concept \nimplementation will be carried out that involves the aforementioned sensors, as well as the \nYou Only Look Once (YOLO) object detection model, with a feature-level data fusion \napproach. Finally, the implemented system will be deployed to an edge device, where the \nhardware performance will be tested and compared to similar work in the literature."
  },
  {
    "keywords": [
      "Strategic programming",
      "Attribute grammars",
      "Zippers",
      "Ztrategic",
      "Programação estratégica",
      "Gramáticas de atributos"
    ],
    "titulo": "On the performance of strategic attribute grammars",
    "autor": "Rodrigues, José Emanuel Silva",
    "data": "2022-09-05",
    "abstract": "Strategic programming is a powerful technique used in language processing to define functions that traverse\nabstract syntax trees. With strategies, the programmer only indicates the nodes of the tree where the work has to\nbe done, and the strategy used to traverse the whole tree and apply the function that works only on the defined\nnodes.\nIn Haskell, there are two libraries that implement strategies: Strafunski and an equivalent library developed\nby DI: Ztrategic. Beyond that, we also have the Kiama library which is implemented in the Scala programming\nlanguage. The Ztrategic library uses memorization in order to save work. Using memorization, the elimination of\nall occurrences of \"bad smells\" in an abstract tree of a program is done only once!\nIn this thesis, we present a detailed study of the performance of the Kiama, Ztrategic, and memoized Ztrategic\nlibraries, using different strategic problems and input languages."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Especificação de uma ontologia para genealogia",
    "autor": "Ribeiro, Frederico Moreira",
    "data": "2017",
    "abstract": "O tema desta dissertação é a especificação de uma ontologia de genealogia, sendo que o\nprincipal objetivo é desenvolver uma ontologia.\nA ontologia será capaz de suportar a descrição da genealogia de um conjunto de indivíduos, desde dados pessoais passando pelas relações de parentesco até eventos significativos\nda vida de uma pessoa, podendo associar fotografias ou registos documentais que\nsejam de interesse.\nPara além disso, pretendia-se desenvolver uma aplicação Web que suportasse todas as\noperações de gestão e manutenção da ontologia.\nEste tipo de aplicação pode ser usada como passatempo ou exercício de simples curiosidade\npelo passado familiar ou como suporte de histórico familiar para fins médicos,\ngenéticos, entre outros."
  },
  {
    "keywords": [
      "681.3.06",
      "658.0"
    ],
    "titulo": "Generating automatically test cases based on models",
    "autor": "Veiga, Nuno Gil Correia Veloso da",
    "data": "2012-12-14",
    "abstract": "I would like to thank the following people for their help and support over the course of the\ncompletion of this thesis.\nTo João Saraiva, my supervisor, for his continued support and encouragement.\nTo Luís Anjos and Nuno Vieira, my co-workers, that helped in everything I needed during\nmy research time in Primavera.\nTo my parents, Carlos and Isabel, that, even though in my life I may have not done everything\nthe way they wanted and wished, were always there for my best interest, caring for me, my\nlife and my future. I feel eternal gratitude for the two of you.\nTo my lovely sister, Catarina, for giving me company even when I didn’t want it, for loving\nme unconditionally and for bringing a new cat into our household. Life with kittens is\nalways better than life without kittens!\nTo my aunts, Nocas, Sissi and Patrícia, that I am sure have always believed in me.\nTo my grandfather, that age only made of him a more interesting, funny and reliable\ncompany. To my grandma, for all the love and food filled of love that fed me during all\nthese years.\nTo my girlfriend Ana, for all the love, patience and understanding, and for staying by my\nside.\nTo all my friends, who make my life outside of research so enjoyable. Particularly, to my\nfriends Ricardo, Renato, Miguel, Pedro Nuno, Bruno and Sebastião, whose company has\nbeen a constant for many years. To Zé and João Nuno, for the great time we have when we\nare together."
  },
  {
    "keywords": [
      "Inteligência artificial",
      "Robots artificialmente inteligentes",
      "Autonomia",
      "Ética",
      "Personalidade jurídica",
      "Artificial intelligence",
      "Artificially intelligent robots",
      "Autonomy",
      "Ethics",
      "Natural/legal personhood",
      "Ciências Sociais::Direito"
    ],
    "titulo": "Reflexão sobre a pessoalização jurídica dos robots artificialmente inteligentes",
    "autor": "Fernandes, Rui Manuel Zilhão",
    "data": "2021-08-02",
    "abstract": "O objeto da nossa dissertação consiste na análise da adequação ou inadequação da \natribuição de personalidade jurídica aos robots artificialmente inteligentes. No ordenamento \njurídico português são atualmente admitidos dois tipos de pessoas jurídicas: as pessoas singulares \ne as pessoas coletivas. Poderá considerar-se a criação de um novo tipo de pessoas jurídicas?\nO florescimento da inteligência artificial, i.e., da disciplina que procura construir máquinas \nque atuem de forma semelhante ao ser humano, capazes de executar tarefas ou funções e tomar \ndecisões com uma eficácia semelhante ou superior àquele, levantou dúvidas relativamente à \nadequação dogmática da tradicional dicotomia entre a personalidade singular e a personalidade \ncoletiva. A criação de robots artificialmente inteligentes capazes de receber e utilizar dados que \ncirculam em rede, bem como de aprender e utilizar experiências anteriores, contribui em larga \nescala para a pertinência de uma análise cuidada do tema. Tanto mais que a implementação de \nrobots artificialmente inteligentes no quotidiano social e económico traz consigo uma panóplia de \nimplicações éticas, as quais constituem desafios que cumpre teorizar e solucionar. \nA personalidade jurídica singular e a personalidade jurídica coletiva não assentam em \nconsiderandos axiológico-jurídicos e filosóficos iguais. A personalidade singular fundamenta-se no \nHomem, enquanto sujeito dotado de uma dignidade originária e inviolável, pelo que tem de ser \nreconhecida a todos os seres humanos, em qualquer circunstância (nunca a pessoa humana \npoderá ser objeto de um direito). Por sua vez, a personalidade coletiva assenta na necessidade de \nfornecer às pessoas singulares os instrumentos jurídicos adequados à prossecução dos seus \npropósitos ou objetivos. Determinados os fundamentos axiológico-jurídicos que subjazem a este \ninstituto jurídico no ordenamento jurídico português, a atribuição de personalidade jurídica aos \nrobots artificialmente inteligentes pressuporá a verificação de um desses considerandos. Haverá \nrazões justificativas e legitimadoras da atribuição de personalidade jurídica aos robots \nartificialmente inteligentes? Qual é o entendimento perfilado pela Comissão Europeia na proposta \nde Regulamento publicada em 21 de abril de 2021? Estas são algumas das questões que nos \npropusemos tratar."
  },
  {
    "keywords": [
      "Cidades inteligentes",
      "Previsão de dados",
      "Séries temporais",
      "Sistemas de análise de consumo energético",
      "Data forecasting",
      "Energy consumption analysis systems",
      "Smart cities",
      "Time series",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistema de análise e previsão de consumo energético para uma cidade inteligente",
    "autor": "Sousa, Bruno Alexandre Almeida",
    "data": "2020-11-30",
    "abstract": "Cidades Inteligentes é um conceito que ainda não se encontra muito bem estabelecido.\nContudo, é de consenso comum que este consiste em fornecer a todos os seus cidadãos\numa melhor qualidade de vida. Nestas cidades inteligentes encontramos múltiplos cená rios aplicacionais que podem ser explorados de forma a alcançar os objectivos necessários.\nUm exemplo destes cenários são os sistemas energéticos que, dada a evolução constante,\nsofrem uma grande sobrecarga. Consequentemente, é necessário recorrer a certas medidas\npara aumentar a eficiência destes. Uma solução é a implementação de sistemas de análise\nde consumo de energia e de previsão de dados. Os dados são extraídos de fontes bastante\ndiversas (sensores, câmaras e outros), dados estes que, posteriormente, são processados e\nexportados para data warehouses. Contudo, com a evolução tecnológica que se tem vindo\na verificar, a quantidade de dados oportunos aumentou significativamente, bem como as\ncaracterísticas relativas à forma de como são recolhidos e tratados. Hoje em dia, a diver sidade destes dados é intensa, dependendo muito das circunstâncias operacionais e dos\nsistemas envolvidos, o que gera vulgarmente cenários aplicacionais novos e estranhos para\nos sistemas que usualmente estão envolvidos no seu tratamento. Neste sentido, existe a\nnecessidade de inovar os processos de tratamento destes dados e, deste modo, aumentar a\noperacionalidade e possibilidade de suportar uma cidade inteligente. Para tal, é necessário\nimplementar processos de ETL e de previsão de dados para haver a capacidade de toma das de decisões de forma a manter uma cidade inteligente. Abordamos, nesta dissertação,\nos sistemas de análise de consumo de energia, visto serem um dos cenários aplicacionais\nmais explorados com a evolução tecnológica. Havendo a necessidade de aumentar a efi ciência desta área identificamos, planeamos e testamos algumas das medidas passíveis de\nimplementar para a previsão de dados futuros que permitam ajudar à tomada de decisão.\nAlcançando, por fim, a seleção de um algoritmo bastante preciso para a previsão destes\ndados."
  },
  {
    "keywords": [
      "OpenThread",
      "Thread",
      "Software verification",
      "Model checking",
      "Alloy",
      "Wireless Mesh Networks",
      "OTNS",
      "IoT",
      "Smart home",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Formalização de um protocolo Mesh para sistemas IoT em Alloy",
    "autor": "Lourenço, Rafael Inácio",
    "data": "2023-11-27",
    "abstract": "Nos dias de hoje os dispositivos IoT fazem parte das nossas vidas, nomeadamente na integração das nossas casas, automação industrial ou monitorização de vários tipos de ambiente. Thread é um protocolo de\nredes mesh wireless que tem uma enorme eficiência energética, segurança, alcance, interoperabilidade,\ndescentralização e com capacidade de regeneração, sendo por isso interessante para integrar em sistemas IoT. Dado isto, a validação e verificação dos requisitos do protocolo é extremamente importante e,\npor isso, nesta dissertação são propostas várias formalizações deste protocolo em Alloy. Posteriormente\nforam realizadas várias verificações das suas propriedades mais relevantes, uma vez que, são estas que\ngarantem o correto funcionamento do algoritmo. Todas foram verificadas sem nenhum contra-exemplo\nencontrado. Para além disso, esta dissertação também explora a relação entre o OpenThread Network Simulator, desenvolvido pela Google, e o modelo produzido em Alloy, com o objetivo de validar este último.\nForam realizadas várias simulações e convertidas em instâncias executáveis nesse modelo. Isto permitiu\ndetetar algumas incongruências entre ambos e corrigir o modelo por forma a ser capaz de executar todas\nas simulações realizadas."
  },
  {
    "keywords": [
      "Microbial community",
      "Systems biology",
      "Genome-scale metabolic model",
      "Community modelling",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Assessment of microbial community interactions using different tools",
    "autor": "Ribeiro, Catarina Moreira",
    "data": "2020",
    "abstract": "Microbial communities participate in many biological processes, directly affecting its\nsurrounding environment. Thus, the study of a community’s behaviour and interactions\namong its members can be very useful in the biotechnology, environmental and human\nhealth fields. Nevertheless, decoding the metabolic exchanges between microorganisms\nand community dynamics remains a challenge.\nComputational modelling methods have gained interest as a way to unravel the interactions\nand behaviour. GSM models allow the prediction of an organism’s response to\nchanges in genetic and environmental conditions. Thus, the extension of such method to a\ncommunity level can help decode a community’s phenotype.\nIn this work, different GSM models and current bioinformatics tools were used to model\nthe metabolism of different microbial communities. The different tools’ performances were\ncompared to assess which is currently the best method to perform an analysis on a community\nlevel. Distinct case studies regarding microbial communities for which its interactions\nwere already known, were selected. To assess the tools’ performances, each tools output\nwas compared to what was expected in theory.\nCOBRA Toolbox's methods proved to be useful to build a community structure from\nindividual GSM models, while pFBA and SteadyCom’s simulation methods can predict\nexchange between the organisms and the environment. Additionally, Dynamic Flux Balance\nAnalysis (dFBA) approaches, such as DFBAlab and DyMMM, can successfully simulate\nmetabolite and biomass variation over time. Nevertheless, these methods are more limited\nas they require specific organism information, which is not always available.\nSeveral GSM models are available for use. Nonetheless, their quality control has to gain\nattention as the simulations’ results are directly affected by the individual models accuracy\nto represent an organism’s metabolism. Thus, community model builders should carefully\nchose a GSM model, or combination of models before performing simulations."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Syntax-directed editor generator",
    "autor": "Ferreira, Jorge Miguel Sol",
    "data": "2017",
    "abstract": "The goal of the master’s thesis work here reported is to develop a system capable of generating\na SDE for any given language definition. A SDE is a type of source code editor that\nknows the programming language grammar and uses this knowledge to guide the editing\nand the execution of a program. This type of editing ensures that a program is syntactically\ncorrect.\nThe editor is intended to provide both syntax-directed editing as well as manual text editing.\nA meta-language must be created to describe the grammar of the editor’s target language.\nThe meta-language will provide annotations to change the display of the text in the editor.\nThat specification, written in the referred meta-language will be the input to generate templates\nfor the syntax directed editor.\nThe SDE generator is available in a standalone JAR application as well as a web version."
  },
  {
    "keywords": [
      "Customer",
      "Data Mining",
      "Installation",
      "Machine Learning",
      "Predict",
      "Service",
      "Telecommunications",
      "Cliente",
      "Instalação",
      "Previsão",
      "Serviço",
      "Telecomunicações",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Predicting problems from Telecom installation processes",
    "autor": "Costa, Diana Sofia Nogueira",
    "data": "2020-12-22",
    "abstract": "Improving customer experience is crucial in any industry, especially in telecommunications, where\ncompetition is a constant factor. Today, all telecommunications companies rely on the massive\namount of data generated daily to get to know the customer, study their behavior, and create new\neffective strategies for their business. The information collected may include network information,\nrepresenting the status of hardware and software components in the network, and the client’s details and calls to support and customer assistance data, which describes problems, consumer\ncomplaints, and all other problem-solving interactions. By combining these mountains of raw data\nwith data mining and machine learning techniques, companies can find solid patterns or systematic\nrelationships that represent valuable information, that is, generate knowledge.\nWithin the most varied user experiences, the process of installing new services can be an event\nthat raises doubts about their operation, degrade the user experience, or, in extreme cases, lead to\nmaintenance interventions. Therefore, the use of advanced predictive models that can predict such\noccurrences become vital. With this, the company can anticipate the cases that will be problematic\nand reduce the number of negative experiences.\nThe main objective of this work is to create a predictive model that, through all the available data\nhistory, can predict which customers will contact the customer service with problems derived from\nthe installation process and have a following maintenance intervention.\nAfter analyzing an imbalanced dataset with approximately 560K entries from a Portuguese telecommunications company, and resorting to the CRISP-DM methodology for modeling, the best results\nwere found with LightGBM, which obtained an AUPRC of 0.11 and AUROC of 0.62. The best tradeoff between precision and recall was found with a threshold model of 0.43 in order to maximize\nrecall while still avoiding a large number of false negatives. The strategies explored in this work and\nthe challenges found may help the company understand which details should improve in its service\nprovision, and which data still need to be investigated in the future."
  },
  {
    "keywords": [
      "BrainCat",
      "DTI",
      "MRI",
      "Multimodalidade",
      "Multimodality",
      "Volumetry",
      "681.3:61",
      "61:681.3"
    ],
    "titulo": "Automatização e otimização das análises multimodalidade de Ressonância Magnética Estrutural (Volumetria) e Tensor de Difusão",
    "autor": "Maia, Liliana Filipa Costa",
    "data": "2012",
    "abstract": "Cada vez mais a relação entre as tecnologias da informação e a saúde se\nestreitam. Concretamente, na neuroimagiologia, essa ligação tem vindo a tornar-se cada vez\nmais importante principalmente após o surgimento da imagem de Ressonância Magnética\n(MRI – Magnetic Ressonance Imaging).\nCom o desenvolvimento da tecnologia, para além das aquisições MRI convencionais, surgiram\noutras técnicas como a aquisição de imagens de tensor de difusão (DTI – Diffusion Tensor\nImaging) e da MRI funcional (fMRI). Estas técnicas permitem a obtenção de uma imagem\ninterior do corpo. Um dos órgãos mais estudados com estas imagens é o cérebro, que é alvo\nde vários estudos mas que devido à sua complexidade ainda é bastante desconhecido.\nEnquanto que com a MRI estrutural pode-se efetuar uma análise volumétrica às diferentes\nestruturas do cérebro, com a DTI é possível verificar a integridade da substância branca\natravés das fibras virtualmente criadas que representam o movimento das moléculas de água.\nVários estudos referem os benefícios de uma análise multimodal com estas duas técnicas.\nPara tratamento e análise destas imagens é necessário uma gestão de várias aplicações\ninformáticas que processam os dados e corregistam as imagens de forma automática. Um dos\ngrandes desafios consta, não só na utilização individual de cada ferramenta na qual é exigido\nalgum conhecimento técnico, como na combinação das várias aplicações que apresentam os\ndados resultantes em diferentes formatos.\nUma solução passa pela pesquisa e definição de fluxos de trabalho para que exista uma\nabordagem simples dos procedimentos a ter com as várias ferramentas e da sua combinação\ncom outras. No entanto, esta solução não impedirá o gasto de recursos de tempo e o trabalho\nmoroso de um estudo que contenha vários sujeitos.\nAssim, neste trabalho, para além de serem apresentados os vários fluxos de trabalho possíveis\npara análise multimodal, será exposto um módulo automatizado que será inserido numa\naplicação de multimodalidade já existente: BrainCat.\nA presente dissertação apresenta um meio de facilitar as análises multimodais para que a\nqualidade quer a nível de investigação científica quer a nível dos diagnósticos clínicos aumente."
  },
  {
    "keywords": [
      "Aprendizagem máquina",
      "Classificação de péptidos",
      "Péptidos de fusão viral",
      "Machine Learning",
      "Peptide Classification",
      "Viral Fusion Peptides",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Building an automated platform for the classification of peptides/proteins using machine learning",
    "autor": "Sequeira, Ana Marta Fernandes Tavares",
    "data": "2019-11-18",
    "abstract": "One of the challenging problems in bioinformatics is to computationally characterize sequences, structures and functions of proteins. Sequence-derived structural and physico-chemical properties of proteins have been used in the development of machine learning models in protein related problems. However, tools and platforms to calculate features and perform Machine learning (ML) with proteins are scarce and have their limitations in terms of effectiveness, user-friendliness and capacity. Here, a generic modular automated platform for the classification of proteins based on their physicochemical properties using different ML algorithms is proposed. The tool developed, as a Python package, facilitates the major tasks of ML and includes modules to read and alter sequences, calculate protein features, preprocess datasets, execute feature reduction and selection, perform clustering, train and optimize ML models and make predictions. As it is modular, the user retains the power to alter the code to fit specific needs. This platform was tested to predict membrane active anticancer and antimicrobial peptides and further used to explore viral fusion peptides. Membrane-interacting peptides play a crucial role in several biological processes. Fusion peptides are a subclass found in enveloped viruses, that are particularly relevant for membrane fusion. Determining what are the properties that characterize fusion peptides and distinguishing them from other proteins is a very relevant scientific question with important technological implications. Using three different datasets composed by well annotated sequences, different feature extraction techniques and feature selection methods (resulting in a total of over 20 datasets), seven ML models were trained and tested, using cross validation for error estimation and grid search for model selection. The different models, feature sets and feature selection techniques were compared. The best models obtained for distinct metric were then used to predict the location of a known fusion peptide in a protein sequence from the Dengue virus. Feature importances were also analysed. The models obtained will be useful in future research, also providing a biological insight of the distinctive physicochemical characteristics of fusion peptides. This work presents a freely available tool to perform ML-based protein classification and the first global analysis and prediction of viral fusion peptides using ML, reinforcing the usability and importance of ML in protein classification problems."
  },
  {
    "keywords": [
      "Public Administration",
      "Documents",
      "CLAV",
      "DGLAB",
      "DMO",
      "ADAR",
      "ST",
      "Web",
      "Application and interfaces",
      "Administração Pública",
      "Documentos",
      "PGD",
      "RADA",
      "TS",
      "Aplicação",
      "Web e interfaces",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "CLAV: tabelas de seleção",
    "autor": "Monteiro, Manuel João Curopos",
    "data": "2022-04-22",
    "abstract": "In the Portuguese Public Administration, there has been a recurring concern to modernize the archive of information generated and received by the different entities. It has provided the use of electronic document management systems, as well as the digitization of documents sent to be archived. These measures are intended to reduce the consumption of paper and printing consumables, as well as optimize processes and modernize administrative procedures. From these measures, the Classification and Evaluation of Public Information (CLAV) emer-ges, a national project, financed by SIMPLEX, which aims to simplify management Public Administration documents. CLAV aims to classify and evaluate all Public Administration documents, based on norms and rules coordinated by the Directorate-General for Books, Archives and Libraries (DGLAB). Many Portuguese Public Administration entities have their classification and assessment instruments published in the official gazette of Portuguese Republic in a Document Management Ordinance (DIM) or Accumulated Documentation Assessment Report (ADAR), which contains the said instrument, the Selection Tables (ST). In the future, it is intended that the SD are created on the platform in an assisted manner. There is already a prototype of this functionality that is now necessary to finalize, as well as specify all the work flow necessary for the final approval of the instrument. It is also intended to develop, in the CLAV web application, the necessary interfaces that allow users to create the STs in an assisted form and to specify and define all the necessary web interfaces for the analysis, approval and removal workflow of a ST."
  },
  {
    "keywords": [
      "Medical informatics",
      "Neuroimaging",
      "fMRI",
      "Functional connectivity",
      "Deep learning",
      "Informática médica",
      "Neuroimagem",
      "Conectividade funcional",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Analysis of brain connectivity in fMRI: a Deep Learning approach",
    "autor": "Ramalhosa, Ivo Miguel Marques",
    "data": "2017",
    "abstract": "The brain functional connectivity extracted from rs-fMRI has been used as a powerful tool to study the different networks in the brain. This neuronal network, found in normal condition, can be associated to different cognitive processes. The applicability of these networks in the future is promising, since is a greater technique to study the effects of several diseases or even treatments on normal brain functional connectivity. Firstly, this question should be addressed: are these networks possible to be described and to be used as features to classify a group or a particular subject?.\nIn order to answer this question, it was settled the use of a Machine Learning method, which has been developed great advances in the recent years, due the good performances in the Deep Learning (DL) method. Therefore, it was created a workflow since the beginning, started with data acquisition until the application of DL methods and the process of creation and fine-tune of these models. In the end, several studies using the functional connectivity were done, namely the assessment of the brain functional connectivity to be used as a “fingerprint”. Additionally, it were performed some tests regarding the groups’ classification.\nAfter settled the correct approach and validate the DL framework, the “fingerprint” study showed a great improvement on impairment classification, even for simple models. We proved that rs-fMRI can be use in research field to identify singular brain patterns as well as the differences between the subjects, which could be applied as group differentiator in a population."
  },
  {
    "keywords": [
      "Comunicação",
      "Peão",
      "V2X",
      "Veículo",
      "Communication",
      "Pedestrian",
      "Vehicle",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistema de alerta rodoviário baseado em comunicações V2X",
    "autor": "Miguel, João Rui de Sousa",
    "data": "2019-12-23",
    "abstract": "Quando falamos em utilizadores em risco num cenário rodoviário, vem-nos à cabeça os peões, ciclistas e possivelmente os motociclistas. Este tipo de utilizadores, particularmente se se tratarem de crianças ou idosos, são especialmente vulneráveis em cenários de pe-rigo quer por erro humano ou por agentes externos. O trabalho realizado no contexto desta dissertação tem o intuito de diminuir o número de fatalidades ocorridas em situações de perigo nas nossas estradas, e tem como objetivo providenciar um método auxiliar de indicação destas mesmas situações de perigo. O tema das comunicações entre veículos tem vindo a ser abordado há alguns anos, mas ainda há muito trabalho que pode ser desenvolvido nesta vertente. Aplicações capazes de comunicar diretamente com veículos circundantes podem aumentar a segurança rodoviária como um todo, não só dos peões como também dos condutores. A expansão da Internet aos veículos vem possibilitar esta abordagem, sendo que é esperado que futuramente todos os veículos produzidos estejam apetrechados com algum tipo de mecanismo de comunicação. A ideia aqui patente é a de alargar os alertas existentes em veículos mais atuais para os dispositivos móveis dos peões, ou em sentido contrário, comunicar a presença, posição ou até direção de um peão ao veículo. A principal dificuldade nesta abordagem continua a ser tecnológica, isto dado o facto de não existir uma tendência para tecnologia comum de ampla utilização. Atualmente os dis-positivos móveis utilizam tecnologias WiFi/ 4G e os veículos sobretudo DSRC (brevemente LTE-V). Esta dificuldade inicial poderá, contudo, ser ultrapassada com recurso a dispositi-vos multi-tecnologia, instalados por exemplo na berma da via pública, sendo estes capazes de agregar e redistribuir informação. Neste trabalho apresenta-se uma proposta para um sistema de alertas de segurança ro-doviária, de baixo custo, para condutores e utilizadores vulneráveis, baseado no uso de dispositivos móveis. A arquitetura do sistema é descrita, apresentado alternativas e jus-tificando as decisões tomadas. O sistema proposto foi implementado na totalidade num protótipo que foi posteriormente testado em cenários realistas com ajuda de um simula-dor construído para o efeito. Os resultados obtidos permitem constatar a sua viabilidade prática e o funcionamento de acordo com o esperado."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Uma arquitetura para implementação NG-PON2",
    "autor": "Lopes, Carlos Miguel Cerqueira Silva",
    "data": "2018-12-20",
    "abstract": "Due to the high appearing of many bandwidth consumer services, as well as the increase\nof the bandwidth consumed by those services, on the last decade the PON (Passive Optical\nNetwork) architectures and the FTTx (Fiber To The Place) concept forcibly evolved, enabling\nlarger disponibility on data rates to the costumer and to the services.\nThere are many PON standards today, for example EPON (Ethernet Passive Optical\nNetwork), BPON (Broadband Passive Optical Network), GPON (Gigabit Passive Optical\nNetwork) and XG-PON (10 Gbps PON). Although having these technologies, the idea that\nPON have unlimited capacity is erroneous, although, even limited, it can be much higher\nthan the capacity available today by the copper network. Given the architectural options,\nimplementation schemes, performance of some equipment, gradual requirements by customers and services, the biggest problems will be spectral austerity/lack and the need to\nsupply customers with an even broader data and service rate , higher than 10 Gbps (provided by XG-PON), and as a consequence, these solutions can be seen as short to medium\nterm.\nIn the wake of these developments, a possible long-term solution to address these shortcomings is a development standard entitled NG-PON2 (Next-Generation PON 2).\nThis work has as main objective the proposal of an architecture of implementation NG-PON2 oriented to the Central Office, being a solution with applicability in the access\nnetworks. In parallel, the NG-PON2 and previous ones will be studied. It will also make\na characterization of current architectures and technologies. The work will also integrate\nthe construction of a real NG-PON2 scenario, encompassing the necessary tests. All this\nwork, from study, architecture design, implementation and testing will be developed at the\ncompany Altice Labs, S.A. in Aveiro."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Modern front-end web development",
    "autor": "Anjo, António Manuel Pereira do",
    "data": "2018-12-20",
    "abstract": "The Internet is always evolving. The way content is generated, displayed and ac cessed is constantly changing with the advancements of technology.\nAs such, new tools, frameworks and technologies are constantly surfacing as a\nway to deal with the challenges of this evolution. Keeping up with an increasingly\nlarge number of options is, for Web Developers, as important as it is challenging.\nWith this challenge in mind, this dissertation aims to offer a deep look into the\ncurrent state of Front-End Web Development by going through relevant concepts\nand doing an in-depth, comparative analysis of the different frameworks. In the\nprocess, data will be collected, a case study will be developed and a developed ap proach will be validated, thus obtaining results and taking conclusions that will help\nmake the best possible decisions in the development process.\nThe author will be, during the work period, part of the Developer Team at Jumpseller,\nworking hands-on with these technologies"
  },
  {
    "keywords": [
      "Lightweight cryptography",
      "NIST",
      "ASCON",
      "Romulus",
      "Cryptanalysis",
      "Criptografia leve",
      "Criptoanálise",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Security analysis of NIST-LWC contest finalists",
    "autor": "Fernandes, João Pedro Dias",
    "data": "2023-01-05",
    "abstract": "Traditional cryptographic standards are designed with a desktop and server environment in mind, so, with the\nrelatively recent proliferation of small, resource constrained devices in the Internet of Things, sensor networks,\nembedded systems, and more, there has been a call for lightweight cryptographic standards with security,\nperformance and resource requirements tailored for the highly-constrained environments these devices find\nthemselves in.\nIn 2015 the National Institute of Standards and Technology began a Standardization Process in order to select\none or more Lightweight Cryptographic algorithms. Out of the original 57 submissions ten finalists remain, with\nASCON and Romulus being among the most scrutinized out of them.\nIn this dissertation I will introduce some concepts required for easy understanding of the body of work, do\nan up-to-date revision on the current situation on the standardization process from a security and performance\nstandpoint, a description of ASCON and Romulus, and new best known analysis, and a comparison of the two,\nwith their advantages, drawbacks, and unique traits."
  },
  {
    "keywords": [
      "Deep learning",
      "Sleep apnea",
      "Distributed deep learning",
      "Electrocardiogram",
      "Apneia do sono",
      "Deep learning distribuído",
      "Electrocardiograma",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Distributed deep learning for sleep apnea detection on ECG signals",
    "autor": "Machado, Ana Margarida da Silva",
    "data": "2020-01-03",
    "abstract": "A huge amount of medical data is being generated each day, leaving the doctors unable\nto analyze such volume and make a good diagnosis for the patient. The emergence of Big\nData frameworks for data analysis leverages the automatic analysis of healthcare data in a\nfaster and accurate manner, by scanning which information is relevant, and, consequently,\ndetecting diseases in earlier stages.\nNowadays, it is estimated that about 9% to 38% of the world’s population has sleep ap nea. Unawareness of the disease’s presence can lead to the development of cardiovascular\ndiseases, and consequently, death. The detection of sleep apnea syndrome through the tra ditional method, Polysomnography (PSG), becomes not only expensive but also inconvenient\nfor the patient. Therefore, systems based on Electrocardiogram (ECG) can improve the qua lity of a patient’s health by overcoming these inconveniences. This master thesis relies on\ndeep learning (DL) networks, such as convolutional and recurrent neural networks for sleep\napnea detection. The computational complexity of these models depends on its size, types\nof layers and data. This complexity also increases the computation time of the training task\nleading to several hours spent training on a single machine. For this work, we propose\na sleep apnea detection system based on ECGs, alongside with a distributed version of it,\nwhich parallelizes the training computation, reducing the overall learning time, while not\ncompromising the model performance.\nThe results obtained for sleep apnea detection encourage the use of electrocardiograms\nfor the detection of this disease. Our model achieved a value of 93% of sensitivity on\nthe Physionet database, being the highest value compared to other studies described in\nthe literature. Besides this, on the distributed environment it was accomplished similar\noutput quality, reducing the training time by approximately 50%, from the centralized to\ndistributed learning.\nThe model was trained with the Sleep Heart Health Study (SHHS) data, achieving the highest\nresults compared to the work described in the literature that used the same dataset. In\ncomparison with the previous dataset, the model trained and tested with the SHHS was\nnot able to attain a similar quality output. However, this corroborates the large diversity of\nthe SHHS data. Moreover, when it was tested if this model could classify the Physionet data,\nit achieved promising results of 73,7%, 73,8%, 68,3% and 63,5% of accuracy, sensitivity, F1-\nscore, and precision, respectively, which lead us to conclude that the SHHS trained model\ncould be able to generalize to new data. In addition to this, on the distributed environment it was achieved equal output perfor mance for SHHS, reducing the training time by approximately 90%."
  },
  {
    "keywords": [
      "RDF4J",
      "REST API",
      "Frontend",
      "RDF",
      "SPARQL",
      "Graph",
      "Grafo",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Developing a RDF4J frontend",
    "autor": "Oliveira, Francisco José Moreira",
    "data": "2021-04-06",
    "abstract": "A few years ago, data was not shared and kept isolated, preventing communication\nbetween datasets. Currently, we have more significant data volumes, and in a world where\neverything is connected, our data is now also following this trend.\nData model focus changed from a square structure like the relational model to a model\ncentered on the relations. Knowledge graphs are the new paradigm to represent and manage\nthis new kind of information structure.\nAlong with the new paradigm, graph databases emerged to support the new requirements.\nDespite the increasing interest in the field, only a few native solutions are available. Most\nare under a commercial license, and the open-source options have very basic or outdated\ninterfaces, and because of that, they are a little distant for most end-users.\nIn this thesis, we introduce the Open Web Ontobud and discuss its design and develop ment. Ontobud is a Web Application aimed at improving the interface for one of the most\nfascinating and influential frameworks in this area: RDF4J. RDF4J is a Java framework to\ndeal with RDF triple storage, management, and query.\nOpen Web Ontobud is an open-source RDF4J web frontend created to reduce the gap\nbetween end-users and the RDF4J backend. We created a web interface that enables users\nwith a basic knowledge of OWL and SPARQL to explore ontologies via resource tables or\ngraphs and extract information from them with SPARQL queries. The interface aims to\nremain intuitive, providing tooltips and help when needed, as well as some statistical data\nin a readily available form.\nDespite the frontend being the main focus, a backend and two databases are also used for\na total of four components in the framework. For the best deployment experience, Docker\nwas used for its simplicity, allowing deployment in just a few commands. Each component\nhas a dedicated image, following a modular design and allowing them to be executed on\nseparate machines if desired."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Energy analysis in the CodeCompass system",
    "autor": "Santos, Mário Nelson Araujo",
    "data": "2017",
    "abstract": "Green computing has an increasing importance in software engineering. Unfortunately,\nthere are lack of tools on this field to help developers to understand and fix issues related\nto unwanted energy consumption.\nThe thesis project will provide Software (sw) eng. with information about energy consumption\nof functions and methods. The CodeCompass system helps software developers to\nunderstand their source code, and it was developed by the Hungarian team members of\nEricsson.\nThus, I will locate hot spots in the software’s source code responsible for abnormal energy\nconsumption, and I will do a plug-in to extend the CodeCompass tool so that it can automatically\nlocate such energy faults, helping software developers to optimize the energy\nconsumption of their software."
  },
  {
    "keywords": [
      "Sistemas de apoio ao ensino",
      "Sistemas de ensino inteligentes",
      "Personalização de processos de aprendizagem",
      "Geração e manutenção de perfis",
      "Tutores artificiais",
      "Cartilha Maternal de João de Deus",
      "Learning support systems",
      "Intelligent tutoring systems",
      "Personalization of learning processes",
      "Profile maintenance",
      "Artificial tutors",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Especificação e desenvolvimento de um módulo de avaliação para um sistema de ensino inteligente",
    "autor": "Martins, Mónica Filipa Casanova",
    "data": "2016",
    "abstract": "Hoje em dia, o recurso às novas tecnologias nas mais diferentes áreas do\nconhecimento tem sido evidente. A área da educação não foi, pois, exceção.\nCada vez mais se tentam arranjar ferramentas tecnológicas que permitam a\nqualquer aluno ter acesso a um processo de aprendizagem mais simples e\neficaz. Dentro desse leque variadíssimo de ferramentas encontramos os ITS\n(intelligent tutoring system). Genericamente, estes sistemas têm como\nobjetivo fornecer instruções a alunos sem a intervenção direta de um\nprofessor. Para que isso possa acontecer, com sucesso, é necessário que estes\nsistemas possuam uma base de conhecimento fiável, com conteúdos\nadequados em todas as vertentes de ensino que promovem. Nesta dissertação\ntivemos como base de trabalho a conceção e o desenvolvimento de um\nmódulo de avaliação para um sistema de ensino inteligente, com capacidade\npara fazer a monitorização de todas as atividades desenvolvidas ao longo de\num processo de aprendizagem ou de aferição de conhecimento. Com esta\nmonitorização pretende-se fazer a criação de perfis de aprendizagem para\ntodos os alunos que utilizem o sistema, de forma a que se possa personalizar\nos processos de ensino, indo em contra às principais necessidades de\naprendizagem dos alunos. Nesta dissertação descrevemos o trabalho\nrealizado no desenvolvimento do módulo de avaliação referido, dando\nparticular atenção aos aspetos relacionados com a seleção e implementação\ndos mecanismos de profiling para a criação automática de perfis de\naprendizagem, aos diferentes métodos de avaliação estudados, à preparação\ndos dados e serviços subjacentes ao seu armazenamento e processamento, à\narquitetura do sistema de avaliação desenvolvido e, por fim, à demonstração\ndas funcionalidades implementadas."
  },
  {
    "keywords": [
      "Wireless",
      "Sensores",
      "Temperatura",
      "Humidade relativa",
      "Luminosidade",
      "Sensors",
      "Temperature",
      "Relative humidity",
      "Luminosity",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "ZeroSkin: sensorização",
    "autor": "Cunha, João Pedro Montenegro",
    "data": "2023-07-04",
    "abstract": "A dissertação insere-se no projeto ZeroSkin, um projeto que pretende fornecer uma \nsolução a baixo custo para renovação da fachada de edifícios que não vão de encontro com os \nobjetivos europeus para 2050 em termos de eficiência energética. A dissertação acrescenta \ninteligência à fachada com a capacidade de sensorizar o meio, recolhendo dados para a integração \nda fachada numa casa inteligente. \nA dissertação resulta em duas versões de protótipos de sensores com comunicação sem \nfios. A comunicação será assegurada pelos microprocessadores Xbee que comunicam através do \nprotocolo de comunicação Z-Wave, evitando interferências com os restantes equipamentos \nwireless presentes na habitação, garantido um alcance até 100 metros em ambiente urbano.\nA primeira das versões possui capacidade de medir a temperatura ambiente, a humidade \nrelativa e a luminosidade. Esta versão consegue medir temperatura num intervalo de -50 até 150 \ngraus celsius, medir humidade relativa entre 30 e 90 % e medir iluminância entre 0 e, \naproximadamente, 1000 lux.\nA segunda versão foi desenvolvida para garantir o correto funcionamento da fachada, \ndetetando falhas através da intrusão de água, possuindo sensores de vazamento instalados nos \npontos críticos da fachada renovada. As medições destes sensores foram dividas em três \nintervalos, ausência de água, presença de alguma humidade e presença de água.\nO protótipo final resultou num protótipo de baixo custo, baixa manutenção, dimensões \nreduzidas e baixa pegada na fachada, baixo consumo energético e autónomo energeticamente.\nA autonomia do módulo de sensores é assegurada através de uma bateria de lítio, capaz \nde alimentar o módulo durante 9 dias, ligada a um gestor de carga, alimentada por um painel\nsolar. Resultando num módulo compacto e capaz de permanecer em funcionamento durante \nlongos períodos, desde que exposto esporadicamente a luz solar.\nA suportar as funcionalidades que o hardware desenvolvido possui, um programa em \nPython controla as comunicações no recetor central, enviando comandos, processando os dados \nrecebidos, terminando a apresentar ao utilizador e gravando em ficheiro os dados processados."
  },
  {
    "keywords": [
      "Estruturação",
      "Framework",
      "Desenvolvimento web",
      "e-commerce",
      "Structuring",
      "Web development",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Framework para a criação de lojas online",
    "autor": "Bravo, José Pedro Vilela",
    "data": "2023-12-20",
    "abstract": "Atualmente, para se obter um modelo de negócio de sucesso, é necessário um investimento no comércio eletrónico e nas tecnologias inerentes. No desenvolvimento de uma loja online, é necessário que seja efetuada uma estruturação da mesma, de forma que o utilizador final consiga encontrar os produtos e/ou serviços desejados efetuando o menor número de cliques, tornando este processo o mais simples e cómodo possível. De forma semelhante, o processo de criação deste tipo de plataformas deve ser um processo relativamente simples e acessível a \nqualquer individuo. Neste contexto, surge a plataforma para geração simples e intuitiva de lojas online consoante as preferências do utilizador. O funcionamento da plataforma consiste na manipulação e integração de um conjunto de componentes customizáveis ou templates, devidamente generalizados, de modo a ser possível gerar uma loja online com as funcionalidades e propriedades definidas pelo utilizador. O objetivo é o desenvolvimento de uma aplicação de generalização que permita automatizar o processo de criação de lojas online. O resultado deste projeto permitirá a um programador web criar uma loja online de forma rápida, através do download do código fonte da loja gerada, tendo apenas de efetuar uma parametrização de um conjunto de atributos específicos consoante o caso concreto a implementar, de modo a satisfazer as necessidades dos clientes."
  },
  {
    "keywords": [
      "681.324",
      "621.39"
    ],
    "titulo": "Large scale privacy preserving bluetooth sensing",
    "autor": "Gonçalves, Nelson Manuel Almeida",
    "data": "2012-03-22",
    "abstract": "Driven by the pervasiveness of mobile devices, location based services are becoming increasingly popular. These services use information about the physical location of users, usually with commercial or informative purposes. However, and particularly for large scale scenarios, this type of services may pose a risk to the privacy of the users. By using location information either directly or indirectly (associated with other information), it is possible to expose personal information that users wish to keep private or even to uncover their identities. This may lead to the rejection of these types of technologies.\nThere are however, non trivial ways to store information without compromising the users’ privacy. This dissertation presents two Bluetooth scenarios where stochastic summarizing techniques are used as a solution to the privacy problem.\nIn the first scenario, Gate Counting, the goal is to provide accurate counting for the number of unique devices sighted while trying to minimize the amount of collected information. For that purpose, we provide an analysis of several stochastic counting techniques that not only provide a sufficiently accurate count for the number of unique devices, but offer privacy guarantees as well, all in a space efficient way.\nFor the second scenario, Causality Tracking, the objective is to study human mobility patterns, also while minimizing the quantity of data gathered. For this purpose, we developed Precedence Filters, a new technique, which is able to provide\naccurate results regarding the popularity of specific routes without compromising the individual privacy of the users.\nBased on these scenarios, this dissertation demonstrates that stochastic summarizing techniques are viable means to the anonymization of location information."
  },
  {
    "keywords": [
      "Concurrency",
      "Process algebra",
      "Quantum",
      "Time",
      "CCS",
      "qCCS",
      "TCCS",
      "TqCCS",
      "Concurrência",
      "Álgebra de processos",
      "Quântica",
      "Tempo",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Integration of time in a quantum process algebra",
    "autor": "Fernandes, Vítor Emanuel Gonçalves",
    "data": "2019-12-30",
    "abstract": "Process algebras are mathematical structures used in Computer Science to study, model,\nand verify concurrent systems. Essentially, a process algebra consists of a language (used\nto specify a system that one wishes to study), a semantic domain to interpret the language\n(which allows the interpretation and the study of the system) and a set of axioms related\nto the language operators (which facilitates the derivation of properties of the system be ing studied). These basic ingredients make process algebras powerful tools, with many\napplications in the development of concurrent systems and many successful stories in in dustry, Bunte et al. (2019); Groote and Mousavi (2014). The three classical examples of a\nprocess algebra are: CCS introduced by Robin Milner, ACP introduced by Jan Bergstra and\nJan Willem Klop, and CSP introduced by Tony Hoare. Of the three, the first stands out\nbecause of its main goal to isolate and study the elementary principles of communication\nand concurrency.\nThe development of Quantum Mechanics supports the design of computational systems\nruled by quantum laws, which, in the context of certain problems, perform significantly\nbetter than any classical computational system. This is exemplified with Shor and Grover\nalgorithms, respectively used in the factorization of integers and in unstructured searching.\nMoreover, Quantum computing has applications in the communications area, having as\nmain examples the quantum teleportation protocol and the BB84 communication protocol.\nHowever, due to their high sensitivity to noise, quantum computers have a very limited\nmemory space, and therefore they usually integrate a QRAM architecture: essentially, a net work of classical computers that process and manage a general task list, invoking quantum\ncomputers only when high-cost computational tasks arise. This highlights the importance\nof extending the theory of process algebras to the quantum domain. In fact, some quantum\nprocess algebras were already proposed in last years: examples include qCCS, developed\nby Mingsheng Ying et al based on CCS, and the CQP algebra, introduced by Simon Gay and\nRajagopal Nagarajan. Related to qCCS, a typing system was developed, where the typable\nprocesses are exactly the valid qCCS processes.\nCurrent quantum process algebras assume the existence of an ideal quantum system,\ni.e. a quantum system immune to noise. In contrast, the aim of this dissertation is to\nstudy and develop a quantum process algebra in which this assumption is discarded. More\nspecifically, we do not assume that a quantum state can be stored indefinitely, it may become\ncorrupted over time, or in other words, have a limited time of coherence. For that goal, 1)\nwe developed a new quantum process algebra that merges the strengths of qCCS and CQP,\nin particular, recursion, memory allocation, and a typing system, so that we can study\ncomplex quantum systems that integrate the QRAM architecture; 2) we extended the new\nprocess algebra with a notion of time so that we could study its effects on quantum states;\nand 3) we developed a number of case studies, via the mentioned extension, in which\nthe coherence time of quantum systems has a central role. This includes, for example, a\nsimplified version of the IBM Cloud, which provides access to a quantum computer via web."
  },
  {
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Data mining",
      "Healthcare",
      "Knowledge discovery process",
      "Machine learning",
      "Conhecimento médico",
      "Inteligência artificial",
      "Mineração de dados",
      "Processo de descoberta de conhecimento",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Applying machine learning algorithms to medical knowledge",
    "autor": "Brito, Maria Ana de",
    "data": "2019-12-23",
    "abstract": "Achieving great and undeniable success in a great variety of industries and businesses has made the term Big Data very popular among the scientific community. Big Data (BD) refers to the ever fast-growing research area in Computer Science (CS) that comprises many work areas across the world. The healthcare sector is widely known to be highly proficient in\nthe production of big quantities of data. It can go from health information, such as the\npatient’s blood pressure and cholesterol levels, to more private and sensitive data, such as\nthe medical procedures history or the report of ongoing diseases.\nThe application of sophisticated techniques enables a profound and rigorous analysis of\ndata, something a human cannot do in real-time. However, a machine is capable of rapidly\ncollect, group, storage and examine vast amounts of data and extract unknown and possi bly interesting knowledge from it. The algorithms used can discover hidden relationships\nbetween attributes that prove to be very useful for a corporation’s work. Buried structures\nwithin the produced data can also be detected by these techniques. Machine Learning (ML)\nmethods can be adjusted and modelled to different input representations - this adaptability\nis one of the factors that contributes to its blooming prosperity.\nThe main goal is to make predictions on data, by building utterly efficient models that can\naccurately take in the data and thus predict a certain outcome. This is especially important\nto the healthcare industry since it can considerably improve the lives of many patients.\nEverything from detecting a type of disease, predicting the chance of morbidity after a\nhospital stay, to aid in the decision making of treatment strategies are vital to patients as\nwell as to clinicians.\nAny improvement over established methods that have been previously studied, tested\nand published are an asset that will improve the patient’s satisfaction about the healthcare\nperformance in medical institutions. This can be achieved by refining those algorithms or\nimplementing new approaches that will make better predictions on the given data.\nThe main objective of this dissertation is to propose ML approaches having acknowledged and evaluated the existent methods used in clinical data. In order to fulfill this goal,\nan analysis of the state of the art of medical knowledge repositories and scientific papers\npublished related to the selected keywords selected was performed. In this line of work,\nit is crucial to understand, compare and discuss the results obtained to those previously\npublished. Thus, one of the goals is to suggest new ways of solving those problems and\nmeasuring them up against the existent ones."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Definição de middleware ”cloud-based” para serviço a aplicações de monitorização de espaços físicos",
    "autor": "Sousa, José Francisco Ferreira Alves de",
    "data": "2017",
    "abstract": "Com a crescente utilização e adoção de dispositivos orientados à Internet of Things, especialmente\ndispositivos orientados ao espaço casa, crescem também o número de protocolos,\nAPIs e frameworks desenvolvidos pelos fabricantes para complementar os seus dispositivos.\nEste elevado número de soluções, sem a adoção de um standard geral que facilita o desenvolvimento\nde aplicações para esta área, serve de motivação para esta dissertação.\nCom isto em mente, esta dissertação tem como objetivo o desenvolvimento de uma camada\nde middleware, que faz a ligação entre os mais diversos dispositivos e as aplicações clientes,\nabstraindo os dispositivos e as funcionalidades, facilitando imenso o desenvolvimento\ndestas aplicações. Este middleware também deverá oferecer algumas comodidades a nível de\nutilização, sob a forma de mecanismos de automação e definição de cenários.\nA dissertação irá expor o estado da tecnologia nesta área, identificando as mais valias e\ntambém os possíveis problemas das mesmas. Também será revisto algum trabalho de investigação\nna área, de modo a entender os esforços já feitos. Feito o estudo sobre o estado de\narte, iremos proceder ao desenho e conceção da arquitetura do middleware, passando para o\ndesenvolvimento do mesmo. Por fim, será feito um caso de estudo, sobre a forma de uma\naplicação mobile, que faz uso do middleware, demonstrando todas as suas funcionalidades e\ncapacidades de interoperabilidade."
  },
  {
    "keywords": [
      "Ontology",
      "Multilingual ontology",
      "Multilingual ontology creation tools",
      "Ontologia",
      "Ontologia multilingue",
      "Ferramentas de criação de ontologias multilingues",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Multilingual ontologies creation",
    "autor": "Monteiro, Simão Freitas",
    "data": "2023-12-15",
    "abstract": "Semantic Web multilingual information has been growing in last years. Consequently we\nassist to the need of creating multilingual ontologies from monolingual ones in order to allow\nfor the semantic interoperability between applications in different natural languages. Cross lingual enrichment is however often based in human interaction, which is a cumbersome\nand error-prone task.\nThis document presents a study of already existing multilingual ontology creation meth ods in order to gain knowledge on what techniques exist, pointing the advantages and\ndisadvantages of each one. From the knowledge obtained with the bibliographic research, a\nnew web-based method was proposed and implemented. The detailed description of this\ndevelopment process is the main content of the present Master’s dissertation.\nThe Multilingual Ontology Creation Tool produces multilingual ontologies in two different\nways. The first is based on translating certain elements of an input ontology and the other\nis done through a triangulation process between two existing multilingual ontologies. In\nthe first approach, that involves translation, a LogFile is also generated offering to the\nuser the possibility to download it for future examination. This LogFile contains detailed\ninformation concerning translation choices the system has made that can affect the quality of\nthe translations. The tool introduced and discussed in this dissertation was tested throughout\nits the development. Lastly, a web application was developed to host the tool in order to\nfacilitate its access and use for the users."
  },
  {
    "keywords": [
      "Ataxia",
      "Balance",
      "Exergames",
      "Motivation",
      "Rehabilitation",
      "Serious games",
      "Equilíbrio",
      "Jogos sérios",
      "Motivação",
      "Reabilitação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Exploring the use of serious videogames in a robotic walker to improve ataxic gait",
    "autor": "Rodrigues, Alexandre Rzepecki",
    "data": "2023-04-17",
    "abstract": "Ataxia is a neurological sign indicative of dysfunction in the cerebellum, a part of the brain that coordinates\nmovement. The typical symptoms include lack of balance when walking or standing, loss of limb coordination,\nchange in speech, and difficulty with fine motor tasks, strongly affecting the person’s daily activities. Cerebellar\nataxia can be inherited or caused by other disorders such as stroke, multiple sclerosis, or cerebral palsy. While\nthere is no known cure for inherited ataxia, the symptoms can be managed through intensive and personalised\nrehabilitation, including physical, speech, and occupational therapy.\nPhysical therapy, also known as conventional therapy, is a standard practice in the rehabilitation of patients with\nataxia. It focuses on performing different physical exercises repeatedly, where a therapist monitors the session and\nregulates the intensity. Although effective, the repetitiveness of conventional therapy can become monotonous,\nbesides being a very time-consuming process, which can be cumbersome for the therapist. Without any additional\nstimulation and decreasing motivation, patients may experience stagnation or even drop out of therapy.\nRobot-assisted Gait Training (RAGT) is being introduced in the rehabilitation of persons with motor disabilities.\nThis technology allows personalised and intensity-adapted training, which could benefit the patient’s recovery.\nHowever, this therapy can also become monotonous, so there is a need for more interactive and appealing\nstrategies. A possible solution to this problem is the development of exergames (serious games) and their\ninclusion in robotic-assisted therapy. This can become advantageous since the robotic devices integrate several\nsensors that read the patient’s movement and can be used as controllers in the game, allowing a more immersive\nexperience.\nConsidering this, this dissertation proposes two serious videogames, which were integrated into a robotic\nwalker, the WALKit Smart Walker, intended for gait ataxia rehabilitation. The first game is cognitive, whose goal is\nto react to a certain shape or sound as quickly as possible. With this game, it is intended to study the influence\nof dual tasking on motor rehabilitation. The second game is a dynamic one whose goal is to incite the patient’s\nbalance control. This game uses an algorithm for torso orientation estimation to control an avatar holding two\nbuckets full of water. The main goal is to give patients biofeedback regarding their postural balance and to exercise\ntheir balance with specific events, or minigames, that encourage them to lean in a way that causes weight transfer.\nBoth games were validated with healthy volunteers, in terms of functionality and usability. The results allowed to\nconclude that both were fun to play and showed potential to aid rehabilitation. Moreover, the results emphasised\nthe relevance of customisation, replayability, and aesthetics in serious games. Future work includes the thorough\nvalidation of both serious games with patients with cerebellar ataxia, to assess their effectiveness in rehabilitation\nalong with WALKit Smart Walker."
  },
  {
    "keywords": [
      "Transactional memory",
      "Concurrent programming",
      "Hot spot optimisation",
      "Memória transacional",
      "Programação concorrente",
      "Otimização de hot spots",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Concurrency hot spot optimisation in transactional memory",
    "autor": "Ribeiro, Rui André Santos",
    "data": "2023-11-27",
    "abstract": "Database management systems have a long history of development and research, with systems like Post greSQL and languages like SQL already being well-established in the industry. Transactional memory\nemerges as a new concurrency control mechanism for concurrent programming, inspired by ideas and\nconcepts from the database world. As is the case with database transactions, transactions in transac tional memory can (and will) conflict when multiple transactions try to modify the same data. This can\nlead to the appearance of hot spots in contended memory regions, quickly degrading the performance of\nan application.\nIn this dissertation, we propose new optimisation techniques for transactional memory hot spots,\nbased on previous research on splitting techniques for numeric database records. We implement the\noptimisations on an existing transactional memory system and measure their impact on performance,\nusing custom-made and reference benchmarks."
  },
  {
    "keywords": [
      "Fusão de dados",
      "Machine learning",
      "Insucesso escolar",
      "Early fusion",
      "Late fusion",
      "Educação",
      "Inteligência Artificial",
      "Data fusion",
      "School failure",
      "Academic performance",
      "Artificial intelligence",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Fusão de dados aplicando modelos de machine learning",
    "autor": "Teixeira, Renata Ferreira",
    "data": "2024-05-20",
    "abstract": "A educação e o sucesso académico são de grande relevância na sociedade atual, nomeadamente para o futuro profissional, económico e pessoal dos jovens. Tendo isto em conta, é de grande interesse e vital importância para as instituições de ensino poder prever a variação das notas dos alunos, principalmente alunos em risco de reprovação, uma vez que se podem alterar métodos de ensino e aplicar medidas corretivas e estratégias de intervenção para apoiar alunos de baixo desempenho, tendo em conta as suas necessidades. A introdução de Inteligência Artificial e de técnicas de Fusão de Dados nesta área pode ser muito interessante, podendo melhorar a eficiência na deteção de alunos em risco de insucesso escolar. Deste modo, esta dissertação visa o desenvolvimento de dois casos de estudo onde se pretende o desenvolvimento de algoritmos e procedimentos relativos à fusão e integração dos dados e a criação de\nmodelos preditivos. No primeiro caso de estudo, é proposto desenvolver modelos de previsão para prever variações nas notas de alunos do ensino básico nas disciplinas de Português e Matemática do primeiro para o segundo período, através da implementação da técnica de early fusion. Como segundo caso de estudo desta dissertação, propôs-se o desenvolvimento de modelos preditivos para a previsão da variação\ndas notas de alunos do ensino secundário nas disciplinas de Português e Matemática do segundo para o terceiro período do 12º ano, havendo, neste processo, a implementação de duas técnicas de fusão de dados - early fusion e late fusion.\nDiante dos melhores modelos candidatos obtidos, comprovou-se que a fusão de dados obtém um bom desempenho na criação de modelos preditivos para a previsão da variação de notas, e que ambas as técnicas de fusão testadas são competentes, aparentando melhorar os resultados da previsão relativamente a modelos criados a partir dos conjuntos de dados de forma separada. Palavras-chave Fusão de dados, Machine Learning, Insucesso escolar, Early fusion, Late fusion, Edu cação, Inteligência Artificial"
  },
  {
    "keywords": [
      "Deep learning",
      "Generative models",
      "Protein design",
      "Evolutionary algorithms",
      "Novel proteins",
      "Algoritmos evolucionários",
      "Design proteico",
      "Modelos generativos",
      "Desenho de novas proteínas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Deep learning generative models for novel enzyme design",
    "autor": "Martins, Miguel",
    "data": "2022-03-24",
    "abstract": "Recent endeavours over the past few years have been applying generative Deep Learning (DL) models to generate novel proteins using an array of different approaches. Such initiatives represent a specially important development towards major contributions to the field of protein engineering. To contribute to this, various DL architectures can be applied to the different datasets to generate proteins with a particular set of properties. The field of DL applied to the generation of novel molecules has been presenting results that encourage further research on this subject. An increasing number of novel, computationally generated, molecules being synthesized with successful results creates grounds for stimulation of new endeavours and diversification of the current applications.\nThe goal of the work presented in this dissertation is to apply different generative DL architectures to the design of novel protein sequences for a targeted set of optimized properties. The developed framework, termed GenProtEA, stands as the main contribution of this work. The framework envisages the implementation of generative DL architectures for the design of novel proteins and leverages the use sampling techniques and Evolutionary Computation to steer the generative process towards a specific set of properties. Evolutionary Algorithms (EAs) can be applied both to single and multi-objective optimization problems which in itself presents an added advantage. \nThe optimization problems were designed considering the literature concerning protein design. The problems ranged from a simple maximization of the average hydrophobicity of the protein sequence to more complex problems such as minimizing two sets of events in a sequence or maximizing a probability of a protein being generated by a defined profile Hidden Markov Model (HMM). The results of the proposed case studies and the respective analysis accompany the framework in this endeavour.\nTwo different generative DL architectures were deployed, trained, and evaluated, using loss and accuracy metrics to perform the analysis.: a Generative Adversarial Network (GAN) and a Variational Autoencoder (VAE). For the GAN architecture, new proteins are sampled varying the latent seed used in the generative process and then selecting the best candidates for each of the case studies. Besides following a same sampling approach to obtain new protein designs, the VAE latent space is explored using EAs. The results of this work show that the use of EAs in the optimization, steering the generative process, can produce the best results, allowing for more variability in the experiments designed and resulting in a much greater set of possibly functional novel proteins."
  },
  {
    "keywords": [
      "3D Application Servers",
      "APEX Framework",
      "CAVE",
      "Motion Capture",
      "OpenSimulator",
      "Second Life",
      "VR Frameworks",
      "Wiimote",
      "Framework APEX,",
      "Frameworks de Realidade Virtual",
      "Captura de Movimento",
      "Servidores de Aplicações 3D",
      "681.3"
    ],
    "titulo": "Integrating a 3D application server with a CAVE",
    "autor": "Moreira, Rui Manuel Ferreira de Carvalho Azevedo",
    "data": "2011-12-07",
    "abstract": "The prototyping of ubiquitous environments has never been an easy task. Under\nnormal circumstances, this type of system can only be tested during deployment,\nresulting in complications in the development process. To solve this issue, developers\nat the Department of Informatics in the University of Minho created the rApid\nPrototyping for user EXperience (APEX) framework, that relies on a 3D application\nserver to create simulations for ubiquitous environments. The goal is to use these\nsimulations as prototypes of the actual environments, allowing early assessment of\nhow users will experience the proposed designs.\nThe focus of this dissertation is in integrating a 3D application server with a Cave\nAutomatic Virtual Environment (CAVE), for a more realistic approach when using\nthe APEX framework. 3D application servers provide the means to develop virtual\nworlds. The CAVE is a Virtual Reality (VR) theatre where a user can interact with\nan immersive environment, through sensors and mobile devices. This dissertation\nshows a set of solutions to achieve the desired integration, as well as a number of\ninteraction devices that can be used for this purpose."
  },
  {
    "keywords": [
      "Interoperabilidade",
      "Middleware",
      "Sistemas Legacy",
      "Web services",
      "Interoperability",
      "Legacy Systems",
      "Web services",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Conceção de Arquitetura de interoperabilidade: o caso Caravela Seguros",
    "autor": "Sousa, Ana Zita Pinto de",
    "data": "2016-12-10",
    "abstract": "Nos dias de hoje, existe cada vez mais a necessidade de interligação/integração entre múltiplos sistemas heterogéneos no que respeita a arquiteturas e tecnologias. Para que seja possível a comunicação entre os diversos sistemas recorre-se a serviços de middleware.\nEstes providenciam Programming Interfaces (APIs) com protocolos e padrões que todos entendem e através dos quais conseguem comunicar.\nAssim, também é possível fornecer soluções de interoperabilidade com sistemas já existentes e que à partida não estarão preparados para a heterogeneidade de sistemas e arquiteturas que entretanto surgiram.\nDesta forma, este tema de dissertação pretende conceber uma solução de middleware que possibilite a interoperabilidade com o sistema existente da Caravela Seguros, através da análise e conceção de uma arquitetura e posteriormente elaborando alguns serviços que provem a validade da solução encontrada. Assim, pretende-se obter um sistema que facilite a utilização dos sistemas da Caravela Seguros por parte das entidades que com ela se relacionem."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Applying data science and machine learning for psycho-demographic profiling of internet users",
    "autor": "Ribeiro, Hugo",
    "data": "2018-11-30",
    "abstract": "There always have been a huge interest in working with public data from online social\nmedia users, with the exponential growth of social media usage, this interest and re searches on the area keep increasing.\nThis thesis aims to address prediction and classification tasks on online social net work data. The goal is to predict psycho-demographic - personality and demographic -\ntraits by doing text emotion analysis on social networks as Twitter and Facebook. Our\nmain motivation was to raise awareness to what can be done with users’ social media\nor network information or usual behaviours on the web, such as from text analysis\nwe can trace their personality, know their tastes, how they behave and so on, and to\nspread the emotion-text relation on social networks subject, because it only started to\nbe studied recently and there’s so much data and information to do it.\nTo perform these tasks mentioned above we carried an extensive review of literature\nof previous works to define the state-of-art of the project and to learn and identify work\nstrategies. Almost all of the past researches, based their results on a vast sample of\nusers and data, but because some frameworks and APIs were shutdown in recent years,\nsuch as MyPersonality from Facebook adding to some frameworks being paid for,\nresulted in a small sample of users’ data to analyze in our thesis which can prejudice\nthe results.\nWe start by gathering data from Twitter and Facebook with users consent. On Twit ter we focused on tweets and retweets, on Facebook we focused on all of what the\nuser typed by using the DataSelfie plugin that stored all that data on a server that\ncan be retrieved later. Our next step was to find emotions on their text data with the\nhelp of a lexicon that categorized words by eight different emotions, two of them were\nput away because we focused only on the six major emotions - this is explained later\n- and we had to remove stopwords and apply stemming to all of the text and do a\nword-matching of every word of our data with every word from the lexicon. After\nthis, we asked our participants to fulfill a \"Big-Five\" personality questionnaire and to\nprovide us their age, so we added the Big-Five traits and age to each users individual\ndataset. We got their final versions, ready to apply machine-learning algorithms to\nfind correlations between emotions and personality or demographic attributes. We\nfocused on practical and methodological aspects of the user attribute prediction task.\nWe used many techniques and algorithms that we thought it were best fit for the data\nwe had and for the goal that we had to achieve.\nWe gathered data in two datasets that we tested, one of them we called \"Mixed Lan guage Dataset\", contains all text entries from each user, and the other \"User Dataset\",\ncontains one entry per user after we analyze every text entry for all users in order to\nhave a more general view on each one. For the first mentioned dataset we achieve\nbest results with the decision trees algorithms, from 58% on the agreeableness trait,\nto 68% on the neuroticism trait. This dataset had a problem with the way data was\nspread, so it was impossible to predict age and gender with efficiency. As for the lat ter, regarding demographic characteristics all of the classifiers had a good classifying\npercentage, from K-nearest’s 73% to Naive Bayes’ 95%. The most solid classifier for\npersonality traits was the one using the CART decision tree algorithm, it ranged from\n50% on the openness trait to 76% on the agreeableness one. There were classifiers with\nterrible results, there were others that were a bit dull, and there were some that stood\nout as we stated above. We had a small sample, and that was a problem as it wasn’t\nconsistent or solid in terms of data value and that can change our results, we believe\nthat our results would be way better if we applied the same mechanisms to a much\nbigger sample.\nConcluding, we demonstrate how we can predict personality or demographic traits\n- BigFive traits, age or gender - from studying emotions in text. As stated above, we\nhope this thesis will alert people for what can be done with their online information,\nwe only focus on psycho-demographic profiling, but there are many other things that\ncan be done."
  },
  {
    "keywords": [
      "SCEUR",
      "Estatísticas",
      "Statistics",
      "OAI-PMH",
      "ContextObjects",
      "Harvest",
      "KEEPS",
      "RCAAP",
      "02:681.3",
      "681.3:02"
    ],
    "titulo": "Serviço Centralizado de Estatísticas de Utilização de Repositórios",
    "autor": "Silva, Hélder de Jesus Almeida da",
    "data": "2011-12-23",
    "abstract": "Nos dias de hoje presenciamos a uma mudança de paradigma no que diz\nrespeito aos repositórios institucionais. Passamos do repositório isolado\nno seu contexto institucional para os consórcios onde conjuntos de repositórios\npartilham ideias, políticas e tecnologias, contribuindo para o\ncrescimento do conhecimento das comunidades em que estão inseridos.\nPodia falar-se da importância que os repositórios institucionais mantêm\njunto da sua comunidade, mas com a mudança de paradigma torna-se\nrelevante explorar os desafios de gestão que o novo contexto apresenta.\nPara gerir um consórcio é necessário possuir indicadores que auxiliem\nna tomada de decisão e que atendam às necessidades de informação que\nas entidades de fomento possuem no que diz respeito ao impacto dos\ninvestimentos em cultura, investigação, inovação e desenvolvimento.\nEsta dissertação apresenta o Serviço Centralizado de Estatísticas de Utilização\nde Repositórios (SCEUR). Trata-se de um projeto inserido no\nâmbito da iniciativa Repositório Científico de Acesso Aberto de Portugal\n(RCAAP) que visa a construção de uma arquitetura que permita recolher,\nprocessar e apresentar de uma forma intuitiva dados estatísticos\nde utilização em repositórios institucionais e também auxiliar a partilha\nde dados estatísticos quer pela disponibilização de add-ons que facilitem\nessa tarefa no software usado pelas instituições quer pela disponibilização\nde informação e recomendações variadas nesse contexto. São também\napresentados projetos internacionais de referência neste contexto,\nnormas existentes e tecnologias usadas para a implementação dos conceitos\nsubjacentes."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aproximação digital das populações às suas instituições públicas",
    "autor": "Monteiro, Vasco F. S.",
    "data": "2018",
    "abstract": "Com o crescimento significativo de utilizadores de dispositivos móveis, nos últimos anos,\nmais do que a necessidade apareceu a oportunidade de criar novas plataformas e serviços\ndigitais que não só facilitam o quotidiano das pessoas, evitando deslocamentos, filas de\nespera e complicações desnecessárias, como tornam a comunicação das pessoas com as\ninstituições num processo mais rápido e cómodo. Como tal, estas alternativas estão gradualmente\na complementar, em alguns casos mesmo a substituir, os métodos antigos.\nEsta dissertação propõe uma solução digital, na forma de uma aplicação móvel, para aproximar\nas populações às suas instituições públicas recorrendo a gamification, isto é, transformando\no processo de comunicação com uma Câmara Municipal ou Junta de Freguesia,\npor exemplo, recorrendo ao uso de elementos de jogos como conquistas, atribuição de\nrecompensas, classificações, entre outras, com o objetivo de estimular a comunicação do\nutilizador com essas instituições permitindo que este possa ao mesmo tempo divertir-se\ne competir com os outros, enquanto explora e ajuda a sua cidade. Para tal, além de um\nserviço de participação de ocorrências na cidade, os utilizadores têm à sua disposição toda\numa narrativa que os leva a completar missões, participar em eventos e conhecer gente\nnova bem como a conhecer verdadeiramente a sua própria cidade. Por sua vez, a instituição\npública responsável pela manutenção e administração do sistema tem à sua disposição uma\nplataforma de administração para manter o conteúdo da aplicação atualizado para os seus\nutilizadores.\nNo decorrer desta dissertação está documentado todo o processo de desenvolvimento da\naplicação móvel e do servidor web, que a suporta, assim como todas as decisões tomadas\ne as razões que as justificam. O documento contém, ainda, exemplos e explicações do funcionamento\nda aplicação, considerações finais sobre o projeto e ideias para trabalho futuro.\nNas considerações finais é feita uma comparação dos objetivos inicialmente traçados para\no projeto e o resultado alcançado, provando que os mesmos foram cumpridos com sucesso."
  },
  {
    "keywords": [
      "Autenticação.Gov",
      "Autenticação",
      "CLAV",
      "Segurança",
      "Authentication",
      "Security",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "CLAV: autenticação e integração na plataforma iAP",
    "autor": "Maia, Octávio José Azevedo",
    "data": "2019-12-23",
    "abstract": "Existe uma preocupação cada vez maior em relação a quantidade de informação gerada e recebida por diversas instituições. Não só no que toca ao consumo excessivo de papel, como também a gestão de grandes quantidades de informação. Com o intuito de simplificar a gestão documental, o governo tem desenvolvido diversas estratégias. Nomeadamente, na Administração Pública (AP), com base em normas e orientações provenientes da Comissão Europeia. O Projeto M51-CLAV-Arquivo digital: Plataforma modular de classificação e avaliação da informação pública (CLAV), surge como uma dessas estratégias. Este visa a classificação e a avaliação de toda a documentação que circula na Administração Pública portuguesa, utilizando um referencial comum que permite o desenvolvimento de instrumentos de natureza transversal a aplicar em contexto organizacional.\nEsta dissertação tem como objetivo primário, a integração dos serviços da plataforma Autenticação.Gov no CLAV, bem como a criação de estratégias apropriadas de gestão de utilizadores, autenticação de pedidos referentes à API pública disponibilizada, autenticação no backend e segurança da aplicação."
  },
  {
    "keywords": [
      "Machine learning",
      "Redes neuronais",
      "Classificação de imagens",
      "Otimizadores",
      "Neural networks",
      "Image classification",
      "Optimizers",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Deteção de veículos em parques de estacionamento através de machine learning",
    "autor": "Coelho, Sara Daniela da Silva",
    "data": "2023-11-20",
    "abstract": "O tema principal abordado nesta dissertação é a classificação de imagens através de redes neuronais. \nO crescimento do estudo desta área da inteligência artificial permite a que, atualmente, os sistemas \nsejam mais eficientes. \nA forma mais ancestral da gestão dos parques de estacionamento passa pela colocação de sensores \nde movimento ou de presença nos lugares de estacionamento. Tal organização leva a grandes despesas \nna aquisição e manutenção do material. \nA implementação de um sistema de gestão centralizado e com recurso a métodos inteligentes, além \nde diminuir as despesas dos materiais, uma vez que uma câmara vem substituir os vários sensores, \ntambém leva a um maior conforto por parte dos utilizadores ao nível da simplicidade em encontrar uma \nvaga no parque de estacionamento.\nA metodologia desta dissertação passa por implementar várias redes neuronais e decidir qual é a que \nobtém um maior número de previsões realizadas com sucesso. Foram utilizados dois tipos de redes \nneuronais: as Multi-Layer Preceptron (MLP) e as Convolution Neural Networks (CNN). Todas as redes em \nestudo foram alimentadas com o dataset criado na plataforma CoppeliaSim, onde são criadas simulações \nde imagens captadas por uma câmara num parque de estacionamento.\nOs resultados dos testes realizados mostram que tanto as redes MLP como as redes CNN obtêm \nbons resultados no projeto implementado, comprovando que as imagens podem ser observadas como \numa sequência de pixéis e, dessa forma, padronizadas.\nO presente trabalho constitui uma forte contribuição na crescente área de estudo das redes neuronais \npois demonstra que, selecionando os parâmetros de rede adequados, é possível aumentar a sua \neficiência. Mais ainda, as redes MLP conseguem apresentar melhores resultados comparativamente às\nredes CNN, preparadas e desenhadas para a interpretação de imagens."
  },
  {
    "keywords": [
      "Justiça",
      "Decisões judiciais",
      "Inteligência Artificial",
      "Processamento de linguagem natural",
      "Machine Learning",
      "Justice",
      "Judicial decisions",
      "Artificial Intelligence",
      "Natural language processing",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Modelação e previsão de decisões judiciais utilizando um repositório de sentenças",
    "autor": "Rodrigues, Joel Soares",
    "data": "2021-12-03",
    "abstract": "O ritmo da evolução tecnológica e a sua relação com o ser humano tem aumentado significativamente ao\nlongo dos tempos, sendo que um dos ramos que mais impacto está a causar no quotidiano das pessoas\né a inteligência artificial. A grande ascensão desta área é um fenómeno transversal a praticamente todos\nos setores da sociedade. Esta dissertação enquadra-se no setor da justiça.\nAo longo dos anos, um dos principais problemas nos sistemas judiciais por todo o mundo é a mo rosidade na resolução dos processos judiciais. Tendo por base esta problemática, as entidades governa mentais adotam cada vez mais reformas na área da justiça com recurso à tecnologia, desejando sistemas\njudiciais cada vez mais eficientes.\nNeste sentido, esta dissertação tem como objetivo o desenvolvimento de uma solução capaz de extrair\nconhecimento a partir de dados jurídicos portugueses. Esta solução é caracterizada por um conjunto de\nmecanismos, com recurso a técnicas de inteligência artificial, que vai desde a extração de dados até à\nrealização de duas grandes experiências: análise de sentimentos e previsão da decisão. Estes mecanismos\npermitiram gerar diversas informações e conhecimento. Por um lado evidenciou-se a pouca relação entre\na carga emocional dos textos dos juízes e a decisão, por outro destaca-se o desenvolvimento de modelos\ninteligentes capazes de prever a decisão com precisões de média de 76%, recorrendo ao conteúdo textual.\nAlém disso, ao longo de todo o processo, também foram extraídas outras informações, como por exemplo,\nas palavras relacionadas com a procedência e improcedência de acórdãos, a legislação que é geralmente\ncitada em conjunto, entre outras. Em paralelo desenvolveu-se um protótipo de uma dashboard com a\napresentação de informações e conhecimento de alto nível sobre os dados."
  },
  {
    "keywords": [
      "616-072.1",
      "616.3",
      "681.3:61",
      "61:681.3"
    ],
    "titulo": "Aquisição, tratamento, arquivo e difusão de exames de endoscopia",
    "autor": "Laranjo, Isabel",
    "data": "2012",
    "abstract": "De entre os diversos tipos de exames de endoscopia, a esofagogastroduodenoscopia assume um papel\npreponderante devido a ser o método ideal para examinar a mucosa do trato digestivo alto, bem como\npara detetar inúmeras patologias gastrenterológicas. O resultado deste tipo de exames é, geralmente, um\nrelatório composto por um conjunto de frames capturados durante o exame, eventualmente acompanhado\npor um vídeo. Hoje em dia, apenas as imagens juntamente com o relatório endoscópico, são arquivadas.\nO facto de o vídeo não ser arquivado pode conduzir a um incómodo no bem-estar do paciente, assim\ncomo a um acréscimo de custos e tempo despendido, pois frequentemente o mesmo é necessário para\nrevisão e validação da hipótese de diagnóstico, bem como para comparação de segmentos do vídeo com\nexames futuros. Mesmo nos casos em que a informação é arquivada, a falta de reutilização e partilha de\ninformação e vídeos entre entidades contribui, mais uma vez, para uma repetição desnecessária de\nexames.\nA existência de um arquivo de vídeos endoscópicos seria uma mais-valia, pois além de resolver os\nproblemas referidos ainda possibilitaria a sua utilização para fins de pesquisa e investigação, além de\ndisponibilizar exames para servirem como referência para estudo de casos similares.\nNeste trabalho é proposta uma solução abrangente para a aquisição, tratamento, arquivo e difusão de\nexames de endoscopia. O objetivo passa por disponibilizar um sistema capaz de gerir toda a informação\nclínica e administrativa (incluindo conteúdo audiovisual) desde o seu processo de aquisição até ao\nprocesso de pesquisa de exames antigos, para comparação com novos casos. De forma a garantir a\ncompatibilidade lexical da informação partilhada no sistema, foi utilizado um vocabulário endoscópico\nestandardizado, o Minimal Standard Terminology (MST). Neste contexto foi planeado um dispositivo\n(MIVbox) orientado à aquisição do vídeo endoscópico, independentemente da câmara endoscópica\nutilizada. Toda a informação é armazenada de forma estruturada e normalizada, possibilitando a sua\nreutilização e difusão. Para facilitar este processo de partilha, o vídeo sofre algumas etapas de\nprocessamento, de forma a ser obtido um vídeo reduzido e as respetivas características do conteúdo.\nDeste modo, a solução proposta contempla um sistema de anotação que habilita a pesquisa por conteúdo,\nservindo assim como uma ferramenta versátil para a investigação nesta área. Este sistema é ainda dotado\nde um módulo de streaming, no qual é transmitido, em tempo real, o exame endoscópico,\ndisponibilizando um canal de comunicação com vídeo unidirecional e áudio bidirecional, permitindo que os\nprofissionais ausentes da sala do exame deem a sua opinião remotamente."
  },
  {
    "keywords": [
      "Deep Learning",
      "Generative models",
      "Molecular design",
      "Multi-objective evolutionary algorithms",
      "Novel sweeteners",
      "Algoritmos evolucionários multi-objectivo",
      "Desenho molecular",
      "Modelos generativos",
      "Novos adoçantes",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Development of deep learning-based tools for the design of new compounds with desired biological activities",
    "autor": "Sousa, Tiago Filipe Escairo",
    "data": "2021-04-21",
    "abstract": "In the last few years, de novo molecular design has increasingly been using generative models, from the\nemergent field of Deep Learning (DL), to propose novel compounds that are likely to possess desired\nproperties/activities, in areas such as drug discovery, materials sciences or biotechnology. A panoply\nof deep generative models, such as Recurrent Neural Networks, Variational Autoencoders, Adversarial\nAutoencoders and Generative Adversarial Networks, can be trained on existing datasets, and provide for\nthe generation of novel compounds, typically with similar properties of interest. Additionally, different\noptimization strategies, including transfer learning, Bayesian optimization, reinforcement learning, and\nconditional generation, can be used to direct the generation process towards desired aims, regarding their\nbiological activities, synthesis processes or chemical features. Various instances of experimental validation\nof these emerging methods have surfaced, with de novo generated molecules being synthesized and\nproving successful in in vitro, and even in vivo, assays. These successful practical realizations encourage\nfurther research into this blooming field.\nThis dissertation aims to explore the application of generative DL to the de novo molecular design, with\na focus on the targeted generation of new compounds. Two frameworks were developed to support this\nendeavor and stand as the main contributions of this work. The first, termed DeepMolGen, standardizes\nthe implementation and usage of various generative DL architectures for molecular design. The second,\ntermed EAMO, employs multi-objective evolutionary algorithms to navigate the latent space of autoencoder\nbased models, optimizing the generation of molecules with desired characteristics. These frameworks were\naccompanied with a systematic and critical review on deep generative models, the related optimization\nmethods for targeted compound design, and their applications.\nFour state-of-the-art architectures were implemented, trained and evaluated under the DeepMolGen\nframework using a standard dataset and common metrics such as validity, uniqueness, novelty and the\nMOSES benchmark. The results showed that DeepMolGen was capable of performing the intended tasks\nand that most of the implemented models performed on par with their publications. Similarly, four case\nstudies from the literature were optimized with EAMO and the results compared to previous works. These\nexperiments showed that EAMO could control abstract chemical properties and is competitive with other\nstate-of-the-art methods. Lastly, the three best performing models were combined with transfer learning\nand EAMO within a pipeline for the generation of sweeteners. The resulting set of 102 promising molecules\nwas reviewed by expert chemists and the pipeline improved with their feedback. A second set of 99\ncompounds was then generated and the preliminary observations pointed to significantly improved results."
  },
  {
    "keywords": [
      "União Europeia",
      "Assinatura eletrónica qualificada",
      "Serviço preservação de longo termo",
      "eIDAS",
      "Validade das assinaturas eletrónicas",
      "Lista de confiança",
      "Europian Union",
      "Qualified electronic signature",
      "Long-term preservation service",
      "Eletronic signature validity",
      "Trust list",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "eIDAS qualified trust services: serviço de preservação",
    "autor": "Fernandes, João Manuel da Silva Gomes",
    "data": "2021",
    "abstract": "De forma a uniformizar o mercado Europeu e conseguir mais confiança nas transações\neletrónicas (sic Considerando 2º do Regulamento eIDAS (2014)), a União Europeia publicou\no Regulamento UE nº 910/2014 (Regulamento eIDAS (2014)), também conhecido como\nRegulamento Eletronic Identification, Authentication and Trust Services (eIDAS). Este normativo\nlegal pretende regular as assinaturas e selos electrónicos, a identificação eletrónica e os\nserviços de confiança dentro do Espaço Europeu. O objetivo deste regulamento é permitir\ntransações seguras e eficazes entre negócios, pessoas e as autoridades públicas.\nPara atingir o seu objetivo, o Regulamento eIDAS introduziu o conceito de serviços\nde confiança qualificados. Os serviços de confiança qualificados permitem às assinaturas\neletrónicas o efeito legal equivalente a uma assinatura manuscrita, quando baseadas num\ncertificado qualificado de assinatura eletrónica emitido por uma entidade que está integrada\nna lista de confiança de um determinado Estado Membro. Estas assinaturas são intituladas\nde assinaturas eletrónicas qualificadas e são reconhecidas nos restantes Estados Membros.\nTribunais (ou outros órgãos encarregados de procedimentos legais) não podem descartá-las\ncomo prova apenas porque são eletrónicas, têm de avaliá-las da mesma forma que fariam\ncom o seu equivalente em papel. (sic Artigo 25º do Regulamento eIDAS (2014))\nA necessidade de preservação de longo prazo de assinaturas eletrónicas é reconhecida\nno seio da União Europeia (UE). No Regulamento eIDAS, entre os serviços de confiança\nqualificados introduzidos, encontra-se o serviço de preservação qualificado. Um serviço\nde preservação qualificado tem como objectivo preservar o estado de validade de uma\nassinatura eletrónica qualificada ao longo do tempo.\nEsta dissertação tem o seu foco no desenvolvimento de uma Prova de Conceito do serviço\nde confiança qualificado de preservação de assinaturas e selos eletrónicos qualificados, que\nse antecipa que comece a ser utilizado massivamente nos próximos anos."
  },
  {
    "keywords": [
      "Retopology",
      "Topology",
      "Remeshing",
      "Quad mesh extraction",
      "Surface registration",
      "3D modeling",
      "Computer graphics",
      "CG animation",
      "CG art",
      "Retopologia",
      "Topologia",
      "Extração de quad meshes",
      "Adaptação de superfícies",
      "Modelação 3D",
      "Computação gráfica",
      "Animação CG",
      "Arte CG",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Retopology: a comprehensive study of current automation solutions from an artist’s workflow perspective",
    "autor": "Silva, António Pedro Carvalho Machado da",
    "data": "2019-12-23",
    "abstract": "Topology (the density, organization and flow of a 3D mesh’s connectivity) constrains the suitability of a 3D model for any given purpose, be it surface showcasing through renders, use in real-time engines, posing or animation. While some of these use cases might not have very strict topology requirements, others may demand optimized polygon counts for\nperformance reasons, or even specific geometry distribution in order to take deformation directions into account.\nMany processes for creating 3D models such as sculpting try to make the user unaware of\nthe inner workings of geometry, by providing flexible levels of surface detailing through\ndynamic geometry allocation. The resulting models have a dense, unorganized topology\nthat is inefficient and unfit for most use cases, with the additional drawback of being hard\nto work with manually.\nRetopology is the process of providing a new topology to a model such as these, while\nmaintaining the shape of its surface. It’s a technical and time-consuming process that clashes\nwith the rest of the artist’s workflow, which is mainly composed of creative processes.\nWhile there’s abundant research in this area focusing on polygon distribution quality\nbased on surface shape, artists are still left with no options but to resort to manual work\nwhen it comes to deformation-optimized topology.\nThis document exposes this disconnect, along with a proposed framework that attempts\nto provide a more complete retopology solution for 3D artists. This framework combines\ntraditional mesh extraction algorithms with adapting manually-made meshes in a pipeline\nthat tries to understand the input on a higher level, in order to solve deficiencies that are\npresent in current retopology tools.\nOur results are very positive, presenting an improvement over state of the art solutions,\nwhich could possibly steer discussion and research in this area to be more in line with the\nneeds of 3D artists."
  },
  {
    "keywords": [
      "Annotation systems",
      "Automomatic tagging",
      "Data analysis",
      "Text mining",
      "Natural language processing",
      "Machine Learning",
      "Sistemas de anotação",
      "Tagging automático",
      "Análise de dados",
      "Text Mining",
      "Processamento de linguagem natural",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Anotação automática de textos para análise e identificação de conteúdo",
    "autor": "Santos, Tiago Miguel Fraga",
    "data": "2022-11-28",
    "abstract": "Automatic text annotation systems are mechanisms that aim to provide assistance to users who need to extract and annotate relevant information in a given text. Usually, this type of system is developed for very specific application domains, in order to facilitate research processes on text content. The works of this dissertation will be developed based on the Tombo da Mitra, a codex that contains the inventory of the properties of the Archbishop’s Table of Braga, in the 17th century. The quantity and diversity of the elements referred to in the book are impressive, as it contains all the names and surnames, settlements, professions, types of land and buildings, among many other elements, which are very important for the study and learning of geography, culture, economy, architecture, religion and portuguese language of the 17th century. The annotation of these elements expressively shows their location in time and space, as well as their potential relationships, facilitating the study of the book and providing linguistic researchers, teachers and students with a valuable instrument to reach and reinforce knowledge about the book. In this dissertation, we present a tool specially designed for the annotation of documents in the Livro das Propriedades, allowing the management and listing of annotation tags and providing a clearer view of the content of the manuscript."
  },
  {
    "keywords": [
      "Business intelligence",
      "Elders",
      "Ethical issues in medicine",
      "Health information and communication technology",
      "Health professionals",
      "Mobile health",
      "Nursing home",
      "Idosos",
      "Lar de Idosos",
      "Problemas éticos na saúde",
      "Profissionais de saúde",
      "Tecnologia de informação e comunicação na saúde",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "A mobile health application to assist professionals: a case study in a portuguese nursing home",
    "autor": "Esteves, Márcia Annie Araújo",
    "data": "2019",
    "abstract": "Over the past few years, the world has been witnessing a huge demographic change: the\naging population has been growing at an alarming rate. This problem has been a matter\nof concern for many countries since it has been posing several challenges to healthcare\nsystems worldwide. In Portugal, which is one of the countries with the largest aging population,\nthis demographic change has led to several issues. In fact, in Portugal, the nursing\nhomes have been getting a higher demand, and health professionals are overloaded with\nwork. Furthermore, the fact that nursing homes still use paper to record information and to\nclinically manage their residents is another tremendous problem since this method is more\nprone to errors and time-consuming.\nIn this context, the present master’s dissertation emerged and consisted in the design\nand development of a mobile application for the health professionals, i.e. the nurses and\ndoctors, working in a Portuguese nursing home, more specifically in one of the nursing\nhomes of Santa Casa da Miseric´ordia de Vila Verde. This mobile application was developed\nto help the health professionals to clinically manage the residents and to assist them at the\npoint-of-care, namely to schedule, perform, and record their daily tasks and to have access\nand manipulate information.\nAdditionally, the present dissertation also included the definition of clinical and performance\nindicators to assist the decision-making process. It is important to mention that a\nmobile solution was chosen since a hand-held device, which can be used anywhere and\nany time, is able to give access and store all the needed information at the point-of-care.\nThereby, this project was developed in order for the nursing home to shift from the\npaper-based to the computer-based management of data as well as to introduce technological\nimprovements in the facility, more specifically, Health Information and Communication\nTechnology. Thus, by taking advantage of the benefits provided by these improvements,\nthe mobile application could help the health professionals to provide better care, namely\nby reducing time-waste and errors, and, consequently, enhance elders’ quality of life. Furthermore,\nthe solution could relieve some of the workload of the health professionals and\nhelp them make more informed and evidence-based decisions and, hence, improve the\ndecision-making process."
  },
  {
    "keywords": [
      "Aprendizagem máquina",
      "Desempenho",
      "Interpretabilidade",
      "Regressão",
      "Interpretability",
      "Machine learning",
      "Performance",
      "Regression"
    ],
    "titulo": "Machine learning interpretability in a context of black box regression models",
    "autor": "Pimentel, João Pedro Torres",
    "data": "2021-08-10",
    "abstract": "As máquinas têm demonstrado várias vantagens em comparação com os humanos, nomeadamente a\nreproduzir e escalar tarefas, apresentando velocidade e precisão elevadas. Todavia, nem sempre é possível\ncompreender o funcionamento dos seus algoritmos. Assim, a necessidade de explicar os resultados destes\ntem vindo a crescer, levando ao aumento da relevância de ferramentas de explicabilidade, já que estas\npossibilitam a redução das divergências entre a interpretação do modelo e o nível de raciocínio humano.\nO principal objetivo desta dissertação passou pelo desenvolvimento de uma técnica drill-down para\navaliar modelos de regressão caixa negra, considerando interações multivariável no âmbito dos preditores.\nAssim, propomos EDRs, uma combinação entre DRs e EDPs. De modo a facilitar a sua análise, foram\nimplementadas múltiplas formas de visualização: boxplots, histogramas e gráficos de densidade, exibindo\ndistribuições completas, uma visualização em grafo para explorar interações entre preditores e tabelas\nde desempenho, comparando os quartis de cada distribuição com uma referência. Com base em pontos\nde corte e uma distribuição de referência, foi ainda efetuada uma extrapolação de contra-factos para\nregressão.\nAplicaram-se quatro algoritmos distintos a uma gama heterogénia de conjuntos de dados com o intuito\nde eliminar qualquer potencial enviesamento de modelo. Estas experiências mostraram que as EDRs\napresentam vantagens em comparação com os EDPs. O número de gráficos a analisar foi reduzido, já\nque apenas os subgrupos interessantes são apresentados. Além disso, podem ser detetadas interações\ncompostas por mais de três condições. Foi, também, considerado um caso de estudo, retratando um\nproblema de seleção de modelo. As EDRs mostraram-se cruciais para compreender como os modelos\nse comportam em relação a combinações específicas de dados e provar que o melhor modelo geral nem\nsempre é o melhor para certos subgrupos. Deste modo, as EDRs podem ser usadas para escolher um\nmodelo ou para gerar ensembles, usando os modelos com melhor desempenho para cada subgrupo.\nApesar das vantagens comparativamente às ferramentas existentes, o uso das regras não esgota o\ndomínio das variáveis, pois não se exibem todas as combinações possíveis, com até três condições. No\nfuturo, pode ser proveitoso estudar uma discretização dos preditores numéricos guiada pelas regras, já\nque esta etapa depende de técnicas externas. Meta-modelos também devem ser definidos para produzir\nensembles baseados no desempenho de cada subgrupo."
  },
  {
    "keywords": [
      "E-commerce",
      "Sistemas de apoio à decisão",
      "Arquitetura de software",
      "Motores de regras",
      "Padrões de desenho",
      "Decision support systems",
      "Software architecture",
      "Rule engines",
      "Design patterns",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistema de apoio à decisão baseado em regras para pricing",
    "autor": "Parente, Nelson Arieira",
    "data": "2020-01-09",
    "abstract": "Desde os primórdios dos tempos que o ser humano procura optimizar e automatizar todos\nos processos que se apresentam como morosos e repetitivos. O processo de pricing de\nprodutos nos retalhistas é um sistema complexo e que consome tempo ao processo interno\ndos mesmos. Este projeto de investigação tem como objectivo o desenvolvimento de uma\nplataforma que seja capaz de optimizar e automatizar o processo de pricing de produtos,\ntendo como base um modelo baseado em múltiplas regras.\nCom o aumento do volume de vendas online, surgem, no âmbito do processo de pricing,\ndiversos problemas relativos aos processos associados à expansão de uma plataforma de\nE-commerce. Aparece assim, a necessidade de um sistema capaz de responder a estes problemas e auxiliar na eficácia, rapidez e tomada de decisão de quem lida diariamente com\nesta questão complexa. Assim, propõe-se o desenvolvimento de um sistema que deverá\nser capaz de auxiliar a tomada de decisão na definição do preço de venda dos produtos\ncomercializados em qualquer plataforma de E-commerce.\nA solução que é desenvolvida ao longo deste projeto de investigação, terá que ser passível de gerir, em tempo real, o processo de pricing de um retalhista, bem como auxiliar\nna decisão da definição de preços. O foco deste projeto de investigação será dado a sistemas de apoio à decisão orientados a modelos, uma vez que é de extrema importância a\nversatilidade e a adaptação do sistema a múltiplos contextos e variáveis.\nComo tal, e de forma a responder às questões de investigação que orientam este projeto\nde investigação, estrutura-se o conteúdo em quatro capítulos fundamentais: o Estado da\nArte, a Metodologia de investigação e Ferramentas de desenvolvimento, o Desenvolvimento\ndo trabalho e as Conclusões.\nDurante o capítulo dedicado ao Estado da Arte abordam-se definições e conceitos essenciais ao capítulo de Desenvolvimento deste projeto, tal como o conceito de Sistemas de Apoio\nà Decisão, a definição do conceito de Motores de Regras e de Algoritmos de Inferência.\nPara estruturar a forma como se irá conduzir este projeto de investigação, no capítulo de\nMetodologia e Ferramentas de Desenvolvimento, apresenta-se a metodologia de investigação e as ferramentas de desenvolvimento aplicadas neste estudo, tal como o ambiente no\nqual a solução final foi desenvolvida.\nO capítulo de Desenvolvimento define-se pela exposição da investigação e lógica aplicada\nno desenvolvimento de um Sistema de apoio à Decisão para o Processo de pricing. Por\núltimo, o capítulo em que se expõem as conclusões deste projeto de investigação, tem como\nobjectivo analisar os princípios teóricos que servem de base a provas de conceito, seguindo-se pela exposição da análise SWOT. O desenvolvimento desta análise é enquadrado na\nmetodologia de investigação definida inicialmente, Design Research, avaliando a solução\ndesenvolvida de modo a perceber se os requisitos iniciais foram cumpridos.\nAssim, e de forma a concluir esta investigação, e relacionar todos os conceitos abordados\ne tecnologias utilizadas, as questões de investigação são respondidas de forma a expor a\nviabilidade da solução apresentada."
  },
  {
    "keywords": [
      "Bloom filter",
      "Data streams",
      "Data structures",
      "Dictionaries",
      "Window models",
      "Fluxo de dados",
      "Estruturas de dados",
      "Dicionários",
      "Modelos de janelas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Bloom filters for stream windows",
    "autor": "Rodrigues, Ana Catarina Gomes",
    "data": "2021-04-06",
    "abstract": "A Bloom Filter is a probabilistic data structure designed to check, rapidly and memory-efficiently, whether an element is present in a set. It has been vastly used in various computing areas and several variants, allowing deletions, dynamic sets and working with sliding windows, have surfaced over the years. In many systems, it becomes relevant to identify the more recent information in the data stream. However, the majority of the sliding window schemes consider the most recent elements of a data stream without taking into account time as a factor. While this allows, e.g., saving the most recent 10000 elements, it does not easily translate into storing data received in the last 60 seconds, unless the insertion rate is stable and known in advance. In this thesis, a new technique is explored, Unproved in terms of time complexity and memory usage compared to the already existing ones, that can save information of a given time period and correctly identify it as present when queried, while also being able to retire data when it becomes stale. This new solution can be employed in a wide range of real world applications, such as in advertising, networking, fraud detection and distributed denial of service attacks prevention."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Mobile AR for large 3D scenes using markers",
    "autor": "Carvalho, Pedro Nicolau Machado",
    "data": "2017",
    "abstract": "Augmented Reality (AR) relies on overlaying 3D objects over a real environment on a screen\ndisplay. Recently this technology has begun to gain a lot of attention by the common\nconsumer because it is starting to be used in devices available to them, one of those devices\nbeing mobile smartphones. Hence, it is relevant to seek the limits and explore the potential\nthat these devices are capable of, in order to build a balanced AR experience.\nThe purpose of this work was to study the limits of developing a localization technique\nfor an Android smartphone which was accurate and fast. This technique was to be used\nin an AR application which allowed the user to view large 3D environments related to the\nreal scene (e.g a room could be turned into an aquarium).\nThe research was initially targeted at finding the right technique to localize the device,\nby assessing the accuracy of algorithms like Inertial Navigation Systems, Monocular Visual\nOdometry and SLAM on a hardware limited device - a smartphone. The results obtained\nwith these algorithms did not meet the precision requirements to avoid misalignments\nbetween virtual and real entities.\nThe solution found was to use 2D markers around the real scene with the help of a\ncomputer tool which comprised a novel feature which aided the user in positioning the\nmarkers with visual cues painted on the 3D model’s surface.\nMarkers detected by the smartphone’s camera allowed the computation of the position\nof the device. These markers are detected using Vuforia’s SDK Image Target Recognition.\nFurthermore, some improvements were implemented to provide more robust tracking for\nlarge 3D Models.\nA framework was devised which consists of the android AR application and the computer\ntool which, aside from helping the user position the markers, can build AR setups\nand deploy them to the mobile app.\nFinally, some tests were made which assess the accuracy of the implementation. Example\nAR scenes were built to examine how the application fares in very different environments,\none being a room and the other a public square and streets in a village."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Study, selection and evaluation of an IoT platform for data collection and analysis for medical sensors",
    "autor": "Rei, João Pedro Nóbrega",
    "data": "2018",
    "abstract": "Every day, huge amounts of data are generated in the healthcare environments from several\nsources, such as medical sensors, EMRs, pharmacy and medical imaging. All of this\ndata provides a great opportunity for big data applications to discover and understand patterns\nor associations between data, in order to support medical decision-making processes.\nBig data technologies carry several benefits for the healthcare sector, including preventive\ncare, better diagnosis, personalized treatment to each patient and even reduce medical costs.\nHowever, the storage and management of big data presents a challenge that traditional data\nbase management systems can not fulfill. On the contrary, NoSQL databases are distributed\nand horizontally scalable data stores, representing a suitable solution for handling big data.\nMost of medical data is generated from sensor embedded devices. The concept of IoT,\nin the healthcare environment, enables the connection and communication of those devices\nand other available resources over the Internet, to perform or help in healthcare activities\nsuch as diagnosing, monitoring or even surgeries. IoT technologies applied to the healthcare\nsector aim to improve the access and quality of care for every patient, as well as to\nreduce medical costs.\nThis master thesis presents the integration of both big data and IoT concepts, by developing\nan IoT platform designed for data collection and analysis for medical sensors. For\nthat purpose, an open source platform, Kaa, was deployed with both HBase and Cassandra\nas NoSQL database solutions. Furthermore, a big data processing engine, Spark, was also\nimplemented on the system.\nFrom the results obtained by executing several performance experiments, it is possible\nto conclude that the developed platform is suitable for implementation on an healthcare\nenvironment, where huge amounts of data are rapidly generated. The results also made it\npossible to perform a comparison between the performance of the platform with Cassandra\nand HBase, showing that the last one presents slightly better results in terms of the average\nresponse time."
  },
  {
    "keywords": [
      "Data cleaning",
      "Data science",
      "Ciência de dados",
      "Limpeza de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Humanized data cleaning",
    "autor": "Dias, José Miguel Silva",
    "data": "2021-03-05",
    "abstract": "Data science has started to become one of the most important skills someone can have\nin the modern world, due to data taking an increasingly meaningful role in our lives.\nThe accessibility of data science is however limited, requiring complicated software or\nprogramming knowledge. Both can be challenging and hard to master, even for the simpler\ntasks.\nCurrently, in order to clean data you need a data scientist. The process of data cleaning,\nconsisting of removing or correcting entries of a data set, usually requires programming\nknowledge as it is mostly performed using programming languages such as Python and\nR (kag). However, data cleaning could be performed by people that may possess better\nknowledge of the data domain, but lack the programming background, if this barrier is\nremoved.\nWe have studied current solutions that are available on the market, the type of interface\neach one uses to interact with the end users, such as a control flow interface, a tabular\nbased interface or block-based languages. With this in mind, we have approached this issue\nby providing a new data science tool, termed Data Cleaning for All (DCA), that attempts\nto reduce the necessary knowledge to perform data science tasks, in particular for data\ncleaning and curation. By combining Human-Computer Interaction (HCI) concepts, this tool\nis: simple to use through direct manipulation and showing transformation previews; allows\nusers to save time by eliminate repetitive tasks and automatically calculating many of the\ncommon analyses data scientists must perform; and suggests data transformations based on\nthe contents of the data, allowing for a smarter environment."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Análise e conceção de um sistema para rastreabilidade em contexto industrial",
    "autor": "Gomes, Nuno André Barbosa",
    "data": "2016",
    "abstract": "Apesar da evolução observada nas últimas décadas na utilização de Sistemas de Informação\n(SI) na gestão de matérias primas (MPs) e informação ao longo dos vários elos de uma\ncadeia de abastecimento, a rastreabilidade de MPs em tempo real é ainda muito limitada\ndevido à falta de integração dos SI com tecnologias de identificação automática.\nA fábrica Bosch Car Multimedia de Braga (Bosch BrgP) não é indiferente a este problema\ne recorre com grande frequência a fornecedores que utilizam rotas dispendiosas em termos\nde tempo, custo e esforço de operacionalização, o que levanta diversos desafios relativamente\naos processos logísticos (planeamento, receção, gestão e abastecimento de MPs), ou\nseja, de todo o fluxo de MP desde os fornecedores até às linhas de produção.\nAtualmente, a Bosch BrgP enfrenta problemas que decorrem da dificuldade em localizar,\nem tempo real e com elevado grau de precisão, todas as MPs (envolvidas) nos diferentes fluxos\nda logística interna (na cadeia de abastecimento interna), o que dificulta o planeamento\ndas necessidades de cliente. Deste modo, a Bosch BrgP tem a necessidade permanente de\nter acesso a informação relativa à localização da MP.\nÉ então pretendido otimizar a logística interna da Bosch BrgP, tornando-a mais competitiva\nno mercado e diminuir os desvios de stock originados pela dificuldade em localizar\nas MPs. De forma a resolver os problemas de visibilidade e rastreabilidade de MPs desde\na sua receção até ao seu consumo, esta dissertação analisa e concebe uma solução para\nrastrear o fluxo das MPs em todo o processo interno responsável pela gestão da cadeia de\nabastecimento interna, que recorre a tecnologia Radio-Frequency IDentification (RFID).\nOs objetivos são alcançados através do desenvolvimento de um sistema de identificação\ne localização de MPs, que garante uma maior visibilidade dos fluxos de MPs, contribuindo\npara a redução do tempo de abastecimento às linhas de produção. Para além disso, o\nsistema deve garantir que toda a movimentação/localização da MP em curso na cadeia de\nabastecimento interna é rastreável e efetuada de modo automático, em tempo real e com\nelevado grau de precisão, contribuindo para a disponibilização dessa informação sempre\nque solicitada.\nEsta dissertação contribui para esse esforço de desenvolvimento com atividades de análise\ne conceção que se concretizam através do levantamento de requisitos (funcionais e não\nfuncionais), da conceção da arquitetura de hardware e software, da definição dos pontos de\ninteroperabilidade com os sistemas legados e da caracterização do novo fluxo de rastreabilidade.\nEstas atividades são documentadas através de diversos esquemas e diagramas (de\nuse-cases e de atividades) que seguem a notação Unified Modeling Language (UML)."
  },
  {
    "keywords": [
      "Green software",
      "Green computing",
      "Android",
      "Energy efficiency",
      "Web browsers",
      "Eficiência energética",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Browser energy efficiency in android",
    "autor": "Gonçalves, Nelson Adriano Sequeira",
    "data": "2022",
    "abstract": "Nowadays, there is a massive growth in energy consumption in the IT sector, which is leaving a huge\nfootprint in terms of energy consumption despite its benefits. With this, the topic of energy consumption\nand how to improve it has become one of the most talked-about topics today.\nSeveral developments have been made to find the most efficient solutions to the various problems that\nusers and developers encounter. But this is far from being an easy task for both, as there is still very little\ninformation available, or sometimes the solutions don’t meet the needs of each one.\nWith this in mind, this dissertation aims to verify which Browser is more efficient in the Android\nenvironment since there is not much information in this area. For this, we selected seven browsers and ran\nfour test scenarios in order to force the browsers. To test, we recorded a script for each Browser in each\nscenario, trying to mimic the use of a regular user. The RERAN tool was used to record and repeat each\nscript five times, and the Trepn tool was used to monitor it. The results obtained allowed us to conclude\nwhich Browser was more efficient among the seven selected."
  },
  {
    "keywords": [
      "61:621.398",
      "621.398:61",
      "614"
    ],
    "titulo": "Sistema inteligente para a simulação do posicionamento e movimento de pessoas ou equipamentos dentro de um hospital",
    "autor": "Salgado, Cátia Matos",
    "data": "2012",
    "abstract": "Os orgãos de gestão dos hospitais optam cada vez mais pela implementação\nde sistemas de monitorização baseados na tecnologia RFID, tendo em vista\nsobretudo a redução de custos associados  a perda e roubo de equipamentos,\no aumento da segurança de pacientes e pro ssionais e a deteção do acesso de\npessoal a zonas não autorizadas. Testes e estudos de otimização, nomeadamente\nem relação  a configuração dos sensores na arquitetura da rede RFID,\ndevem ser realizados para aumentar o desempenho destes sistemas. A simulação surge neste contexto como uma importante ferramenta de apoio,\numa vez que permite que os estudos sejam endereçados a partir de um ambiente\ncomputacional, evitando as desvantagens inerentes à realização de testes\nno próprio hospital. Recentemente, o estudo da questão de conservação de\nenergia, em redes de sensores de tracking de objetos, tem atraído muita atenção. A previsão de trajetórias pode ser utilizada para determinar o\nconjunto de sensores que num determinado momento devem ser desativados\npara reduzir consumos energéticos e, consequentemente, aumentar o tempo\nde vida do sistema.\nO objetivo deste trabalho é integrar as duas temáticas referidas - simulação e previsão - no desenvolvimento de um sistema inteligente capaz\nde simular e prever a trajetória de uma entidade numa área preenchida com\nsensores. A área escolhida para a simulação consiste no piso pediátrico de\num hospital no Norte de Portugal, onde um sistema RFID de monitorização\nse encontra atualmente implementado para a monitorização de pacientes.\nA análise do sistema desenvolvido é realizada através do estudo de cenários,\nonde estatísticas obtidas em diferentes condi cões de simulação são analisadas\nsob diversos prismas. Os resultados provam que o sistema proposto é capaz\nde simular com sucesso o movimento de uma ou mais entidades num ambiente\nhospitalar real, e de estimar localizações com uma precisão média de\n62% para um modelo de simulação que tem em consideração a existência de\nvariabilidade e aleatoriedade no movimento. A previsão é realizada através\nda aplicação de um algoritmo de Data Mining, denominado SK-Means, ao\nhistórico de percursos de cada entidade, e permite a obtenção de padrões de movimento específicos e distintos para cada uma delas. As localizações previstas são apresentadas ao utilizador em tempo real, de forma dinâmica.\nPretende-se que no futuro o sistema possa ser implementado no hospital\npara permitir a visualização em tempo real dos pacientes."
  },
  {
    "keywords": [
      "Natural language processing",
      "Post generator",
      "Social networks",
      "Twitter",
      "Fake-news",
      "Fact-checking",
      "Emotions",
      "Personality",
      "Engagement",
      "Processamento de linguagem natural",
      "Gerador de posts",
      "Redes sociais",
      "Emoções",
      "Personalidade",
      "Envolvimento"
    ],
    "titulo": "Post generator for social media based on emotions and personality",
    "autor": "Barbosa, Maria Laura de Araújo",
    "data": "2022-12-19",
    "abstract": "Social networks are currently one of the main sources of fake-news dissemination. On the other hand, fact-checking agencies, which emerged with the aim of solving the problem of fake-news, find it difficult to spread their content on social media. The engagement of fake-news is considerably greater than that of fact-checking. \nThis dissertation contributes to defining a set of heuristics applicable to fact-checking posts in mi-croblog environments, in order to increase their reach and engagement with the reader. The systematic review, inspired by the guidelines defined by Kitchenham, made it possible to identify the main strategies used in the dissemination of fake-news and fact-checkings on social media. The results showed that the dissemination and viralization of posts depends on two aspects: the dissemination strategy and the engagement strategy created with the reader. The first is outside the scope of study in this document. Regarding the second, the systematic review showed that the inclusion of emotions and personality in social media posts is an efficient strategy to improve reader engagement. Furthermore, engagement seems to depend on the type of language and elements that make up the post. Since we are working in the context of fake-news in which ethical limits are frequently tested and extrapolated, it is important to reinforce that the defined approach is ethically valid. \nIn this sense, the course of the dissertation continued with the design and development of a Post Generator Algorithm based on emotions and personality capable of increasing the engagement of fact-checking posts. The algorithm was tested in an experiment carried out with twenty participants, where patterns were searched between the posts obtained by the algorithm and those created by the fact-checking journal Snopes and the interactions achieved in each of them. This experiment, carried out in a lab environment, proved the hypothesis that emotions and personality in Microblog posts increase user engagement with fact-checking posts."
  },
  {
    "keywords": [
      "Computer programming exercises generation",
      "Natural language processing",
      "AI-based language models",
      "Geração de exercícios de programação",
      "Processamento de linguagem natural",
      "Modelos de linguagem baseados em IA",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automatic generation of programming exercises",
    "autor": "Freitas, Tiago Carvalho",
    "data": "2023-12-28",
    "abstract": "This document is a thesis report for the final project of the Master Degree in Informatics\nEngineering, accomplished at Universidade do Minho in Braga, Portugal. The project\nconsisted in the development of a tool capable of semi-automatically generating computer\nprogramming exercises named GOLIATH. It applies Natural Language Processing (NLP)\nmethods and techniques, commonly used to construct short texts in predefined formats, to\nbuild typical Computer Programming exercises. In doing so, GOLIATH intends to support\nteachers and students in their educational endeavours by providing dynamically generated\nexercises. It aims to ease the burden of creating study material and also provide constant and\nimmediate access to new exercises. GOLIATH was implemented to include two AI-based\nmodels: one to generate text (Keys-To-Text) and another to generate source code (CodeT5).\nA template-based exercise generation mechanism was added to its functionalities, taking\nadvantage of a DSL designed specifically for this project. This DSL was fully designed\n(and its processor implemented) to allow the use of expressive templates that enable the\ngeneration of more than one version of an exercise. The entire application was tested and\nfeedback was overall positive, pointing to the better aspects of the application and providing\nuseful feedback for future iterations. With its objectives achieved, GOLIATH may become a\nvaluable contribution to the education support tools landscape."
  },
  {
    "keywords": [
      "Natural language processing",
      "Social behavior",
      "Text analysis",
      "Sentiment analysis",
      "POS Tagging",
      "Processamento de linguagem natural",
      "Comportamento social",
      "Análise de texto",
      "Analise de sentimentos"
    ],
    "titulo": "Online-SoBA: text analysis to study social behaviors",
    "autor": "Mendes, Paulo Jorge",
    "data": "2022-09-29",
    "abstract": "This document reports a Master’s Project that fits in the 5th year of the Integrated Master’s in\nInformatics Engineering from the Universidade do Minho. This Master work, Online-SoBA\n(Online Social Behavior Analysis), aims at developing a semi-automatic system capable of\nanalyzing short texts that correspond to comments written in reaction to ’posts’ in Portuguese\nsocial media (social networks or online newspapers), in order to identify behaviors that\ncharacterize social opinions about a given topic along a given period of time. To meet the\nchallenge posed, these short texts are analyzed and the information is extracted according to\nschemes that describe the phenomena to be studied, that is, sentence structures in natural\nlanguage that characterize the referred behaviors. In order to achieve this goal, a pipeline\nwas implemented to process the texts contained in the NetLang Corpus, and later a web\nplatform was developed to present the generated results to the end user, as well as to allow\na fluid navigation over them. The report introduces the system architecture designed to\nattain the objectives, discusses the implementation decisions and the development steps, and\nshows the interface created for a fruitful knowledge extraction. Before closing the document\nwith the conclusions, an experiment conducted is described and the results are analyzed to\nassess the SoBA from the end user perspective."
  },
  {
    "keywords": [
      "Ciências Médicas::Outras Ciências Médicas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Plataforma computacional para apoio à decisão em neonatologia e pediatria",
    "autor": "Coimbra, Ana Cecília Sousa da Rocha",
    "data": "2016-11",
    "abstract": "O erro humano está frequentemente associado ao calculo manual de dosagem de\nmedicamentos, sendo a sua incidência maior na população pediátrica do que na\npopulação adulta. Desta forma, esta dissertação tem como objetivo a finalização e\nimplementação de uma plataforma de apoio à decisão médica de modo a reduzir\nesse erro.\nA codificação de episódios de internamento é de extrema importância para a\ngestão financeira hospitalar, uma vez que esta é utilizada para o calculo de financiamento\nhospitalar através da utilização de GDH. Neste sentido, foi desenvolvida\numa plataforma de apoio à gestão financeira que permite auxiliar na codificação\nde episódios de internamento de qualquer unidade da instituição de saúde, mesmo\ndas unidades de Neonatologia e Pediatria.\nO desenvolvimento de ambas as plataformas foi acompanhado por um médico\nresponsável, tendo sido testadas e realizadas as devidas alterações. Encontrandose\na primeira na fase final de testes e a segunda em funcionamento em ambiente\nhospitalar. Por fim foram lançados dois questionários direcionados à avaliação de\ncada plataforma."
  },
  {
    "keywords": [
      "Tuberculosis",
      "Whole genome sequencing",
      "Microevolution",
      "Phylogeny",
      "Multidrug resistance",
      "Tuberculose",
      "Microevolução",
      "Filogenia",
      "Multirresistência a antibióticos",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Inferring epidemiology and microevolution of Mycobacterium tuberculosis strains from deep-sequencing data of patient samples",
    "autor": "Barbosa, Bárbara Andreia Andrade",
    "data": "2017",
    "abstract": "Tuberculosis, caused by the intracellular pathogen Mycobacterium tuberculosis is an infectious disease that remains a global public health problem where approximately one-third of the world population have been at least in contact and is latently infected with.\nWhole genome sequencing has revolutionized the investigation of mycobacterial genomes. The application of this technology has provided innovative understandings into the evolution of the Mycobacterium tuberculosis due to recent studies reporting conflicting findings on its genomic stability, particularly during the evolution of drug resistance in modern lineages.\nTo address this question we focused on understanding the genotypic and epidemiological factors that influence the spread and fitness of this bacterium by analyzing deep –sequencing data of 85 patient samples from Central Asia. Samples were part of a larger study of 399 clinical isolates of newly diagnosed patients with pulmonary TB collected between 2012 and 2013 at the NCTLD in Tbilisi, Georgia.\nAll the samples were mapped against H37Rv strain. We focused on single-nucleotide polymorphisms to reconstruct models for molecular evolution, using Maximum Likelihood and Bayesian Inference methods. 84% of our population belongs to the Beijing lineage, associated with the massive spread of multidrug-resistant strains. Relationship between mutations on rpoB and rpoC were associated with drug resistance to rifampicin and mutations on pncA region also demonstrated to be related with drug resistance to pyrazinamide.\nFurthermore we found that the amount of variation accumulated within a patient can be as high as that observed between patients along, what we assume to be, a chain of transmission. Intrapatient diversity was found in all of the follow up patients.\nOur study adds new data to the understandings of the variability among Mycobacterium tuberculosis strains in an intra and interpatient microevolution scenario."
  },
  {
    "keywords": [
      "Machine learning",
      "Sleep staging",
      "Sleep/wake detection",
      "Sleep classification",
      "Sleep monitoring",
      "Sleep onset latency",
      "Cardiorespiratory features",
      "Actigraphy",
      "Aprendizagem-maquina",
      "Classificação de sono",
      "Detecção sleep/wake",
      "Latência de sono",
      "Features cardiorrespiratórias",
      "Monitorização do sono",
      "Actigrafia",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "On the improvement of sleep onset latency detection and sleep-wake classification using cardiorespiratory features",
    "autor": "Azevedo, Cármina Augusta Pereira",
    "data": "2016-08-12",
    "abstract": "This document describes an investigation performed at Philips Research (Eindhoven, The\nNetherlands) which aimed at improving the performance of Philips current sleep/wake classification\nmethods using portable devices based on unobtrusive cardiorespiratory signal\nmodalities. Particularly, this research focused on improving the detection of the Sleep Onset\nLatency (SOL) parameter.\nUsing a data set with recordings of healthy subjects, several alternative classification models\nwere built, evaluated and compared to the current classifier.\nIt was found that the performance of the current classifier, regarding SOL detection, decreases\nwith increasing SOL, leading to an underestimation of this parameter, and possibly\nundervaluation of symptoms of sleep disruptions or even sleep disorders, during medical\ndiagnosis.\nThe main issue associated to this fault is that the current classifier is trained with examples\nfrom the entire night and therefore, for subjects with extended SOL periods, fails to capture\nthe characteristics of wake before the initiation of sleep.\nIn this report a new method of distinguishing sleep from wake, to be applied with recordings\nof subjects with SOL over 30 minutes, is proposed. The new method comprises two steps:\none specially dedicated to identify wake before the initiation of sleep (and more accurately\ndetect the moment of Sleep Onset (SO)), and the other one to distinguish wake after SOL.\nHence, it requires the use of two classifiers which differ regarding techniques for feature\nselection and are trained with examples of different periods of the night recordings."
  },
  {
    "keywords": [
      "Armazenamento definido por software",
      "Cache",
      "Armazenamento",
      "Adaptabilidade",
      "Multi-inquilino",
      "Qualidade de serviço",
      "Maximização da taxa de sucesso",
      "Controlo de desempenho",
      "Software-defined storage",
      "Storage",
      "Adaptability",
      "Multi-tenancy",
      "Quality of service",
      "Hit ratio maximization",
      "Performance control",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Towards programmable and adaptable caches",
    "autor": "Peixoto, José Pedro Ribeiro",
    "data": "2024-05-20",
    "abstract": "Serviços web como Facebook e Reddit, entre outros, lidam com exigentes cargas de trabalho, processando milhões de pedidos por segundo. Como tal, estes dependem do armazenamento em memória (cache) em quase todas as camadas da infraestrutura para fornecer alta responsividade, reduzir a carga na rede e no armazenamento, e diminuir os custos operacionais. No entanto, à medida que a complexidade das implementações de cache, a heterogeneidade das cargas de trabalho, e o número de inquilinos aumentam, também aumenta a dificuldade em gerir eficientemente estes sistemas de cache.\nNomeadamente, os atuais sistemas de armazenamento em cache como a CacheLib, a poderosa biblioteca de cache da Meta, gerem os recursos de forma rígida e monolítica, o que é adverso aos seus inquilinos e cargas de trabalho altamente dinâmicas. Por um lado, as políticas manuais, rígidas, e propensas a erros, mesmo que ótimas pontualmente, acabam por condicionar a eficácia das caches à medida que a carga de trabalho varia. Por outro lado, as implementações típicas unificam controlo e dados, e à medida que a sua base de código cresce, é progressivamente mais difícil de as manter e desenvolver. Por\núltimo, a investigação tipicamente propõe caches com novas políticas de gestão que continuam a sofrer dos mesmos problemas devido ao seu desenho tradicional de cache e dos seus pressupostos. Assim, esta dissertação propõe o Holpaca, um novo desenho de cache programável e adaptável, para permitir uma configuração polivalente dos recursos, como partições e despromoção, entre outros, e ajustar o sistema em função do comportamento dos inquilinos e das cargas de trabalho. Contrariamente aos trabalho anteriores, este desenho segue uma abordagem desacoplada que separa a lógica de controlo dos mecanismos de cache, para promover o desenvolvimento independente e mais fácil das políticas de gestão sem comprometer a responsividade percebida pelo utilizador, nem reiniciar o sistema todo. Para demonstrar a aplicabilidade da nossa solução, prototipámos o Holpaca como uma extensão da CacheLib, combinando o seu desenho programável com o nosso desenho de armazenamento em\ncache adaptável com control desacoplado e garantias multi-inquilino. A nossa avaliação mostra ganhos promissores ao gerir dinamicamente os recursos de memória, nomeadamente para maximizar a taxa de sucesso global e respeitar múltiplas garantias de qualidade de serviço em ambientes multi-inquilino heterogéneos, tudo isto sem sobrecarga percetível."
  },
  {
    "keywords": [
      "Conhecimento de código",
      "Dados de controlo de versão",
      "Fragmentação de conhecimento",
      "Mapeamento de código em grafos",
      "Perda de conhecimento.",
      "Code knowledge",
      "Code mapping graphs",
      "Knowledge fragmentation",
      "Knowledge loss",
      "Version control data",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Knowledge analysis in code mapping graphs",
    "autor": "Pereira, Isabel Sofia da Costa",
    "data": "2021-10-27",
    "abstract": "Os mapas de código, que podem ser representados como grafos, são usados para descrever \ncompletamente um sistema de software. Quando inspecionamos um mapa de código, somos capazes \nde observar o software de uma nova perspetiva e, portanto, entendê-lo melhor. Por exemplo, podemos \nanalisar o seu comportamento bem como as dependências que possam existir entre os vários elementos \ndo sistema. Nesta dissertação, estudámos um mapa de código, em forma de grafo, contendo dados de \ncontrolo de versão, provenientes de projetos que estavam guardados em repositórios Git. O grafo referido \ncontém vários tipos de informação sobre o sistema, incluindo métricas de código, como complexidade, \ne dados sobre os desenvolvimentos realizados. Uma vez que a estrutura organizacional definida pelos \nautores que desenvolvem o sistema pode originar problemas de qualidade no código, o nosso estudo \nconcentrou-se nos problemas relacionados com os autores dos programas desenvolvidos, usando \nprincipalmente alguns dos seus dados de desenvolvimento. Após explorados os problemas relacionados \ncom os autores, agrupámos os seus dados consoante as suas equipas e analisámos os problemas \ndetetados, nomeadamente fragmentação e perda de conhecimento, tendo como perspetiva de análise a \nprópria equipa de desenvolvimento. Nesse sentido, desenvolvemos um programa que é capaz de detectar \nos referidos problemas e de os revelar ao utilizador de forma que a sua identificação seja feita quase que \ninstantaneamente, o que, como se sabe, facilita muito a gestão de um projeto de software."
  },
  {
    "keywords": [
      "Sistemas conexionistas",
      "Redes neuronais artificiais",
      "Deep learning",
      "Ciência de dados",
      "Visão computacional",
      "Deteção de anomalias",
      "Autoencoders",
      "Variational autoencoders",
      "Monitorização automática de pavimentos",
      "Connectionist systems",
      "Artificial neural networks",
      "Data science",
      "Computer vision",
      "Anomaly detection",
      "Automatic pavement monitoring",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Connectionist systems for image processing and anomaly detection",
    "autor": "Gomes, Luís Filipe Fernandes",
    "data": "2021-08-10",
    "abstract": "A Inteligência Artificial (IA) e a Ciência de Dados estão cada vez mais presentes no nosso quotidiano e os\nbenefícios que trouxeram para a sociedade nos últimos anos são notáveis. O sucesso da IA foi impulsionado\npela capacidade adaptativa que as máquinas adquiriram e está estreitamente relacionada com a sua habilidade para aprender. Os sistemas conexionistas, apresentados na forma de Redes Neurais Artificiais (RNAs), que se inspiram no sistema nervoso humano, são um dos mais importantes modelos que permitem a aprendizagem. Estes são utilizados em diversas áreas, como em problemas de previsão ou classificação, apresentando resultados cada vez mais satisfatórios. Uma das áreas em que esta tecnologia se tem destacado é a Visão Computacional (Computer Vision (CV)), permitindo, por exemplo, a localização de objetos em imagens e a sua correta identificação. A Deteção de Anomalias (Anomaly Detection (AD)) é outro campo onde as RNAs vêm surgindo como uma das tecnologias para a resolução de problemas. Em cada área são utilizadas diferentes\narquiteturas de acordo com o tipo de dados e o problema a resolver. Combinando o processamento de imagens\ne a deteção de anomalias, verifica-se uma convergência de metodologias que utilizam módulos convolucionais\nem arquiteturas dedicadas a AD. O objetivo principal desta dissertação é estudar as técnicas existentes nestes\ndomínios, desenvolvendo diferentes arquiteturas e modelos, aplicando-as a casos práticos de forma a comparar\nos resultados obtidos em cada abordagem. O caso prático principal consiste na monitorização de pavimentos\nrodoviários por meio de imagens para a identificação automática de áreas degradadas. Para isso, dois protótipos de software são propostos para recolher e visualizar os dados adquiridos. O estudo de arquiteturas de\nRNAs para o diagnóstico da condição do asfalto por meio de imagens é o foco central no processo científico\napresentado. Os métodos de Machine Learning (ML) utilizados incluem classificadores binários, Autoencoders\n(AEs) e Variational Autoencoders (VAEs). Para os dois últimos modelos, práticas supervisionadas e não supervisionadas são também comparadas, comprovando a sua utilidade em cenários onde não há dados rotulados\ndisponíveis. Usando o modelo VAE num ambiente supervisionado, este apresenta uma excelente distinção entre\náreas de pavimentação em boas condições e degradadas. Quando não existem dados rotulados disponíveis, a\nmelhor opção é utilizar o modelo AE, utilizando a distribuição de semelhanças das reconstruções para calcular o\nthreshold de separação, atingindo accuracy e precision superiores a 94%). O processo completo de desenvolvimento mostra que é possível construir uma solução alternativa para diminuir os custos de operação em relação\naos sistemas comerciais existentes e melhorar a usabilidade quando comparada às soluções tradicionais. Adicionalmente, dois estudos demonstram a versatilidade dos sistemas conexionistas na resolução de problemas,\nnomeadamente no projeto de estruturas mecânicas, possibilitando a modelação de campos de deslocamento e\npressão em placas reforçadas; e na utilização de AD para identificar locais de aglomeração de pessoas através\nde técnicas de crowdsensing."
  },
  {
    "keywords": [
      "Retalho",
      "Preço",
      "Motores de regras",
      "Solvers",
      "Otimização de preço",
      "Fore-cast",
      "Retail",
      "Price",
      "Rule engines",
      "Price optimisation",
      "Forecast",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "A gestão e otimização de preço no retalho especializado: motores de regras versus solvers",
    "autor": "Marciel, Cecília Catarina Domingues",
    "data": "2018-12-14",
    "abstract": "The continuous social and economic development has led, over time, to an increase in\nconsumption, as well as a greater demand from the consumer for what he buys. In this\nsense retailers have the need to respond to these challenges and explore new opportunities.\nNaturally, the selling price of a product assumes a fundamental role in the purchase\ndecision, and in that way the retailers must carefully analyze and define the best price for\neach product, based on several factors, such as: perceived value of the product, positioning\nof the product, the company strategy, competition.\nFaced with all these challenges, the use of Information Systems is essential for retailers so\nthat it can support them in the pricing decision. These information systems are becoming\nincreasingly complex, including demand forecasts, and making recommendations based on\nbalanced buying patterns due by the economic evolution of markets.\nIn a first phase the ideia was to make a study on two main price recommendation systems:\nRules Motors and Price Optimization. As the objective of the dissertation is to change the\nalgorithm of Regular Price Optimization of the software tool Profimetrics, part of the study\nwas conducted according to the methodology of the tool. After an analysis of the company’s\ncurrent algorithm, the changes were made to perfect it. Subsequently, we used the case\nstudy methodology, in the application of the algorithm developed to a retail company.\nThrough this case study it was possible to make a brief diagnosis in order to compare\nthe current algorithm of the company with the developed algorithm."
  },
  {
    "keywords": [
      "Nitrosomonas europaea",
      "Genome-scale metabolic model",
      "Nitrogen",
      "Eutrophication",
      "Engenharia e Tecnologia::Biotecnologia Ambiental"
    ],
    "titulo": "Reconstruction of the genome-scale metabolic model of Nitrosomonas europaea",
    "autor": "Raposo,  Pedro Miguel Brígida",
    "data": "2017",
    "abstract": "Nitrogen is one of the four most common elements in any cell and thus, it is needed\nto sustain all kinds of life, making the nitrogen cycle crucial to life on Earth. However human\nactivities have doubled the transfer of the reactive nitrogen into the biosphere, largely\nthrough the excessive use of fertilizers. This lead to eutrophication of aquatic systems, a\nnegative ecosystem response usually associated with reduction of the biodiversity in it.\nThis work is set to improve the removal technique of reactive nitrogen by transforming\nit into non-reactive nitrogen - through Nitrosomonas europaea, an essential and ubiquitous\nbacteria in the nitrogen cycle. By using it in wastewater treatment plants, it is possible to\novercome a limiting step of this transformation, which ultimately helps to stop eutrophication.\nN. europaea is the most studied ammonia-oxidizing bacteria to date and has various\npathways that involve different compounds of nitrogen, making it metabolically versatile\nand, therefore, suitable for wastewater treatments. In this work, it was reconstructed a\ngenome-scale metabolic model of N. europaea, using merlin (a specialized software for this\ntask), to allow performing in silico simulations with different environmental conditions,\nproviding knowledge of its underlying metabolic fluxes.\nThis reconstruction was made through computational means (including several iterative\nsteps such as automatic and manual annotation of the genome, curation of the\nmetabolic pathways, among others), was validated through laboratorial means (by growing\nthe organism in a chemostat and quantifying the compounds of its biomass), and was\nsupported by literature in many cases.\nThis validation was represented by the accuracy of the model (a comparison between\nthe in vivo with the in silico data), and was equal to 98, 36 %.\nNow, with a metabolic model of the organism, a guided approach may be developed\nto optimize the conversion of ammonia into nitrite, to be later metabolized by other organisms\nto produce molecular diatomic nitrogen (inactive nitrogen), thus providing a solution\nto eutrophication."
  },
  {
    "keywords": [
      "VILT",
      "UX/UI",
      "Resource allocation management",
      "Project management",
      "Gestão de alocação de recursos",
      "Gestão de projetos"
    ],
    "titulo": "Improve human resources management via UX/UI",
    "autor": "Silva, Maria Moutinho Figueiredo da",
    "data": "2022-12-19",
    "abstract": "Resource allocation is critical to optimize billable allocation and avoid allocation conflicts. Tools are needed to\ncorrectly book and share allocations during the normal operation of a company.\nZeus is an intranet built in-house by VILT, a digital solutions company, which is used to connect every employee in the company, containing important personal and shared information, as well as useful services. Starting from scratch, it was developed according to the needs of VILT’s staff, making it a fully customized intranet. One of the many features in Zeus that was requested by VILT’s Professional Services Managers is the allocation map page. This feature allows managers to manage multiple human resources and their projects. Each manager has their own team to manage, and they currently face some limitations related to the resource allocation management process. These limitations are noticeable to the managers, resulting in a lack of adherence from some of them, which resulted in a divergence of the methods used in resource allocation management inside the company. There is a need to create an improved solution that takes into account: the managers’ insights and ideas, inspiration from existing software, and inspiration from the managers’ alternative solutions. The main goal for the new solution is having an easy view of who is available and when, as well as the option to easily manage that information.\nWith this dissertation, we want to study the UX/UI Design process and different resource allocation solution,\nand use it to design a resource allocation solution for VILT."
  },
  {
    "keywords": [
      "61:681.3",
      "681.3:61",
      "616-073",
      "612.8",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Exploration and application of machine learning algorithms to functional connectivity data",
    "autor": "Veloso, Telma Alves",
    "data": "2014",
    "abstract": "Methods for the study of the functional connectivity in the brain have seen several\ndevelopments over the last years, however not yet in a fully realized manner. Machine learning and\ncomplex network analysis are two promising techniques that together can help the process of better\nexploring functional connectivity for future clinical applications.\nMachine learning and pattern recognition algorithms are helpful for mining vast amounts\nof neural data with increasing precision of measures and also for detecting signals from an\noverwhelming noise component (Lemm, Blankertz, Dickhaus, & Müller, 2011). Complex network\nanalysis, a subset of graph theory, is an approach that allows the quantitative assessment of\nnetwork properties such as functional segregation, integration, resilience, and centrality (Rubinov\n& Sporns, 2010). These properties can be fed into classification algorithms as features. This is a\nnew and complex approach that has no standard procedures defined, so the aim of this work is to\nexplore the use of fMRI-derived complex network measures combined with machine learning\nalgorithms in a clinical dataset.\nIn order to do so, a set of classifiers is implemented on a feature dataset built with brain\nregional volumes and topological network measures that, in turn, were constructed based on\nfunctional connectivity data extracted from a resting-state functional MRI study. The set of classifiers\nincludes the nearest neighbor, support vector machine, linear discriminant analysis and decision\ntree methods. A set of feature selection methods was also implemented before the classification\ntasks. Every possible combination of feature selection methods and classifiers was implemented\nand the performance was evaluated by a cross-validation procedure.\nAlthough the results achieved weren’t exceptionally good, the present work generated\nknowledge on how to implement this recent approach and allowed the conclusion that, for most\ncases, feature selection improves the performance of the classifier. The results also showed that\nthe decision tree algorithm produces relatively good results without being associated with a feature\nselection method and that the SVM classifier, together with RFE feature selection method, produced\nresults on the same level as other work done with a similar approach."
  },
  {
    "keywords": [
      "Machine Learning",
      "DeViSE",
      "Medical Imaging",
      "Semantic Model",
      "Aprendizagem Máquina",
      "Imagem Médica",
      "Modelo Semântico",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Visual Semantic Embedding Model based on DeViSE for medical imaging",
    "autor": "Diogo, Ludgero da Silva",
    "data": "2021-02-22",
    "abstract": "During the last decades, artificial intelligence algorithms have been evolving to the point that they can \nachieve some amazing results like, identify and navigate roads, identify fraudulent transactions, \npersonalize crops to individual conditions, discover new consumer trends, predict personalized health \noutcomes, optimize merchandising strategies, predict maintenance, optimize pricing and scheduling in \nreal-time, diagnose diseases, among many others.\nHowever, although it can do all of that, it needs all the data to be correctly label, in other words, it can \nnot, for example, diagnose a disease, such as a stroke, if it does not know what a stroke is, so if the \nalgorithm has never been trained to identify strokes a new algorithm has to be created or the current one \nhas to be retrained, similar issues happen in the other examples.\nThis work focuses on this problem and tries to solve it by using a related in a high dimensional vector \nspace, called semantic space, where the knowledge from known classes can be transferred to unknown \nclasses."
  },
  {
    "keywords": [
      "Quantum machine learning",
      "Machine learning",
      "Classification",
      "Clustering",
      "Swap test",
      "Distance metric",
      "Metrica de distancia",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Classification and clustering using swap test as distance metric",
    "autor": "Sousa, Tomás Rodrigues Alves de",
    "data": "2023-06-04",
    "abstract": "This master’s thesis explores the advantages of using a quantum-based distance metric in a Machine\nLearning (ML) algorithm. It compares the performance of such a hybrid algorithm with an entirely classical\nalgorithm. Quantum Machine Learning (QML) has been growing in recent years. Some studies suggest that\nQML may even provide a polynomial speed-up for data categorization compared to traditional ML. However,\nanalyzing the benefits is not straightforward, as QML algorithms often rely on abstract, oracle (black-box)\nmodels that frequently rely on Quantum Random Access Memory (QRAM). Furthermore, loading classical\ndata onto quantum registers limits the applicability of QML, imposing a bottleneck. We used the Swap\nTest to measure the overlap between two quantum states to achieve our objective. Then we replaced the\nclassical distance metric in a distance-based machine learning algorithm with the quantum-based distance\nmetric. Our research showed that the Swap Test could be used as a distance metric in classical algorithms,\ndespite the fact that the results obtained are not better than the classical metrics. In the final discussion,\nwe present some ways that can improve the obtained results."
  },
  {
    "keywords": [
      "Seleção de fornecedores",
      "Inteligência artificial",
      "Gestão da cadeia de fornecimento",
      "Machine learning",
      "Supplier selection",
      "Supply chain management",
      "Artificial intelligence",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Machine learning applied to companies management",
    "autor": "Ribeiro, Pedro Miguel Ferreira",
    "data": "2022-12-21",
    "abstract": "O mais recente progresso reconhecido pela comunidade europeia por Indústria 5.0 atende as evoluções imergentes no mundo da indústria e revela haver uma necessidade de evolução nos sistemas de\nERP’s (Enterprise Resource Planning). É esperado que estes sistemas que auxiliem na gestão das empresas de uma forma mais dinâmica, assim tornando-se mais autónomos, atendendo a toda a informação\naglomerada no sistema.\nO PRODUTECH, formalmente conhecido como o “Cluster” português das Tecnologias de Produção,\né uma rede estabelecida por empresas de tecnologia de produção capazes de reagir às dificuldades do\nsector de produção com soluções criativas, adaptáveis, integradas e competitivas. Incluído dentro deste\nconsórcio está a PRIMAVERA, sendo uma empresa portuguesa pioneira no desenvolvimento de soluções\nde gestão, mais particularmente, sistemas ERP. Estes sistemas constam com elevados volumes de dados,\no que poderá levar a operações complexas no tratamento e análise dos mesmos.\nNeste relatório de dissertação de mestrado é documentada uma visão de solução para dar resposta\na assistência computacional na escolha de fornecedores, bem como a aglomeração dos conhecimentos\ndos diversos conceitos que envolvem a Indústria, visando a exploração de técnicas de data science e de\nmachine learning.\nEsta implementação teve em consideração aspetos importantes na gestão estratégica das empresas\natendendo as necessidades das mesmas, sendo possível a adaptação da solução para cada caso em\nparticular."
  },
  {
    "keywords": [
      "681.3:658.0",
      "658.0:681.3"
    ],
    "titulo": "Dashboarding: projeto e implementação de painéis analíticos",
    "autor": "Barros, Rui Miguel Pereira da Costa",
    "data": "2013",
    "abstract": "Na atualidade, graças às elevadas capacidades computacionais e gráficas existentes, é possível\ndotar os sistemas de processamento analítico com ferramentas de visualização e manipulação de\ninformação muito atrativas e de fácil utilização, em particular quando utilizamos para isso\ndashboards. Os dashboards tornam a interação com a informação proveniente de um sistema de\nprocessamento analítico mais interativa e eficaz, muito graças à modularidade inerente aos seus\ncomponentes gráficos e à sua qualidade inata de representar a informação graficamente. A\nmodularidade também é uma característica importante uma vez que permite modificar o sistema\nutilizando apenas cliques do rato, enquanto que, por sua vez, a representação gráfica da\ninformação facilita a sua análise e interiorização (Few, 2006a). Estas qualidades, entre outras,\nfazem com que os dashboards sejam uma ferramenta fulcral na análise da informação e no\nsuporte à tomada de decisão no seio de uma empresa, tendo sempre em mente que o sucesso de\numa empresa está dependente da capacidade que os seus responsáveis e funcionários têm de\ntomar decisões acertadas em tempo útil. Em geral, os dashboards podem ser utilizados para\nmonitorizar o desempenho de uma empresa, tanto a nível global como a nível individual, definir\nestratégias de marketing, analisar tendências, entre outros. Nesta dissertação pretendeu-se\ninvestigar a utilização de dashboards em sistemas de processamento analítico, abordando desde o\nseu desenho até à sua implementação e exploração prática. Complementarmente, de forma a\ndemonstrar a utilidade e vantagens desse tipo de instrumentos, procedeu-se à implementação de\num sistema piloto, incorporando na sua estrutura uma coleção de dashboards providos de\nmecanismos de auto-adaptabilidade aos requisitos dos utilizadores."
  },
  {
    "keywords": [
      "910:681.3",
      "681.3:910",
      "621.39"
    ],
    "titulo": "An open source virtual globe for android",
    "autor": "Almeida, Paulo Adelino Dias",
    "data": "2013",
    "abstract": "Virtual globes have a number of key bene ts as a platform for communicating\nand visualizing geospatial data over traditional technologies. Virtual globes have\nincreased in popularity and several implementations are available that cater to\ndi erent audiences from education to industry.\nDespite these advantages, an open source virtual globe solution is still not\navailable for mobile environments.\nOur goal is the development on an open source globe for Android, able to\nreceive 3D scenes from a W3DS server. We present the architecture and the implementation\ndecisions. We choose to develop the virtual globe on top of osgEarth\nwhich takes advantage of the OpenSceneGraph toolkit. Based on this decision, we\nexplain how osgEarth was extended to consume new 3D data sources and how it\nwas ported to the Android platform. Porting to Android requires major changes in\nthe OpenGL API usage. Embedded devices only support a subset of the OpenGL\nAPI.\nWe provide a virtual globe application that runs natively on the Android operating\nsystem. It is implemented on top of the osgEarth framework. osgEarth\nwas ported to Android and expanded to support additional features. Pointers to\nthe source code repositories are provided.\nWith the work developed in this project, mobile virtual globe solutions can\nbe customized and deployed, providing powerful visualizations and more intuitive\ninteractions."
  },
  {
    "keywords": [
      "Clustering",
      "Cross-industry standard process for Data Mining",
      "Data Mining",
      "Decision Trees",
      "Dengue Fever",
      "Knowledge discovery in databases",
      "Árvores de Decisão",
      "Descoberta de conhecimento em bases de dados",
      "Febre de Dengue",
      "Mineração de dados",
      "Processo padrão inter-indústrias para mineração de dados",
      "Segmentação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "A Data Mining approach towards effective Dengue outbreak prediction in Seremban, Malaysia",
    "autor": "Coutinho, Celso Filipe Nogueira",
    "data": "2016-01-11",
    "abstract": "In Malaysia, the incidence rate of Dengue Fever and Dengue Haemorrhagic Fever has reached the\nlevel of epidemic, and its numbers keep growing. In the last few years, a big effort has been put\ninto developing methods for predicting dengue outbreaks. However, the path for undertaking\neffectively those predictions, and therefore save Human lives, is still a very long one. This\ndissertation work focused on the use of Data Mining techniques, for discovering hidden patterns on\ndata obtained by crossing information related to patients infected with dengue in Malaysia and\nmeteorological data coming from the areas where those patients got infected."
  },
  {
    "keywords": [
      "Dungeon Crawler Stone Soup",
      "Inteligência Artificial",
      "Aprendizagem por Reforço",
      "Deep Reinforcement Learning",
      "Artificial Inteligence",
      "Reinforcement Learning",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Learning to play DCSS with Deep Reinforcement Learning",
    "autor": "Gonçalves, André Almeida",
    "data": "2019-12",
    "abstract": "DCSS is a roguelike game in which the player must explore and find artifacts. In every step\nof the game, there are decisions to make, and the complexity of the game resides in the vast\namount of options available to the player at any given time. The commands can be divided\ninto classes such as movement, combat, inventory management and usage, spell casting,\nand divine abilities.\nWe aim to implement an intelligent bot that will be able to play the game. To do so, we\nwill use DRL.\nDRL is where deep learning and RL meets. It uses the same principles of RL, to learn to\nperform a task, receiving rewards for every action made. The difference is that the action\nwe will perform is chosen by a DNN, and therefore, we call it DRL\nPyTorch[11] will be as the framework used to implement a NN that will be able to find\nthe solution for every small decision that can be made during gameplay. If all of those\ndecisions are merged, an intelligent bot should be able to play with some degree of success.\nThe aim of the intelligent bot is to learn a task, which in this case, is playing the game,\nwithout programming any real behavior."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of integrated models of hepatocyte cells",
    "autor": "Ferreira, Jorge Miguel Lourenço",
    "data": "2017",
    "abstract": "Metabolism acts a machinery by maintaining the functionality of the cell in response\nto several perturbations, keeping a balance in the levels of crucial metabolites and cell\ncomponents and producing energy by breaking down certain compounds. A better understanding\nof these mechanisms cannot be restricted to the knowledge of the function of\nspecific tissues or cell types, it also requires knowledge about their interactions.\nThe human liver has a high number of physiological functions related to the metabolism,\nsuch as the production of the bile, hormones and vitamins. The hepatocytes have a major\nimpact in human metabolism, being the most metabolically active cell types in humans.\nMalfunction on the metabolism of this type of cells is related to several diseases, like\nhepatitis, cirrhosis or non-alcoholic fatty liver disease (NAFLD), where the last one is\nconsidered a manifestation of obesity.\nA particular pathway has been associated not only with obesity, but also with cancer\nand type 2 diabetes, the mechanistic TOR (mTOR) pathway. Signalling of this pathway\nhas an effect on most of cellular functions and regulates growth and proliferation. It has\nbeen shown that alterations in this pathway can lead to fat accumulation in the liver of\nobese people. A better understanding of this complex pathway may help researchers to\nunveil more information on how this pathway works and how it can help in the treatment\nof several diseases.\nThe increase of high-throughput data, due to the advances in sequencing and other\nexperimental techniques, allowed us to better understand the molecular characteristics of the cell. A useful tool to process all this information are Genome-scale metabolic\nmodels (GSMMs). A GSMM is a list of mass-balanced reactions, which can be related to\ncellular compartments, like the cytoplasm. Given high-throughput data, GSMMs can be\nutilized for the simulation of the metabolism of a certain cell type through a constraintbased\nmodelling framework. There are several algorithms/ tools to create tissue-specific\nmetabolic models (based on a generic human model, such as Recon2) including tINIT,\nMBA or mCADRE.\nAlthough all these methods still face a number of issues, the generated models can\nsimulate human tissues and can be a good starting point for a better understanding of\ncomplex diseases. An important limitation of these models is the fact that they only\nrepresent the metabolic layer of the cells, while for models to be able to support accurate\nsimulations, a number of other important sub-systems (e.g. regulation, signalling) should\nalso be taken into account. This models (Integrative models) combine the information\nand material flow of the three previous mentioned sub-systems, delivering a more robust\ntool with more predictive strength."
  },
  {
    "keywords": [
      "Passive optical network",
      "Central office",
      "Network function virtualization",
      "Virtual network function",
      "Software-defined network",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Virtual network function development for NG-PON Access Network Architecture",
    "autor": "Araújo, Igor Virgílio Aquino Martins de",
    "data": "2022-02-19",
    "abstract": "The access to Internet services on a large scale, high throughput and low latency has grown\nat a very high pace over time, with a growing demand for media content and applications\nincreasingly oriented towards data consumption. This fact about the use of data at the edge\nof the network requires the Central Offices (CO) of telecommunication providers, to be pre pared to absorb these demands. COs generally offer data from various access methods, such\nas Passive Optical Network (PON) technologies, mobile networks, copper wired and oth ers. For each of these technologies there may be different manufacturers that support only\ntheir respective hardware and software solutions, although they all share different network\nresources and have management, configuration and monitoring tools (Fault, Configuration,\nAccounting, Performance, and Security management - FCAPS) similar, but being distinct and\nisolated from each other, which produces huge investment in Capital Expenditure (CAPEX)\nand Operational Expenditure (OPEX) and can cause barriers to innovation. Such panora mas forced the development of more flexible, scalable solutions that share platforms and net work architectures that can meet this need and enable the evolution of networks. It is then\nproposed the architecture of Software-Defined Network (SDN) which has in its proposal to\nabstract the control plane from the data plane, in addition to the virtualization of several Net work Function Virtualization (NFV). The SDN architecture allows APIs and protocols such\nas Openflow, NETCONF / YANG, RESTCONF, gRPC and others to be used so that there is\ncommunication between the various hardware and software elements that compose the net work and consume network resources, such as services AAA, DHCP, routing, orchestration,\nmanagement or various applications that may exist in this context.\nThis work then aims at the development of a virtualized network function, namely a VNF\nin the context of network security to be integrated as a component of an architecture guided\nby the SDN paradigm applied to broadband networks, and also adherent to the architecture\nOB-BAA promoted by the Broadband Forum. Such OB-BAA architecture fits into the initia tive to modernize the Information Technology (IT) components of broadband networks, more\nspecifically the Central Offices. With such development, it was intended to explore the con cepts of network security, such as the IEEE 802.1X protocol applied in NG-PON networks\nfor authentication and authorization of new network equipment. To achieve this goal, the\ndevelopment of the applications was based on the Golang language combined with gRPC\nprogrammable interfaces for communication between the various elements of the architec ture. Network emulators were initially used, and then the components were ”containerized”\nand inserted in the Docker and Kubernetes virtualization frameworks. Finally, performance\nmetrics were analyzed in the usage tests, namely computational resource usage metrics (CPU,\nmemory and network I/O), in addition to the execution time of several processes performed\nby the developed applications."
  },
  {
    "keywords": [
      "Redes sociais",
      "Erãs públicos",
      "Espaços inteligente",
      "Modelos de integração para redes sociais",
      "Social networks",
      "Public displays",
      "Smart places",
      "Integration models for social networks",
      "681.324",
      "316.77"
    ],
    "titulo": "Integração do Facebook em ecrãs públicos",
    "autor": "Soares, Abel Jorge Ferreira Barbosa",
    "data": "2012-12-13",
    "abstract": "As redes sociais online mudaram para sempre a forma como as pessoas comunicam, partilham experiências e vivem umas com as outras. Na área dos sistemas ubíquos, a comunidade tem demonstrado interesse em trazer para espaços públicos inteligentes componentes de interação social que advêm das redes sociais online. Estas integrações, onde omundo social virtual se cruza com um mundo de interações sociais físicas e localizadas têm motivações óbvias, pelo impacto positivo que podem ter em melhorar as interações sociais localizadas, no entanto, introduzem várias questões que devem ser estudadas. Porque as\ninterações sociais que ocorrem num mundo virtual são necessariamente diferentes das que ocorrem no mundo físico real, ainda não é evidente a melhor forma de introduzir componentes de interação social online num espaço com ecrãs públicos.\nEste estudo tem como objectivo avaliar as hipóteses de integração de redes sociais em espaços com ecrãs públicos em três áreas, a capacidade técnica dos mecanismos de integração, o mapeamento de dados e entidades das redes sociais em ecrãs públicos e a adaptação de processos existentes nas redes sociais nos ecrãs públicos. Partindo das motivações, necessidades e possibilidades de integração são definidos uma série de casos de uso representativos, expostos modelos de integração, assim como são implementadas aplicações no âmbito desses casos de uso. Desta forma espera-se ter uma noção clara e genérica das possibilidades de integração de componentes sociais virtuais em espaços com ecrãs públicos."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Geração de descrições de computação para a cloud",
    "autor": "Martins, Luís Henrique",
    "data": "2017",
    "abstract": "Nos últimos anos, a procura por soluções que tirem partido de um computação segura na\ncloud é um conceito em expansão e de grande interesse. A atratividade deste tema tem\nmotivado a apresentação de inúmeras propostas de protocolos que tiram partido dessas\ncaracterísticas. Contudo, a grande maioria desses protocolos requerem que as funcionalidades\na executar se apresentem descritos a um nível de abstração muito baixo, concretamente\nna forma de circuitos lógicos Booleanos.\nObviamente que não é simples nem produtivo trabalhar a esse nível de abstração, pelo\nque surge uma necessidade de converter descrições de programas realizado numa linguagem\nde alto-nível nesses circuitos. Este projeto baseia-se no estudo dessa transformação,\nassegurando que a mesma é correta garantindo a preservação da semântica do código\nfonte.\nPara a realização desta transformação será proposto um compilador certificado, que terá\na intenção de gerar descrições de circuitos Booleanos a partir de programas C. Para a\nprodução destas descrições será tido em conta a sua eficiência de forma a melhorar a sua\nperformance mantendo a fiabilidade do mesmo."
  },
  {
    "keywords": [
      "Industrial internet of things",
      "Internet of things",
      "Industry 4.0",
      "Manufacturing",
      "Coreflux",
      "Sensor network",
      "Computer communication",
      "Machine to machine",
      "Big Data",
      "Data gathering",
      "Manufacturing process",
      "Manufacturing optimization",
      "Automation",
      "Internet das coisas industrial",
      "Internet das coisas",
      "Indústria  4.0",
      "Manufatura",
      "Rede de sensores",
      "Comunicação por computador",
      "Máquina a máquina",
      "Obtenção de dados",
      "Processo industrial",
      "Otimização industrial",
      "Automação",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Aplicação de MQTT em ambiente industrial, trazer indústrias legadas para a I4.0",
    "autor": "Silva, João Manuel ferreira da",
    "data": "2021-08-10",
    "abstract": "Taking into account the characteristics of industrial environments, sometimes resistant to\ninnovation so as not to harm productive factors, technologies that are outdated are often\nadopted. In addition, there is a clear need and desire for obtaining data related to the pro duction process in order to optimize it and, therefore, obtaining a better performance of the\nindustrial equipment. Complementary, there is the need for transferring information about\nrefined concepts among multiple departments to assist tasks such as process engineering\nand order management.\nThe purpose of this dissertation is to build a platform developed by Setlevel, called\nCoreflux, where the building modules are capable of providing not so up-to-date technologies\nwith the capabilities to integrate Industry 4.0 concepts. Based on the concept of Internet of\nThings, the work aims at providing obsolete equipment and technologies with the capacity to\nbehave as newer equipment, while maintaining the same benefit that these older technologies\noffer to the industrial sector, such as reliability and availability."
  },
  {
    "keywords": [
      "Model-driven software engineering",
      "Domain-specific language",
      "Transformação  de modelos;",
      "Geração automática de código fonte",
      "Geração de código portável",
      "Cross-platform generation",
      "Cross-platform code",
      "681.3.06"
    ],
    "titulo": "MDA SMART: uma ferramenta multiplataforma baseada em modelos",
    "autor": "Costa, Rogério Araújo",
    "data": "2012-12-12",
    "abstract": "Atualmente, o maior desafio no desenvolvimento de software é referente à a portabilidade das aplicações para as várias plataformas disponíveis, especialmente pela crescente heterogeneidade nos componentes de hardware, de middleware e de software base.\nO desenho de modelos abstratos de software é uma das formas mais elegantes e eficientes para solucionar este desafio. A Model-Driven Software Engineering (MDSE)  ́é uma metodologia de desenvolvimento em que os modelos são chave em todo o ciclo de vida do projeto, desde a captura de requisitos, passando pelas fases de modelação e desenvolvimento, e por fim nos processos de teste e instalação.\nO objetivo primário desta dissertação foca-se na construção de uma ferramenta, o MDA SMART, capaz de interpretar modelos abstratos de software, parametrizáveis, e de gerar automaticamente código fonte para várias plataformas. A ferramenta, caracterizada por uma arquitetura robusta e extensível, é idealizada para permitir a manipulação de modelos\nde forma ágil, para ser modular o suficiente para integrar novos perfis meta-modelo e para escalar eficientemente para novas plataformas.\nO MDA SMART resulta da articulação de uma Domain-Specific Language (DSL) para a gestão dos meta-modelos e consequentes processos de transformação. Na utilização da DSL são obtidos processos de transformação rigorosos, com elevado desempenho e que visam maximizar a consistência e portabilidade dos modelos através de medidas ajustadas a destoarem a heterogeneidade entre as plataformas. Adicionalmente, a ferramenta visa compatibilizar os modelos de lógica de negócio com os referentes às interfaces gráficas que, conjugados, vão permitir a obtenção de modelos e código fonte com alto nível de consistência e completude."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Mobile ray-tracing",
    "autor": "Santos, Tiago Manuel da Silva",
    "data": "2019",
    "abstract": "The technological advances and the massification of information technologies have\nallowed a huge and positive proliferation of the number of libraries and APIs. This large\noffer has made life easier for programmers in general, because they easily find a library,\nfree or commercial, that helps them solve the daily challenges they have at hand.\nOne area of information technology where libraries are critical is in Computer Graphics,\ndue to the wide range of rendering techniques it offers. One of these techniques is ray\ntracing. Ray tracing allows to simulate natural electromagnetic phenomena such as the\npath of light and mechanical phenomena such as the propagation of sound. Similarly, it also\nallows to simulate technologies developed by men, like Wi-Fi networks. These simulations\ncan have a spectacular realism and accuracy, at the expense of a very high computational\ncost.\nThe constant evolution of technology allowed to leverage and massify new areas, such as\nmobile devices. Devices today are increasingly faster, replacing and often complementing\ntasks that were previously performed only on computers or on dedicated hardware.\nHowever, the number of image rendering libraries available for mobile devices is still very\nscarce, and no ray tracing based image rendering library has been able to assert itself on\nthese devices. This dissertation aims to explore the possibilities and limitations of using\nmobile devices to execute rendering algorithms that use ray tracing, such as progressive\npath tracing. Its main goal is to provide a rendering library for mobile devices based on ray\ntracing."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Plataforma inteligente de apoio à decisão médica no transplante de órgãos",
    "autor": "Reis, Rita Soares",
    "data": "2019-09-04",
    "abstract": "A alocação adequada de órgãos para transplantação é crítica e crucial. No entanto, o \nnúmero de órgãos a ser doados não é suficiente dada a quantidade de pacientes em lista \nde espera. Assim, a determinação do maior número possível de potenciais dadores, de \nforma eficiente e eficaz torna-se essencial e pode contribuir para melhorar a taxa de \nsucesso de transplantação de órgãos.\nAo longo dos últimos anos, a utilização de Tecnologias de Informação (TIs) e de \nferramentas computacionais em vários setores económicos, incluindo o setor da saúde, \ncresceu exponencialmente, já que têm potencial para transformar e melhorar a prestação \nde cuidados de saúde. \nAssim, e aliando a necessidade da eficiência na descoberta de potenciais dadores com \na emergência das TIs na saúde, surge a necessidade de uma plataforma Web de apoio à \ndecisão clínica. O objetivo desta plataforma é automatizar o processo de descoberta de \ninformaçãoútil e acionável, através da utilização de tecnologias como Business Intelligence\n(BI) e Data Mining (DM), ajudando na tomada de decisão clínica diária. Assim, esta é\nresponsável pela recolha, gestão, armazenamento e sinalização de potenciais dadores.\nNo âmbito deste projeto de dissertação, foi redesenhada e otimizada a plataforma Web \nOrganite, atualmente implementada no Centro Hospitalar do Porto (CHP). Envolveu \ntransformações tanto no design da interface do utilizador, como no modo como a \ninformação está organizada na plataforma, de forma a melhorar a experiência do \nutilizador e a interação com os dados clínicos. Foi ainda desenvolvida uma metodologia, \ncom base em técnicas de Data Mining, para construir um modelo preditivo que avalia \nquais os pacientes que dão entrada no hospital que têm maior probabilidade em ser\npotenciais dadores de órgãos. O objetivo é tornar mais simples e eficaz o processo de \nidentificação de potenciais dadores, contribuindo positivamente na tomada de decisão do \nGabinete de Coordenação de Colheita e Transplantação (GCCT), e impactando na redução \nda lista de doentes que aguarda um transplante."
  },
  {
    "keywords": [
      "Health information systems",
      "Clinical Records",
      "Electronic Health Records",
      "Interoperability",
      "Physical Medicine",
      "Sistemas de informação em saúde",
      "Registos clínicos",
      "Registos de saúde eletrónicos",
      "Interoperabilidade",
      "Medicina Física",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Plataforma de registo clínico para medicina física e reabilitação",
    "autor": "Domente, Alexandru",
    "data": "2022-04-12",
    "abstract": "Clinical records are a fundamental component of the correct treatment and follow-up of patients. The management of patient flow is complex and in the current pandemic times, the work organization must be increasingly focused and oriented towards simple and effective records. Communication among the different departments is fundamental and must ensure that data and information flows without breaks. The provision of health care to patients is a vital importance in a society and the more advanced the medicine is the better is the treatment. The impact of technologies drastically benefits the treatment chosen by the doctor and the respective diagnoses, so the more information the doctor has regarding of the state of health and all kinds of interventions of his patients, the more prepared he will be to face and the more appropriate the methods will be used for each one. The main objective of this dissertation is to study the applicability of tools to develop a web platform to be used in the management of information to health professionals. An organic platform that can be molded to various situations and that are capable to supporting clinical records where each health professional is able to visualize the tasks according to their specialty, such as physiotherapy and rehabilitation professionals. A web platform that allows any professional in the area, a simple registration and access, offering a practical history of patients with all the necessary information and all the interventions they have done."
  },
  {
    "keywords": [
      "681.3:61",
      "61:681.3",
      "616-073",
      "616.8"
    ],
    "titulo": "Processamento e análise de dados de Ressonância Magnética funcional (RMf) em repouso e tensor de difusão (DTI)",
    "autor": "Lopes, Vanda Carolina da Silva",
    "data": "2013",
    "abstract": "Os estudos de conectividade cerebral têm-se tornado bastante relevantes no meio\ncientífico e clinico. No entanto, estes estudos não são fáceis de realizar, uma vez que\nas aplicações existentes foram desenvolvidas por investigadores para as suas\nnecessidades, não sendo apropriadas para um profissional de saúde. Assim, foi criada\na aplicação BrainCAT que permite que os estudos de conectividade cerebral de\nRessonância Magnética funcional e tensor de difusão sejam realizados de forma\nintuitiva e que seja facilmente manuseada por um utilizador com poucos\nconhecimentos na área.\nAssim, o objetivo desta dissertação centra-se na aplicação BrainCAT. Assim, esta\ndissertação pode ser dividida em cinco parte: teste da aplicação com um número\nelevado de casos, levantamento de falhas e de formas de melhorar a aplicação,\nimplementação das melhorias através da programação em objective-C, testar\nnovamente a aplicação para os mesmos casos e, por fim, comparar os resultados das\nduas análises.\nAs melhorias implementadas baseiam-se em formas de tornar a aplicação mais\nintuitiva e de melhorar os resultados. A nível de melhorias de resultados centrou-se na\netapa da normalização, uma vez que esta era a que apresentava mais problemas.\nAssim, entre outras implementações, foram adicionadas etapas como a remoção do\npescoço, a verificação da extração do crânio na imagem estrutural e a normalização\nnão linear.\nRelativamente à comparação entre resultados, verificou-se que com a extração do\npescoço da imagem estrutural, a etapa seguinte de extração do crânio melhorou. A\nverificação do resultado desta imagem, facilita o utilizador e previne que casos não\nsejam vistos. A normalização linear apresentou resultados melhores, contudo\napresenta uma maior sensibilidade a artefactos ou erros de pré-processamento."
  },
  {
    "keywords": [
      "Machine learning",
      "Educational data mining",
      "Learning analytics",
      "Classification",
      "School success",
      "Classificação",
      "Sucesso escolar",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Análise e previsão de comportamento com base no perfil do utilizador usando algoritmos de machine learning",
    "autor": "Lacerda, Maria Beatriz Araújo",
    "data": "2024-05-20",
    "abstract": "The decision to drop out of school is one that carries profound implications, not only for the individual involved but also for the wider society. This complex issue intersects with various socioeconomic, cultural, and personal factors, leading to a range of long-term effects that extend far beyond the immediate impli cations of leaving school. For individual consequences, dropping out of school often results in reduced employment opportunities and earning potential. Without the foundational education provided by a high school diploma or equivalent, individuals may find themselves limited to lower-wage jobs with fewer bene fits and job security. This, in turn, can lead to a lifetime of financial instability and limited social mobility. The repercussions of school dropouts ripple through society. Economically, a lower-educated workforce can mean reduced productivity and innovation, impacting national competitiveness and economic growth.\nSocietally, there can be an increase in dependency on welfare systems and public assistance, as indivi duals without adequate education may struggle to support themselves and their families. Education is closely linked to health outcomes. Individuals who leave school prematurely are at a greater risk of en gaging in unhealthy behaviors such as substance abuse and may have limited access to healthcare and health information. This not only affects their personal health and well-being but also places a burden on public health systems. High dropout rates often disproportionately affect marginalized communities, exacerbating existing inequalities. This perpetuates a cycle of poverty and limited educational attainment,\nimpacting generations. Education plays a critical role in fostering civic awareness and participation. In dividuals who drop out of school may be less likely to engage in civic activities, vote, or participate in community organizations, leading to a less engaged and informed citizenry. In a global context, the edu cational attainment of a population is a key factor in a country’s ability to compete and cooperate on the\ninternational stage. Higher dropout rates can impede a nation’s development and its ability to address global challenges effectively. Evaluating user profiles plays a crucial role in anticipating risk situations. Therefore, the focus of this master thesis is to process data using machine learning and deep learning methods. The aim is to develop detailed profiles that include information on the emotional state of individuals, identification of risk alerts predictions of potential warning situations, and strategies to be adopted. This master thesis aims to use these advanced technologies to create an effective risk prediction and response system. So, it’s present a study on the classification of middle and secondary school students based on their interactions with a bot and their school’s management system, in order to identify risk behaviors and predict success or failure in school. Through the use of machine learning techniques, the answers given by students will be analyzed to extract relevant characteristics and predict their math and portuguese grades. The results of this study will provide insight into the potential use of student data from this age group as a means of identifying at-risk behaviors and predicting student success in school, and will inform the\ndevelopment of more effective interventions and support for at-risk students."
  },
  {
    "keywords": [
      "Cirurgia assistida por computador",
      "Inteligência Artificial",
      "Interação humano-computador",
      "Leap motion",
      "Reconhecimento de gestos manuais",
      "Artificial Intelligence",
      "Computer-assisted surgery",
      "Hand gestures recognition",
      "Human-computer interaction",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Leap motion and artificial intelligence for surgical navigation: automatic hand gestures recognition",
    "autor": "Magalhães, Bruno Miguel Fernandes",
    "data": "2023-12-15",
    "abstract": "Globalmente, 310 milhões de cirurgias são realizadas a cada ano e existe uma probabilidade de \n2% a 5% de infeções do local cirúrgico. Problemas relacionados com a esterilização de equipamentos são\numa das razões. Estas infeções impactam negativamente a saúde física e mental do paciente \ncomprometendo a sua qualidade de vida. As cirurgias assistidas por computador estão a ajudar os \ncirurgiões a realizar operações mais seguras e a permitir aos pacientes menos tempo de recuperação.\nNo entanto, este meio de interação geralmente depende de dispositivos de contato físico, como rato e \nteclado, que expõem a sala de cirúrgica a condições assépticas. O Leap Motion ultrapassa o problema \ndos dispositivos físicos uma vez que não precisa de nenhum tipo de interação física.\nEsta dissertação tem como objetivo conceber, desenvolver e validar uma abordagem de interação \nhomem-computador intuitiva e sem contacto, baseada no reconhecimento automático de gestos manuais \natravés do Leap Motion, seguindo uma conceção centrado no utilizador. Para tal, foi primeiramente \nrealizado um protocolo junto dos utilizadores finais para determinar que gestos eram mais intuitivos. \nPosteriormente, foram criados dois grandes datasets (um de imagens da mão e um com características \nda mão) para alimentar modelos de inteligência artificial que pudessem reconhecer os gestos manuais \nde qualquer pessoa. O melhor modelo desenvolvido, com 96.25% de precisão nos dados de teste, foi \nbaseado no algoritmo Support Vector Machine e foi, de seguida, integrado na ferramenta de \nreconhecimento de gestos manuais que através das previsões do modelo, executa a respetiva ação no \necrã, removendo a necessidade de periféricos com contacto físico.\nA partir de uma validação preliminar realizada junto de voluntários da Universidade do Minho e \numa validação clínica realizada junto de cirurgiões do hospital Trofa Saúde Braga Centro, verificou-se \nque os utilizadores demoram mais tempo a realizar o mesmo conjunto de tarefas com a ferramenta de \ndeteção de gestos manuais do que com o uso tradicional do rato. Contudo, foi possível observar que há \numa curva de aprendizagem da ferramenta e que estes tempos diminuem com a experiência. Por fim, o \nSystem Usability Scale, que é um teste padronizado de avaliação de usabilidade, revelou que a aplicação \ndesenvolvida atinge um resultado de 76.67 ± 9.86, porém há uma perceção de usabilidade maior na \nvalidação preliminar do que na validação clínica (67.5 ± 6.37). Através de uma última questão aberta \npôde-se ainda perceber que a sensibilidade do cursor é o que precisa de mais atenção e constitui o ponto \nprincipal do trabalho futuro, juntamente com melhorias na interface gráfica."
  },
  {
    "keywords": [
      "5G simulation",
      "5G testbed",
      "5G virtualization",
      "Softwarization",
      "Emulator",
      "5G",
      "Ambiente de testes",
      "Virtualização",
      "Software",
      "Emulador",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Implementation of a virtualized 5G network",
    "autor": "Magalhães, Tiago",
    "data": "2023-08-03",
    "abstract": "Many organizations have developed open software components for 5G (Fifth Generation) networks and\nrecognize the importance of new technologies based on virtualization and softwarization.\nWith these solutions, it is possible to implement a 5G virtualized network without having access to a\nmobile network, which has many restrictions.\nImplementing a 5G testbed is essential because it allows the creation of a framework that can enable\nthe development and research of new solutions related to 5G.\nThis dissertation proposes a solution that uses open-source software to emulate the access network\nand deploys software modules that implement core network functionalities. Moreover, network capabilities,\nas well as interoperability, are described."
  },
  {
    "keywords": [
      "681.3.06"
    ],
    "titulo": "Analysing call graphs for software architecture quality profiling",
    "autor": "Couto, Luís Diogo Monteiro Duarte",
    "data": "2012-06-14",
    "abstract": "Risk assessment is an important topic for financial institution nowadays, especially in the context of loan applications or loan requests and credit scoring. Some of these institutions have already implemented their own custom credit scoring systems to evaluate their clients’ risk supporting the loan application decision with this indicator. In fact, the information gathered by financial institutions constitutes a valuable source of data for the creation of information assets from which credit scoring mechanisms may be developed.\nHistorically, most financial institutions support their decision mechanisms on regression algorithms, however, these algorithms are no longer considered the state of the art on decision algorithms. This fact has led to the interest on the research of new types of learning algorithms from machine learning able to deal with the credit scoring problem.\nThe work presented in this dissertation has as an objective the evaluation of state of the art algorithms for credit decision proposing new optimization to improve their performance. In parallel, a suggestion system on credit scoring is also proposed in order to allow the perception of how algorithm produce decisions on clients’ loan applications, provide clients with a source of research on how to improve their chances of being granted with a loan and also develop client profiles that suit specific credit conditions and credit purposes.\nAt last, all the components studied and developed are combined on a platform able to deal with the problem of credit scoring through an experts system implemented upon a multi-agent system. The use of multi-agent systems to solve complex problems in today’s world is not a new approach. Nevertheless, there has been a growing interest in using its properties in conjunction with machine learning and data mining techniques in order to build efficient systems. The work presented aims to demonstrate the viability and utility of this type of systems for the credit scoring problem."
  },
  {
    "keywords": [
      "Complexity",
      "Grammar",
      "Language-based-tool",
      "Programming language",
      "Static code analysis",
      "Complexidade",
      "Gramática",
      "Ferramenta baseada em linguagens",
      "Linguagem de programação",
      "Análise de código estático",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Programming language complexity analysis and its impact on Checkmarx activities",
    "autor": "Pinto, Gonçalo Rodrigues",
    "data": "2022-12-15",
    "abstract": "Tools for Programming Languages processing, like Static Analysers (for instance, a Static\nApplication Security Testing (SAST) tool, one of Checkmarx’s main products), must be\nadapted to cope with a given input when the source programming language changes.\nComplexity of the programming language is one of the key factors that deeply impact the\ntime of giving support to it.\nThis Master’s Project aims at proposing an approach for assessing language complexity,\nmeasuring, at a first stage, the complexity of its underlying context-free grammar (CFG).\nFrom the analysis of concrete case studies, factors have been identified that make the\nsupport process more time-consuming, in particular in the stages of language recognition\nand in the transformation to an abstract syntax tree (AST). In this sense, at a second stage, a\nset of language features is analysed in order to take into account the referred factors that\nalso impact on the language processing.\nThe main objective of the Master’s work here reported is to help development teams to\nimprove the estimation of time and effort needed to adapt the SAST Tool in order to cope\nwith a new programming language.\nIn this dissertation a tool is proposed, that allows for the evaluation of the complexity of a\nlanguage based on a set of metrics to classify the complexity of its grammar, along with a set\nof language properties. The tool compares the new language complexity so far determined\nwith previously supported languages, to predict the effort to process the new language."
  },
  {
    "keywords": [
      "Ablação laser",
      "BCZT/CFO",
      "Compósitos magnetoelétricos",
      "Havriliak-Negami",
      "Magnetoeletricidade",
      "Laser ablation",
      "Magnetoelectric composites",
      "Magnetoelectricity",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Propriedades elétricas em heteroestruturas multiferróicas",
    "autor": "Moreira, João Filipe Martins",
    "data": "2023-12-11",
    "abstract": "Os filmes finos multiferróicos magnetoelétricos, que apresentam um acoplamento entre a ordem \nmagnética e elétrica, têm atraído recentemente bastante interesse científico e tecnológico. A possibilidade de controlar a magnetização (polarização) por meio de um campo elétrico (magnético) permite estudar as interações electro-magnéticas à nanoescala, criar nanoestruturas multifuncionais e alargar a gama de potenciais aplicações. Nesse âmbito, combinando um material piezoelétrico com um material magnetostritivo, as interações mecânicas entre as fases induzem um acoplamento magnetoelétrico. Esse acoplamento entre a fase piezoelétrica e a fase magnetostritiva dá origem a diversas aplicações em microeletrónica, por exemplo, memórias de múltiplos estágios ou sensores. Neste contexto, foram produzidas compósitos magnetoelétricos constituídas por 0.85[0.6Ba(Zr0.2Ti0.8)O3 − 0.4(Ba0.7Ca0.3)TiO3] − 0.15SrTiO3 (0.85BCZT-0.15STO), um bom piezo/ferroelétrico, livre de chumbo, e CoFe2O4 (CFO), um material ferromagnético com alta magnetostrição. A técnica utilizada para a deposição das heteroestruturas em bicamada foi a ablação laser (PLD). Neste trabalho, foi a estudada a influência da espessura do CFO em dispositivos do tipo Pt(111)/BCZT-STO/CFO/Au, e posteriormente, a influência do substrato no dispositivo Nb:STO(001)/BCZT-STO/CFO/Au, comparando com os resultados obtidos para as estruturas anteriores. Após a sua produção, estes dispositivos foram caraterizados a nível estrutural por difração de raios-X com incidência rasante (GIXRD), a nível morfológica por microscopia eletrónica de varrimento (SEM) e a nível da sua composição química por espetroscopia de dispersão de energia (EDS). Posteriormente foi realizada a caracterização ferroelétrica e dielétrica. Os resultados obtidos por SEM e GIXRD mostram que as camadas de BCZT-STO e CFO nos dispositivos de Pt/BCZT-STO/CFO/Au são policristalinas e apresentam crescimento colunar. Já para a estrutura Nb:STO/BCZT-STO/CFO/Au, verificou-se o crescimento epitaxial dos filmes de BCZT-STO e CFO que constituem a bicamada. As medidas de GIXRD confirmam a formação da fase tetragonal do BCZT-STO e a estrutura espinela cúbica do CFO. Os ciclos P-E, obtidos para todos os dispositivos, permitem concluir que compósito apresenta comportamento ferroelétrico. As propriedades dielétricas dos filmes foram estudadas pela medição da capacidade (C) e das perdas dielétricas (tan 𝛿) em função da frequência e temperatura. Verifica-se que BCZT-STO apresenta um comportamento relaxor. Através \ndo modelo de Havriliak-Negami e incluindo a contribuição da condutividade, modelizou-se e ajustou-se a constante dielétrica imaginária, para obter os tempos de relaxação e energias de ativação, de maneira a saber os principais mecanismos de condução presentes nestas estruturas."
  },
  {
    "keywords": [
      "681.324",
      "621.39"
    ],
    "titulo": "Framework e Cliente WebRTC",
    "autor": "Amaral, Vasco Manuel de Frias",
    "data": "2013",
    "abstract": "WebRTC is a standard technology which allows real-time communications between browsers,\nwithout installing additional plugins. In this way, for each device (computers, smartphones,\netc.) with an installed browser, it is possible to perform peer-to-peer real-time communications\nnatively, for instance, video and voice calls, chatting or instant messaging, file sharing and screen\nsharing.\nThis recent technology has grown exponentially both in implemented solutions and in browsers\ncompatibility. WebRTC is therefore an evolutionary technology with a strong growth, where\nmore solutions Over-The-Top (OTT) could appear and where the telecommunications operators\ncould invest creating their own service solutions.\nFacing the lack of standards regarding the communication between WebRTC endpoints, this\nproject studies in depth theWebRTC technology in order to identify its potentiality and to assess\nin which way it could impact on the telecommunications world. This project also aims to create\na framework that helps developing WebRTC applications and services at a higher level.\nAs proof-of-concept aWebRTC client is developed to allow testing the services implemented\nin the framework. The evaluation results address functionality tests, attesting that the implemented\nfeatures of the framework work properly, and measure the CPU and memory consumption\nof WebRTC technology."
  },
  {
    "keywords": [
      "E-mail",
      "Análise",
      "DKIM",
      "SPF",
      "DMARC",
      "Analysis",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Melhoria da segurança do e-mail através de DKIM, SPF e DMARC",
    "autor": "Carneiro, Luís Miguel Rocha",
    "data": "2024-01-05",
    "abstract": "As ameaças cibernéticas constituem um grande problema para as empresas. Desde phishing a ran somware, são distribuídas principalmente através de correio eletrónico, através do qual os atacantes\nfazem-se passar por fontes confiáveis, convencendo deste modo a vítima a clicar no link e/ou anexo\nmalicioso. Este problema agrava-se com a prática de spear-phishing, especialmente prevalente no con texto empresarial, que personaliza o e-mail com informações específicas relevantes para o(s) alvo(s), o\nque aumenta a eficácia do ataque.\nOs objetivos do trabalho realizado foram a pesquisa e o desenvolvimento de ferramentas que permitam\nidentificar estes e-mails maliciosos. As soluções propostas tiram partido de protocolos como o DKIM, o\nSPF e o DMARC para ajudar a classificar a confiabilidade de um e-mail suspeito. Dado um e-mail, o\nobjetivo é obter o resultado para cada um destes protocolos, que pode ter sido calculado por outros\nservidores de correio eletrónico antes de chegar ao destinatário ou é calculado após a sua receção.\nAmbos os tipos de validações foram desenvolvidas e ficaram operacionais, com a exceção da validação\n“metódica” do DKIM (feita in house), devido ao seu posicionamento limitado pela arquitetura imposta.\nOs testes confirmaram o bom funcionamento e resiliência das soluções contra e-mails provenientes de\ndomínios diferentes."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Do Registo de Saúde Eletrónico à administração de medicamentos assistida",
    "autor": "Dias, Inês São José Simões",
    "data": "2019",
    "abstract": "Ao longo dos últimos anos, tem-se assistido a um crescente avanço ao nível das Tecnologias\nda Informação (TI) e o caso da aplicabilidade das TI à área da saúde não é\nexcepção, dando origem ao que se designa de Tecnologias de Informação da Saúde\n(TIS). Assim, com o decorrer do tempo e dos ditos avanços, surgiram novas ferramentas,\ntecnologias e Sistemas de Informação Hospitalar (SIH) com o intuito de melhorar\na qualidade da prestação dos serviços das instituições de saúde e, do lado do utente,\ncom o objetivo de proporcionar um acesso cada vez mais eficiente aos cuidados de\nsaúde. Um dos avanços mais significativos na prossecução da interoperabilidade entre\nsistemas e na centralização da informação é o Registo de Saúde Eletrónico (RSE). Este\nsistema integra dados do utente provenientes de várias fontes, tornando-se um ativo\nválido no que diz respeito ao suporte da decisão clínica. Paralelamente a isso, permite\nainda o acesso a aplicações para a realização de processos operacionais, tais como a\nprescrição de medicamentos e exames de forma eletrónica.\nPartindo destes pressupostos, foi então estudado o estado atual dos avanços destes\nSIH em Portugal, por forma a perceber de que forma seria possível, com os recursos\nexistentes atualmente, munir o utente de melhores e mais informações acerca da sua\nsaúde. Por isso, o principal objetivo deste projeto de dissertação é desenhar e desenvolver\numa aplicação móvel capaz de apoiar o utente no cumprimento das suas obrigações\nde saúde, sejam elas consequência de eventos numa determinada instituição ou mesmo\na toma de medicamentos prescritos. Para além disso, é também pretendido que, para\nalém do possível apoio conseguido através da aplicação criada, se consiga ainda criar\numa comunidade de auxílio ao utente, através da criação de um agregado. A principal\nmotivação é, portanto, uma melhoria na qualidade da saúde do utente, através de um\nacompanhamento monitorizado e o mais individualizado possível.\nMetodologicamente, partiu-se de uma análise completa aos dados provenientes do\nPortal do Serviço Nacional de Saúde (SNS) e de outras instituições de saúde, com o\nintuito de contornar a inexistência de uma API e conseguir extrair e tratar os dados e,\nposteriormente, carregá-los na Base de Dados (BD) que alimenta a aplicação. Ultrapassada essa dificuldade, comprova-se então a possibilidade de agregar toda a informação\nde um mesmo utente numa só aplicação, com a devida autenticação."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Model-based testing of user interfaces",
    "autor": "Gonçalves, Marcelo José Rodrigues",
    "data": "2017",
    "abstract": "The Graphical User Interface (GUI) is crucial for the success of an interactive system. Incorrect\noperation of the GUI may inhibit the proper functioning of the system. Ensuring the quality of a\nsystem is essential to its success. A practical way to ensure it is through software testing. Model-\nBased Testing (MBT) is a black-box technique that compares the behaviour of the system under\ntest against the behaviour of an oracle (the system's model). These tests are intended to verify\nthat the implementation of a software meets its speci cation.\nApplying MBT tools to GUIs enables the systematic testing of the system by automatically\nsimulating user actions on the user interface. However, applying the MBT process to GUIs creates\nseveral challenges such as the mapping between the actions in the model and the actions in the\napplication interface. This dissertation will focus on the model-based testing of graphical user\ninterfaces. The main objective is to further the development of the TOM Framework. The TOM\nFramework supports the process of MBT applied to GUIs, in particular, web-based GUIs, enabling\nthe creation and execution of user interfaces tests and thus increasing the probability of error\ndetection in the tested interfaces."
  },
  {
    "keywords": [
      "Dispositivos móveis",
      "Smartphones",
      "Aplicação mobile",
      "Aplicação web",
      "iOS",
      "Android",
      "React native",
      "React",
      "Node.JS",
      "MySQL",
      "Mobile devices",
      "Mobile application",
      "Web application",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "eHealth and patient relationship management",
    "autor": "Pereira, Rui Paulo Couto Brandão",
    "data": "2022-12-19",
    "abstract": "Os dispositivos móveis têm ganho uma enorme relevância ao longo dos últimos anos, especialmente os smartphones, que nos dias de hoje, são indispensáveis para a maioria das pessoas, tanto para a sua utilização pessoal como para a sua utilização profissional. Neste contexto e com algumas dificuldades encontradas nas unidades hospitalares, especialmente a falta de comparência dos utentes às respetivas consultas médicas e a falta de uma relação fluída entre o utente e a sua unidade hospitalar, foi proposta uma possível solução, de modo a combater esses problemas e a integrar os dispositivos móveis no acesso do utente aos cuidados de saúde. No levantamento dos problemas enumerados surgiu a ideia de criar uma aplicação mobile, precisamente para que a interação dos utentes com as suas unidades hospitalares sejam facilitadas e fluídas. Para complementar este projeto e de modo a obter um protótipo completamente funcional, foi criada ainda uma\naplicação web, para os profissionais de saúde, com a capacidade de responder aos pedidos dos utentes, feitos\natravés da aplicação mobile. Para procurar as funcionalidades a implementar nesta aplicação foi feita uma análise de mercado e um levantamento do que os utentes mais necessitam nas deslocações aos hospitais.\nRelativamente à estrutura da solução, a aplicação mobile foi pensada para ser utilizada em qualquer\nsistema operativo de um smartphone, quer seja iOS ou Android, por essa razão foi desenvolvida em React Native, a aplicação web em React, o servidor em Node.JS e a base de dados relacional em MySQL."
  },
  {
    "keywords": [
      "Green software",
      "Technical debt",
      "Code smells",
      "Refactoring",
      "Energy debt",
      "Code analysis",
      "Software systems engineering",
      "Débito técnico",
      "Débito energético",
      "Análise de código",
      "Engenharia de sistemas de software",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Energy debt: applying technical debt to energy consumption",
    "autor": "Maia, Daniel Fernandes Veiga",
    "data": "2020-12-16",
    "abstract": "The purpose of this dissertation is to analyse the impact of certain practices in long term\npower usage and expand on the concept of Technical Debt by introducing this aspect of\nenergy consumption, dubbing the resulting notion as Energy Debt.\nThis dissertation presents energy debt as a range of excess of energy required to execute\ncode. It holds a minimum and maximum cost which depends on a set of factors during\nruntime.\nWe analyse existing research regarding energy consumption to compile a detailed set of\nenergy smells and the expected energy savings when they are eliminated via refactoring.\nThen, we present the debt model that computes excessive energy expenditure of a software\nsystem. This debt model is based on the number and variety of occurrences of energy smells\npresent on the software’s source code.\nLastly, we’ve developed a tool which we dubbed E-Debitum, which extends the Sonar Qube framework to detect energy smells and compute energy debt."
  },
  {
    "keywords": [
      "Mobile identity",
      "Digital identity",
      "SIAM",
      "SOA",
      "ESB",
      "Service",
      "Integration",
      "WSO2",
      "Mobile ID",
      "Identidade móvel",
      "Identidade digital",
      "Serviços",
      "Integração",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Mobile identification as a service",
    "autor": "Rodrigues, Rafaela Cristina Riço",
    "data": "2022-03-03",
    "abstract": "The benefits of using mobile identification applications as substitutes for physical documents are obvious, whether\nthese are university student cards, company employee identification cards, the citizen card or driving license.\nHowever, as these applications grow in popularity and complexity, new requirements and needs arise that\nneed to be addressed without disturbing the normal behavior of the application.\nOften the data needed to provide an authentication service is spread across multiple servers, which need to\nbe integrated. This becomes more complicated and complex when an application provides more than one form\nof authentication (a driving license and a student card require data provided by different services).\nIn this dissertation we are going to look for solutions that allow to develop an architecture that is prepared\nto integrate new services at runtime and allows the management of the system, maintaining its dynamic and\nindependence from third parties, regardless of the technology and form of communication used by them.\nSo, this dissertation presents the state of the art regarding the integration of multiple service providers and the\ndesign and implementation a proposed solution, using the WSO2 products to do so. This process is performed\nin the context of the mobile ID, that is a implementation of a mobile driving license based on the ISO/IEC 18013-5:2021."
  },
  {
    "keywords": [
      "Health Information Systems",
      "Interoperability",
      "AIDA",
      "Web Services",
      "Medical Acts",
      "ADSE",
      "Sistemas de Informação na Saúde",
      "Interoperabilidade",
      "Atos médicos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "New generation of interoperable artefacts in medical informatics",
    "autor": "Nunes, Filipe André Martins",
    "data": "2021-08-10",
    "abstract": "A healthcare institution produces huge amounts of data on a daily basis. These data come from n sources\nand therefore have m different formats. As it is evident, this heterogeneity of information sources can be a huge\nobstacle, since increases the need for this information to be available and shared among the various information\nsystems. The exchange of clinical information between Health Information System (HIS) is crucial for the effective\nprovision of care, significantly improving the performance of the institutions.\nIn the healthcare industry, the ability of different information and software systems to communicate and share\ndata, as well as use the data exchanged, is called interoperability. This knowledge can be exchanged around\nthe healthcare ecosystem thanks to the use of standards and data sharing models, regardless of the application\nbeing used. However, the lack of interoperability remains a concern. The Agency for the Integration, Diffusion\nand Archive (AIDA) proposes to achieve levels of interoperability never before implemented. For this purpose,\nweb services are used for the processing, dissemination, and archiving of clinical information.\nIn the context of this master’s dissertation, the aim is to develop and explore new information technology\nartefacts to help the administrative and accounting teams of the Hospital da Santa Casa da Misericórdia de\nVila Verde. This solution aims to fill the existing gaps between the hospital and the Assistência na Doença\naos Servidores do Estado (ADSE). The first gap occurs in the dentistry speciality, where it is not possible to\nverify the patient’s ADSE status to perform some dental medicine act under co-payment. The second gap\noccurs in the invoicing process through ADSE in real-time, in order to perform some medical appointments\nand/or acts resulting from them since the hospital’s accounting team cannot verify if a patient complies with the\nADSE requirements. The gaps identified can make ADSE refuse to reimburse the hospital for the medical acts\nperformed, which makes the administrative and accounting work unpleasant. In this situation, the hospital will\nhave to find a solution that suits the patient and the ADSE. Very often, the hospital has to bear the costs.\nIn order to overpass these problems with the validation and invoicing process of medical acts through ADSE,\nthis project consists of two web applications that enable the hospital’s information systems to interoperate with\nthe ADSE web service."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Power aware scheduler for heterogeneous environments",
    "autor": "Magalhães, João Paulo Fontoura Moutinho",
    "data": "2017",
    "abstract": "Efficient or green computing is becoming a key issue in current programming techniques,\ngoing beyond high performance computing, by simultaneously considering issues such as\nenergy or power consumption. In heterogeneous environments, where different processors\nand accelerators co-processors may coexist, there is a real opportunity to reduce the overall\nenergy consumption of the system by using scheduling decisions in run-time that can have\na good and quick response to changes in the different components.\nCurrent tools to aid the development of efficient applications lack yet these run-time facilities.\nThis motivated the development of a new framework with a power-aware scheduler\nfor heterogeneous environments, PASH-Frame, whose prototype is the key object of this\ndissertation. This work extended previous performance-based scheduling work to include\nrun-time power-aware features, adding tools to measure power consumption at each device\nand using different scheduling decisions to get the best outcome according to pre-defined\ntargets by the end user.\nTo evaluate the overall behaviour of PASH-Frame, several tests were performed: 1000\nSAXPY tasks with vector sizes varying from 16 thousand elements to 256 thousand; 200\nSGEMM tasks with matrices varying from 64 thousand elements to 16 million and finally\na test that combines the two previous ones. Results show that the scheduling algorithm\nimplemented in the framework can achieve good results in some cases, in spite of not being\nable to make some critical decisions when it comes to energy consumption reduction like\nforcing a component to idle to save energy."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Extending the BiYacc framework with ambiguous grammars",
    "autor": "Macedo, José Nuno Castro de",
    "data": "2018",
    "abstract": "Contrarily to most conventional programming languages where certain symbols are used so\nas to create non-ambiguous grammars, most recent programming languages allow ambiguity.\nThis results in the necessity for a generic parser that can deal with this ambiguity without\nloss of performance.\nCurrently, there is a GLR parser generator written in Haskell, integrated in the BiYacc\nsystem, developed by Departamento de Informática (DI), Universidade do Minho (UM), Portugal\nin collaboration with the National Institute of Informatics, Japan. In this thesis, this necessity\nfor a generic parser is attacked by developing disambiguation filters for this system which\nimprove its performance, as well as by implementing various known optimizations to this\nparser generator. Finally, performance tests are used to measure the results of the developed\nwork."
  },
  {
    "keywords": [
      "Archiving",
      "Documents",
      "Public Administration",
      "CLAV",
      "DGLAB",
      "Classification",
      "Evaluation",
      "Interface",
      "Notification",
      "Arquivo",
      "Documentos",
      "Administração Pública",
      "Classificar",
      "Avaliar",
      "Interface",
      "Notificações",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "CLAV: interface e sistema de notificações",
    "autor": "Canela, Ricardo Manuel Cerineu",
    "data": "2022-07-19",
    "abstract": "Document archiving as a fundamental resource for operational efficiency and a resource\nfor organizational and social memory has several inherent implications. In addition to the\nhigh number of documents produced and the consequent accumulation of paper, the greatest\ndifficulty of this process is in the classification and determination of the final destination of\nthe documents. The analysis based on the semantics of the document is a complicated process,\nleading to the elimination of important evidences and case-by-case unnecessary conservation.\nThe Portuguese public administration with the intent of bridging semantic interoperability\nhas implemented policies that allows uniform representations, recognized by all parties involved\nin any process, regardless of the respective area of intervention. In conjunction with technological development, the CLAV initiative appears, promoting measures for de-materialization of\nprocesses and the consequent adoption of a electronic document management system. This\nproject was born out of an initiative by DGLAB. With the objective of taking advantage of\nthe standardization of public entities functions, creating a link between the entities and the\narchivist, aiming to classify and evaluate documentation efficiently.\nThis dissertation has as main objectives the reformulation of the user interface, in a clear,\nconcise, familiar and efficient way, both in use and development, and the creation of a\nnotification system that allows different types of users to monitor the status of the document\nclassification and evaluation process."
  },
  {
    "keywords": [
      "Sensores biomédicos",
      "Monitorização biomédica",
      "Simple network management protocol",
      "Management information base",
      "Health sensors",
      "Health monitoring",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Monitorização de dados sensoriais biomédicos através de SNMP",
    "autor": "Antão, Sónia Carolina de Carvalho Silva",
    "data": "2016",
    "abstract": "A população mundial está a envelhecer, sendo que, na Europa, 5% da população tem\nmais de 80 anos e estima-se que este número venha a triplicar nos próximos 20 anos. Esta\nevolução traz consigo um novo conjunto de desafios sociais e económicos, nomeadamente no\nâmbito da prestação de cuidados médicos.\nUma das vertentes mais importantes da prestação de cuidados médicos é a\nmonitorização de pacientes. Os sensores biomédicos atuais são dispendiosos e funcionam\ncom sistemas computacionais protegidos pelos fabricantes, com tecnologias próprias não\npartilhadas. Assim, torna-se útil a definição dum novo paradigma capaz de diminuir\nrelevantemente os custos de aquisição, instalação e manutenção desses sistemas de\nmonitorização.\nNesta dissertação apresenta-se um modelo genérico de monitorização biomédica\ncapaz de ser implementado em unidades hospitalares, visando a recolha de dados sensoriais\nbiomédicos, o seu pré-processamento e eventual integração numa base de dados global (numa\ncloud local, por exemplo). A arquitetura permite a monitorização automática de pacientes de\nvariados serviços de saúde com a menor intervenção humana possível, independentemente do\ntipo de sensor utilizado, com baixo custo de implementação e de aplicação universal.\nO sistema desenvolvido utiliza mecanismos normalizados para a representação da\ninformação a monitorizar assim como para a comunicação entre as entidades da arquitetura, e\nque são baseados nas tecnologias amplamente utilizadas para a gestão de redes Internet.\nNomeadamente, foram criadas definições para novas bases de dados específicas para\nmonitorização e configuração de sensores biomédicos utilizando o paradigma das\nManagement Information Bases. Além disso, o protocolo de comunicação entre as entidades\nda arquitetura proposta é o Simple Network Management Protocol (SNMP).\nComo prova de conceito foi implementado, com sucesso, um protótipo que ilustra a\narquitetura proposta, incluindo o hardware dum sensor biomédico básico de baixo custo e o\nsoftware dum agente SNMP e duma simples aplicação biomédica capaz gerar alertas em\nsituações clinicamente pre-definidas por uma equipa médica."
  },
  {
    "keywords": [
      "PVSio-Web",
      "Prototyping",
      "User interfaces",
      "Usability",
      "Prototipagem",
      "Interface",
      "Usabilidade",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "A library of user interface widgets prototypes for car dashboards",
    "autor": "Pacheco, Henrique Jorge Caldas",
    "data": "2017",
    "abstract": "Ensuring the good usability and user experience of software systems is invaluable, and for\nthat, following a standardized usability engineering process is fundamental. Prototyping\nplays a crucial role in this process, enabling the proper validation of the usability guidelines\nbefore reaching the actual implementation phase. This dissertation focuses on the construction\nof a JavaScript widgets library to ease the process of prototyping user interfaces. This\nlibrary will later be incorporated in the prototyping software tool PVSio-Web."
  },
  {
    "keywords": [
      "Interoperabilidade",
      "Software de Integração",
      "Fluxo de Mensagens",
      "Indicadores de Desempenho",
      "Interoperability",
      "Integration Software",
      "Message Flow",
      "Performance Indicators",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Indicadores de desempenho em plataformas HL7",
    "autor": "Arieira, Bruno Manuel Borlido",
    "data": "2021-04-06",
    "abstract": "Nos dias de hoje torna-se essencial garantir a disponibilidade de toda a informação que circula em unidades na área da saúde, com a finalidade de auxiliar os profissionais de assistência médica. Por conseguinte, não deve existir qualquer tipo de barreira inerente à comunicação entre os sistemas de informação, mesmo que estes sejam distintos quanto à sua implementação. É neste contexto que surge a interoperabilidade, isto é, a capacidade de diferentes sistemas estabelecerem a comunicação e troca de dados entre si, de forma eficaz, sem esforço adicional por parte do utilizador. Este conceito é promovido pela utilização de padrões específicos para troca de informação no domínio da saúde, como por exemplo o Health Level 7 (HL7), e de sistemas de software, que proporcionam a normalização e partilha dos dados entre todos os sistemas, designados plataformas de integração. Tendo em conta o elevado volume de mensagens a que este tipo de ferramentas poderá estar exposto, é indispensável que estejam operacionais 24 horas por dia, sem que exista qualquer tipo de sobrecarga de mensagens que comprometam o seu desempenho. Surge assim o objetivo principal da presente dissertação, a aquisição de métricas relevantes que proporcionam a performance ideal dos motores de integração, promovendo um fluxo de mensagens contínuo sem qualquer tipo de quebra. O software utilizado como objeto de estudo designa-se Mirth Colmai (também denominado NextGen Connect). Embora este tipo de sistemas de integração seja amplamente utilizado nas unidades de saúde, existe pouco trabalho de investigação inerente à avaliação do desempenho de ferramentas de interoperabilidade neste domínio específico. Tendo em consideração que a comunicação entre sistemas utilizada pela maioria deste tipo de plataformas é suportada por soluções Message Orientai Middleware (MOM), foram utilizados indicadores relacionados com abordagens de filas de mensagens e com boas práticas de processamento de fluxo de dados. Deste modo, para além de se promover os casos de análise e avaliação do comportamento da plataforma conforme métricas consideradas, também permite aos utilizadores deste tipo de ferramentas de software deterem uma noção das circunstãncias onde retiram o melhor aproveitamento do seu funcionamento."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Dispositivo de realidade virtual para melhoria da marcha em pacientes com a doença de Parkinson",
    "autor": "Dias, João",
    "data": "2017",
    "abstract": "In recent years there have been many improvements to medical procedures, involving the\nuse of augmented reality technology to provide new innovative approaches to difficult tasks\nthat are often required of the patients, requiring less physical exertion from the to achieve\nthe same results or simply looking at the problem in a new perspective. Virtual reality\ntechnology has the capability of creating an interactive, motivating environment in which\npractice intensity and feedback can be manipulated to create individualised treatments to\nretrain movement.\nCurrently there is a very large amount of people suffering from minor to severe functional\nlimitations, impairments such as loss of range of motion, decreased reacting times, disordered\nmovement organisation, and impaired force generation create deficits in motor control\nthat effect the personss capacity for independent living and economic self-sufficiency.\nThe use of augmented reality is starting to be used in more medical scenario’s and in the\ntreatment of many diseases generally co-related with motor difficulties or recovery treatments.\nOne of the diseases that has been looked more prominently for augmented reality development\nis the Parkinson’s disease which causes its patients to suffer severe gait constriction\nand whose generalised gait treatments didn’t produce a significant improvement in the patients\ngait without the use of heavy medication.\nOne other important detail to take notice is that the Parkinsons disease causes the patient\nto abruptly enter a freezing state without any kind of warning which can lead the patient\nto fall and severally harm itself depending on the situation at hand.\nThe objective of this thesis is to explore the possibilities of the use of augmented reality in\nan attempt to improve gait in patients suffering from Parkinson’s disease. For this purpose\nmany augmented reality glasses were analysed selecting the best one in terms of affordability,\ncomfort and utility. The application developed has the objective of improving the\npatients gait by displaying an augmented reality supper- imposed path for the patient to\nfollow matching auditory cues with each of the patients steps and also helping the patient\nof he suddenly finds himself affected by a ”freezing” episode."
  },
  {
    "keywords": [
      "IT infrastructure monitoring",
      "Microservices",
      "Healthcare",
      "Backend architecture",
      "Containerization",
      "Monitorização de infraestruturas IT",
      "Micro-serviços",
      "Ambiente hospitalar",
      "Arquitetura de backend",
      "Containerização",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Monitoring architecture for services and servers in healthcare environment",
    "autor": "Ramos, Vasco António Lopes",
    "data": "2022-12-13",
    "abstract": "Information systems are continuously evolving in nature and complexity. Infrastructure concerns such as availability, efficiency, and disaster recovery have been some of the most important drivers regarding how infrastructure is planned and executed. As these mechanisms evolved, so did the underlying foundation for their functioning — Information Technology (IT) infrastructure monitoring. Inside the healthcare environment, it is important to discuss IT infrastructure monitoring and disaster prevention and recovery since availability and communication are vital for the proper functioning of healthcare units, whether acting in isolation or on a network. When acting on a network, it is especially important to be able to easily monitor and observe each unit from a single point of access so that actions can be swiftly taken when there is a problem. Considering the wide range of available solutions and heterogeneous nature of IT infrastructure, even within the healthcare industry, the majority of the solutions either focus too much on a particular problem of some industry, healthcare or not, or are too generic and can’t fulfill the needs of an increasingly connected and interdependent healthcare industry. This Dissertation proposes a web and microservices-based IT infrastructure monitoring backend solution with a multi-site and multi-organization scheme at its core that is designed to be scalable, easily deployed and integrated with existing tools, and simple to further extend and improve. This solution has two main components, one server which is the central point of the solution, the guardian server, and the other one, which is the local client to be installed on each organization’s infrastructure. The produced backend solution was tested and validated in two healthcare organizations which provided useful feedback and a positive answer to the usefulness of a monitoring solution, such as the one developed in this Dissertation, in improving the efficiency and reliability of the organizations’ IT infrastructure and, therefore, their healthcare services. A formal evaluation of the solution was also carried out with a combination of a Strengths, Weaknesses, Opportunities and Threats (SWOT) analysis and a risk assessment report, both mechanisms providing useful insights on the strengths and limitations of the solution, as well as possible improvement points."
  },
  {
    "keywords": [
      "P2P",
      "Monitorização de redes",
      "Network monitoring",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Um sistema P2P para detecção de anomalias de rede",
    "autor": "Mendonça, Ricardo César Cangueiro",
    "data": "2020-06-16",
    "abstract": "Precise and efficient monitoring is vital to ensure that a network works according to the\nintended behavior, as well as quickly acting to the problems found. The task of monitoring\na network becomes complex with increasing network size and heterogeneity. The available\nnetwork monitoring and management solutions are not only costly but also difficult to use,\nconfigure and maintain.\nThis work aims to continue the development of a P2P system to detect network anomalies\nto help network administrators. The system should be easily used by ISP network\nadministrators.\nInitially, state-of-the-art research will be presented, where various technologies related to\nthe objective of the dissertation will be highlighted. Multiple requirements of monitorization\nsystems will be exposed and there will be a description of the applicability of a P2P\nsystem in a monitorization system.\nAfterward, all the processes of creation of the system and the existing entities will be\ndescribed as well as their communication capabilities. All the ways an administrator can\ninteract with the system will also be presented. Then, the process of definition of a metalanguage\nwill be exposed, to allow the administrators to configure and pre-program the\nnetwork in an effective and varied manner. The process of optimization of the monitoring\nsystem will be described, to reduce the traffic of the P2P network. It will be also described\nhow the system became fault-tolerant, recovering its state if any entity has a problem.\nSubsequently, the implementation of each one of the developed mechanisms and the architecture\nof the system will be exposed, where it’s explained which technologies were used\nand justified some paths chosen to achieve the dissertation objectives.\nThe system will be tested in a network emulator and the resulting data from the created\nmechanisms will be analyzed to validate the correct behavior of what was developed."
  },
  {
    "keywords": [
      "Reinforcement learning",
      "Distributed databases",
      "Middleware",
      "Optimization",
      "Machine learning",
      "Aprendizagem por reforço",
      "Bases de dados distribuídas",
      "Otimização",
      "Aprendizagem máquina",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automatic parameter tuning using reinforcement learning",
    "autor": "Ferreira, Luís Manuel Meruje",
    "data": "2020-12-10",
    "abstract": "Every major Database Management System (DBMS) and most components in a distributed\nsystem in use today, closed or open source, comprise a set of configuration parameters which\nhave substantial influence over the performance of the system. The correct configuration and\ntuning of these parameters often leads to a performance level that is orders of magnitude\ngreater than that achieved by default configurations.\nThe number of parameters tends to increase as new versions are released. Moreover, the\noptimal values for these parameters vary with the environment, namely the workload to\nwhich the system is being subjected to, and the physical characteristics of the hardware it is\nrunning on.\nIt is common to delegate the responsibility of parameter tuning to a system administrator.\nThe problem with this approach is that it requires both extensive prior experience with\nthe specific system and workload at hand, and a large amount of the administrator’s time.\nMoreover, variables may establish extensive and non-trivial correlations between them that\nare very difficult to identify and tune.\nThis dissertation introduces an automated and dynamic approach to parameter tuning\nusing a reinforcement learning approach, while also adopting the use of deep neural\nnetworks to tackle the fact that complex relations between variables may exist.\nTwo use cases were implemented to showcase our approach, in the context of a distributed\ndatabase. One where we adjust tuning variables specific to each replica and another where\nwe adjust the shard configuration of the cluster (i.e. what shard is allocated to what replica).\nThe reinforcement learning agents act at the middleware level, where all replication logic is\nheld. The performance was measured in terms of the reward achieved by those agents as\nwell as the values for the individual performance metrics that make up that reward. For\nthe use case that concerns individual replica configurations, a maximum gain in reward of\n105.41% was observed in one of the replicas as well as a maximum gain of 484.31% in one of\nthe individual performance metrics. In the second scenario, of shard reallocation, we saw\nimprovements in reward value up to 28.72% and of up to 69.92% for individual metrics."
  },
  {
    "keywords": [
      "LiDAR",
      "Deep learning",
      "FPGA",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Real-time implementation of 3D LiDAR point cloud semantic segmentation in an FPGA",
    "autor": "Delgado, Pedro Paulo Fontes",
    "data": "2023-01-05",
    "abstract": "In the last few years, the automotive industry has relied heavily on deep learning applications for \nperception solutions. With data-heavy sensors, such as LiDAR, becoming a standard, the task of \ndeveloping low-power and real-time applications has become increasingly more challenging. To obtain \nthe maximum computational efficiency, no longer can one focus solely on the software aspect of such \napplications, while disregarding the underlying hardware. \n In this thesis, a hardware-software co-design approach is used to implement an inference application \nleveraging the SqueezeSegV3, a LiDAR-based convolutional neural network, on the Versal ACAP VCK190\nFPGA. Automotive requirements carefully drive the development of the proposed solution, with real-time \nperformance and low power consumption being the target metrics. \n A first experiment validates the suitability of Xilinx’s Vitis-AI tool for the deployment of deep \nconvolutional neural networks on FPGAs. Both the ResNet-18 and SqueezeNet neural networks are \ndeployed to the Zynq UltraScale+ MPSoC ZCU104 and Versal ACAP VCK190 FPGAs. The results show \nthat both networks achieve far more than the real-time requirements while consuming low power. \nCompared to an NVIDIA RTX 3090 GPU, the performance per watt during both network’s inference is 12x \nand 47.8x higher and 15.1x and 26.6x higher respectively for the Zynq UltraScale+ MPSoC ZCU104 and \nthe Versal ACAP VCK190 FPGA. These results are obtained with no drop in accuracy in the quantization \nstep. \n A second experiment builds upon the results of the first by deploying a real-time application containing \nthe SqueezeSegV3 model using the Semantic-KITTI dataset. A framerate of 11 Hz is achieved with a peak \npower consumption of 78 Watts. The quantization step results in a minimal accuracy and IoU degradation \nof 0.7 and 1.5 points respectively. A smaller version of the same model is also deployed achieving a \nframerate of 19 Hz and a peak power consumption of 76 Watts. The application performs semantic \nsegmentation over all the point cloud with a field of view of 360°."
  },
  {
    "keywords": [
      "Secure multiparty computation cryptography",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Secure multiParty computation protocols",
    "autor": "Torres, Daniel Casanova Faria",
    "data": "2015-03-13",
    "abstract": "Secure Multiparty Computation (abrv. MPC) is a group of techniques that enable multiple entities to compute some function on their private inputs. Moreformally,itenablesasetofplayers{P1,...,Pn},eachofthemholding private inputs xi, where i is the index of the player, to evaluate a function f(x1,...,xn) = (y1,...,yn) so that every player Pi learns yi. Multiparty Computation has many application scenarios such as private auctioning, computation outsourcing, or private information retrieval. These and other applications make MPC a very appealing object of study, since it may be a solution to problems in different ﬁelds. The objective of this thesis is twofold. First, we aim to provide a comprehensive guide to the current state of multiparty computation protocols. Wedosobyinspectingthetwocategorieswheremostgeneralfunctionality secure function evaluation protocols are inserted - boolean or arithmetic circuit evaluation. Moreover, we show how to implement MPC protocols with current constructions, and what problems are inherently connected to the chosen representations. Second, we document some of the design choices behind a partial implementation of a concrete secure arithmetic circuit evaluation protocol - the SPDZ protocol by Damgård et al. [10]. We explain the protocol in general terms, and then go into the details of some subprotocols, namely those that were implemented."
  },
  {
    "keywords": [
      "Hypertension",
      "Diabetes",
      "Management",
      "Monitoring",
      "Application",
      "Hipertensão",
      "Diabetes",
      "Gestão",
      "Monitorização",
      "Aplicação"
    ],
    "titulo": "Aplicação mobile para gestão e telemonitorização de hipertensos e diabéticos",
    "autor": "Gomes, Pedro Miguel Queirós",
    "data": "2022-12-19",
    "abstract": "Currently, the chronic diseases of hypertension and diabetes have a huge prevalence not only in Portugal but\nalso in Europe, which leads to the need to develop mechanisms that allow greater control of the health condition of chronic patients with hypertension or diabetes. With this need to increase management control, mobile applications are presented as a solution to improve the management and monitoring of the health condition of chronically ill patients. Due to their mobility and connectivity with medical devices, they are increasingly positioned as the most complete answer that users can turn to better manage their health condition. However, existing applications are mostly manual control or are dependent on a specific medical device. It was in this context that Altice Labs proposed this dissertation.\nThus, the purpose of this dissertation is to develop a mobile application for the management and monitoring of\nhypertensive and diabetic patients, in order to allow greater efficiency in the management and telemonitoring of the chronic diseases of hypertension and diabetes. The application must be integrated with the SmartAL platform by Altice Labs and allow the user to easily and intuitively record vital signs and daily activity, as well as food. After carrying out studies on hypertension and diabetes and carrying out an extensive analysis of the market for mobile applications, the application was developed according to the proposed objectives, having been validated through a pilot test with an end user. In the conclusion, a reflection is carried out on the work carried out, the results obtained and the future work."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Realidade aumentada em aplicações móveis para planeamento cirúrgico ortopédico",
    "autor": "Inácio, José Alberto Mestre Conceição",
    "data": "2017",
    "abstract": "Cada vez mais as necessidades e os requisitos do paciente acompanham os desenvolvimentos tecnológicos na área da cirurgia médica. Tal acontece para que estes obtenham uma intervenção o mais eficiente e seguro possível por parte dos serviços de saúde e dos seus profissionais. No entanto, hoje em dia ainda é difícil implementar e operar diferentes tipos de tecnologias em ambientes médicos devido às desvantagens que estes podem trazer para os seus utilizadores e por todo o processo de aprendizagem que estas requerem. Numa primeira abordagem, este trabalho tem como objetivo esclarecer conceitos e reunir algumas soluções existentes para a resolução destes problemas, assim como as respetivas tecnologias utilizadas pelas mesmas. Posteriormente é elaborado e apresentado um conceito e protótipo de uma aplicação móvel de planeamento cirúrgico ortopédico que implementa tecnologias de Realidade Aumentada. A solução proposta pretende ajudar o cirurgião desde a fase de planeamento até a própria fase de intervenção cirúrgica. Para além de alguns exemplos e da apresentação do trabalho realizado para a solução, é também descrito o processo de implementação e a arquitetura do sistema. Tendo em conta o protótipo desenvolvido, são discutidas as vantagens da sua utilização num contexto cirúrgico e levantados alguns pontos de interesse futuro a serem estudados e implementados para a sua melhoria."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Plataforma autónoma de recolha e análise de infraestruturas tecnológicas",
    "autor": "Mota, Nelson Duarte Cardoso da",
    "data": "2018-12-12",
    "abstract": "The growth of technological infrastructures in companies and institutions, along with the\nlack of specialized human resources to manage them effectively, sometimes creates misinformation\nabout the actual status of these infrastructures, making them unstable or even\nunsafe.\nIn that context, the need arises to create an autonomous and analytical information collection\nsystem. This system should be flexible and extensible enough to adapt to the various\nconfigurations of equipments and software available, capable to produce a report with as\nmuch information as possible, as well as some recommendations to improve infraestructure\noverall status, using only open source technology.\nThis dissertation arises from this need and aims to design and implement this platform."
  },
  {
    "keywords": [
      "Software engineering",
      "Web application",
      "Clinic software",
      "Software development",
      "Engenharia de software",
      "Aplicação web",
      "Software para clínicas",
      "Desenvolvimento de software",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of a web clinical management application",
    "autor": "Cerqueira, Rúben Correia",
    "data": "2023-10-09",
    "abstract": "The time of doing all the work manually is passing by as the influence of developing\ntechnology is increasing. Most of the tasks done in a business can be automated by software.\nBecause of that, the growing demand for this kind of technology is making IT companies\ndevelop any software that is required.\nThis report - the dissertation that describes a thesis in Informatics Engineering - covers\nsome of these technologies, focusing on the clinic area. The development of a clinic web\napplication was proposed by Wintouch to help clinic businesses boost their productivity and\norganization. This Master’s work, herein reported, began with the research of the state of the\nart, studying what the market is like, and analyzing what are the drawbacks of the existing\nsimilar applications. The lessons learned at that stage were relevant to design a new web\napplication that can stand out above competitors. The design of the application’s architecture\nis discussed below along with the technologies used to best fit the application to reach the\nobjectives proposed and meet the desired requirements. The report presents a detailed\naccount of the outcomes of the development process, encompassing both backend and\nfrontend implementations. Notable features and functionalities are thoroughly documented,\nalongside a reflection of the challenges encountered during the development journey."
  },
  {
    "keywords": [
      "Arquitetura de aplicações",
      "Assistentes digitais",
      "Dispositivos IoT",
      "Google assistant",
      "Applications architecture",
      "Digital assistants",
      "IoT devices",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Gestão e controlo de dispositivos IoT através da interação com assistentes digitais",
    "autor": "Peixoto, Júlio Dinis Sá",
    "data": "2019-12-23",
    "abstract": "A crescente adoção de assistentes digitais faz com que os casos de uso para os quais estes \nsão utilizados sejam cada vez mais abrangentes. Isto aliado ao facto dos dispositivos IoT estarem cada vez mais acessíveis, leva a que comece a ser comum os utilizadores controlarem\ndiversos dos seus dispositivos através dos assistentes digitais.\nO presente documento retrata a dissertação com o tema Gestão e controlo de dispositivos IoT através a interação com assistentes digitais. O principal foco desta passa pela investigação\nda gestão de múltiplos dispositivos de IoT, inseridos no mesmo ambiente e que possam \nser geridos/controlados através de uma plataforma de interação por voz, neste caso um \nassistente digital.\nEmbora já existam soluções disponíveis para desenvolvimento de aplicações neste sentido, estas são muito recentes carecendo tanto ao nível do número de funcionalidades como \nda flexibilidade oferecida para sua utilização.\nEsta investigação resulta numa proposta de arquitetura para aplicações deste domínio e\nda implementação de um sistema que permita fazer a gestão e controlo de um conjunto de dispositivos inteligentes inseridos no contexto de uma smart home. A arquitetura deste\nsistema e das aplicações que o constituem dever ao possibilitar a inclusão de novas funcionalidades inexistentes nos assistentes digitais atuais e a implementação de algumas destas \nfuncionalidades devera ser apresentada como casos de estudo ao longo do presente documento."
  },
  {
    "keywords": [
      "Arquitetura de microsserviços",
      "Aplicações baseadas em microsserviços",
      "Decomposição de monólitos",
      "Java",
      "Refactoring",
      "Microservice architecture",
      "Microservice-based applications",
      "Monolithic decomposition",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Refactoring Java monoliths into executable microservice-based applications",
    "autor": "Freitas, Francisco José Oliveira",
    "data": "2022-05-17",
    "abstract": "Nos últimos anos assistiu-se a uma mudança drástica na forma como o software é desenvolvido. Os\nprojectos de software de grande escala estão a ser construídos através de uma composição flexível de\npequenos componentes possivelmente escritos em diferentes linguagens de programação e com os processos de deploy independentes – as chamadas aplicações baseadas em microsserviços. Isto tem sido\nmotivado pelos desafios associados ao desenvolvimento, manutenção e evolução de grandes sistemas\nde software, mas também pelo aparecimento da cloud e pela facilidade que trouxe em termos de escalabilidade horizontal, reutilização e flexibilidade na propriedade e no deploy. O crescimento dramático da\npopularidade dos microsserviços levou várias empresas a aplicar grandes refactorings aos seus sistemas\nde software. Contudo, esta é uma tarefa desafiante que pode demorar vários meses ou mesmo anos.\nEsta dissertação propõe uma metodologia capaz de transformar automaticamente aplicações desenvolvidas em Java sob uma arquitetura monolítica em aplicações baseadas em microserviços. A metodologia proposta é direccionada para as aplicações que tiram partido da técnica ORM para relacionar classes\ncom as entidades da base de dados, através de anotações no código fonte. A nossa abordagem recebe\ncomo input o código fonte e uma proposta de microsserviços, e aplica técnicas de refactoring às classes\npara tornar cada microsserviço independente. Esta metodologia cria uma API para cada chamada de métodos de classes que se encontram noutros serviços, e as entidades da base de dados também sofrem\nrefactoring para serem incluídas no serviço correspondente. A metodologia proposta foi implementada\natravés da construção de uma ferramenta que suporta o refactoring de aplicações desenvolvidas em Java\nSpring e que utilizam as anotações da JPA para o mapeamento entre as classes e as entidades.\nRealizou-se um análise quantitativa e qualitativa em 120 projetos open-source aleatoriamente recolhidos do GitHub. Na avaliação quantitativa procurou-se perceber a aplicabilidade da metodologia e na\nanalise qualitativa, através da execução de testes unitários, procurou-se avaliar se aplicação original e a\naplicação baseada em microserviços gerada são funcionalmente equivalentes.\nOs resultados são promissores sendo a metodologia capaz de realizar o refactoring em 69% dos\nprojetos, sendo o resultado da execução dos testes unitários igual em ambas as versões dos projetos."
  },
  {
    "keywords": [
      "Gamificação",
      "Plataformas educacionais",
      "Sistemas de avaliação de conhecimento",
      "Técnicas",
      "Mecânicas e dinâmicas de jogo",
      "Aprendizagem",
      "Motivação",
      "Gamification",
      "Educational platforms",
      "Knowledge assessment systems",
      "Techniques",
      "Mechanics and game dynamics",
      "Learning",
      "Motivation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aplicação de técnicas de gamificação em sistemas educacionais",
    "autor": "Cunha, Miguel Afonso Machado da",
    "data": "2021",
    "abstract": "Em contexto educativo e numa sociedade em permanente mudança, há uma necessidade premente de adaptar\nas metodologias pedagógicas, no sentido de adequar os processos de ensino e de aprendizagem às características\ndos alunos, implementando diferentes abordagens, que permitam captar a evolução do aluno e incrementar\na sua atenção, motivação e empenho nas matérias a estudar, como forma de promover o sucesso da aprendizagem.\nDurante os últimos anos várias iniciativas de investigação e de aplicação têm sido desenvolvidas com o\nobjetivo de integrar técnicas e modelos de Gamificação no domínio dos sistemas de ensino e de aprendizagem,\ncom vista ao desenvolvimento da motivação e desenvolvimento dos alunos nas mais variadas áreas do conhecimento.\nNesta dissertação demonstramos a aplicação de técnicas de Gamificação em sistemas educacionais,\natravés da incorporação de elementos de jogos nas suas várias vertentes, convencidos que estas permitem contribuir\npositivamente para o desenvolvimento dos processos de aprendizagem e, em particular, para o aumento\nda concentração e interesse dos alunos nesses sistemas. Para esse fim, utilizámos um sistema específico de\navaliação de conhecimento, com o objetivo de combater a diminuição da motivação e potenciar o uso e a exploração\ndesse sistema. Tendo isso presente, começamos por analisar os benefícios que a Gamificação pode\nter em contextos educativos, analisando aspetos que permitam aumentar a motivação dos alunos, melhorar o\nseu processo de aquisição de conhecimento e, consequentemente, o seu sucesso académico. Posteriormente,\nconcebemos um modelo de gamificação específico e fizemos a sua implementação no sistema de avaliação\nreferido recorrendo a linguagens como a Python, a JavaScript e a HTML, entre outras."
  },
  {
    "keywords": [
      "Microserviços",
      "Cliente-servidor",
      "Machine learning",
      "Detetação de fraudes",
      "Classificação",
      "Microservices",
      "Monolithic",
      "Fraud detection",
      "Classification",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of a fraud detection microservice platform",
    "autor": "Afonso, Carlos Manuel Marques",
    "data": "2022-12-13",
    "abstract": "O mundo ciber-físico deixou de optar por verificações manuais e promoveu a adoção de\nsistemas mais eficientes e fiavéis para detetar transações fraudulentas. Estes sistemas visam\notimizar e também melhorar a forma como estas transações são validadas.\nPara atingir estes objetivos, foram criados ou adaptados modelos de aprendizagem\nautomática para realizar estas tarefas. São cuidadosamente testados e desenvolvidos para\natender às necessidades dos utilizadores para garantir que não se envolvem em negócios\nfraudulentos e para evitar tentativas maliciosas de roubar ou fazer qualquer dano ao\nutilizador final.\nNos últimos anos, o DTx tem vindo a desenvolver um sistema capaz de hospedar este tipo\nde algoritmos e disponibilizá-los para sistemas de produções em ambientes ciber-físicos.\nNo início deste trabalho, a DTx propôs conceber e criar uma plataforma que pudesse ser\nimplementada num ambiente em nuvem e também capaz de acolher um módulo de IA que\nesteja qualificado para prever entradas de Churn em extratos de telecomunicações.\nNesta dissertação, o grande objetivo foi criar uma plataforma baseada numa arquitetura\nde microserviços, de forma a fornecer uma solução para os requisitos especificados pelo\nDTx e torná-la uma solução simples, mas eficiente.\nDe forma abrangente, esta dissertação começa por expor um estudo profundo sobre\no estado atual da arte dos sistemas ciber-físicos, ambientes em nuvem, algoritmos de\naprendizagem automática e plataformas que podem acolher este tipo de sistemas. Em\nseguida, apresenta-se então as especificações do sistema, a forma como foi implementado,\nos seus diversos serviços e, finalmente, uma análise dos resultados onde é possível ver que\nmaior parte dos requisitos foram atingidos."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Platooning Simulation in ITS Communications",
    "autor": "Ribeiro, Bruno Daniel Mestre Viana",
    "data": "2016-10",
    "abstract": "Vehicular Ad Hoc Networks (VANETs) is a term used to describe networks\nof moving vehicles equipped with devices that allow spontaneous communication\nwith other vehicles and infrastructures. Developing collaborative\ndriving applications for VANETs is currently a hot topic and has an increasing\npopularity in the Intelligent Transportation Systems (ITS) domain. The\ngoal of this thesis is to study the development and testing of advanced ITS\napplications, using Platooning as a use case. It presents a state of the art\non typical ITS applications, its evaluation and corresponding implementation\nand testing methods. The Platooning Management Protocol (PMP)\nwas then implemented and tested by means of simulation, resorting to the\nV2X Simulation Runtime Infrastructure (VSimRTI) framework, which couples\nSimulation of Urban MObility (SUMO) and Network Simulator 3 (ns-3).\nResults show that it is able to work in a smooth and efficient manner: the maneuvers\nhappen during an acceptable interval, the proposed communication\nrequirements are met and the lane capacity is increased."
  },
  {
    "keywords": [
      "Network security",
      "Anomaly detection",
      "Deep Learning",
      "Generative adversarial networks",
      "Segurança das redes de computadores",
      "Deteção de anomalias",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Network anomaly detection using adversarial Deep Learning",
    "autor": "Valente, Maria Elisa Maciel",
    "data": "2021-04-06",
    "abstract": "Computer networks security is becoming an important and challenging topic. In particular, one\ncurrently witnesses increasingly complex attacks which are also bound to become more and more\nsophisticated with the advent of artificial intelligence technologies.\nIntrusion detection systems are a crucial component in network security. However, the limited\nnumber of publicly available network datasets and their poor traffic variety and attack diversity are a\nmajor stumbling block in the proper development of these systems.\nIn order to overcome such difficulties and therefore maximise the detection of anomalies in the\nnetwork, it is proposed the use of Adversarial Deep Learning techniques to increase the amount and\nvariety of existing data and, simultaneously, to improve the learning ability of the classification models\nused for anomaly detection.\nThis master’s dissertation main goal is the development of a system that proves capable of improving the detection of anomalies in the network through the use of Adversarial Deep Learning techniques,\nin particular, Generative Adversarial Networks. With this in mind, firstly, a state-of-the-art analysis and\na review of existing solutions were addressed. Subsequently, efforts were made to build a modular solution to learn from imbalanced datasets with applications not only in the field of anomaly detection in\nthe network, but also in all areas affected by imbalanced data problems. Finally, it was demonstrated\nthe feasibility of the developed system with its application to a network flow dataset."
  },
  {
    "keywords": [
      "Mobile application",
      "Music",
      "Remote server",
      "Streaming",
      "Aplicação móvel",
      "Música",
      "Servidor privado",
      "Streaming",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Serviço de acesso remoto a coleções musicais em servidores privados",
    "autor": "Oliveira, Jorge Miguel da Silva",
    "data": "2021-12-03",
    "abstract": "Streaming content has completely changed the way we consume it. The arising of the\nInternet, and streaming services, has allowed people to access a big collection of music\nwithout buying it, just by subscribing to a service or to use it for free with the consumption\nof publicity.\nBut, and for those who have a local collection? They can’t use these platforms unless the\nservice allows the user to buy the content, store it in a local environment, play it there, and\nthe user could play it remote by streaming.\nThe use of clouds appears to be a good solution for this group of people. Unfortenelly,\nwhen the collection is really big, the additional costs of storage of the cloud could be a big\nproblem. In addition, normally, the user interface of these services isn’t enjoyable, i.e., isn’t\nuser-friendly.\nSo, a hybrid system could be the ideal, a streaming service with cloud services that allow\nstoring the private collection. This is the best for the user but will force them to store all the\ncollection or at minimum a part of the collection which they want to access, in an external\nsource, with a strict organizational structure. For a new user, this couldn’t be a problem but\nis for a user who already has a big collection.\nThis dissertation has proposed a remote access service to these private collections so that\nwith just a simple gadget (smartphone, pc, ...) the user can access the content stored on their\nprivate server anywhere on the planet, without change the original location of the collection.\nThis service was built using only normalized and open-source technologies.\nBased on the proposed architecture, was also developed a prototype with the main\nfunction of the system, like the possibility to play music from the private collection on\na smartphone Android. The conclusion of the tests made to the prototype was that this\nalternative solution could be very good for the people who want that their collection remains\nin one place, and, at the same time, can play it remotely."
  },
  {
    "keywords": [
      "Advanced driver assistance systems",
      "Smartphones",
      "Inertial sensors",
      "Vertical acceleration",
      "Correlation coefficient",
      "Dynamic time warping",
      "Sistemas avançados de assistência ao condutor (Advanced driver assistance systems)",
      "Sensores inerciais",
      "Aceleração vertical",
      "Coeficiente de correlação",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "A feasibility study on the use of smartphone sensors for development of Advanced Driver Assistance Systems",
    "autor": "Santos, Nuno Miguel Teixeira dos",
    "data": "2017",
    "abstract": "Technological evolution is impacting several industries, e.g., by allowing them to deliver higher levels\nof functionality. The automotive industry is an example of how technology is supporting the development\nof new solutions in vehicle safety and comfort.\nAdvanced Driver Assistance Systems (ADAS) are cases of solutions that evolved significantly in\nrecent years. This is possible not only due to the progress of electronic solutions but also because\nof higher quality in software. The smartphone is an example of this evolution with a broad range\nof applicability since these devices have been used to develop ADAS, making them an interesting\ncost-effective platform to develop such systems.\nPrevious research has shown smartphones’ ability to output sensors data with the necessary quality\nfor a broad number of applications with special focus in inertial sensors. However, such studies\ntend to be difficult to reproduce or lack the desired detail levels of their experimental methods. Concerns\nabout how good are smartphone sensors and their use to develop ADAS emerge when reading\nexisting literature, particularly, how the context of collecting data is controlled and which variables\nimpact the collection process.\nIn order to assess the feasibility of using smartphones as sensing devices, questions arise on how\ndifferent parts of the collection setup affect the quality of data collected. Motivated by those questions,\na study considering four different hypotheses is proposed to assess the impact of a controlled set of\nvariables, namely: brands of inertial sensors, car mounts, sensor sampling rates, and vehicles. A set\nof controlled experiments is performed to assess the impact of each variable in the collection process\nof inertial sensors, more precisely the vertical acceleration.\nTo perform the experiments, three special-purpose tools were developed. Smartphones used in\nthe experiments feature an application to collect and export their sensors data. A researcher of an\nexperiment operates another smartphone application to annotate road anomalies found while driving.\nA desktop application automates the computation and statistical validation of the vertical acceleration\ncorrelation from different setups.\nDynamic Time Warping was used to compute the correlation coefficient of vertical acceleration\nas measured by different devices. Results show a baseline correlation coefficient of 0.892 with a\nstandard configuration of software and hardware. When one of the independent variables is changed,\nthe resulting coefficients range from 0.827 to 0.848.\nRandomization tests were executed to statistically validate experiments results, making use of a\nRandom Shuffle algorithm on surrogate data. Such tests rejected all four proposed null hypotheses\nregarding dissimilarities on vertical acceleration sensed by different setups.\nFrom the controlled experiment a deeper understanding of the variables influencing data collection\nwith smartphones was obtained. Results showed that varying the inertial sensors, car mounts, rates\nof sampling, or vehicles had a low impact on vertical acceleration sensed by smartphones. This is\na good indicator that smartphones can be used to develop ADAS without the need to standardize\nevery part of the collection setup. Thus, it possible to foresee the deployment of a system to a wider\naudience by taking advantage of existing equipment."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Correct translation of imperative programs to single assignment form",
    "autor": "Azevedo, Marta Vasconcelos Castro",
    "data": "2017",
    "abstract": "A common practice in compiler design is to have an intermediate representation of the source code\nin Static Single-Assignment (SSA) form in order to simplify the code optimization process and make\nit more efficient. Generally, one says that an imperative program is in SSA form if each variable is\nassigned exactly once.\nIn this thesis we study the central ideas of SSA-programs in the context of a simple imperative language\nincluding jump instructions. The focus of this work is the proof of correctness of a translation\nfrom programs of the source imperative language into the SSA format. In particular, we formally introduce\nthe syntax and the semantics of the source imperative language (GL) and the SSA language; we\ndefine and implement a function that translates from source imperative programs into SSA-programs;\nwe develop an alternative operational semantics, in order to be able to relate the execution of a source\nprogram and of its SSA translation; we prove soundness and completeness results for the translation,\nrelatively to the alternative operational semantics, and from these results we prove correctness of the\ntranslation relatively to the initial small-step semantics."
  },
  {
    "keywords": [
      "ROC curves",
      "Statistic",
      "Software",
      "R",
      "Shiny",
      "Package",
      "Checklist",
      "Curvas ROC",
      "Estatística",
      "Software",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Análise através da curva ROC: que ferramentas utilizar?",
    "autor": "Quintas, José Pedro dos Santos",
    "data": "2020-05-11",
    "abstract": "ROC (Receiver Operating Characteristic) curve is a statistic tool that allows the evaluation\nof the accuracy of a classification system. These curves are drawn on a two-dimensional\ngraph, with the ordinate representing the true positive fraction or sensitivity and the abscissa\nrepresenting the false positive fraction or 1-specificity. The index that evaluates the accuracy\nof these graphs is represented by the area under the curve (AUC) where the larger that area\nis the bigger the test performance is.\nIts first appearance dates to the year of 1950. Nevertheless, computationally , the first\nsoftware only appeared around 1993 and since then several tools have been made available\nfor its analysis. Regarding the theoretical part of the subject, there is a vast bibliography\nexisting which introduces all the necessary concepts to analyze a ROC curve visually\nand statistically. However, only a few of those documents discuss the evaluation and the\ncomparison of software that attain these same curves, consisting of old works in which\nthe vast majority corresponds to software that when compared to the current scenario are\noutdated or fell out of use.\nThe R software environment with a programming language mainly for statistical use is\ncurrently one of the best tools to perform the ROC analysis. The variety of packages in this\nwork environment make it an interesting study product, which allows us to take advantage\nof the different features in different the packages or enjoy the same features but by different\nmeans and formats. Like R there are several tools that can perform this same analysis, as is\nthe case of STATA software, which receives regular updates that have been improving this\ntool recurrently. With the versatility of allowing us to work from a command line or through\nmenus predefined by the software itself, it makes it a very accessible and convenient tool to\nexplore.\nThe R language is also related to the package called shiny, which can create browser\napplications through its own commands, making it possible to transpose the different\ncommands of packages R into a single application. Due to the wide variety of ROC packages\nin R, it is interesting to link them to shiny. Therefore, a library in the application format was\ndesigned to group the different packages on the same browser page. The result of this is\nROSY application available on https://pquintasbcl.shinyapps.io/ROSY/.\nDue to the increasing use of ROC analysis in different systems, it is essential to explore\nthe best computational methods to process it in a correct way. Therefore, in this work the\nresearch and selection of different software/tools to perform this type of analysis is done,\nbased on the different existing bibliographic documents in order to compare them and create a checklist, which will allow us to visualize the fundamental characteristics present in each\nsoftware analyzed."
  },
  {
    "keywords": [
      "681.324"
    ],
    "titulo": "Analysis of user profile in social networks",
    "autor": "Rodrigues, Adão Carlos Fernandes",
    "data": "2013-12-16",
    "abstract": "With this work it is intended to create / identify user profiles through their actions on social networks.\nThis identification is to determine, in a specific way, which profile each user has, linking between the following dimensions and their sets of variables: sociodemographic characteristics (gender, age, education, situation before the economic activity indicator and occupational class) the specific type of aggregate practices conducted over the internet (study, work, services, search for information, communication and entertainment), the context of use of social networks (home, school, workplace or other), frequency of use (daily, frequent or sporadic) and, finally, the range of motivations of users of social networks (professional, informational, recreational or other).\nAfter a careful analysis of these dimensions, we are able to separate the different types of users use only analyzing their sets of variables that are associated with each other. This analysis also allows to deepen knowledge about the various uses of social networks, and may also be useful to the market in that it provides substantive information concerning the forms of articulation between the social characteristics of users and their activities, schemes and contexts of use."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Business intelligence: indicadores de desempenho num bloco operatório",
    "autor": "Leal, Sara Rafael Marinho",
    "data": "2017",
    "abstract": "A predominância de um mundo orientado pelas Tecnologias de Informação (TI) é uma\nnoção que está bem presente no quotidiano, desde a mais ínfima tarefa rotineira ao dia-adia\nnum posto de trabalho. De todas as atividades onde as TI incidem, destaca-se neste\ntrabalho a aplicação das mesmas numa área cujo foco é de extrema importância – a saúde.\nO setor da saúde, tal como outras áreas organizacionais, não é uma exceção à influência\ndas tecnologias e tornou-se uma área que integra a informação como um bem fundamental\npara seu o bom funcionamento.\nO crescente aumento do volume de dados de registos eletrónicos e de fontes diversas\ncom que os profissionais de saúde lidam todos os dias originou uma nova necessidade –\na transformação desses dados em informação para extração de conhecimento. Consequentemente,\na dificuldade do processamento de tamanho volume de informação potenciou o\naparecimento dos sistemas de Business Intelligence (BI), capazes de lidar com a quantidade\nde dados armazenada e cujo objetivo passa por apresentar informação sob a forma de conhecimento\npara suportar o processo de tomada de decisão.\nA grande motivação para a implementação de sistemas de BI surgiu da possibilidade\nde conceção de uma forma de disponibilizar a informação de forma rápida, eficaz e visualmente\napelativa cuja interpretação seja algo intuitiva. A informação relevante pode\nser disponibilizada em diversos formatos, como por exemplo num Dashboard – técnica de\nvisualização interativa crucial na análise da informação e no suporte à decisão.\nO pressuposto principal desta dissertação é evidenciar que na modalidade cirúrgica de\numa unidade hospitalar é também possível transmitir informação que permita auxiliar os\nprofissionais de saúde na gestão de um bloco operatório. Deste modo, foram construídos\nindicadores de diversas categorias que poderão ser relevantes face às possíveis necessidades\nhospitalares.\nNa seleção da tecnologia a utilizar para o desenvolvimento da plataforma de BI optou-se\npelo Power BI, ferramenta de Business Intelligence bastante intuitiva e que permite a partilha\ndos elementos visuais que vão influenciar a leitura do profissional de saúde responsável\npela gestão da unidade cirúrgica.\nApós o desenvolvimento dos Dashboards, pode-se afirmar que o resultado foi satisfatório,\numa vez que foram criados indicadores de desempenho que permitem perceber a importância\nde um sistema de BI para a gestão mais eficiente de uma unidade cirúrgica."
  },
  {
    "keywords": [
      "Hardware-dependent systems",
      "Software simulation",
      "Software testing",
      "Software development",
      "API",
      "AI",
      "Industry 4.0",
      "Sistemas dependentes de hardware",
      "Simulação por software",
      "Teste de software",
      "Desenvolvimento de software",
      "IA",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Vision System Hardware: simulation and test automation",
    "autor": "Baixo, Ivo Alexandre Pereira",
    "data": "2023-11-27",
    "abstract": "This dissertation addresses the challenge of efficiently developing and testing software in hardware-dependent systems, with a specific focus on simulating hardware used in the Vision Software project of Smartex. Smartex is a company that applies AI solutions to the textile industry, particularly to the quality inspection cycle in fabric production.\nThe primary motivation behind this research is to overcome delays in the software development life cycle caused by the software having physical hardware dependencies, whose unavailability can injure the software development life cycle. To achieve this, this dissertation aims to develop software capable of simulating the Vision Software’s hardware components and provide an application programming interface (API) for seamless interaction with the simulation software. The research objectives encompass several key aspects. Firstly, the development of simulation software to faithfully simulate the hardware components of the Vision System. Secondly, the creation of an automated test pipeline that leverages the simulation software to enhance Smartex’s Quality Assurance processes for the Vision Software. Additionally, the reliability and effectiveness of the hardware simulation\nsolution will be thoroughly tested. Lastly, the impact of this work on Smartex’s software life cycle, specifically for the Embedded Systems and Quality Assurance teams, will be carefully measured and evaluated. This dissertation offers a comprehensive approach to hardware simulation through the use of software as well as automated software testing, effectively addressing a critical challenge in modern software development. The achieved solution significantly improved the overall software life cycle for the Embedded\nSystems and Quality Assurance teams at Smartex, enabling the testing and development of the Vision Software without the need for physical hardware dependencies."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Logical clocks for could databases",
    "autor": "Gonçalves, Ricardo Jorge Tomé",
    "data": "2011-09-27",
    "abstract": "Cloud computing environments, particularly cloud databases, are rapidly\nincreasing in importance, acceptance and usage in major (web) applications,\nthat need the partition-tolerance and availability for scalability purposes, thus\nsacrificing the consistency side (CAP theorem). With this approach, use of\nparadigms such as Eventual Consistency became more widespread. In these\nenvironments, a large number of users access data stored in highly available\nstorage systems. To provide good performance to geographically disperse\nusers and allow operation even in the presence of failures or network partitions,\nthese systems often rely on optimistic replication solutions that guarantee\nonly eventual consistency. In this scenario, it is important to be able to\naccurately and e ciently identify updates executed concurrently.\nIn this dissertation we review, and expose problems with current approaches\nto causality tracking in optimistic replication: these either lose information\nabout causality or do not scale, as they require replicas to maintain\ninformation that grows linearly with the number of clients or updates. Then,\nwe propose Dotted Version Vectors (DVV), a novel mechanism for dealing\nwith data versioning in eventual consistent systems, that allows both accurate\ncausality tracking and scalability both in the number of clients and servers,\nwhile limiting vector size to replication degree. We conclude with the challenges\nfaced when implementing DVV in Riak (a distributed key-value store),\nthe evaluation of its behavior and performance, and discuss the advantages\nand disadvantages of it."
  },
  {
    "keywords": [
      "Synthetic data",
      "Traffic sign recognition",
      "Convolutional neural networks",
      "European traffic signs",
      "Dados sintéticos",
      "Reconhecimento de sinais de trânsito",
      "Redes neuronais convolucionais",
      "Sinais de trânsito europeus",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Synthetic data approach for traffic sign recognition",
    "autor": "Silva, Diogo Lopes da",
    "data": "2019-12-23",
    "abstract": "Currently, Advanced Driver Assistance Systems (ADAS) have been gradually increasing their\npresence in everyday life, thanks in part to its ability to recognize several distinct types\nof objects in the road, namely, traffic signs. These systems employ Convolutional Neural\nNetworks (CNNs), a type of classification algorithms that relies on an enormous amount of\ndata in order to be effective. Current traffic sign datasets suffer from a scarcity of samples\ndue to the necessity of compiling and labeling them manually. Such task is highly resource\nand time consuming. Thus, researches resort to other mechanisms to deal with this problem,\nsuch as increasing the architectural complexity of the neural networks or performing data\naugmentation.\nThis work addresses the data shortage issue by exploring the feasibility of developing a\nsynthetic dataset. Such set would not require gathering and labelling manually thousands\nof real word traffic sign images, requiring only easily collectable information and no human\nintervention.\nThe only data required is a set of templates for each sign given that a particular sign may\nhave more than one template. This is required to cope with outdated pictograms that are\nstill present in streets and roads.\nWe apply several colour and geometric processing methods to the templates aiming to\nachieve a look similar to real signs, from the CNN point of view. One of such methods is\nthe usage of Perlin noise to both simulate shadows and avoid the clean and homogeneous\nlook that templates have.\nTwo use cases for synthetic data usage are presented: considering the synthetic dataset\nas a standalone training set, and merging synthetic data with real samples when real data\nis available. The first option provided results that not only clearly surpass any previous\nattempt on using synthetic data for traffic sign recognition, but are also encouragingly\nplacing the accuracies obtained close to state-of-the-art results, with much simpler networks.\nThe second approach provided results on three distinct test datasets that consistently beat\nstate-of-the-art results, either in accuracy or in simplicity of the network."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Flexible molecular alignment: an industrial case study on quantum algorithmic techniques",
    "autor": "Oliveira, Marta Sofia Saraiva",
    "data": "2020-07-30",
    "abstract": "Flexible molecular alignment is a complex and challenging problem in the area of Medic inal Chemistry. The current approach to this problem does not test all possible alignments,\nbut makes a previous analysis of all the variables and chooses the ones with potentially\ngreater impact in the posterior alignment. This procedure can lead to wrong ”best align ments” since not every data is considered.\nQuantum computation, due to its natural parallelism, may improve algorithmic solutions\nfor this kind of problems because it may test and/or simulate all possible solutions in an\nexecution cycle.\nAs a case study proposed by BIAL and in collaboration with IBM, the main goal of\nthis dissertation was to study and create quantum algorithms able to refactor the problem\nof molecular alignment in the new setting of quantum computation. Additionally, the\ncomparison between both classical and quantum solutions was defined as a subsequent\ngoal.\nDuring this dissertation and due to its complexity, in order to produce a practical solu tion to this problem, we resorted to a manageable number of conformations per molecule,\nrevisited the classical solution and elaborated a corresponding quantum algorithm. Such\nalgorithm was then tested in both a quantum simulator and a real device.\nDespite the privileged collaboration with IBM, the quantum simulations were not pro duced in viable time, making them impractical for industry applications. Nonetheless, tak ing in consideration the current point of development of quantum hardware, the suggested\nsolutions still has potential for the future."
  },
  {
    "keywords": [
      "Kamailio",
      "Containers",
      "Virtualization",
      "Docker",
      "Kubernetes",
      "VoIP",
      "SIP",
      "Cloud",
      "Microservices",
      "RTP",
      "Virtualização",
      "Microsserviços",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Kamailio-KSIPADP: support for cloud and kubernetes environments and evolution towards SaaS",
    "autor": "António, Pedro Miguel da Silva Paiva",
    "data": "2023-11-29",
    "abstract": "The traditional application format perfectly suited the necessities of the software industry a few years\nago, but today’s needs often call for distributed applications, based on microservices, and containerized.\nThis approach is advantageous at several levels: better usage of available resources, automatic deploy ment with integrated scaling and fault tolerance, as well as the inherent security granted by the isolation\nfrom the host’s operating system.\nFurther, in order to avoid the acquisition and maintenance costs associated with the physical equipment,\ncompanies are now migrating their workloads to the Cloud, which can be further supported through a\nmicroservice based architecture, thus capitalizing on the benefits of both.\nMicroservice based architectures lead to an effort to rethink current applications, so that they can be\nmigrated to a containerized solution and deployed in a cloud environment. One case is the application\nKSIPADP, developed by Altice Labs, and the subject case of this dissertation. It is based on Kamailio\nand acts as a Session Initiation Protocol (SIP) Server with several components. Over the course of this\ndissertation, KSIPADP is first migrated into a containerized Docker environment, followed by a single\ninstance deployment in Kubernetes, and finally into a scalable Kubernetes implementation that integrates\nadditional services like RTPEngine or a Media Server. Every aspect of the process that led to the project’s\nsuccessful completion is detailed over the course of the dissertation, alongside all the tests that were\nperformed in order to validate the solution’s correct functionality."
  },
  {
    "keywords": [
      "Monitorização",
      "Prevenção",
      "Falhas",
      "Multi-agente",
      "Monitoring",
      "Preventing",
      "Fault management",
      "Multi-agent",
      "681.3:61",
      "61:681.3"
    ],
    "titulo": "Monitorização e prevenção de falhas em sistemas baseados em Agentes",
    "autor": "Vieira, Nuno Manuel Rodrigues",
    "data": "2012",
    "abstract": "Os avanços tecnológicos verificados nos dia de hoje assim como a quantidade\nde informação e comunicação que lhes estão associados, atribuem um\npapel de grande importância aos sistemas de monitorização. É no seio desta\nevolução, que a competição existente entre os vários setores de mercado não\nestão acessíveis a erros e falhas, principalmente ao nível dos equipamentos\ntecnológicos. Assim, neste contexto de intolerância, assiste-se a uma proliferação\ne a uma utilização cada vez maior de sistemas de monitorização e\nprevenção.\nMais importante que monitorizar é prevenir. Ter a capacidade de evitar\numa falha, e permitir a resolução de algum problema atempadamente é uma\nmais valia para o desempenho, atribuindo fiabilidade e qualidade ao serviço.\nA gestão e a verificação de equipamentos, bem como de processos associados\naos mesmos permitem um maior controlo e domínio dum sistema.\nAssim esta dissertação tem como objetivo principal a implementação dum\nsistema para monitorizar a atividade de um ou mais sistemas multi-agente,\ncom capacidade para intervir e avisar o administrador do sistema quando\nocorre um problema.\nO sistema construído, que assenta numa estrutura formada por três unidades\ndistintas, unidade de análise, unidade de processamento e unidade de\ninterface com o utilizador, permitem a implementação de processos para troca\ne integração de informação, exercendo assim uma comunicação fundamental\nentre sistemas e utilizadores . É um sistema direcionado principalmente\nà gestão e monitorização do desempenho de diferentes equipamentos assim\ncomo dos processos em execução nos mesmos. Este trabalho foi desenvolvido em colaboração com um Hospital no Norte do País e o ambiente escolhido\npara a implementação da plataforma foi unicamente laboratorial.\nA concretização deste projeto esteve dividida em quatro fases: Início,\nPesquisas, Construção da Aplicação e Escrita da Dissertação. Na primeira\nfase, o Início, foi feito o levantamento dos principais requisitos para o sistema,\ndefinição dos objetivos, e elaboração de um plano de trabalho. Na segunda\nfase, Pesquisas, procedeu-se ao levantamento de informação sobre os conceitos\nteóricos relacionados com o tema em causa, como artigos científicos que\ndão suporte ao tema, entre outros trabalhos já publicados na mesma área.\nNo final desta etapa já se encontravam definidas as ideias base da aplicação\na construir de acordo com as necessidades. Na terceira fase deste projeto, a\nConstrução englobou a modelação e implementação da plataforma de monitorização,\nde acordo com as especificações definidas anteriormente. A quarta\netapa englobou a escrita da dissertação, o que incluiu um enquadramento\ndos conceitos teóricos em função da aplicação desenvolvida."
  },
  {
    "keywords": [
      "681.3:658.0",
      "658.0:681.3"
    ],
    "titulo": "Manutenção de Caches para sistemas OLAP",
    "autor": "Marques, Pedro Carvalho",
    "data": "2011-07-13",
    "abstract": "Nos dias que correm a utilização de sistemas multidimensionais de dados faz parte do quotidiano de qualquer organização de média ou de grande dimensão. Este tipo de sistema, cuja principal finalidade consiste em auxiliar os seus utilizadores nas suas atividades de tomada de decisão, tem por principais características a flexibilidade na exploração de dados e a celeridade na disponibilização de informação. Apesar de todos os mecanismos já existentes, que contribuem para este objetivo, é, por vezes, muito difícil manter os níveis de desempenho desejados pelos utilizadores. Como consequência disto, foram estudadas outras formas para reduzir a carga imposta ao servidor multidimensional de dados – o servidor OLAP. Um destes mecanismos é a criação de caches que armazenam informação já consultada e que, aquando de um pedido, o satisfazem sem terem a necessidade de consultar a sua fonte. Devido à natureza dos utilizadores dos sistemas OLAP, é possível determinar com bastante precisão os seus padrões de acesso e de exploração, isto é, quais as consultas efetuadas por um determinado utilizador ao longo de um dado período de tempo. Levando esta análise para um pouco mais à frente é, ainda, possível prever com antecedência qual a sequência exata de consultas que será efetuada por um determinado utilizador quando este iniciar uma qualquer sessão de exploração de dados. Após esta fase de previsão, consegue-se decidir quais as queries que deverão ser pré-materializadas, armazenando-as numa cache, de forma a servir o maior número possível de pedidos do utilizador a partir desta.\n\nA técnica proposta nesta dissertação centra-se na problemática que gira em torno da simplificação do número de queries que deverão ser pré-materializadas. O objetivo final desta técnica consiste em manter um balanço positivo entre o tempo dispendido a realizar esta pré-materialização e o que seria gasto caso tudo fosse calculado apenas quando requisitado."
  },
  {
    "keywords": [
      "Cryptography",
      "Post-quantum",
      "NTRU",
      "Criptografia",
      "Pós-quântica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Integrating post-quantum cryptography (NTRU) in the TLS protocol",
    "autor": "Fontes, Afonso Pires",
    "data": "2019-11-21",
    "abstract": "We aim to integrate new “suites”, using post-quantum authentication and encryption tech niques, in the TLS protocol. Namely, this project is dedicated to integrating algorithms\nbelonging to the NTRU family of cryptossystems in the OpenSSL library and in the Python\npackage “Cryptography”.\nEven though all the algorithms included in this project have already been imple mented as part of their submissions to the NIST Post-Quantum Standartization project,\ncurrently there doesn’t seem to exist a way to perform prototyping and testing of these cryp tossystems in real-life use cases, and it would be interesting to create such tools.\nWe also aim to test if these algorithms could be further optimized for speed and\nefficiency by comparing the reference implementations (submited to NIST and publicly avail able) with our own implementations that perform some required mathematical operations in\na very efficient manner (by using specialized number theory libraries)."
  },
  {
    "keywords": [
      "Delirium",
      "GLM",
      "Regressão logística multinomial",
      "Multinomial logistic regression",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "GLM’s em Data Science como ferramenta na seleção de fatores de risco: um exemplo de previsão de delirium",
    "autor": "Coelho, Alexandra Moreira",
    "data": "2023-12-16",
    "abstract": "Globalmente, 25% da população sofre de distúrbios mentais, sendo possível implementar metodologias\nque possibilitem a deteção e previsão numa fase mais precoce. Concretamente, o delirium é uma disfunção neuropsiquiátrica aguda, prevalente em doentes admitidos em contexto hospitalar de internamento\ne terapia intensiva. Sendo uma manifestação multifatorial é normalmente subdiagnosticada e negligenciada. O delirium pode ser categorizado, de acordo com o perfil de atividade motora, em hipoativo e\nhiperativo. Neste contexto, surge o tema da dissertação que visa desenvolver uma aplicação capaz de\nprever a ocorrência de delirium e dos seus subtipos, com base na metodologia dos GLMs.\nOs modelos de regressão logística multinomial são frequentemente implementados para identificar as\nvariáveis mais contributivas, dado que permitem modelar a relação entre os preditores e uma variável dependente multicategórica. As etapas que precedem a implementação do algoritmo dizem respeito ao pré-processamento dos dados. No decorrer do processo de modelação, aplicou-se o ADASYN para gerar amostras sintéticas devido ao desbalanceamento das classes da variável dependente. Posteriormente, foi realizada a seleção de variáveis recorrendo a diversas técnicas, sendo que o método Elastic Net com um alpha\nde 0,1 foi o que demonstrou um melhor desempenho. Para tal, este modelo foi implementado na aplicação disponível em https://alexandra-coelho.shinyapps.io/Delirium_detection/.\nPara o subtipo hipoativo, permitiu a seleção de 27 variáveis, tendo obtido uma AUC-PR de 0,307 e\numa AUC-ROC de 0,691. As variáveis mais contributivas incluem o período de internamento em dias, o\nalcoolismo, os analgésicos, os cardiotónicos, assim como, o grupo de diagnóstico referente à toxicidade e\ndrogas. Relativamente ao subtipo hiperativo, o modelo determinou 29 variáveis relevantes, onde obteve\num valor de AUC-PR de 0,074 e de 0,531 para a AUC-ROC. Das variáveis mais impactantes destacam-se a PCR, a idade, a pO2, os critérios SIRS e o local de proveniência no SU, nomeadamente, o UDC1.\nEspecula-se que os baixos valores associados essencialmente ao subtipo hiperativo são devidos à baixa\nrepresentatividade desta categoria. Apesar deste modelo preditivo ainda poder ser melhorado, assume-se\ncomo uma ferramenta útil para os profissionais de saúde aquando o diagnóstico do delirium no SU."
  },
  {
    "keywords": [
      "Help desk system",
      "Customer relationship management",
      "Onboarding",
      "Sistema de help desk",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of a help desk assistant",
    "autor": "Moreira, Manuel João Ferreira",
    "data": "2024-01-08",
    "abstract": "Throughout the years, companies have continuously sought ways to improve their customer service, and\nhelp desk systems eventually evolved to meet these needs. Recent advancements in technology such as\nthe introduction of chatbots have led to more automated and self-service options in help desk systems.\nImplementing an effective help desk system can be a great way to attract new customers, through\nonboarding methods, and strengthen relationships with existing clients by addressing issues quickly and\nefficiently, resulting in increased customer satisfaction and loyalty. This dissertation aims to establish\nthe optimal help desk system that will not only assist in preserving a positive relationship with existing\nclients but also attract new ones, ultimately contributing to enhanced customer satisfaction, retention,\nand operational efficiency for Wintouch’s cloud application.\nWith this in mind, Wintouch is looking for ways to make its help desk system better in its cloud application. They are hoping to have automated self-service customer support options as well as the more\ntraditional customer service techniques, so any user can have the ability to choose their preferred method,\nensuring their customer relationship remains in the best shape possible.\nHence, this dissertation aims to establish the optimal help desk system that will assist in preserving\na positive relationship with existing clients while also attracting new ones."
  },
  {
    "keywords": [
      "Decentralized identifiers",
      "Digital identity",
      "Distributed ledger technology",
      "Privacy",
      "Self-sovereign identity",
      "Verifiable credentials",
      "Credenciais verificáveis",
      "Identidade auto-soberana",
      "Identidade digital",
      "Identificadores descentralizados",
      "Privacidade",
      "Tecnologia de registo distribuído",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "DIDs, Claims, Credentials e Blockchains (Self-sovereign Identity)",
    "autor": "Peixoto, Ricardo Jorge Marques",
    "data": "2021-07-26",
    "abstract": "The high growth in the use of digital identities creates the need to develop mechanisms\nthat can protect the personal data of each individual. The way identity is treated today\nprevents each of us from being able to control our personal information. This is due to the\ncentralized architecture in which the personal data are inserted, that is, all these data are kept\ntogether and controlled by the entities responsible for providing the most varied services,\nwhich is wrong since the identity belongs to the person and thus it must be responsible\nfor controlling that identity. Centralized identity management brings within itself several\nproblems, whether intentional (that is, data correlation for profiling) or unintentional (that\nis, data breach).\nTo face this problem, multiple entities across the world are developing decentralized\nidentity managment systems based on a self-sovereign identity architecture where each\nindividual is responsible for managing and storing a set of credentials, each with parts of\ntheir personal information. A self-sovereign identity architecture allows users to provide only\nsmall parts of their personal information or even to omit any type of personal identification,\nusing cryptographic techniques like selective disclosure and zero-knowledge proofs, which\nallows them to have more control over their privacy.\nTaking into account the current problems of digital identity, this dissertation aims to\nexplore the state of the art and develop a proof of concept, through the implementation of\na system based on self-sovereign identity, which is able to cover the use cases for digital\nidentity. Thus, this document shows the architecture implemented, with a blockchain,\nresponsible for the storage of all public data, and a user agent, responsible for facilitating all\ninteractions of the various users with the developed system.\nThe proof of concept developed allows not only to validate that it is possible to correct\nmany of the problems associated with centralized identity management, but also to explore\nnew cryptographic strategies in order to improve the way each of us manages our own\nidentity."
  },
  {
    "keywords": [
      "Criptografia",
      "Jasmin",
      "AES-GCM",
      "ECDSA",
      "Curva P-384",
      "Cryptography",
      "P-384 Curve",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Implementação de biblioteca criptográfica em Jasmin",
    "autor": "Sousa, Luís Enes",
    "data": "2024-05-20",
    "abstract": "O processo de implementação de algoritmos criptográficos exige uma grande preocupação com a eficiên cia do código desenvolvida, mas sem descurar os aspetos de correção e segurança. Uma implementação\nque apresente bom desempenho, mas não dê garantias totais da sua correção pode ser vulnerável a\nataques, enquanto que uma outra implementação que garanta a segurança a custo da eficiência pode\ncomprometer a usabilidade ou até ser descartada em contextos com recursos computacionais limitados.\nA framework Jasmin pretende conciliar essa dicotomia, possibilitando desenvolver software criptográfico\nde elevado desempenho e altos níveis de confiabilidade, através da combinação de construções de alto\ne baixo nível.\nNo projeto apresentado nesta dissertação recorre-se à linguagem Jasmin para codificar dois algor timos criptográficos largamente utilizados: o algoritmo de cifragem autenticada com dados associados\nAES-GCM e o algoritmo de assinatura digital baseado na curva elíptica P-384. Ao longo do mesmo tam bém são implementadas outras primitivas criptográficas, como o algoritmo de cifra de bloco AES-CTR\ne a função de hash SHA-384, sempre tendo em consideração os conceitos de segurança inerentes ao\ndesenvolvimento de software criptográfico, tais como a política constant-time."
  },
  {
    "keywords": [
      "Computer vision",
      "Deep learning",
      "Object detection",
      "Object fingerprint",
      "Object tracking",
      "Detecção de objectos",
      "Monitorização de objectos",
      "Visão por computador",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of an algorithm for counting vehicles and pedestrians based on video",
    "autor": "Matos, Miguel",
    "data": "2019-12-30",
    "abstract": "The population density in the urban environment has increased significantly, consequently\nincreasing the number of vehicles and people on the public road. Possible monitoring of\nthis flow allows better problem management, and the enhancement of solutions in a smart\ncity context, solutions that promote regular traffic in a city.\nThis work presents a solution for counting vehicles and people in a video to use the\nsolution developed in cities of Portugal. The solution combines deep learning techniques\nand traditional computer vision techniques, combining object detection, classification, ob ject tracking, and fingerprint concepts. For each concept is presented the state of the art\ntechniques and techniques used in similar problems.\nTo choose the best fingerprint methods, a comparative study of different techniques was\nproduced. With a dataset of vehicle and people images, the following techniques were con sidered: Fourier Transform, Scale Invariant Feature Transform (SIFT), Color Co-occurrence\nHistogram (CCoH), and Autoencoders, of which CCoH showed better results.\nThe solution pipeline consists of the YOLOv3 algorithm for the object detection part, hav ing the algorithm a convolutional neuronal network for object classification; Kalman Filter\nfor object tracking was chosen in conjunction with the CCoH technique for object finger print. The pipeline ends with the matching of the newly detected objects with previously\ndetected objects, using the Hungarian algorithm for this correspondence.\nIn order to extract features using the defined pipeline, a python library has been devel oped, allowing visualization of its operation and easy integration with video sources (video\nfiles and cameras). Object counting, area definition, line intersection, heatmap’s, and object\ncollision are examples of features that can be obtained by the library.\nAs a global solution, a web application was developed, including a frontend application,\na backend, a relational database, and a service to perform video processing with the help\nof the developed library. The web application is in use and in a production environment."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of an adaptable multicast overlay network",
    "autor": "Sampaio, André Filipe da Silva",
    "data": "2017",
    "abstract": "Multicast is a group communication paradigm created in order to reduce, as much as\npossible, the amount of data generated to the network. However, limited deployment of IP\nMulticast protocols has motivated an interest in alternative approaches which implement a\nsimilar process of Multicast at an application-level (using solely end-systems and not the\nrouters). In this context, different methodologies are presented, entitled Application-Layer\nMulticast or Overlay Multicast, which may vary in the way they operate.\nThis dissertation’s objective is to develop and experiment a prototype of an overlay multicast\nsystem. This system should be easily configurable and adaptable in order to assume\ndifferent strategies when establishing the multicast distribution tree. It is also expected\nto explore and integrate collaborative mechanisms between the overlay network and the\nInternet Service Providers (ISP).\nWith the presented context, the first step to take is an investigation on the state of the\nart, where technologies relevant to this work will be presented. After this initial step, the\ndeveloped system’s architecture will be described, one which enables different ways of\nbuilding and maintaining the multicast distribution tree. The envisioned system can operate\nindependently, integrating mechanisms where the distribution tree relies solely on peer\ndecisions, which will be firstly addressed. Then, this work will move on to collaborative\nmechanisms between the overlay’s management (the central node) and the Internet Service\nProviders. Based on the proposed system architecture, several mechanisms are explored,\nnot only focusing on alternative ways to build distribution trees, but also mechanisms\nallowing for some traffic engineering objectives involving the Internet Service Providers.\nUsing the CORE network emulator, all the proposed mechanisms are tested, and results\nare analyzed to corroborate the system’s correct operation."
  },
  {
    "keywords": [
      "681.3:61",
      "61:681.3",
      "614.253.83"
    ],
    "titulo": "Sistema de notificação de eventos adversos em ambiente hospitalar",
    "autor": "Martins, Renata Reis Pires",
    "data": "2013",
    "abstract": "O tema da gestão do risco começou a ser debatido aquando da publicação do relatório do\nInstituto de Medicina, em 2000 “To Err Is Human: Building a Safer Health System”, que\ncaptou a atenção da população mundial, ao revelar que 44000 a 98000 pessoas morrem nos\nEstados Unidos da América como consequência direta de erros nos cuidados de saúde. É a\npartir da última década que este tema passa a ser de extrema importância, e se começam a\ntomar medidas para o prevenir. As organizações de saúde voltaram as suas políticas para a\ngestão do risco, contudo, tem-se notado que estas têm grandes dificuldades a aprender com o\nerro. De forma a colmatar este problema é necessário registar todos os eventos adversos. No\nmínimo esses registos servem para identificar perigos, e minimizar a ocorrência do mesmo\nevento.\nO objetivo deste trabalho é o estudo e a concepção de um sistema de notificação e tratamento\nde eventos adversos integrável em sistemas de informação hospitalar existentes. Além disso,\ndá a conhecer a importância que os sistemas de notificação de eventos adversos têm na\nsociedade atual, ao fornecer um estado de arte desta temática, e apresentando sistemas já\nutilizados, e com benefícios já reconhecidos.\nA avaliação do sistema desenvolvido foi efetuada através da sua integração na Intranet de\ntodos os hospitais do Grupo Trofa Saúde, que no futuro se irá refletir como um indicador de\nqualidade dos cuidados prestados nesta organização de saúde."
  },
  {
    "keywords": [
      "Docker",
      "Docker swarm",
      "Kubernetes",
      "Orchestration",
      "Containers",
      "Containerization",
      "Services",
      "Build",
      "Deploy",
      "Web applications",
      "Orquestração",
      "Containerização",
      "Serviços",
      "Build",
      "Deploy",
      "Aplicações web",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Using container-based virtualization on web apps production environment: dipcode development cycle",
    "autor": "Guimarães, Luís Miguel Pinheiro",
    "data": "2022-05-14",
    "abstract": "With the fast evolution of the internet over the last years, the top priority on software\ndevelopment has shifted from what? to when?. Reduced time-to-market is now the\ncompetitive edge that all companies strive for.\nThe usage of container-based virtualization technologies keep the multiple environments\nwhere a development team works similar enough, that their work is made easier when devel oping and testing new features, which in turn results in a significantly faster delivery. The\nnature of this tecnhology also brings numerous advantages when it comes to management,\nmonitoring and maintaining resources, allowing for an ease of adjustment, based on the\nclient needs.\nThroughout this dissertation is presented an extended base of knowledge about container\ntechnologies, especially Docker, as well as what are the basic techniques to use when building\nan application inside such infrastructure, from the writing of the Dockerfile to the adaptation\nof the multiple pipelines responsible to deploy the application."
  },
  {
    "keywords": [
      "Cancer genomics",
      "Exploration",
      "Problog",
      "Stomach cancer",
      "TCGA",
      "Data processing",
      "Estudos genómicos",
      "Exploração",
      "Cancro do Estômago",
      "Processamento de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Probabilistic logic programming for cancer genomics",
    "autor": "Fernandes, João Pedro Alves",
    "data": "2018-12-14",
    "abstract": "Over the past years, research on cancer genomics has been boosted by the advances in high \nthroughput sequencing technologies. The Cancer Genome Atlas (TCGA) project is an effort to map \nthe genomic alterations possibly associated with specific types of tumours and aims to improve the \nprevention, diagnosis and treatment of cancer. The generation of large and heterogeneous datasets, \nas a result of TCGA and other similar projects, creates the need to use advanced bioinformatics and \ncomputational tools for the analysis of cancer genomic data.\nDespite different bioinformatics frameworks have been established in order to explore and perform \ncomprehensive analysis of cancer datasets, the area of logic and probabilistic logic programming has \nnot been sufficiently explored in the analysis of cancer data.\nThe main goal of this thesis was to explore Problog – a probabilistic logic programming (PLP) \nlanguage – to encode interactions on heterogeneous cancer genomics datasets that may lead to new \ninsights. To accomplish this objective, our work consisted in the elaboration of a python program and \na Problog framework. The used datasets involved stomach cancer genomic data.\nThe python program – ProceOmics – aimed to process and format cancer genomic data so it \ncould be used by Problog programs. The Problog framework – Problog Knowledge Base (KB) –\nintended to codify the data previously processed by ProceOmics. To evaluate the consistency of the \ndeveloped framework and explore possible relations between the different types of genomic data, \nqueries were formulated to the Problog KB.\nThus, this thesis provides a tool that establishes a link between the genomic data contained in \npublic databases with probabilistic logic programs. We hope this work may help to overcome future\nefforts to use PLP on genomic data analysis."
  },
  {
    "keywords": [
      "Deep Learning",
      "Estação de tratamento de águas residuais",
      "Machine Learning",
      "Séries temporais",
      "Sustentabilidade",
      "Sustainability",
      "Time series",
      "Wastewater treatment plant",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Monitorização da qualidade da água numa estação de tratamento de águas residuais: uma abordagem baseada em Machine Learning",
    "autor": "Pereira, João Paulo Ribeiro",
    "data": "2023-12-28",
    "abstract": "A monitorização da qualidade da água é uma tarefa fundamental que tem de ser incluída em qualquer\nprocesso operacional de uma Estação de Tratamento de Águas Residuais (ETAR), pois permite verificar\nse os efluentes, quando descarregados no meio ambiente, cumprem os valores padrão definidos por lei.\nSe estes valores não forem monitorizados com eficácia, a poluição da água continua a intensificar-se e,\nconsequentemente, a água torna-se cada vez mais escassa. Devido ao problema da poluição da água,\ntem-se verificado um aumento na escassez da água, em Portugal, durante as últimas décadas.\nA introdução de abordagens de Machine Learning (ML) neste tipo de operações pode vir a ser\nbastante importante, devido à sua capacidade de melhorar a eficiência da monitorização da qualidade da\nágua e da previsão das substâncias da mesma. Uma das principais características que esta abordagem\noferece é a interpretação de padrões e tendências nos dados, que não são facilmente identificáveis por\noutras técnicas, e ainda a interpretação de relações não lineares nos dados.\nDeste modo, este trabalho visa a implementação de modelos de ML baseados em Convolutional\nNeural Networks (CNN), Long Short-Term Memory (LSTM), Transformer, e Transformer-LSTM,\npara a previsão dos valores da condutividade e do caudal, no afluente duma ETAR, de modo a apoiar\na monitorização da qualidade da água, e a celeridade do processo de tomada de decisão neste tipo de\ninfraestruturas. Diante dos melhores modelos candidatos obtidos, verificou-se que os modelos baseados\nem Transformer alcançaram os melhores resultados na previsão da condutividade, nas duas abordagens\nconsideradas (multivariate e univariate), enquanto que os modelos baseados em CNN alcançaram o\nmelhor desempenho na previsão do caudal, também nas duas abordagens supramencionadas."
  },
  {
    "keywords": [
      "Accelerometer",
      "Anomaly detection",
      "Impacts",
      "Semi-supervised learning",
      "Acelerómetro",
      "Aprendizagem semi-supervisionada",
      "Deteção de anomalias",
      "Impactos",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Development of a classification algorithm for vehicle impacts: an anomaly detection approach",
    "autor": "Pereira, Rita Alexandra Ferreira",
    "data": "2022",
    "abstract": "In the past decade, Machine Learning has been heavily applied to automobile industry solutions, the most\npromising being development of autonomous vehicles.\nNew mobility services are available today as alternatives to owning a car, like ride hailing and carsharing.\nHigh costs associated with the maintenance of the vehicle and the reduced rate of vehicle use\nthroughout the day are some of the factors for the popularity of these services.\nCar-sharing is self-service mode of transport that provides its members with access to a fleet of vehicles\nparked in various locations throughout a city.\nDamages are expected to happen when vehicles are used and the required repair implies costs to fleet\noperators. Systems able to detect these damages will promote a better use of these vehicles by vehicle\nusers.\nVehicle damages result from impacts with other objects, for instance, other vehicles or structures of\nany kind and these impacts inflict deformations to the vehicle exterior structure. Most of these impacts\ncan be perceived or detected by the forces involved as result of the impact.\nAnomaly Detection is a technique applicable in a variety of domains, such as intrusion detection, fraud\ndetection, event detection in sensor network or detection ecosystem disturbances.\nThe objective of this thesis is the study and development of a semi-supervised intelligent system for detection\nand classification of vehicle impacts with an Anomaly Detection approach, using the accelerometer\ndata, and following a strategy that would allow exploring a Machine Learning cycle.\nThis thesis was developed under an internship in the company Bosch Car Multimedia S.A, located in\nBraga."
  },
  {
    "keywords": [
      "681.3.06"
    ],
    "titulo": "Comparison of software development methodologies based on the SWEBOK",
    "autor": "Simão, Elísio Maciel",
    "data": "2011-11-09",
    "abstract": "We are facing a period where software projects have a huge dimension involving\nsmall resources, high risk and a wide range of available approaches. In this\nscenario the Software Development Methodologies (SDMs) can prove to be a\nuseful ally, but very dangerous and even fatal if misused. The big issue around\nthis matter is how to choose the appropriated SDM that  ts a speci c project.\nIn the given scope, this dissertation describes a framework for comparing SDMs\ndelivering a set of procedures that should be followed when the choice of an\nSDM is made. The dissertation approaches the framework by applying it to a\ngroup of SDMs that were selected by their popularity and signi cance. This\nexercise is done to prove the concept of the framework and to provide a base\ncomparison, with each chosen SDM, that can, and should, be extended by those\nwho choose to use the framework.\nThe classi cation is achieved by de ning a scale that goes from total satisfaction\nto no satisfaction, with an intermediate level of partial satisfaction, that is applied\nto a set of keys. These keys are based in SWEBOK (Software Engineering\nBody Of Knowledge) that describes and explains the di erent Knowledge Areas\n(KA) stating their common issues and best practices. To explain the framework,\nthe dissertation analyzes each KA and evaluates the selected SDMs by\nassessing how their approach complies with SWEBOK's knowledge areas, using\nthe previous stated scale.\nThe framework delivered can be enriched by its user who should provide weights\nto each KA regarding the project in which the SDM will be used and previous\nexperiences"
  },
  {
    "keywords": [
      "681.3.06",
      "681.324"
    ],
    "titulo": "Conceção e linhas de orientação para o desevolvimento de aplicações em HTML5 : o case study Primavera BSS",
    "autor": "Costa, Vítor Nuno Rodrigues",
    "data": "2013",
    "abstract": "Nos últimos anos temos assistido a grandes evoluções nas aplicações web,\nproporcionando uma interação cada vez mais apelativa, tanto no seu aspeto como na sua\nusabilidade. Com estas evoluções foi surgindo o conceito de Rich Internet Applications\n(RIA). As RIA são aplicações web que tem características e funcionalidades que eram\nusualmente desempenhadas por software para desktop dada a sua complexidade. O\nconceito passa por transferir processamento para o cliente (browser), permitindo uma\ninteração mais próxima com o utilizador, respostas mais rápidas e uma menor\nsobrecarga sobre o lado do servidor. Esta ideia permite uma abordagem ao\ndesenvolvimento de aplicações empresariais no contexto web, sendo possível manter a\ncapacidade de processamento das aplicações desktop tirando partido do melhor do\nmundo web e permitindo chegar mais facilmente aos clientes.\nA PRIMAVERA, como interessada neste tipo de aplicações, já desenvolveu a\nFramework Athena assente nas tecnologias Microsoft Silverlight e WCF RIA Services,\nmas dado que a evolução do Microsoft Silverlight foi descontinuada por parte da\nMicrosoft, existe a necessidade de procurar uma alternativa. O HTML5 surge como\nprincipal alternativa, mas é preciso perceber se este pode efetivamente constituir-se\ncomo tal.\nAssim, esta dissertação será inicialmente focada na análise comparativa entre as\ntecnologias Microsoft Silverlight e HTML5, passando depois pela implementação de\nprotótipos, que no final, permitam à PRIMAVERA tomar uma decisão sobre a\nsubstituição da tecnologia.\nO caso de estudo será uma aplicação desenvolvida em HTML5, JavaScript e\nCSS, que poderá ser instalada em dispositivos móveis com qualquer sistema operativo.\nEstas aplicações, denominadas aplicações híbridas, implicam também uma elevada\ncomplexidade no lado do cliente e envolvem vários dos conceitos relacionados com o\ndesenvolvimento de RIA em HTML5."
  },
  {
    "keywords": [
      "MAL",
      "Ivy Workbench",
      "Utilizadores",
      "Guia",
      "VS Code",
      "Verificação",
      "Users",
      "Guidance",
      "Formal verification",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "A new models editor for the IVY Workbench",
    "autor": "Mendes, Rui Filipe Moreira",
    "data": "2022-12-21",
    "abstract": "Para que as interfaces de sistemas críticos possuam um nível de qualidade que permita o seu uso em\nsegurança, devem passar por um processo rigoroso de análise. A verificação formal de interfaces é uma\ndas formas de realizar essa análise. Para tal, é importante que os desenvolvedores dessas interfaces\nconsigam editar e criar os modelos que acharem mais adequados para as suas interfaces. Tanto os\ndesenvolvedores mais experientes como os menos experientes. A Ivy Workbench é uma ferramenta que\npermite descrever o funcionamento das interfaces e verificar propriedades sobre o seu comportamento,\nde forma a identificar potenciais problemas na interação. Deste modo, fornece informação relevante para\nos desenvolvedores que utilizem o Ivy, para que se possa melhorar o software sem ter de necessariamente\npassar por um processo de teste manual longo e exaustivo.\nO atual editor do Ivy é difícil de manter e não fornece ajuda suficiente nem guia novos utilizadores\nadequadamente. Por isso, é necessário que haja uma melhor forma de editar os modelos na linguagem\nModel Action Logic (MAL), a linguagem de programação da Ivy Workbench. O objetivo desta dissertação\né construir uma solução que permita que todos os tipos de desenvolvedores consigam construir os seus\nmodelos através de orientações do próprio editor. É bastante desafiante desenvolver uma solução deste\ngênero, que permita alcançar o nível de apoio pretendido, dado que precisamos de ter em conta com o que\né que os utilizadores estão mais confortáveis e quais as ferramentas que usam com maior regularidade,\npara que seja possível desenvolver uma solução o mais abrangente possível.\nPara que se concretize o principal objetivo, enquanto também se alcança o máximo número de utilizadores, optou-se por desenvolver uma extensão de VS Code. Trata-se do editor de código mais utilizado\ne fornece várias ferramentas para desenvolvedores de extensões, assim como uma vasta documentação.\nÉ possível tirar partido das funcionalidades que esta ferramenta já apresenta, típicas de um Integrated\nDevelopment Environment (IDE) comum, que nos permitem criar novas formas para os utilizadores da\nIvy escreverem modelos MAL, e fazendo isso, aumentar a sua produtividade.\nDepois da extensão estar concluída, é expectável que esta solução seja mais fácil de manter no\nfuturo, e mais utilizadores achem esta nova solução menos complexa para trabalhar, levando a que estes\nse sintam mais satisfeitos a utilizar a ferramenta e a própria linguagem, ajudando assim o crescimento\nda utilização da Ivy Workbench assim como da qualidade do software."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Space colonisation based procedural road generation",
    "autor": "Fernandes, Gabriel Dias",
    "data": "2018",
    "abstract": "Procedural generation of content has been studied for quite some time and it is increasingly\nrelevant in scientific areas and in video-game and film industries. Procedural road layout\ngeneration has been traditionally approached using L-Systems, with some works exploring\nalternative avenues. Although originally conceived for biological systems modelling, the\nadequacy of L-Systems as a base for road generation has been demonstrated in several\nworks.\nIn this context, this work presents an alternative approach for procedural road layout\ngeneration that is also inspired by plant generation algorithms: space colonisation.\nIn particular, this work uses the concept of attraction points introduced in space colonisation\nas its base to produce road layouts, both in urban and inter-city environments. As will\nbe shown, the usage of attraction points provides an intuitive way to parameterise a road\nlayout. The original Space Colonization Algorithm (SCA) generates a tree like structure,\nbut in this work, the extensions made aim to fully generate a inter-connected road network.\nAs most previous methods the method has two phases. A first phase generates what\nis mostly a tree structure growing from user defined road segments. The second phase\nperforms the inter connectivity among the roads created in the first phase.\nThe original SCA parameters such as the killradius help to control the capillarity of the\nroad layout, the number of attraction points used by each segment will dictate its relevance\nestablishing a road hierarchy naturally dependent on the distribution of the attraction\npoints on the terrain. An angle control allows the creation of grid like or more organic\nroad layouts.\nThe distribution of the attraction points in the terrain can be conditioned by boundary\nmaps, containing parks, sea, rivers, and other forbidden areas. Population density maps can\nbe used to supply an explicit probabilistic distribution to the attraction points. Flow-fields\ncan be used to dictate the flow of the road layout. Elevation maps provide an additional\nrestriction regarding the steepness of the roads.\nThe tests were executed within a graphic toolbox developed simultaneously. The results\nare exported to a geographical information file format, GeoJSON, and then maps are rendered\nusing a geospatial visualisation and processing framework called Mapnik.\nFor the most part, parameter settings were intuitively reflected on the road layout and\nthis method can be seen as a first step towards fully exploring the usage of attraction points\nin the context of road layout."
  },
  {
    "keywords": [
      "Parallel computing",
      "Distributed computing",
      "Task oriented computing",
      "HPX",
      "APEX",
      "Application tracing",
      "Computação paralela",
      "Computação distribuída",
      "Computação orientado às tarefas",
      "Rastreamento de aplicações"
    ],
    "titulo": "HpxTrace: monitorização de aplicações em HPX",
    "autor": "Pinto, José António Pereira",
    "data": "2022-10-18",
    "abstract": "As the complexity of the parallelism in high performance computing continues to increase, traditional tools such as\nMPI and OpenMP are starting to become inadequate. Within this context, the HPX C++ library emerges as the\nfirst implementation of ParalleX, an execution model that describes message-driven and task-centric computing\nwith the use of lightweight threading, futures, global address spaces and other mechanisms, in order to mitigate\nproblems such as starvation, latency, management costs, and resource contention.\nIn terms of monitoring HPX applications, this work includes the study of: i) HPX performance counters, a\nframework that provides real-time access to a number of performance metrics; and ii) APEX, a tool with an event\nAPI that enables runtime auto-tuning of application parameters and the generation of execution profiles, with the\naim of finding areas of application. This resulted in an extensive report on the inner workings of these tools, which,\ndue to its importance, was incorporated into the dissertation itself. During the development of this project, a new\nHPX counter was defined that monitors component variables and exemplifies how the framework of counters can\nbe adapted to handle situations outside of its initial scope.\nThis dissertation’s main contribution is HpxTrace, a tracing tool for HPX applications that was modeled after\nDtrace and built with APEX’s policy engine. HpxTrace provides a system of probes that trigger upon the occurrence\nof certain events. The user can associate actions with each probe to collect and manipulate event data. Events\ninclude readings from HPX counters, execution of tasks, message exchanges, and events instrumented in the\napplication by the users themselves.\nTo describe the actions each probe should take, a scripting language was developed using the Spirit library\nfrom the Boost library collection. In addition to providing the basic operations and aggregations found in Dtrace,\nthe language had to be adapted to the distributed nature of HPX. This was accomplished by taking measures\nsuch as separating variables into local and global contexts and adding synchronization mechanisms.\nTo validate HpxTrace, two implementations of stencil algorithms that solve heat distribution problems were used\nas case studies. HpxTrace detected the differences in the optimizations between the two versions and their impact\non performance.\nFollowing testing, it was concluded that HpxTrace can be used to analyse and measure the performance of\nHPX applications. Additionally, it proved to be particularly useful in comprehending the internal behavior of HPX,\nwhich is valuable for tasks like finding performance optimizations, as well as debugging and learning the platform\nitself."
  },
  {
    "keywords": [
      "Comércio eletrónico",
      "Plataforma E-Commerce",
      "Loja online",
      "E-Commerce",
      "E-Commerce platform",
      "Online store",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Geração de lojas online orientada para uma framework – funcionalidade de gestão de utilizadores",
    "autor": "Teles, Tiago Martins",
    "data": "2023-07-28",
    "abstract": "O desenvolvimento de todo o código necessário para uma loja online é um trabalho \ndemorado e complexo. Todas as lojas online têm diferentes exigências e condições, no \nentanto, existem pontos comuns entre vários tipos de loja. Supondo a possibilidade de extrair \nesses aspetos comuns, seria possível a criação de um esqueleto de código com todos os \ncomponentes básicos, sobre os quais um programador poderia acrescentar e moldar a loja \nonline, reduzindo assim o tempo necessário para a desenvolver. \nA elaboração desta dissertação começou pela criação de uma loja online que suportasse um \nnegócio fictício de venda de bicicletas e produtos relacionados. Este passo serviu para \nidentificar e avaliar quais seriam os parâmetros necessários especificar, e em que sentido \nfariam diferença na construção de um website de vendas bem como, distinguir os \ncomponentes comuns e as suas relações.\nA presente dissertação tem como propósito principal o desenvolvimento de uma ferramenta \npara a criação de lojas online. Esta ferramenta pretende ser usada por programadores para \nproduzir o código padrão que é comum a qualquer implementação de uma solução de loja \nonline. Através da recolha de parâmetros sobre a loja online a ser criada, são construídos os \nficheiros necessários para a implementação do site.\nA aplicação concebida focou-se na utilização de apenas alguns dos componentes básicos, \nnomeadamente, “Utilizadores” e “Categorias” no back-end, “Registo e Login” e “Perfil” no \nfront-end, mostrando assim ser possível usar a framework desenvolvida para uma \nimplementação parcial de uma loja online. A abordagem modular permite expandir a \naplicação, criando e adicionando outros componentes à estrutura parcial existente."
  },
  {
    "keywords": [
      "PID",
      "mDL",
      "Carteira digital",
      "EUDIW",
      "Identidade digital",
      "Identidade móvel",
      "CMD",
      "Digital wallet",
      "Digital Identity",
      "Mobile Identity",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Ecossistema da Carteira de Identidade Europeia (EUDIW): componente ’Person Identification Data (PID) provider’",
    "autor": "Silva, Rolando José Soares",
    "data": "2024-05-20",
    "abstract": "No seguimento do regulamento Europeu de Identificação Electrónica (eIDAS), tem vindo a ser promovida a Carteira de Identidade Digital Europeia (European Digital Identification Wallet - EUDIW). Trata-se de um mecanismo de identificação desmaterializado que oferece garantias de segurança e privacidade aos cidadãos, e que se pretende interoperável nos vários estados membros. Esta dissertação aborda a\nemissão de Personal Identification Data (PID) para diferentes países no contexto da EUDIW, e ainda uma\nemissão da versão digital da carta de condução automóvel (mobile Driver License — mDL). O estudo compreende uma análise de soluções semelhantes disponibilizadas em Portugal, assim como uma análise dos standards e regulamentos que se enquadram no âmbito desses sistemas. Foca-se depois no desenho e implementatção do protótipo para a componente de Personal Identification Data Provider e da mDL."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Development of web-based tools for metabolomics data analysis and mining",
    "autor": "Cardoso, Sara Manso de Sousa",
    "data": "2017",
    "abstract": "The recent advances in metabolomics experimental techniques have provided novel approaches\nfor many research issues in the biological fields. Indeed, the ability to identify\nand quantify numerous compounds in biological samples provides significant advances in\nfunctional genomics, biomarker identification, sample characterization or drug discovery\nand development. To take full advantage of these data advanced bioinformatics methods\nfor data analysis and mining have been required.\nA number of methods and tools for metabolomics data analysis have been put forward\nrecently, being one of the major limitations still faced the lack of integrated frameworks for\nextracting relevant knowledge from these data and being able to integrate these data with\nprevious biochemical knowledge. Also, the lack of reproducibility in many data analyses\nor data mining processes is a strong obstacle for biological discovery.\nIn recent work from the host group, specmine, a metabolomics and spectral data analysis/\nmining framework, in the form of a package for the R system, has been developed to\naddress some of these issues.\nIn this thesis, an integrated web-based platform for metabolomics data analysis and mining,\nnamed WebSpecmine, was designed and developed, based on the specmine package, thus\nproviding an easier and friendly user interface. This website provides means for analysing\nmetabolomics data from different formats, including tasks such as pre-processing, univariate\nand multivariate analysis and metabolite identification. This web-based platform was\ndeveloped collaboratively and, therefore, this work focused mainly in data from nuclear\nmagnetic ressonance and mass spectrometry.\nAlso, the package faced some limitations regarding types of analysis not yet provided,\nsuch as metabolite identification for other data formats besides Mass Spectrometry coupled\nto Liquid Chromatography. Therefore, the extension of the metabolite identification feature\nwas addressed, by implementing such analysis for Nuclear Magnetic Ressonance data in\nthe specmine package, as well as making it available in the website.\nThe website was validated by applying it to reproduce the pipelines from previous studies\nthat made use of the specmine package. Furthermore, a case study involving banana\npeels and the analysis of their characteristics and potential made use of the newly created\nwebsite to further validate its functionality. All the analyses here executed were stored and\nare available in the web application, as public projects."
  },
  {
    "keywords": [
      "Chemical vapor deposition",
      "Hexagonal boron nitride",
      "Molybdenum diselenide",
      "Two-dimensional materials",
      "Deposição química de vapores",
      "Diseleneto de molibdénio",
      "Materiais bidimensionais",
      "Nitreto de boro hexagonal",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Large-scale, controlled growth of  two-dimensional materials by chemical  vapor deposition",
    "autor": "Fernandes, João Henrique de Castro",
    "data": "2021-12-16",
    "abstract": "In this work, the optimization of two atmospheric pressure chemical vapor deposition systems was carried \nout in order to grow two different bidimensional materials, namely hBN and MoSe2. The first is an insulator \nwith a structure similar to graphene and it is seen as an optimal candidate for several applications, in \nparticular photonics and optoelectronics. The latter is a prominent semiconductor belonging to the family \nof two-dimensional transition metal dichalcogenides which demonstrated outstanding optoelectronic \nproperties, such as thickness-dependent photoluminescence, combined with lightweight and flexibility. \nSeveral deposition parameters were investigated in parallel with an extensive characterization \nmethodology carried out by optical microscopy, Raman spectroscopy, atomic-force microscopy, scanning electron microscopy, x-ray photoelectron spectroscopy, energy dispersive x-ray spectroscopy and \ntransmission electron microscopy. As a final result, reliable experimental procedures have been \nestablished which lead to the growth of few-layer polycrystalline hBN films (up to 20cm2\n) and μm-sized \nsingle crystals of monolayer MoSe2. Selected samples were tested in experimental devices. The\nfluorescent properties of 2D hBN films were probed to quantify its performance as a single-photon \nemission source at room temperature. A sensing device based on 2D MoSe2 was assembled to investigate \nthe optical response of the material to various degree of tensile strain."
  },
  {
    "keywords": [
      "Cranial implants",
      "Medical imaging",
      "Computer-aided design (CAD)",
      "Deep learning",
      "3D Shape completion",
      "Denoising autoencoders",
      "Implantes cranianos",
      "Imagem médica",
      "Desenho assistido por computador",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Automated computer-aided design of cranial implants: a deep learning approach",
    "autor": "Morais, Ana Rita André",
    "data": "2018",
    "abstract": "Over the past decade, there has been an increase in the number of surgeries for reconstruction\nof cranial defects with implants. Computer-aided Design (CAD) software enables\nthe design of patient-specific cranial implants, therefore increasing the reliability of the\nreconstruction, but there is often a lack of appropriate software. Either the software is expensive\nto acquire which limits its availability or it is not user-friendly for clinicians and\ntherefore very time-consuming to use. This thesis proposes a Deep Learning (DL) approach\ntowards the automated CAD of cranial implants, allowing the design process to be less\nuser-dependent and even less time-consuming.\nThe problem of reconstructing a cranial defect, which is essentially filling in a hole in a skull,\nwas posed as a 3D shape completion task and, to solve it, a Volumetric Convolutional Denoising\nAutoencoder (DAE) was implemented using the open-source DL framework PyTorch.\nIn order to train the autoencoder, a large amount of 3D skull models was required, and\nthese were obtained by processing an open-access dataset of Magnetic Resonance Imaging\n(MRI) brain scans. The 3D skull models were represented as binary voxel occupancy grids\nand experiments were carried out for different voxel resolutions (303, 603 and 1203). For\neach experiment, the autoencoder was evaluated in terms of quantitative and qualitative 3D\nshape completion performance. The obtained results showed that the implemented Volumetric\nDAE is able to perform shape completion on 3D models of defected skulls, allowing\nfor an efficient and automatic reconstruction of cranial defects with a single forward pass\nof the trained model. Even though the current computational resources impose limitations\nin the resolution of the 3D skull models, the results presented in this thesis make it possible\nto conclude that DL can be considered a promising approach towards the automated\nreconstruction of cranial defects."
  },
  {
    "keywords": [
      "Biblioteca ELPA",
      "Computação paralela",
      "Disselénio de tungsténio",
      "Eficiência computacional",
      "Intel KNL",
      "Quantum Espress",
      "Computational efficiency",
      "ELPA-library",
      "Parallel computing",
      "Tungsten diselenide",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Performance tuning to determine electronic properties of materials with Quantum Espresso",
    "autor": "Caldas, Sérgio Manuel Rodrigues",
    "data": "2019-12-23",
    "abstract": "Desde os anos 70, a teoria do funcional da densidade tem sido uma das técnicas mais\nutilizadas em física quântica para resolver a equação de Schrödinger, para determinar as\npropriedades eletrónicas dos materiais, usando funções de onda eletrónica e a energia de\ncada eletrão. O método de resolução do campo autoconsistente (sigla em inglês SCF) é um\nprocesso iterativo que calcula a densidade dos eletrões a partir de funções de onda. Estes\ncálculos com a equação de Schrödinger são realizados múltiplas vezes de forma sucessiva\naté se atingir a convergência autoconsistente.\nO SCF neste processo iterativo é atualmente calculado em pacotes de software dedicado,\ncomo o Quantum ESPRESSO (QE), um produto open-source em Fortran 90, para cálculo da\nestrutura eletrónica dos materiais. Sendo estes cálculos computacionalmente intensivos, a\nsua execução paralela permite melhorar o desempenho do processo de cálculo. O Quantum\nESPRESSO (QE) suporta paralelismo em ambiente de memória distribuída, com message\npassing interface (MPI) e, mais recentemente, em memória partilhada, com OpenMP.\nA presente dissertação apresenta várias propostas de instalação e configuração desta fer ramenta. Estas propostas sugerem diferentes estratégias de paralelismo tendo em vista\nobter melhorias de desempenho deste tipo de simulações, comparativamente a um estudo\nanterior realizado nas mesmas condições de experimentação. Para o presente estudo foram\nutilizados processadores multicore e many-core do cluster SeARCH. Estes testes exploraram\no impacto de versões multiprocesso com múltiplos fios de execução por processo, introduzi das em versões mais recentes do QE com desenvolvimento de paralelizações híbridas.\nAtravés de diferentes casos de teste, diferentes instalações e parâmetros configuráveis\n(número de pools) este trabalho explorou e procurou obter um ambiente de execução que\nmelhor favorecia o desempenho de simulações do tungsten diselenide (WSe2) no cluster\nSeARCH. Os resultados obtidos nestes testes, onde se aconselham certas configurações e\nse desaconselham outras, destinam-se a ajudar as comunidades de Física a encontrar um\nambiente de execução afinado em termos de desempenho, para o caso concreto deste tipo\nde simulações."
  },
  {
    "keywords": [
      "Automação",
      "Containerisação",
      "Integração contínua",
      "Orquestração de containers",
      "Automation",
      "Container orchestration",
      "Containerisation",
      "Continuous integration",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Programmable environments for quick orchestration of deployments",
    "autor": "Poleri, Maria Helena Ribeiro",
    "data": "2020-12-22",
    "abstract": "Com o surgimento da computação em cloud, tem havido uma crescente adoção da containerisação\n(containerisation) e orquestração de containers para o desenvolvimento de software. As empresas que\naderem a práticas de ContinuousIntegration/ContinuousDelivery(CI/CD)e microserviços beneficiam\nmuito da adoção destas tecnologias, pois os containerstem permitido o aprovisionamento mais rápido e\nautomatizado de aplicações, melhorando a sua escalabilidade e capacidade de tolerância a faltas.\nA Feedzai é uma empresa que usa algoritmos de machine learning para combater a fraude financeira,\nusando um sistema distribuído complexo constituído por múltiplas tecnologias. Escrever configurações\npara ambientes de teste nestas condições é frequentemente um desafio para o engenheiro de testes,\nespecialmente se feito manualmente, resultando num maior custo em horas-humano necessárias para\ndesempenhar esta tarefa. Quando se trata de ambientes de teste, estas configurações dependem muito da\ntopologia requerida pelo teste, o que resulta num potencialmente grande e crescente número de ficheiros\nde configuração para gerir. É obrigatório resolver este problema cedo, de forma a antecipar uma stack\ntecnológica difícil de gerir à medida que os casos de teste que terão de ser cobertos crescem. Este\nproblema foi alvo de várias tentativas de solução por parte de engenheiros na Feedzai, mas as soluções\nresultantes provaram, com o passar do tempo, ser insuficientes, resolvendo apenas parte do problema.\nEsta dissertação apresenta a arquitetura e principais decisões de implementação do ProgrammableEnvironmentsforQuickOrchestrationofDeployments(Pequod), uma framework que se propõe a resolver o problema supramencionado ao permitir ao programador lançar um ambiente composto por uma stack tecnológica arbitrária usando uma qualquer tecnologia de containerisação/orquestração escolhida pelo\nmesmo. Com esta ferramenta, o programador apenas escolhe quais os componentes que serão lançados e descreve as dependências entre os mesmos; a lógica de configurar estes componentes usando a\ntecnologia escolhida é executada pelo Pequod, sem que o programador tenha de ficar familiarizado com\nesta. O desenho da Domain-SpecificLanguage(DSL)que permite ao programador definir o ambiente de\nforma transparente é também discutido aqui.\nO presente documento apresenta também uma avaliação das capacidades desta framework usando dois\nprodutos distintos da Feedzai. Os resultados desta avaliação revelaram que esta nova framework está em\nconformidade com os objetivos delineados de início, resolvendo os problemas que as soluções antecessoras não resolviam."
  },
  {
    "keywords": [
      "DSL",
      "Architectural degradation",
      "Code anomalies",
      "Usability",
      "CDN framework",
      "Metrics",
      "681.3.06",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automatic detection of architectural violations in evolutionary systems",
    "autor": "Albuquerque, Diego de Lara e",
    "data": "2014-07-18",
    "abstract": "Software applications evolve over the years at a cost: their architecture modularity tends to be degraded. This happens mainly because software application maintenance often leads to architectural degradation. In this context, software architects need to elaborate strategies for detecting architectural degradation symptoms and thus maintaining the software architectural quality. The elaborations of these strategies often rely on tools with domain-specific languages (DSLs), which help them to specify software architecture rules. These tools also enforce the adherence of these rules in the evolving program. However, their adoption in mainstream software development is largely dependent on the usability of the language. Unfortunately, it is also often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. Usability is a multi-faceted quality characteristic, which is challenging to quantify before a DSL is actually used by its stakeholders. There is even less support and experience on how to quantitatively evaluate the usability of DSLs used in software maintenance tasks. To this end in this dissertation, a usability measurement framework was developed based on the Cognitive Dimensions of Notations (CDN). The framework was evaluated both qualitatively and quantitatively using two textual DSLs for architecture rules in the context of two evolving object-oriented systems. The results suggested that the proposed metrics were useful: (1) to early identify the DSL usability limitations to be addressed, (2) to reveal specific features of the DSLs favoring software maintenance tasks, and (3) to successfully analyze eight usability dimensions that are critical in many DSLs. However, along with these results this evaluation also revealed that this kind of tools lack support for communication among the stakeholders, creating a gap in the software development. To solve this problem we proposed heuristics for tools that use DSLs for detecting architecture degradation symptoms. These heuristics will permit the exchange of information between the stakeholders, thereby, also increasing the tool usability. Finally, we chose TamDera as the tool to implement these heuristics in our study domain. Therefore, we implemented in the new version of TamDera the communication support for the stakeholders by using a new architecture and a new environment with the developed heuristics."
  },
  {
    "keywords": [
      "Genómica",
      "Alterações genéticas",
      "Exomas",
      "Distribuição alélica",
      "População",
      "Genomics",
      "Variants",
      "Exomes",
      "Allele distribution",
      "Population",
      "Ciências Naturais::Outras Ciências Naturais"
    ],
    "titulo": "Characterization of genetic variants in 70 portuguese individuals",
    "autor": "Martins, Daniel Eduardo Fernandes",
    "data": "2018-11-23",
    "abstract": "A análise genómica das populações tem contribuído significativamente para o aumento do número de \nSNVs descritos em bases de dados. Estudos populacionais prévios têm contribuído com 18 a 57% novas \nvariantes. A nova informação genética é particularmente relevante enquanto referência para propósitos \nclínicos. Iniciativas à escala global como o 1000 Genomes Project (1kG) incluem populações Ibéricas, \ncontudo, nenhum indivíduo Português foi incluído no mesmo grupo. Tanto quanto se sabe, nenhum \nindivíduo Português foi incluído no projeto gnomAD, o maior conjunto de dados genómicos atual. \nAcreditamos que uma coleção de informação genómica referente à população Portuguesa poderia trazer \ngrandes benefícios ao diagnóstico molecular em pacientes Portugueses. \nAs alterações genéticas detetadas em 70 indivíduos Portugueses foram inseridas em uma base de dados \nnão-relacional. A informação publicada pelos projetos 1kG e gnomAD para cada alteração incluída nas \nmesmas foi adicionada à referida base de dados. Frequências alélicas reportadas para sete populações \nincluídas na base de dados do gnomAD, cinco populações do 1kG e 5 subpopulações Europeias do \nmesmo projeto foram comparadas contra os valores calculados para os nossos dados. As diferenças das \ndistribuições alélicas foram testadas com o Fisher’s Exact test. Os p-values obtidos foram corrigidos de \nacordo com a sua False Discovery Rate (FDR). \nOs exomas de indivíduos Portugueses analisados continham 224,155 alterações genéticas filtradas de \nacordo com critérios de qualidade definidos no presente estudo. Aproximadamente 16,4% das variantes \nnão se encontravam descritas nas bases de dados dos projetos 1kG e gnomAD. Os resultados obtidos \nendossam evidências, previamente descritas na literatura, de uma correlação entre as diferenças \ngenéticas das populações comparadas em relação à população Portuguesa e a distância geográfica das \nmesmas a Portugal. Diferenças significativas entre distribuições alélicas da população estudada e outras \nsubpopulações Europeias foram encontradas para 7,284 alterações genéticas distribuídas por 2,571 \ngenes. Os resultados obtidos sugerem a existência de marcadores genéticos populacionais e podem \nmotivar futuros estudos com vista a detetar marcadores genéticos específicos da população Portuguesa. \nO estudo apresentado representa uma contribuição significativa para, não só enriquecer iniciativas \ngenómicas de grande escala, mas também para estabelecer uma referência auxiliar para análises \ngenéticas a doentes Portugueses."
  },
  {
    "keywords": [
      "Sistemas de apoio à decisão clínica",
      "openEHR",
      "Interoperabilidade semântica",
      "Motor de inferência",
      "Módulo de apoio à decisão",
      "Guidelines",
      "Interoperabilidade semântica",
      "Registos eletrónicos de saúde",
      "Clinical decision support system",
      "Semantic interoperability",
      "Inference engine",
      "Decision support module",
      "Electronic health records",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistema de apoio à decisão clínica baseado em regras utilizando o modelo openEHR",
    "autor": "Silva, Sarah Tifany da",
    "data": "2022-05-18",
    "abstract": "Atualmente os Sistemas de Apoio à Decisão Clínica (SADC) são um componente essencial para qualquer sistema informático clínico. A sua correta integração nos sistemas existentes permite uma maior\neficiência na aquisição e disponibilização de dados importantes e relativos a cada paciente. Deste modo, é\nimperativo que um SADC garanta a interoperabilidade semântica, isto é, a capacidade de comunicar com\noutras plataformas sem que hajam perdas de significado no seu conteúdo. É necessário também garantir\nque estes sistemas sejam de fácil integração, de modo a possibilitar a agregação da maior quantidade\nde dados clínicos possível dos diferentes sistemas existentes nas instituições.\nCom isto, surge então a necessidade de optar por padrões que garantam a interoperabilidade semântica deste sistemas e que facultem meios para a sua rápida integração. Neste sentido, emerge o\nopenEHR, um padrão e-health aberto que oferece os meios necessários para garantir as propriedades\nsupracitadas anteriormente. Adicionalmente, mas não menos importante, este apresenta um módulo de\napoio à decisão que permite codificar a lógica de decisão presente nos planos e guidelines clínicos.\nSão várias as definições existentes para o conceito de guideline, no entanto, pode-se assumir, para\nefeitos de simplificação, o guideline como sendo um conjunto de declarações que visam guiar o profissional de saúde na tomada de decisão no seu dia-a-dia. Este tipo de sistema é relevante pois visa minimizar\npossíveis erros de teor humano que podem facilmente ser cometidos por parte dos profissionais que estão sujeitos diariamente a vários factores como turnos extensos, cansaço, entre outros e que em última\ninstância podem compremeter a tomada de uma decisão acertada.\nAssim, a presente dissertação tem como finalidade desenvolver um Sistema de Apoio à Decisão\nClínica, baseado em regras e no modelo openEHR, que permite a criação, gestão e execução de guidelines\nclínicos. Para além disso, foi-se mais além e desenvolveu-se uma interface gráfica que faculta ao utilizador\num meio simples, intuitivo e de agradável visualização."
  },
  {
    "keywords": [
      "681.324"
    ],
    "titulo": "Geração de perfis Web baseada em assinaturas",
    "autor": "Monsanto, Paulo Alexandre Ricardo",
    "data": "2013-03-22",
    "abstract": "No mundo atual, com o crescimento da Internet e o consequente aumento de informação e serviços que são oferecidos pelas empresas e organizações no seu ambiente torna-se premente desenvolver técnicas que facilitem a navegação dos utilizadores por este enorme espaço virtual. A forma como interagimos com os diversos sítios presentes na Internet define um determinado comportamento, os nossos hábitos, os nossos costumes. De facto, no nosso dia-a-dia, e depois de frequentar durante bastante tempo um mesmo estabelecimento, apreciamos o cuidado com que, por vezes, sem nada dizer, o que mais apreciamos é posto à frente e à nossa disposição sem que sejamos consultados. Simplesmente conhecem-nos. Os sítios na Internet cada vez mais tentam ter esse mesmo cuidado com os seus utilizadores. Todavia, a comunidade cibernauta é, como sabemos, muito vasta e heterógena e, como tal, saber os hábitos e costumes de tantos indivíduos é uma tarefa complicada.\n\nO uso de perfis é uma ação normal na caracterização de utilizadores, seja por questões de segurança ou funcionais. Um determinado utilizador pode sempre enquadrar-se num ou noutro perfil, em que cada um deles determina o acesso a este ou àquele tipo de informação ou funcionalidade usualmente oferecida por um sítio presente na Internet. Este tipo caracterização pode permitir o agrupamento de utilizadores por diversos perfis, facilitando a gestão de informação e serviços, aproximando-os às necessidades reais dos utilizadores. Contudo uma das questões relacionadas com este tipo de caracterização de perfis é o facto de ela ser estática ao longo do tempo. Os nossos comportamentos e hábitos, como é conhecido, podem não o ser. O conhecimento de “Quem nós somos” num sítio pode sofrer alterações ao longo do tempo. As nossas características de consumo e as nossas preferências podem mudar, o que nos define perante ele, a nossa assinatura, pode ter variações.\n\nAs características de utilização que os diversos sítios presentes Internet valorizam mais varia de sítio para sítio. Questões de consumo, por exemplo, serão provavelmente mais valorizadas em sítios que ofereçam produtos e serviços, em detrimento de outras questões, dependendo, obviamente dos objetivos de negócio do sítio em questão. Com certeza que, a definição de quais as características mais importantes num determinado contexto irá definir os atributos da assinatura dos utilizadores para esse sítio, tendo cada utilizador um valor diferente de acordo com esses atributos. Cada utilizador terá a sua assinatura. O valor da assinatura dos utilizadores ao longo do tempo tem que ser determinada por processos de cálculo específicos e ajustados ao contexto em questão e à informação disponível. O uso de técnicas de mineração de dados e de extração de conhecimento é, assim, essencial para este processo.\n\nA definição e o cálculo da variação de assinaturas de utilizadores para um dado sítio permitem a realização de várias análises. Uma delas é a análise do perfil de um utilizador ao longo do tempo. A variação da assinatura de um utilizador poderá indiciar uma alteração no seu perfil de comportamento perante o sítio que frequenta. O sítio, sabendo dessa alteração, poderá reagir de forma dinâmica e de diversas formas. Por exemplo, alterando o conteúdo ou a estrutura do próprio sítio. Neste trabalho de dissertação foram exploradas estas temáticas. Mais especificamente pretendeu-se aprofundar a definição de assinatura de um utilizador Web e a sua associação aos diversos padrões de utilização de um sítio. No âmbito deste trabalho, esses padrões foram extraídos recorrendo a técnicas de mineração de dados a partir de diversas fontes de informação disponibilizadas pelos servidores que alojam os sítios. As técnicas utilizadas na extração desse conhecimento são também abordadas ao longo desta dissertação, com o objetivo de fornecer uma perspetiva global tanto do processo de mineração de dados em si, como da posterior associação do conhecimento extraído às assinaturas definidas para os utilizadores de um sítio específico que escolhemos como alvo para o nosso estudo."
  },
  {
    "keywords": [
      "614:681.3",
      "681.3:614",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Usabilidade e aceitação de tecnologia : estudo de casos em sistemas de informação hospitalar",
    "autor": "Novo, Ana Cristina Gomes Ribeiro",
    "data": "2014",
    "abstract": "O aumento exponencial da informação médica e a procura pela otimização da prestação de cuidados de saúde, conduziu à introdução das tecnologias\nde informação na área da saúde. Não existe qualquer dúvida de que a introdução do Processo Clínico Eletrónico (PCE) na área da saúde é uma mais\nvalia para a qualidade dos cuidados prestados aos utentes. Para além de\npromover uma maior qualidade dos serviços prestados, a implementação e\nintegração deste tipo de sistemas em ambiente hospitalar, aumenta a segurança dos utentes e reduz os custos. No entanto, a adoção e aceitação de\nsistemas de PCE na área médica não se tem revelado uma tarefa fácil. Um\ndos principais fatores apontados para o fracasso da adoção destes sistemas\nestá associado ao seu baixo nível de usabilidade.\nDentro desta problemática, a presente dissertação pretende avaliar a usabilidade\ne a aceitação dos sistemas de informação em ambiente hospitalar que\ngarantem o registo eletrónico da informação relativa aos seus utentes. Para\nalém disto, é realizada também uma avaliação global das funcionalidades\ninerentes ao PCE implementado no Centro Hospitalar do Alto Ave (CHAA),\ncom o intuito de alcançar um ambiente hospitalar livre de papel."
  },
  {
    "keywords": [
      "Primary Health Care",
      "Emergency department",
      "Crowding",
      "Health kiosk",
      "Self-Service Kios",
      "Cuidados de Saúde Primários",
      "Serviço de Urgência",
      "Lotação hospitalar",
      "Quiosque de saúde",
      "Quiosque de auto-atendimento",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "E-anamnesis: a clinical observation electronic platform for emergency departments",
    "autor": "Santos, Fátima Jacinta Pereira dos",
    "data": "2021-04-08",
    "abstract": "One of the reasons for the increased number of visits to emergency departments is the\nprimary health care inability to handle urgent needs and provide all the health services\nneeded to assess complex conditions. A significant amount of these visits are due to the\nabnormal flow of patients whose clinical condition is of low severity and could ideally be\nresolved with self-care and primary health care.\nThe crowding in emergency departments causes operational and logistical problems and\nhas undesirable consequences for patients, health professionals and hospitals. Delays in\ntreatment interventions and increased mortality, medical errors and waiting times are just a\nphew examples of critical consequences that can occur, resulting in a significant barrier to\nthe quality of health care delivery.\nWith the advances in technology, several institutions have found in self-service an alternative\nfor the patient’s collection of health information autonomously. These devices can be used\nby low clinical severity patients (with the blue, green or yellow bracelets from Manchester\ntriage) to reduce waiting time in the emergency departments.\nThis dissertation proposes a technological solution to improve both the time and quality\nof the anamnesis procedure performed by medical staff in the emergency department. The\nintroduction of a self-service kiosk in the emergency department waiting room will make it\npossible to quickly and intuitively collect the patient’s past medical history, usual medication,\nmain complaint symptoms and vital signs. Subsequently, this data will be made available\nto the physician before each clinical observation. The hypothesis considered is that by\nproviding a selective, structured and uniform anamnesis information’s presentation of each\npatient, medical staff observation can proceed much faster and accurately, focusing on the\nconfirmation of the most relevant aspects. The primary purpose of this solution is to reduce\nthe period of clinical observation and thus improve the response capacity of the emergency\ndepartment with the same resources."
  },
  {
    "keywords": [
      "Schemas",
      "JSON",
      "XML",
      "Data generation",
      "Synthetic data",
      "DataGen",
      "DSL",
      "Dataset",
      "Grammar",
      "Randomization",
      "Open source",
      "Data science",
      "REST API",
      "PEG.js",
      "Geração de Dados",
      "Dados sintéticos",
      "Gramática",
      "Aleatoriedade",
      "Ciência de dados",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Synthetic data generation from JSON/XML schemas",
    "autor": "Cardoso, Hugo André Coelho",
    "data": "2022-12-19",
    "abstract": "The objective of this dissertation is the development of an application capable of automatically\ngenerating synthetic datasets that are representative and, possibly, very large, directly from\nJSON and XML schemas, in order to facilitate the testing of software applications and\nscientific endeavors in areas such as Data Science or Application Development.\nFor this purpose, it is intended to develop a new version of DataGen, an online open-source\napplication that allows the quick prototyping of datasets through its own Domain Specific\nLanguage (DSL) of specification of data models. DataGen is able to parse these models and\ngenerate synthetic datasets according to the structural and semantic restrictions stipulated,\nautomating the whole process of data generation with spontaneous values created in runtime\nand/or from a library of support datasets.\nThe objective of this new product, DataGen From Schemas, is to expand DataGen’s use\ncases and raise the datasets specification’s abstraction level, making it possible to generate\nsynthetic datasets directly from schemas. This new platform builds upon its prior version\nand acts as its complement, operating jointly and sharing the same data layer, in order to\nassure the compatibility of both platforms and the portability of the created DSL models\nbetween them. Its purpose is to parse schema files and generate corresponding DSL models,\neffectively translating the JSON or XML specification to a DataGen model, then using the\noriginal application as a middleware to generate the final datasets.\nThe present dissertation details the entire creative process behind the development of this\napplication: firstly, it frames the topic of study and its initial phase of investigation, debating\nrelevant technologies and existing related work; then, the ideation phase of the product is\naddressed, projecting an adequate arquitecture and the reasons behind its design choices,\nas well as surveying technical requirements for DataGen From Schemas, while taking into\naccount the conclusions reached through prior research; afterwards, the development phase\nis covered, carefully explaining the elaborated components, their properties and the data flow\nbetween them, for both the JSON and XML modules; finally, the reader is presented with\nconclusions taken from this project’s development and possible future work to implement,\nin order to improve the current solution."
  },
  {
    "keywords": [
      "Real-time analytics",
      "Big data",
      "NoSQL databases",
      "Machine learning",
      "Biometrics",
      "Mouse dynamics",
      "Indicadores em tempo real",
      "Bases de dados NoSQL",
      "Biométricas comportamentais",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Real-Time intelligence",
    "autor": "Araújo, Daniel Coutinho",
    "data": "2016",
    "abstract": "Over the past 20 years, data has increased in a large scale in various fields. This explosive\nincrease of global data led to the coin of the term Big Data. Big data is mainly used to describe\nenormous datasets that typically includes masses of unstructured data that may need\nreal-time analysis. This paradigm brings important challenges on tasks like data acquisition,\nstorage and analysis. The ability to perform these tasks efficiently got the attention\nof researchers as it brings a lot of oportunities for creating new value. Another topic with\ngrowing importance is the usage of biometrics, that have been used in a wide set of application\nareas as, for example, healthcare and security. In this work it is intended to handle\nthe data pipeline of data generated by a large scale biometrics application providing basis\nfor real-time analytics and behavioural classification. The challenges regarding analytical\nqueries (with real-time requirements, due to the need of monitoring the metrics/behavior)\nand classifiers’ training are particularly addressed."
  },
  {
    "keywords": [
      "Machine learning",
      "CAA",
      "Medical imaging",
      "MRI",
      "Biomarkers",
      "Artificial intelligence",
      "AAC",
      "Imagiologia médica",
      "Imagem por ressonância magnética",
      "Biomarcadores",
      "Inteligência artificial",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Characterizing and revealing biomarkers on  patients with Cerebral Amyloid Angiopathy using artificial intelligence",
    "autor": "Silva, Fátima Solange Lima Rezende da",
    "data": "2020-10-06",
    "abstract": "Cerebral Amyloid Angiopathy is a cerebrovascular disorder resulting from the deposition of an \namyloidogenic protein in small and medium sized cortical and leptomeningeal vessels. A \nprimary cause of spontaneous intracerebral haemorrhages, it manifests predominantly in the \nelder population. Although CAA is a common neuropathological finding on itself, it is also \nknown to frequently occur in conjunction with Alzheimer’s disease, being sometimes \nmisdiagnosed. \nCurrently, CAA diagnosis is generally conducted by post-mortem examination or, in live \npatients by the examination of an evacuated hematoma or brain biopsy samples, which are \ntypically unavailable. Therefore, a reliable and non-invasive method for diagnosing CAA would \nfacilitate the clinical decision making and accelerate the clinical intervention.\nThe main goal of this dissertation is to study the application of Machine Learning (ML) to reveal \npossible biomarkers to aid the diagnosis and early medical intervention, and better \nunderstand the disease. Therefore, three scenarios were tested: Classification of four \nneurodegenerative diseases with annotation data obtained from visual rating scores, age and \ngender; Classification of the diseases with radiomic data derived from the patient’s MRI; and \na combination of the previous experiments. The results show that the application of Artificial \nintelligence in the medical field brings advantages to support the physicians in the decision making process and, at some point, make a correct prediction of the disease label.\nAlthough the results are satisfactory, there are still improvements to be done. For instance, \nimage segmentation of cerebral lesions or brain regions and additional clinical information of \nthe patients would be of value."
  },
  {
    "keywords": [
      "Automatic speech emotion recognition",
      "Semi supervised learning",
      "Human emotion",
      "Unlabeled dataset",
      "Aprendizagem semi supervisionada",
      "Emoção humana",
      "Dados sem anotação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Emotion recognition: recognition of emotions through voice",
    "autor": "Andrade, Guilherme Marques",
    "data": "2022-04-11",
    "abstract": "As the years go by, the interaction between humans and machines seems to gain more and more importance\nfor many different reasons, whether it's taken into consideration personal or commercial use. On a time\nwhere technology is reaching many parts of our lives, it's important to keep thriving for a healthy progress\nand help not only to improve but also to maintain the benefits that everyone gets from it. This relationship\ncan be tackled through many points, but here the focus will be on the mind.\nEmotions are still a mystery. The concept itself brings up serious questions because of its complex nature.\nTill the date, scientists still struggle to understand it, so it's crucial to pave the right path for the growth on\ntechnology on the aid of such topic. There is some consensus on a few indicators that provide important\ninsights on mental state, like words used, facial expressions, voice.\nThe context of this work is on the use of voice and, based on the field of Automatic Speech Emotion\nRecognition, it is proposed a full pipeline of work with a wide scope by resorting to sound capture and\nsignal processing software, to learning and classifying through algorithms belonging on the Semi Supervised\nLearning paradigm and visualization techniques for interpretation of results. For the classification of the\nsamples,using a semi-supervised approach with Neural Networks represents an important setting to try\nalleviating the dependency of human labelling of emotions, a task that has proven to be challenging and,\nin many cases, highly subjective, not to mention expensive. It is intended to rely mostly on empiric results\nmore than theoretical concepts due to the complexity of the human emotions concept and its inherent\nuncertainty, but never to disregard prior knowledge on the matter."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistema de apoio à decisão na área de nutrição",
    "autor": "Abelha, Carlos Vasco Pinheiro da Costa",
    "data": "2018-12-20",
    "abstract": "Over the past few years the use of information technology (IT) and computer applications have been distributed among various sectors, including food distributors. The reason for this to be happening is the capability of technology to transform and improve radically the quality of the service of a company. The main objective of food retail/food distribuition\ncompanies in healthcare, as well as social services, is the ability to provide the best service to its patients and the capability to deliever high quality meals, while reducing the costs and the waste involved on this long process. Therefore, for this to happen, decisions need\nto be made as fast and effective as possible. This being the case, for a bright future of an IT in the food distribution sector it is required the development of systems capable of ma naging all the resources, providing a better service and a greater satisfaction among users, through the use of costumized meal plans, focusing on quality. Hereupon, in the context of this master’s dissertation, the aim of this present work was develop and explore Web\nInformation Systems to support decisions making process in nutrition. Thus, it includes the development of a Web platform that works as a Information system for a food retail institution and as a decision support system that creates meal plans for all its clients taking into account detailed information present in the organization.\nThe tools were designed in order to help the administration of Cantina Social do Hospital da Misericordia de Vila Verde in their daily work."
  },
  {
    "keywords": [
      "Arquiteturas de software",
      "CQRS",
      "Microsserviços",
      "Padrões de desenho",
      "Saga",
      "Software architecture",
      "Microservices",
      "Design patterns",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Estudo sobre a importância dos princípios e padrões das arquiteturas orientadas a microsserviços",
    "autor": "Pereira, José André Martins",
    "data": "2023-07-13",
    "abstract": "Com o atual crescimento do desenvolvimento de software e aumento da procura para solução \nde diversos problemas, começa-se a verificar a incapacidade de certas abordagens \narquiteturais para lidarem com alguns dos desafios atuais. Efetivamente, alguns destes \ndesafios estão relacionados com o aumento da adesão das pessoas à tecnologia, \npersonalização das aplicações, implementação rápida de novas funcionalidades, crescimento \ne complexidade das aplicações, otimização da produtividade das equipas de desenvolvimento, \nadequação das melhores tecnologias, entre outros. Deste modo, as arquiteturas orientadas a \nmicrosserviços resolvem alguns destes problemas atuais. Este projeto de dissertação tem \ncomo objetivo estudar as arquiteturas orientadas a microsserviços, os seus princípios, padrões \ne testar a aplicabilidade dos mesmos a um caso de estudo do mundo real, um sistema E commerce, com a finalidade de resolver alguns dos problemas e desafios desta abordagem \narquitetural. As categorias de padrões de arquiteturas orientadas a microsserviços estudadas \nsão: Decomposição, Manutenção de Dados, Mensagens Transacionais, APIs Externas, \nDescoberta de Serviços e Segurança. Dos padrões estudados e implementados, os que tiveram \nresultados interessantes foram o Saga e CQRS, devido a resolverem problemas relacionados \ncom a manutenção dos dados, que se torna complexa com a característica distribuída deste \ntipo de arquiteturas. A avaliação da aplicabilidade destes padrões ao caso de estudo, faz-se\nem comparação do desenho e implementação, com o estudo do estado de arte, através dos \npontos positivos e negativos, bem como são efetuados testes à aplicação desenvolvida para \nconclusão de resultados. Por fim, foram efetuados testes de carga, para verificar a capacidade \nde escalabilidade, mas principalmente o impacto que as decisões arquiteturais têm na \nperformance e disponibilidade das funcionalidades."
  },
  {
    "keywords": [
      "Chatbot",
      "Conversational agent",
      "Helpdesk",
      "Natural language processing",
      "Agente conversacional",
      "Suporte",
      "Processamento de linguagem natural"
    ],
    "titulo": "Chatbot for VILT’S helpdesk team",
    "autor": "Ribeiro, Diogo Pinto",
    "data": "2022-10-18",
    "abstract": "The existence of helpdesk teams is a common occurrence in companies nowadays. These teams\nare an expensive resource and can serve a limited number of users, which evidences the importance of\nhelpdesk teams operating as efficiently as possible. One common occurrence is the existence of a set of\nrepeated actions that could be automated to allow a helpdesk team to focus on other tasks, thus allowing\nto increase its productivity, as well as serving a larger quantity of users with a lower response time.\nIn this context, VILT, an international software company, came up with a Master’s thesis proposal to\nautomatize the answering of technical queries from its programmers and the triggering of some usual\nroutines and procedures needed to help the referred collaborators. The proposal of a chatbot to solve this\nproblem is an interesting approach given the ease of use and popularity of chatbots in the industry. A\nchatbot is capable of answering a variety of questions with a lower response time compared to a human\ncounterpart and is capable of delivering these answers at any given time.\nThis Master’s dissertation in the area of Informatics Engineering describes the proposal and develop ment of Triton, a chatbot built to answer VILT’s demands referred to above. Triton provides an interface\nwith which users may interact, being capable of solving a predetermined set of common problems that\nwere previously handled by a member of the helpdesk team. Throughout the development of this chatbot,\nthe users were included in different processes so that the final solution may be as adequate as possible\nto their needs. A comprehensive evaluation of the implemented solution is presented in this dissertation,\nand the results show that Triton is indeed capable of helping VILT’s employees with common problems\nand questions, therefore assisting the helpdesk team."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Distribuição e sincronização de áudio digital em redes sem fios",
    "autor": "Gylytskyy, Oleksii",
    "data": "2018",
    "abstract": "Como em todas as áreas tecnológicas, também os utilizadores de sistemas áudio procuram soluções o mais práticas, convenientes e eficazes possível. As tecnologias sem ﬁos têm vindo, gradualmente, a substituir as soluções com ﬁos (ou cabladas) nos sistemas de distribuição de áudio, tanto, digital como analógico. O mercado está repleto de soluções que cumprem este propósito em variados graus de usabilidade e capacidades técnicas, sendo que o maior desaﬁo é colocado pelo problema da sincronização na distribuição individual e independente de canais áudio no formato digital. As soluções atuais são quase exclusivamente versões comerciais, utilizando algoritmos e tecnologias proprietárias relativamente complexas e, como tal, não gratuitas. Este aspeto tende a atrasar o desenvolvimento de novos sistemas. Esta dissertação documenta o projeto que teve como principal objetivo provar que é possível construir um sistema de distribuição independente e simultânea de canais áudio digitais, alcançando níveis ótimos de sincronização usando algoritmos e mecanismos alternativos sobre tecnologias bem conhecidas de rede local sem ﬁos. Assim, este relatório apresenta a investigação elaborada sobre tecnologias universais e gratuitas, que facilitem o desenvolvimento de um novo mecanismo de livre acesso para a distribuição digital de áudio sobre redes locais IEEE 802.11. Esse mecanismo e os algoritmos associados são discutidos em detalhe, sendo depois apresentados os passos da implementação e teste desse mecanismo num sistema protótipo. O protótipo desenvolvido oferece um nível excelente de sincronização dos canais áudio em todos os dispositivos reprodutores do sistema, mesmo quando sujeito a condições de stress numa rede sobrecarregada. A eﬁcácia do sistema foi medida analiticamente e não foi detetada nenhuma deterioração signiﬁcativa na sincronização dos canais áudio nos sistemas reprodutores ﬁnais. Também nos testes empíricos de audição os utilizadores não conseguiram detetar qualquer nível de dessincronização. Espera-se que a tecnologia desenvolvida, implementada e testada, permita o aparecimento futuro de novas soluções gratuitas, ou comercialmente mais competitivas, mas que poderão rivalizar com as soluções atuais em termos das capacidades técnicas de distribuição de áudio digital, sobre redes locais de utilização genérica e comum."
  },
  {
    "keywords": [
      "prototype",
      "industry",
      "autonomous vehicles",
      "user interfaces",
      "software development",
      "software design",
      "software architecture",
      "microservices",
      "logistics",
      "protótipo",
      "indústria",
      "veículos autónomos",
      "interfaces gráficos",
      "desenvolvimento de software",
      "design de software",
      "arquitetura de software",
      "microsserviços, logística"
    ],
    "titulo": "Software development for monitoring and supporting intralogistics vehicles: From a laboratory prototype to an industrial prototype",
    "autor": "Fernandes, Ana Esmeralda Alves",
    "data": "2019",
    "abstract": "Technological progress and advancements are constant and natural. The extreme impact\nand the daily use of everything related to technology on everyone’s day to day life is irrefutable.\nThe inclusion of robots and automated vehicles and machines is increasing because\nof the advantages linked to their use.\nThis reality led to the beginning of a project, Smart Autonomous Mobile Units (SAMU),\nincluded in the iFactory program which comes from a partnership between Bosch Car\nMultimedia (CM), and University of Minho. The project consists of the automation of\nspecific industrial vehicles used in the movement of materials and finished products that\nsupport the intralogistics processes.\nThis dissertation presents the software process behind the transformation of a Laboratory\nPrototype to an Industrial Prototype. A laboratory prototype was the result of the previous\nstages of development since this prototype was created and tested only in a controlled\nenvironment. For the next step of the SAMU project, the Proof of Concept, it is mandatory\nto increase the maturity level of this prototype. The PoC will be performed on a real\nindustrial environment having a lot more adversities than the prior tests setting where\neverything was under control. On the Bosch Braga plant the autonomous vehicles have to\ninteract with manually-driven vehicles as well. For this reason, the necessity of creating\na system that could coordinate the allocation of logistic services to each vehicle had to\nemerge.\nA new solution was structured and developed following the basics of software design\nand architecture to modify and complement the former system with all the functions it is\nrequired to have. The final solution for the central platform of the industrial prototype is\ncomposed of seven microservices and each of them have a different purpose and business\nlogic behind. The improvement of the central platform led to the development of new\nfunctionalities, such as the organization of the vehicles’ fleet and the visualization of the\nvehicles dynamically moving across their area of operation.\nThe created solution was proven, by the results, to be functioning properly and as intended.\nThe use of microservices reinforces their advantages for this specific context, since\nthe deployment becomes easier and their maintenance is more straightforward. Another\nimportant aspect is that the addition of new functionalities is also a more effortless process.\nConsidering that the previous solution was being developed with microservices the preservation\nof this type of architecture was the right answer for the continuation of development."
  },
  {
    "keywords": [
      "681.324",
      "616-053.2"
    ],
    "titulo": "Mobile application to support pediatric medical practice",
    "autor": "Oliveira, Rui Emanuel Barros",
    "data": "2013",
    "abstract": "The objective of this project is to develop a mobile application in order to aid pediatricians performing their\nwork. The necessity of this application was initially identified by a pediatrician working in Santo António\nHospital of Oporto, after also verifying the interest of some of his coworkers. The bibliography also states\nsome situations where mobile applications may be helpful, such as: errors in the administration of drugs or\nthe difficulty pediatricians face in performing needed mathematical operations.\nIt is made a review of pediatric applications, mobile mostly, in order to know what kinds of applications are\nalready available for pediatricians. It is presented the analysis of 5 distinct applications, from medical\ncalculators for emergency situations to decision support systems that given a set of clinical characteristics\nit is provided a list of diagnosis to consider.\nFollowing it is done a study of requirements elicitation and prioritization. Its objective is to know the\ntechniques and tools already studied in the bibliography, as well as to identify the most appropriate ones\nfor this project. Several elicitation and prioritization techniques were used in this project. It is also used a\ntool to register the requirements.\nIn order to develop a mobile application that may run on the majority of smartphones in the market, it is\nmade an analysis of the smartphone operating systems market share, as well as of market share\nprojections for the next few years. After identifying the target operating systems for the app it is made a\nstudy of the mobile cross-platform development frameworks. The framework choice considered the elicited\nrequirements and the operating systems with the greatest market share.\nAfter a learning period of the involved technologies, the pediatric app is developed using the gathered\nrequirements and following the results of the requirements prioritization. The development of the\napplication was always followed by a pediatrician, and as a result the application was tested and refined\nduring that time. Finally, the application is released as well as a questionnaire to evaluate it."
  },
  {
    "keywords": [
      "Modelo gerador de imagens",
      "Variational autoencoder",
      "VQ-VAE",
      "PixelCNN",
      "VQ-GAN",
      "Image generation model",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Geração de imagens de faces com alta resolução utilizando variational autoencoders",
    "autor": "Gonçalves, Tiago Rico",
    "data": "2024-01-15",
    "abstract": "Atualmente, a Inteligência Artificial, e em especial a subárea da aprendizagem automática e\nprofunda, é alvo de enorme interesse por parte das comunidades académica e empresarial.\nDas áreas aplicacionais da aprendizagem automática, a par com o processamento de língua\nnatural, a visão por computador é aquela que tem suscitado mais interesse e gerado mais\nresultados científicos. A geração de imagens é um dos problemas que se enquadra na visão\npor computador.\nCom esta dissertação pretende-se estudar os modelos geradores de imagens, e de entre\nas alternativas para atacar este problema, o foco do trabalho são os modelos Variational\nAutoencoders. Na fase inicial da dissertação é feito um levantamento bibliográfico do estado\nda arte do tema do trabalho, visando adquirir os conhecimentos necessários para concretizar\na parte experimental.\nOs conhecimentos adquiridos na fase de levantamento bibliográfico foram aplicados na\nfase seguinte, onde se desenvolveu, treinou e avaliou modelos capazes de gerar imagens\nnovas com faces humanas. O foco foi a arquitetura Vector Quantized Variational Autoencoder\n(VQ-VAE), auxiliada por um modelo autorregressivo PixelCNN. No entanto, foram também\nexplorados outros modelos geradores, tendo em mente complementar o estudo em causa, e\nconsequentemente, poder tirar conclusões mais abrangentes.\nApós a implementação dos modelos, foi possível concluir que dentro de todos os modelos\ntestados o VQ-VAE apresentou o melhor desempenho, quer seja a nível qualitativo através\nda inspeção visual das faces geradas, quer seja a nível quantitativo com a aplicação da\nmétrica Frechet Inception Distance. Além do VQ-VAE, o outro modelo que se destacou foi o\nVector Quantized Generative Adversarial Network, comprovando assim o potencial da aplicação\nda quantização de vetores nos modelos geradores."
  },
  {
    "keywords": [
      "Voxel",
      "Screen-space",
      "Smoothing",
      "Reflection",
      "Refraction",
      "Ray-casting",
      "Fluid",
      "Particles",
      "Suavização",
      "Reflexões",
      "Refrações",
      "Fluidos",
      "Partículas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Real-time rendering of particle based fluid simulations",
    "autor": "Fernandes, José Carlos Pereira",
    "data": "2022",
    "abstract": "Particle based methods have been been increasing in popularity in fluid simulations as of late due to the increases\nseen in computational power in the form of various many core devices like graphics-cards (GPUs) or high-core\ncount CPUs.\nThe increasing popularity of particle based fluid simulations resulted in a need for higher fidelity renders which\nportray the particle data in a cohesive substance rather than isolated particles. Rendering the particle data in\nreal-time however is a difficult problem due to the fact that the simulations tend to be computationally intensive\nand, as a result, the rendering has to deal with computational constraints that make it difficult to achieve visual\nquality while, at the same time, keeping the interactive aspect of the simulation.\nCurrently there are various approaches to rendering particle data which thread a balance between visual qual-\nity and performance. These techniques can vary widely by having entirely different algorithms/approaches and\nhaving different objectives, some of them focusing on having the best looking surfaces, while others pretending\nto achieve the shortest render times, or even trying to have a low memory footprint.\nComputational constraints have kept certain techniques from being usable in the interactive realm. This con-\nstraints have, however, been loosening up due to hardware advances which has, in turn, been lowering the gap\nbetween high-quality renders and interactive rendering and thus opening the door to new rendering approaches.\nThis thesis aims to explore the implementation of real-time fluid rendering in conjunction with a preexisting\nparticle-based simulation. The thesis will be divided in two halves with the first half covering a screen-space\nimplementation, a family of techniques which aims to render the fluid surface while having a low computational\ncost, while the second half explores a volumetric render implementation based on a voxel grid while leveraging\nthe computational power of a modern GPU.\nThe screen-space implementation is able to represent the frontal surface of the fluid, and also implements the\nextraction of a back surface. This enables it to enhance its visual fidelity at a low computational cost. This work\nalso compares different approaches used in the process of smoothing these surfaces, which enable a higher\nsurface cohesion while tackling the preservation of the fluid’s edges.\nThe volumetric implementation uses a voxel grid to represent the fluid, enabling it to render multiple refractions\nthus achieving a more realistic render. The volumetric implementations is also able to represent occluded fluid\nfeatures such as air bubbles by leveraging the GPU memory, and a more accurate colouring of the fluid."
  },
  {
    "keywords": [
      "Análise de dados",
      "Dashboarding",
      "Indicadores para melhoria do ensino",
      "Índices de bem-estar",
      "Sistemas de avaliação de conhecimento",
      "Dashboarding",
      "Data analysis",
      "Indicators for teaching improvement",
      "Knowledge assessment systems",
      "Well-being indexes",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Índices de bem-estar para sistemas de avaliação",
    "autor": "Senra, Fábio Carvalho",
    "data": "2021-10-27",
    "abstract": "Os índices são uma excelente ferramenta de comunicação, dada a sua simplicidade, a sua facilidade em compactar a informação e em simplificar medidas complexas. O facto dos índices compostos permitirem a divisão em várias dimensões e associar-lhes ponderações torna a sua compreensão mais simples. O aumento da sua utilização vem justificar a sua versatilidade, visto que, são utilizadas nas mais variadas áreas, desde a saúde até à educação. No enorme espetro da aplicação dos índices enquadram-se os de bem-estar, que são usualmente utilizados para prever o impacto de programas e medidas construídas, tendo em conta as suas necessidades de implementação. Com o passar do tempo esta gama de índices foi sendo aplicada em vários outros domínios como, por exemplo, a progressão social ou a economia. Nesta dissertação foi desenvolvido um sistema de índices baseado na utilização de um tutor inteligente, cujo principal objetivo é ajudar alunos e professores fornecendo um sistema personalizado de formação e de avaliação de conhecimento multidisciplinar. Os alunos podem recorrer a este tutor para realizar várias atividades relacionadas com a aferição do conhecimento que possuem num dado domínio de estudo. Alguma da informação mais relevante que deriva dessa utilização é armazenada num Data Warehouse, que a manterá de acordo com as dimensões de análise do índice. Posteriormente, os dados destas dimensões são trabalhados de acordo com três métodos distintos: um sistema de equalização regular (Equalizador), no qual o utilizador define os pesos de aferição como desejar, o Analytical Hierarchy Process, em que o utilizador estabelece comparações entre todas as dimensões que originam os seus pesos e, por fim, através da classificação individual, na qual o utilizador classifica cada dimensão consoante uma escala e a sua opinião pessoal."
  },
  {
    "keywords": [
      "Deduplicação",
      "Segurança",
      "Armazenamento",
      "Hardware confiável",
      "SGX",
      "Deduplication",
      "Security",
      "Storage",
      "Trusted hardware",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "S2Dedup: SGX-enabled secure deduplication system",
    "autor": "Miranda, Mariana Martins de Sá",
    "data": "2020-12-14",
    "abstract": "Com os atuais avanços tecnológicos, cada vez mais informação está a ser digitalizada, resultando assim num aumento exponencial nos dados guardados em formato digital. Este crescimento sem precedentes tem levantado preocupações acerca do espaço e custo dos sistemas de armazenamento, criando uma necessidade de explorar mecanismos que visem mitigar este problema. Uma estratégia que se foca neste problema é a técnica conhecida por deduplicação, que se baseia no facto que dados idênticos estão a ser gerados e armazenados repetidamente, resultando assim num consumo desnecessário de espaço de armazenamento em disco. Deste modo, a deduplicação propõe uma análise dos dados armazenados e, subsequentemente, a eliminação de cópias redundantes, economizando espaço e custos de armazenamento. Serviços como DropBox e Google Drive aplicam essa estratégia, contudo, o processamento de dados que pertencem a vários utilizadores fomenta preocupações de privacidade e segurança, especialmente quando este é realizado em fornecedores de serviços de armazenamentos terceiros. A abordagem tradicional para resolver estes problemas é os utilizadores enviarem os seus dados já cifrados. Contudo, usar uma cifra probabilística implica que dados idênticos podem resultar em textos cifrados diferentes, o que torna impossível encontrar copias redundantes e, consequentemente, aplicar a deduplicação. Deste modo, propomos o S2Dedup, um sistema de deduplicação seguro que explora tecnologias emergentes de segurança assistida por hardware. Mais especificamente, a solução proposta recorre ao Intel SGX (Software Guard Extensions), de forma a permitir a deduplicação de dados entre utilizadores em infraestruturas de armazenamento de terceiros, sem descuidar da segurança e privacidade dos seus dados. Além disto, o S2Dedup foi projetado para oferecer suporte a vários esquemas de segurança, cada um oferecendo diferentes níveis de espaço poupado, desempenho e privacidade. Esta característica é fundamental para garantir a aplicabilidade do S2Dedup a uma gama ampla de sistemas com requisitos diferentes. Um protótipo do S2Dedup é implementado e avaliado com cargas de trabalho sintéticas e realistas, assim como comparado com as soluções alternativas de deduplicação seguras do estado da arte. Os resultados mostram que é possível implementar técnicas de segurança mais robustas e ao mesmo tempo manter bons resultados de desempenho e até mesmo alcançar, em alguns casos, uma melhoria na eficácia da deduplicação em comparação com as soluções do estado da arte."
  },
  {
    "keywords": [
      "Internet of Things",
      "Machine Learning",
      "Data Mining",
      "Domotics",
      "Smart Home",
      "Smart Environments",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Enhance Smart Home capabilities by learning from usage patterns and IoT devices",
    "autor": "Mirra, José Alexandre da Silva",
    "data": "2018-12-12",
    "abstract": "Domotics represent a field of consumer electronics related primarily to home automation.\nAlthough smart environments have existed for decades and even for longer in the imaginary\nof people and sci-fi, the new age of Internet of Things (IoT) and the low-cost Do It\nYourself (DIY) electronics/maker market has brought smart homes and the understanding\nof domotics closer to everyone.\nIn recent years, the interest in the fields of domotics and IoT has increased. This recent academic\nand industrial interest has contributed to the evolution of the Smart Home concept,\nbeing more and more appealing to our civilization. This has prompted new commercial solutions\non the market, which are discovering ways to expand and increase their own value\nby integrating new features, especially from the IoT market.\nThe proliferation of IoT devices leads to new sources of information. In addition, the relation\nbetween the environment occupant and the environment is responsible for adding\nvalue to this data. Afterwards, the data can then be used in a meaningful way, by machine\nlearning algorithms to learn usage patterns. Furthermore, this data may or may be\nunrelated to the data incoming from sensors.\nThis Masters Project will focus on learning strategies to allow Smart Homes to become\nintelligent, in a sense that they anticipate needs and actions, thus enabling extended features\nto the home automation system. The user should be able to give feedback on his ”feeling”\nregarding the decision making and suggestions, possible by our system implementation."
  },
  {
    "keywords": [
      "Blockchain",
      "Acordo distribuído",
      "Faltas bizantinas",
      "Distributed consensus",
      "Byzantine faults",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Acordo bizantino mútavel para a Blockchain",
    "autor": "Silva, Fábio Luís Baião da",
    "data": "2019-12-23",
    "abstract": "O principal componente de um sistema blockchain e o protocolo de acordo distribuído\nque tem de ser capaz de tolerar faltas bizantinas na chegada a decisões. Existem muitas \nimplementações de blockchain, cada uma utilizando diferentes protocolos de acordo, porém\ntodos eles revelam limitações. Implementações cujo protocolo é da categoria Proof of, apesar\nde escalarem, implicam compromissos entre desempenho e coerência. Protocolos ditos\ntradicionais (e.g. PBFT) são muito restritos na escalabilidade que oferecem, não conseguindo manter o desempenho ao aumentar o numero de participantes. Para além disso, cada protocolo foca-se em características particulares com padrões de comunicação específicos,\npelo que para alterar algum destes aspetos e necessário substituir o protocolo de acordo. \nNeste trabalho propõe-se um protocolo que combina a tolerância a faltas bizantinas com as características do protocolo Mutable Consensus que admite diferentes padrões de comunicação aplicáveis a diferentes ambientes. Adicionalmente, um desses padrões que privilegia uma comunicação por difusão epidémica (gossip) oferece grande escalabilidade, permitindo assim construir um protocolo que também possa escalar."
  },
  {
    "keywords": [
      "681.324"
    ],
    "titulo": "Descarga temporal de páginas web",
    "autor": "Pereira, Pedro Vasco Neto",
    "data": "2013",
    "abstract": "There is a plethora of information inside the Web. Even the most famous commercial\nsearch engines cannot download and index all available information. For\nthis reason, from the last years until now, there are several research works on the\ndesign and implementation of focused crawlers in a particular topic, and also on\ngeographic scope crawlers.\nThose who follow carefully the research on the area of Web crawling are witnessing\nthat the temporal dimension has not the importance it deserves in the\nliterature.In the opposite direction, there is an increasing interest on time dimension\nin other areas of information retrieval namely retrieval models, result sets\npresentation, clustering, classification, and others.\nTherefore, the challenge we have set ourselves in this work, was to develop a\ncrawler whose purpose is to deal with time constraints. The importance of this dimension\nis certainly quite amplified when combined with the topic or geography,\nbut now we wanted to study it in isolation.\nThe used approach is quite direct. It is based on an algorithm for temporal\nsegmentation ofWeb pages and follows links only in segments within the temporal\nscope of the restriction.\nThis system is designed forWeb pages written in Portuguese though its design\nphilosophy can be applied to other languages.\nIn addition and for increase results effectiveness, the used algorithm prioritized\nthe downloading of pages with more links within the temporal scope. The\nprecision of results is around 75%."
  },
  {
    "keywords": [
      "Automated guided vehicles",
      "Automated driving",
      "Path calculation",
      "Controlled environment",
      "Impact prediction",
      "Veículos automaticamente conduzidos",
      "Condução automatizada",
      "Cálculo de rota",
      "Ambiente controlado",
      "Previsão de impacto",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automated driving: an approach to automated guided vehicles in a controlled environment",
    "autor": "Lopes, Luís Francisco Mendes",
    "data": "2022-12-19",
    "abstract": "The growing need for process optimisation is one of the main drivers of technological\nmodernisation in companies. Process automation has been one of the main initiatives in\nthis modernisation task. In many scenarios, the movement of any kind of vehicles or other\nmoving objects requires human hands.\nIn this context, this Master’s Thesis, included in the fifth year of the Integrated Master\nin Informatics Engineering in the University of Minho, reports the work developed in a\nbusiness context with the help of Deloitte Technologys S.A. that welcomed the student in a\ncurricular internship program. The aim is to develop an implementation of of a simulator\nof automated guided vehicles in a controlled environment, which can reveal itself as an\ninitiative for the modernisation of the logistic process of moving mobile objects or vehicles.\nIn order to develop the Master’s Project, a work methodology was outlined. This methodology is supported on four crucial stages to reach the proposed objectives: documentary,\nproposal, pre-development and development. The documentary analysis was made to\ndevelop a better understanding of the context and to study other developed projects and\ntools that could be necessary for the proposal and development. In the solution proposal we\nstarted by dividing the problem in several components, in a kind of \"divide and conquer\"\nmethod. During this process, a conceptual map was developed, which will be analysed in\nthe document and will guide us to the first Conceptual Architecture Diagram. The next step\nis through the analysis of the Conceptual Architecture Diagram to study which modules will\nbe out of the scope of what is necessary for the full functioning of the Master project, taking\ncertain components as assumptions and substituting some modules by other solutions, so\nthe development can be faster however in a way to not neglect the required functionality for\nthe full operation of the solution. After these steps, the development of the solution will\nfollow, putting into practice all the knowledge acquired in the previous stages.\nThe Master’s Project culminated in five micro-services: Vehicle, Controller, Map, Vehicle/Sessions Micro-Service and Alert; and a web application that allows the user to check\nwith a GUI data about vehicles, sessions, alerts and the map of the controlled environment.\nThese micro-services and the web application working simultaneously allow having a vehicle\nsimulator automatically driven in a controlled environment."
  },
  {
    "keywords": [
      "Image segmentation",
      "Semantic segmentation",
      "Deep learning",
      "Self-driving",
      "Automotive security",
      "Segmentação semântica de imagens",
      "Veículos autónomos",
      "Redes neuronais convolucionais",
      "Segurança automóvel",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Application of semantic segmentation through data acquired from sensors",
    "autor": "Monteiro, Filipe Pimenta Oliveira",
    "data": "2021",
    "abstract": "Today, AI is very important in our lives as its used all around us without our knowledge. From simple things such as personal assistants like Alexa and Siri, and advertising algorithms focusing on our tastes -\nNetflix on the recommendation of movies or, even more common, the presentation of advertising based\non our search history -, to robots and to smart houses, cities or even vehicles. The presence of AI is\nincreasing and even if we are still far away from our ’General AI’ ideology, a machine capable of anything\nautonomously, each day we get closer.\nIn the last decade multiple applications of AI have been through breakthroughs. For example, the first\nimplementations of autonomous vehicles were introduced by Tesla and other companies. A number of\ndiscoveries must have been made to achieve this revolution of AI performance and, among them, is two of\nthe most important developments: Object Detection and Semantic Segmentation, closely related to each\nother. These are responsible for understanding the environment so the machine can take actions, being\nthe latter an improvement of the first in terms of sensibility error associated to each entity detected as well\nas being able to detect its corresponding type, in a pixel level. These machines require more and more\ndata to analyse, having many types of sensors in order to collect information, such as radars, cameras,\nLiDAR, among others.\nThis work falls in the study of the use of Semantic Segmentation techniques and its application on\ncategorising data from image related sensors in order to explain its breakthroughs and challenges, as well\nas improving and overcoming such obstacles. Data will consist mainly of scans from outdoor/self-driving\ncars POV (KITTI360) with the ability to be used with other types of data such as indoor scans (COCO), to\nexplain both road and more day-to-day images semantic compositions, applied on a state-of-art solution.\nConsecutively we will perform a process of optimisation in order to reduce computation costs. Currently\nthe works of DeepLab (with the research of deeplabv3[1]) have achieved a high success on Semantic\nSegmentation overcoming previous problems such as handling component boundaries with more refined\nlines while keeping it fairly easy to run on more less powerful machines, being the start point for this work."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "The influence of emotions on human computer interaction",
    "autor": "Teixeira, André Francisco Soares Carvalho Alves",
    "data": "2018",
    "abstract": "Emotion is an essential part of what means to be human, but it is still disregarded by\nmost technical fields as something not to be taken into account in scientific or engineering\nprojects. However, the understanding of emotion as an aspect of decision making processes\nand of modelling of human behaviour is essential in order to create a better connection between\nhumans and their tools and machines. This dissertation focuses on the measurement\nof emotion of users through the use of non-intrusive methods, like measuring inputs and\nreactions to stimuli, along with the creation of a tool that measures the emotional changes\ncaused by visual output created by the tool itself. Usage of the tool in a test environment\nand the subsequent analysis of the data obtained will allow for conclusions about the effectiveness\nof the method, and if it is possible to apply it to future studies on human emotions\nby investigators in the fields of psychology and computation."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "HLA Binding Intelligence (HABIT): an integrated web-server for generation and advanced interpretation of peptide-HLA binding predictions",
    "autor": "Martins, Joana dos Santos",
    "data": "2017",
    "abstract": "Human T cells are essential in the control of pathogen infections, cancer and autoimmune\ndiseases. Immune responses mediated by T cells are aimed at specific peptides, designated\nT cell epitopes, that are recognized when bound to Human leukocyte antigen (HLA)\nmolecules. The HLA genes are remarkably polymorphic in the human population driving\na broad and fine-tuned capacity of their encoded proteins to bind a wide array of peptide\nsequences. Amino acid variants in epitopes might impact the HLA-peptide interaction and\nconsequently the level and type of generated T cell responses. Having the tools to effectively\npredict and measure the impact of amino acid variants in HLA binding will be of\ngreat value for the future of personalized host directed therapies.\nMultiple algorithms based on Machine learning (ML) have been implemented to estimate\nHLA-peptide binding. However, there is still no tool capable of performing integrated\nanalyses, namely comparing wild type and mutant sequences by predicting all overlapping\npeptides including amino acid positions of interest. This requires that researchers have\nprogramming skills to analyse prediction data for HLA peptide binding and to extract\npotentially meaningful conclusions from that data.\nThe main objective of this thesis was to design and implement a web server, called HLA\nBinding Intelligence (HABIT), that automates HLA binding prediction and advanced interpretation\nof the impact of peptide variants in inducing adaptive immune responses. HABIT\nintegrates the best overall predictors for peptide-HLA binding (class I and II), NetMHCpan\nand NetMHCIIpan, respectively. The application features an intuitive and user-friendly interface\navailable online for academic users, including comparative studies of wild type and\nmutant sequences, statistical tests and calculations of human population coverage. The performance\nof the web server was tested with a case study representative of tuberculosis, the\nR233L mutation found in the acetyl-/propionyl-CoA carboxylase beta chain (AccD2) protein. The\nresults obtained through the interactive graphical interface allowed the automated identification\nof the most promising peptide sequences to be used in the design of T cell-mediated\nimmunotherapy approaches.\nOverall, HABIT provides a quick and easy way to answer major questions of rational\nimmunotherapy studies, namely “What is the influence of an amino acid variants on HLA\nbinding?”, “Do wild type and mutant epitopes show statistically significant differences in\npredicted HLA binding properties?” and “What is the predicted population coverage of the\nepitopes?”. Thus, HABIT constitutes a promising and reliable advance in the discovery of\nmolecular determinants that influence the variation in T cell mediated immune responses."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Digital signage  : aplicação baseada em eventos usando API Google Calendar",
    "autor": "Branco, Pedro Miguel Pereira Tomé",
    "data": "2011-12-19",
    "abstract": "O uso de painéis de digital signage ou displays (ou, em português, painéis digitais) está cada vez mais a ser implementado nos locais públicos e semipúblicos, uma vez que é uma forma actual de apresentar, de um modo dinâmico, informação, publicidade e conteúdos de entretenimento. Desta forma, as pessoas expostas a esta tecnologia passam a ter um acesso mais rápido e actualizado à informação sobre tudo o que as rodeia.\n\nOs ecrãs dos displays têm evoluído no sentido da melhoria da sua qualidade. Além disso, a descida dos preços torna esta tecnologia bastante mais acessível para ser aplicada em vários sectores, nomeadamente na indústria, no comércio, na educação, na saúde, etc., substituindo os meios tradicionais publicitários e informativos, baseados em papel.\n\nUma rede de painéis digitais permite a actualização dos seus conteúdos de forma remota, com base num servidor central que controla toda a informação apresentada na rede, enquanto que nos meios tradicionais a actualização de conteúdos é muito mais cara, demorada e de difícil gestão.\n\nComo as pessoas olham pouco tempo para os painéis digitais, exige-se um esforço muito grande no estudo do local onde estes são instalados e na modelação dos conteúdos a serem apresentados, de forma a que o ambiente se torne o mais atractivo possível e, deste modo, se promova a atenção de quem passa pelos painéis. Também existem displays que usam tecnologias mais sofisticadas, permitindo a adaptação automática dos conteúdos apresentados em função do contexto envolvente, fazendo com que a informação seja mais direccionada ao público, sem a necessidade de controlo humano.\n\nDado o crescente sucesso, à escala planetária, da digital signage, têm surgindo cada vez mais soluções de software aplicadas nesta vertente. Por conseguinte, o tema principal desta dissertação incidirá sobre o desenvolvimento de uma aplicação web para painéis de digital signage, sugerida pela empresa Ubisign, com o objectivo de permitir configurar visualizações com informação sobre eventos provenientes do Google Calendar.\n\nUma vez que a promoção de eventos geralmente exige custos elevados de design e produção, é necessário minimizá-los no contexto das redes de digital signage. Para tal, existem determinadas ferramentas on-line de gestão de eventos, como o Google Calendar, com a função de permitir que o utilizador especifique eventos que vão ocorrer e efectue o seu escalonamento com base em calendários. Do ponto de vista de um developer, estas ferramentas evitam a necessidade de implementar outros softwares de gestão de eventos, permitindo também desenvolver outras aplicações que comuniquem com estas ferramentas, através de APIs apropriadas e bem documentadas.\n\nPara tirar partido disto, a aplicação a desenvolver terá de recorrer à API Google Calendar para disponibilizar, de forma personalizada, informação sobre eventos que estejam planeados para um dado sítio com uma rede de digital signage instalada e, para esse efeito, terá de ser integrada no serviço Ubisign.com."
  },
  {
    "keywords": [
      "614:681.3",
      "681.3:614"
    ],
    "titulo": "Extração de conhecimento nas listas de espera para consulta e cirurgia",
    "autor": "Oliveira, Olívia Raquel Ferreira",
    "data": "2012",
    "abstract": "O desenvolvimento industrial, a constante inovação e a necessidade de melhoria\ncontínua por parte das organizações originou um crescimento exponencial do volume\nde dados armazenados existindo, por isso, uma maior quantidade de informação relacionada\ncom cada instituição. Cada vez mais as instituições dependem do tipo de dados\nque armazenam e acumulam.\nAtualmente, uma organização tem de fazer uma gestão eficiente das suas bases de\ndados de modo a extrair o máximo de conhecimento possível para apoiar no processo\nde tomada de decisão e assim garantir competitividade no mundo dos negócios e dos\nmercados. A necessidade de implementação de um sistema de apoio à decisão no seio\nde uma instituição fez emergir o conceito de Business Intelligence: processo responsável\npela transformação dos dados em informação útil e organizada, e a subsequente\nconversão dessa informação em conhecimento valioso para a tomada de decisão.\nA área da saúde é bastante susceptível, tanto a nível clínico como administrativo, à\nqualidade e rapidez das decisões tomadas, uma vez que estas decisões colocam sempre\nem causa a vida humana. Assim, é de extrema importância a utilização de sistemas de\napoio à decisão nas unidades de saúde.\nO propósito deste projeto prende-se, essencialmente, com a exploração e aplicação\nde uma ferramenta de BI aplicada no contexto da saúde. Por outro lado, pretende-se\ntambém avaliar a aplicabilidade de uma ferramenta open source em sistemas complexos\ne integrados como os das instituições hospitalares. Neste sentido, a ferramenta\nexplorada e avaliada foi o Pentaho. Foi realizada uma monitorização e simulação dos\ndados clínicos relativos às listas de espera para consulta e cirurgia e à ocupação das\nsalas do bloco operatório de um hospital no norte de Portugal.\nVerificou-se que o Pentaho, enquanto ferramenta open source, é inteiramente capaz\nde ser implementada e integrada numa instituição hospitalar, com a potencialidade\nde uma ferramenta proprietária. Sendo assim conclui-se que o Pentaho é uma ferramenta\nde BI bastante eficiente, capaz de apresentar soluções válidas e atrativas para a\nresolução de problemas e o para suporte à tomada de decisões."
  },
  {
    "keywords": [
      "FTTH",
      "Synoptics maps",
      "Trace maps",
      "CAD",
      "XLSX",
      "Mapas de traçado",
      "Mapas de sinóticos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Fiber networks synoptics",
    "autor": "Gomes, António Sérgio Costa",
    "data": "2024-01-19",
    "abstract": "Due to the highly complex nature of FTTH networks’ infrastructure, a great deal of planning and documen tation is required to assure quality of service and ease the maintenance and upgrade of such structures.\nThe creation of FTTH networks projects is divided in four distinct phases one of which is the creation of\nSynoptics Maps. This dissertation focuses on these Synoptics Maps, particularly automating the creation\nof such documents. These are, in essence, schematics of different areas of the network and they allow for\neasier consultation of the network’s layout and details which are not easily observed on trace maps since\nthey represent the entire network branch, often being too complex to interpret. Currently, Synoptics Maps\nare created manually, through the help of Computer Aided Design (CAD) tools but this process is time\nconsuming and prone to human error. It is estimated that each Synoptics Map takes up to two working\nhours to complete and each project requires multiple of these, depending on the area’s size, number of\ncostumers in that area, and other variables. Furthermore, errors can be costly for the company building the\nnetwork since it can cause unnecessary materials to be allocated or insufficient quality of service for that\narea’s demands. As a way to help mitigate these issues, Proef proposed the creation of a tool that, when\nprovided with network data in the form of Trace Maps and other relevant information, creates Synoptics\nMaps automatically.\nThe dissertation began with an initial study of the problem and it’s context and, in partnership with\nProef, the requirements where devised, discussed and agreed upon. Then, after selecting the most ad equate development tools, a first prototype was developed. As specified, it was a desktop application\nthat, as specified above, takes a network’s trace map and XSLX tables and creates Synoptics Maps. The\ndevelopment process encountered some delays because it relied on examples provided by the company\nwhich had some errors that where difficult to spot and correct. This, along with the realization that Trace\nMaps do not contain all the necessary data for creating Synoptics Maps, caused successive changes to\nthe requirements. As a result, a partial solution was achieved, serving as a foundational point for the\ndevelopment of a more comprehensive solution in the future."
  },
  {
    "keywords": [
      "681.324",
      "007.5"
    ],
    "titulo": "3D virtual environments' generation",
    "autor": "Gomes, Tiago Emanuel Oliveira",
    "data": "2013",
    "abstract": "The development and testing of ubiquitous environments (places enhanced with\nsensors, public displays and personal devices) usually presents high costs, both due\nto the need to acquire specific hardware (sensors, displays, etc.), and the need to\nuse, or even to build, a space wherein the physical system will be implemented.\nConsider, for example, the impact of testing a new ambient intelligence system to\nprovide information in a hospital or in an airport. It is hardly feasible trying to\nprototype the system in the target environment due to the costs (e.g. of redesign)\nand problems associated with such approach. The use of three-dimensional virtual\nenvironments then arises as a solution to this problem. Using them, it becomes\npossible to simulate the use of technology without needing to purchase hardware,\nand without interfering with the physical environments in which the final system\nwill be installed.\nThree-dimensional application servers such as SecondLife (secondlife.com) or\nOpenSimulator (opensimulator.org) provide an easy way to develop virtual worlds.\nA platform for the prototyping of ubiquitous environments is being developed at\nthe Department of Informatics of the University of Minho, which is based on\nOpenSimulator: the APEX (rApid Prototyping for user EXperience) framework.\nAt the moment, each new world has to be modelled manually, using an OpenSimulator\ncompatible viewer, which makes this part of the process time-consuming\nand inefficient.\nThis project’s objective is to study three-dimensional virtual environment modelling\napproaches, and to develop a module that integrates one of these approaches\nin the APEX framework to streamline the virtual worlds generation process. The\ntool developed is presented in this dissertation. It has reduced significantly the\nenvironment generation development time in the APEX framework. Moreover, a\ncase study was developed during the project where the tool was used to build the\nenvironment. Despite the tool has been developed to meet the needs of the APEX\nplatform, it can be quite useful for other OpenSimulator users."
  },
  {
    "keywords": [
      "Arquitetura de microsserviços",
      "Arquitetura monolítica",
      "Arquiteturas de software",
      "Padrões de microsserviços",
      "Microservice architecture",
      "Microservice patterns",
      "Monolithic architecture",
      "Software architectures",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Construção de uma plataforma de e-commerce: uma abordagem baseada numa arquitetura de microsserviços",
    "autor": "Correia, Luís Rafael Barbosa",
    "data": "2021",
    "abstract": "As arquiteturas monolíticas estão, em grande parte, presentes na maioria das plataformas de e-commerce, o que\nleva a um processo de modificação mais complicado e entregas demoradas ao cliente, uma vez que não está\npreparada para trabalho em paralelo.\nA arquitetura de microsserviços veio proporcionar outra forma de desenvolvimento destas plataformas, permitindo\no trabalho em simultâneo por diferentes equipas, produzindo novas entregas para o cliente de forma mais\nacelerada e segura. Todavia, esta possui alguns desafios e complexidades, o que leva muitas vezes à escolha de\numa arquitetura monolítica para o desenvolvimento da aplicação.\nA maioria das aplicações não são imutáveis, pois mesmo estando entregues ao cliente são sujeitas a\nmodificações. Esta necessidade de modificar a aplicação leva a preocupações acerca da rapidez com que as\nnovas funcionalidades são entregues ao cliente. É preciso tomar decisões no início do desenvolvimento sobre\nque arquitetura seguir, de modo a tomar a decisão mais vantajosa. No caso de aplicações monolíticas a mudança\npara uma arquitetura de microsserviços facilita este aspeto, bem como muitos outros. Contudo, esta separação\npode-se tornar quase impossível se o monolítico não for bem preparado para uma eventual futura mudança.\nUma das maiores dificuldades numa migração de um monolítico para microsserviços, relaciona-se com\na definição do que deve ser cada microsserviço e na comunicação entre estes. A migração deve partir da\nidentificação de partes do código que possam ser isoladas sem ter muito impacto no resto do código. Com o\ndesenho de um diagrama de packages é possível obter uma visão sobre a estrutura do sistema, percebendo que\ncomponentes são mais fáceis e mais difíceis de extrair. Deve-se começar por extrair aqueles que contém menos\ndependências, adquirindo as vantagens de uma migração incremental que permite que sejam reduzidos os erros\nefetuados, porém, pode haver situações em que se queira extrair um componente com mais dependências.\nÉ necessário compreender o porquê da migração para uma arquitetura de microsserviços. Esta decisão\nnão deve ser tomada apenas porque a arquitetura de microsserviços está em voga, mas sim por razões\nfundamentadas. Dentro destas razões encontra-se a rapidez com que as mudanças são efetuadas e colocadas\nem produção, pois é mais fácil realizar modificações e voltar a instalar os microsserviços sem que toda a aplicação\ntenha que reiniciar. Isto permite uma melhor estruturação da equipa, possibilitando que várias equipas possam\ntrabalhar em simultâneo para a mesma aplicação, não prejudicando em nada outros microsserviço. Outra razão\né a necessidade de escalar os microsserviços independentemente, providenciando maior robustez, pois a falha\nde um serviço não leva à falha de toda a aplicação ou então pela escolha de tecnologia, podendo-se implementar\nos microsserviço com a tecnologia que seja mais adequada e eficiente."
  },
  {
    "keywords": [
      "Criptografia",
      "Privacidade",
      "Privacy enhancing technologies",
      "Segurança",
      "Cryptography",
      "Privacy",
      "Security",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Privacy and security in Data Mining",
    "autor": "Nunes, Mafalda Guimarães",
    "data": "2021-04-06",
    "abstract": "Na atualidade, cada vez mais informação pessoal é recolhida, armazenada, analizada e partilhada.\nTal é possível e encorajado devido a avanços na área tecnológica, nos métodos de armazenamento de\ndados e nas capacidades analíticas, incluindo machine learning.\nApesar deste novo paradigma de recolha de dados oferecer grandes oportunidades de negócio, a\nquantidade crescente de informações, que podem ser bastante sensíveis, levanta graves questões de\nprivacidade e segurança. Existe uma necessidade cada vez maior de adaptar os mecanismos existentes\nde exploração de dados, com garantias de segurança, a cenários reais. Este trabalho explorará alguns\ndesses mecanismos, tomamdo em consideração requisitos funcionais e de desempenho.\nPara compreender melhor o contexto legal e os mecanismos de exploração de dados com garantias\nde segurança disponíveis, também conhecidos como Privacy Enhancing Technologies (PETs), realizou-se\numa pesquisa preliminar. De seguida, vários cenários de exploração de dados adequados para a empresa\nAltice/MEO foram definidos e possíveis soluções apresentadas. Entre esses cenários, um foi escolhido\npara implementação, devido aos desafios tecnológicos associados e à sua relevância a nível do negócio.\nO cenário escolhido enquadra-se na área de e-Health, dizendo respeito à cifragem dos dados geridos\npor uma plataforma de vida assistida. Estes dados só podem ser decifrados por utilizadores autorizados,\no que não inclui o servidor da plataforma. Assim sendo, este servidor deve ser capaz de realizar as\noperações necessárias sobre os dados cifrados. Para solucionar este problema, um esquema foi proposto,\num protótipo desenvolvido e uma avaliação comparativa de usabilidade efetuada.\nA avaliação de usabilidade revelou que o custo da aplicação do esquema proposto, em termos de\ndesempenho, ocupação do espaço e gestão de chaves, é aceitável. Devido à natureza muito sensível\ndas informações envolvidas, a melhoria das garantias de privacidade e segurança, com manutenção da\nfuncionalidade, supera o referido custo.\nApesar de ainda existir a necessidade de desenvolver novas PETs e melhorar a eficácia das exis tentes, os resultados obtidos permitem concluir que atualmente existem implementações que podem ser\naplicadas a cenários reais."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Safe storage of medical images in NoSQL databases",
    "autor": "Martins, Diana Sofia Chaves",
    "data": "2016",
    "abstract": "Databases are a critical point for medical institutions since, without them, information systems\ncan not work. In such way, the usage of a secure, easily scalable and highly available\ndatabase is crucial when dealing with medical data.\nNowadays, medical institutions use RDBMSs – or SQL databases – in their medical images’\ninformation systems. However, this type of databases has drawbacks at various levels.\nFor instance, they show scalability issues in distributed environments or when dealing\nwith high numbers of users. Besides, their data schema is complex and difficult to create.\nIn terms of functionalities, this kind of databases show a huge set of features, which add\nextra unnecessary complexity when dealing with large complex datasets.\nAn alternative to RDBMSs are NoSQL databases. These are databases design to deal with\nlarge amounts of heterogeneous data without giving up high performance rates. However,\nboth SQL and NoSQL databases still present major concerns regarding security. In the\nmedical field, since there is sensitive and personal data being exchanged, their protection\nis a crucial point when implementing a storage solution for medical purposes.\nHaving this said, this master thesis focuses on the development of a medical imaging\ndatabase with increased availability by the employment of a NoSQL HBase backend. In\norder to protect the data being stored, a secure version of HBase protected with symmetric\ncipher was also implemented. Both protected and unprotected versions were implemented,\ntested, and compared with an open source toolkit.\nFrom the results obtained by executing performance tests, it is possible to conclude that\nthe HBase backend shows a better overall performance in terms of latency and throughput\nin comparison to the open source toolkit. However, the appliance of the encryption service\nimplies higher latency and lower throughput."
  },
  {
    "keywords": [
      "Internet",
      "Fibra ótica",
      "Redes fixas",
      "BoM",
      "Automatização",
      "Optical fiber",
      "Fixed networks",
      "Automation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aplicação FTTx BoM",
    "autor": "Martins, Joel Salgueiro",
    "data": "2024-01-19",
    "abstract": "Atualmente com a massificação da Internet ao redor do mundo é inegável que a implantação das tecno logias de suporte à Internet para os seus consumidores tem de passar por um processo de planeamento\nantes da sua implantação, de modo a garantir máxima eficiência económica e máxima qualidade nos\nserviços de comunicação.\nA tecnologia mais recente apta para a construção de novas redes fixas é a fibra ótica, capaz de\ntransmitir dados recorrendo a sinais luminosos ao invés de sinais elétricos, garantindo elevadíssimas\ntaxas de transferências de dados assim como confiabilidade e segurança acrescida.\nO planeamento destas redes é de elevada importância para o sucesso do projeto de rede. Com\neste criam-se diversos resultados (um dos quais, a lista de materiais (BoM)) que permitem minimizar os\ncustos envolvidos, tanto na implantação como na futura manutenção da rede, resultando em economias\nde dinheiro significativas a longo prazo.\nApesar das vantagens do planeamento destas redes, nas empresas, geralmente este é efetuado super ficialmente, utilizando ferramentas genéricas que não automatizam a maioria das fases de planeamento\ne obrigam à intervenção manual que, devido à sua natureza minuciosa e ao erro humano, obriga a um\nprazo de execução prolongado que acaba por causar impactos negativos altamente significativos. Para\nmitigar este problema a Proef, empresa promotora desta dissertação, decidiu projetar quatro sistemas\ncapazes de automatizar ao máximo cada uma das fases do planeamento, aspirando tornar este mais\nrápido e fiável.\nA presente aplicação, FTTx BoM, incorpora-se na fase de construção da lista com todos os materiais\nnecessários (BoM) e, visa automatizar a sua construção recorrendo apenas a informação dos documen tos resultantes da fase de Survey, juntamente de regras de construção pré-definidas pela operadora\nfinanciadora que, tipicamente é um provedor de serviço de Internet (ISP).\nO BoM desempenha um papel relevante na aprovação final por parte da operadora que financia\no projeto e, atualmente, como a sua elaboração é feita manualmente, implica um período significativo\nfrequentemente estendido devido a erros e inconsistências encontradas em fases avançadas, acarretando prejuízos reputacionais e financeiros."
  },
  {
    "keywords": [
      "Authentication",
      "Burp suite",
      "Cookie detection",
      "Machine Learning",
      "Security",
      "Autenticação",
      "Deteção de cookies",
      "Segurança",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aprendizagem automática aplicada à deteção de vulnerabilidades",
    "autor": "Ribeiro, Paulo Filipe Silva",
    "data": "2021-12-02",
    "abstract": "The HTTP protocol is a stateless protocol, that means, each request made by the user\nis an independent request, there is no notion of state. So, to add it to the applications\nwe need an additional tool to implement this notion of state. For this, cookies are used,\nallowing the websites to identify the authenticated users. A cookie is a file stored in the\ncustomer’s browser and sent together with HTTP requests, allowing the website to recognize\nthe customer and send a response corresponding to the request made.\nThis dissertation aims to strengthen the protection of data associated with authentication\nsessions through the identification and analysis of authentication cookies using machine\nlearning techniques. If web applications are vulnerable to malicious attacks, such as Broken\nAuthentication or XSS (Cross-Site Scripting), attackers can gain access to the information\nstored in the cookie. Using this information they can steal the user’s session, being able to\nauthenticate themselves in the web application to obtain access to data/services.\nUsing machine learning techniques, we can identify within a set composed of several\ntypes of cookies, which cookies are associated with authentication. The objective is the\nrecognition of this type of cookies, since this is the one that needs greater security, taking\ncare in case the attacker even gaining access to this file, there is no possibility of deciphering\nthe information that puts the users session at risk.\nIn addition to the classification of cookies, the detection and analysis of the encoding used\nwill be carried out. The tool will then be integrated into the security testing software, Burp\nSuite, working as an extension in order to facilitate and reduce the time necessary for a QA\nanalyst to spend checking cookies."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Community based repository for georeferenced traffic signs",
    "autor": "Novais, Hélder Manuel Pereira",
    "data": "2017",
    "abstract": "In traffic environments, road signs have a key role to control, warn, and command or prohibit\nthe driver of certain actions. Traffic sign maintenance is essential to prevent negative\nevents. In order for these traffic signs to play the role they were designed for, periodic onsite\ninspections are essential and followed out to determine if signs are in good condition\nand visible, both during the day and night. However, periodic inspections are time and cost\nconsuming.\nAnother issue is related to the drivers’ awareness to the traffic signs on the road. Many\nfactors, both internal and external to the driver, may potentially contribute to him missing a\nsign. Given the purpose of this dissertation, we will focus primarily on the external factors\nsuch as the sign being damaged or occluded, or distractions caused by the many gadgets\ninside the vehicle. Due to all these extraneous influences, a traffic sign recognition system\nmay help the driver to respect these signs and increase significantly their safety, as well as\nthe others around them.\nSome high-end vehicles already have such a warning system, at least for danger signs.\nHowever, drivers with these vehicles represent a small fraction of the total driving force.\nThis dissertation aims at bringing such a system to a much broader audience.\nSmartphones are one of the most used devices by society today, mostly due to the many\nfunctionalities they provide in day to day life and their relative accessible monetary value.\nThe increased computational power and cameras’ quality improvement of these devices\nover the years make them good candidates to support the access to this kind of technology\nto all. In other words, smartphones of this day and age have the necessary resources to be\nused as instruments for sign recognition.\nHence, we propose a dual purpose community based approach. On the one hand, each\ndriver can use his mobile device to detect, recognize and geolocate traffic signs, contributing\nto the traffic sign central repository. Detection is performed using Cascade Classifiers,\nwhile a Convolutional Neural Network supports the recognition phase. The repository,\nbased on the information received from the clients, can be used to provide sign status\nreports and to enable more direct and timely inspection instead of relying on prescheduled\nglobal inspections. On the other hand, drivers would have access to the database of traffic\nsigns, therefore being able to receive real-time notifications regarding traffic signs such as\nspeed limit signs, school proximity, or road construction signs. Hence, allowing the system\nto perform its function even if the recognition phase is not active when used in a low\ncomputational power device."
  },
  {
    "keywords": [
      "Business intelligence",
      "Tableau",
      "Manipulação de dados",
      "Manipulação de estruturas",
      "Interação com dados",
      "Plugin",
      "Data manipulation",
      "Structures manipulation",
      "Data interaction",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Persistência dinâmica de dados para soluções de business intelligence",
    "autor": "Luís, Marcos de Morais",
    "data": "2021",
    "abstract": "Existem hoje em dia diversas soluções de Business Intelligence no mercado que permitem a\nanálise de informação de forma intuitiva, permitindo o acesso a utilizadores de negócio das\nmais diversas áreas. Estas soluções vieram assim tornar o processo de análise de informação\nágil, permitindo que esteja presente em mais processos de negócio, executados por utilizadores\nnão especialistas. Estes processos de negócio exigem por vezes a manipulação de dados\nmanualmente. A tarefa de manipulação manual de dados provenientes de diversas fontes\né um processo complexo, onde há a necessidade de proceder à implementação de queries,\npipelines customizados, entre outras tarefas específicas. Estas tarefas estão assim fora do\nalcance de profissionais sem conhecimento técnico de programação e desenvolvimento\nde software. Existe assim a necessidade de construção de uma ferramenta que permita a\ninteração e manipulação de dados sem conhecimento técnico que seja transversal a diferentes\ncontextos com a possibilidade destes também terem diferenças ao nível de tecnologias de\nbase de dados.\nO Tableau, ferramenta de visualização e interação com dados provenientes de diversas\nfontes, anunciou que iria disponibilizar uma api para aceder e interagir com a informação\npresente nos dashboards, possibilitando assim o desenvolvimento de software por terceiros.\nPor conseguinte, surgiu então a oportunidade de criar um produto que no formato de\nplugin tivesse como objetivo preencher a lacuna de manipular e interagir com os dados\npresentes numa base de dados sem estar constrangido a um cenário ou tecnologia específica.\nDesta maneira conseguimos expandir o conjunto de ações disponíveis aos utilizadores para\nalém das atuais que estão de momento restritas apenas à visualização e interação com dados\nprovenientes de diversas fontes, sem qualquer possibilidade de alteração e preservação de\nnova informação.\nEste documento relata o plano de trabalhos sobre a manipulação de dados e estruturas, em\nmúltiplos contextos com a possibilidade de alteração de tecnologia de base de dados. Tendo\nisto em conta, está estipulado o desenvolvimento de um produto de um motor dinâmico de\nmanipulação de queries dinâmico, que permita a interação por parte dos utilizadores com\ndiferentes contextos e que secundariamente seja de fácil integração em diferentes ferramentas\nde visualização através de interfaces gráficas."
  },
  {
    "keywords": [
      "Arquiteturas de software",
      "Micro-serviços",
      "Open banking services",
      "Padrões",
      "Frameworks",
      "Lithium",
      "Software architectures",
      "Microservices",
      "Patterns",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Arquitetura de micro-serviços e o caso da Framework Lithium da PRIMAVERA BSS",
    "autor": "Gonçalves, Fábio Daniel de Sá",
    "data": "2023",
    "abstract": "A PRIMAVERA BSS é uma empresa tecnológica portuguesa ao serviço da gestão empresarial. Empresa\npioneira, em Portugal, a desenvolver soluções de gestão para Windows. A nível tecnológico, a PRIMAVERA\nBSS disponibiliza um vasto leque de serviços, nomeadamente softwares de gestão.\nO desenvolvimento dos micro-serviços na empresa assenta numa framework proprietária — a framework\nLithium. Esta assegura uma arquitetura comum e padrões de desenho aplicando práticas de\nModel-driven Development (MDD).\nOs micro-serviços são cada vez mais uma arquitetura muito utilizada na indústria. Grandes empresas,\nadotaram esta arquitetura e muitas outras seguem a tendência, ao migrar as suas aplicações para esta\narquitetura. Contudo, existe ainda uma dificuldade em construir um sistema neste estilo muito devido\nà falta de informação ou conhecimento acerca dos padrões disponíveis, e por isso mesmo, este estilo\narquitetural necessita de ser amplamente estudado.\nPosto isto, um dos objetivos desta dissertação é suprir esta lacuna, através do estudo de elementos\nimportantes, que devem ser considerados durante o desenvolvimento de aplicações/sistemas baseados\nna arquitetura de micro-serviços. Outro objetivo passa por estudar a framework acima referida bem como\nrespetivas alternativas.\nEm suma, para além de se ter procedido à pesquisa e estudo de arquiteturas concorrentes, bem como\nde frameworks alternativas à Lithium, também foi desenvolvida uma aplicação baseada na arquitetura dos\nmicro-serviços."
  },
  {
    "keywords": [
      "Educational software",
      "Educational systems",
      "Data warehousing",
      "Multidimensional Data Systems",
      "Data analysis",
      "Data visualization",
      "Dashboarding",
      "Software educativo",
      "Sistemas educacionais",
      "Sistemas multidimensionais de dados",
      "Análise de dados",
      "Visualização de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Incorporação de um sistema analítico numa plataforma computacional de avaliação",
    "autor": "Oliveira, Bárbara Nadine Freitas",
    "data": "2022-01-31",
    "abstract": "It is now possible to prove that technology has proven to be a strong ally in the most\ndiverse areas, from economics and management to health or banking. Education is therefore no exception. The insertion of technology and software that provide students with\neducational and motivational support for their learning has been a major challenge. It is\nbased on this lack of support that a project called “Leonardo” emerged at the University\nof Minho. This project considers the development of educational software, which aims\nto provide students with a supervised learning method that will allow students to improve their knowledge and also observe their results. All of this will allow you to draw\nlessons that support your progress in a given area of study. This dissertation proposes,\ntherefore, the development of a data warehousing system, which allows the collection\nof information about users and their interactions with the referred system. In this way,\nit will be possible to obtain a multidimensional data analysis platform, which allows to\nmonitor the student’s state of knowledge over time, thus allowing him / her to evaluate\nhis / her progress, in a given field of study, during the interaction with a student. the system."
  },
  {
    "keywords": [
      "Oracle Retail",
      "Integração Low Code",
      "Adaptadores spring",
      "Computação em nuvem",
      "Transformação digital",
      "Low Code Integration",
      "Spring adapters",
      "Cloud computing",
      "Digital transformation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Integração de soluções Cloud Oracle Retail com uma plataforma de integração low code",
    "autor": "Gomes, José Miguel Ferreira",
    "data": "2023-12-04",
    "abstract": "Nesta dissertação, é explorada a interseção vital entre tecnologia, integração de sistemas e necessidades em constante evolução no setor do retalho. O estudo começou com uma análise aprofundada dos requisitos tecnológicos e operacionais do retalho moderno, destacando-se a importância crítica da integração eficiente de dados para acompanhar as expectativas dos clientes.\nOs objetivos centrais deste trabalho foram investigar, identificar e implementar uma framework Low\nCode capaz de integrar soluções Oracle Retail Cloud com aplicações externas, com o intuito de simplificar, acelerar e reduzir os custos associados a esse processo. Para alcançar esses objetivos, a pesquisa envolveu uma revisão exaustiva das tecnologias existentes, a seleção criteriosa das ferramentas de mercado mais adequadas e a análise cuidadosa das estratégias de segurança relacionadas com integração em cloud. A conclusão bem-sucedida deste trabalho culminou com a evolução do Retail Consult Integration Broker para atender aos requisitos de computação em nuvem e de baixo código, uma solução adaptável e robusta que atendeu às metas definidas. Durante o processo de desenvolvimento, ajustes foram feitos para garantir uma solução ainda mais refinada. Esta framework não apenas cumpriu os objetivos iniciais, mas também representou uma contribuição significativa para a modernização do setor do retalho. Além de fornecer uma solução técnica, esta dissertação ressalta a importância estratégica de adotar abordagens Low Code e de computação em nuvem no contexto das operações retalhistas. A combinação dessas tecnologias oferece uma resposta eficaz às constantes mudanças do mercado, proporcionando às empresas a flexibilidade necessária para inovar, adaptar-se rapidamente e atender às expectativas dos clientes num ambiente tecnológico em constante evolução."
  },
  {
    "keywords": [
      "Avaliação de usabilidade",
      "Contratação eletrónica",
      "Web",
      "Arquitetura de informação",
      "Dashboard",
      "Usability evaluation",
      "E-procurement",
      "Information architecture",
      "Évaluation de l'utilisabilité",
      "Architecture de l'information",
      "681.324:347.4",
      "347.4:681.324"
    ],
    "titulo": "Melhorar a usabilidade de aplicações web : mais resultados com menos esforço",
    "autor": "Silva, Fábio Samuel Coelho da",
    "data": "2013",
    "abstract": "A contratação eletrónica é apresentada como uma obrigação a ser cumprida pelos diversos estados, no contexto da união europeia. Até ao ano 2016, todas as contratações públicas deverão ser executadas com total transparência, segurança e fiabilidade, através da utilização de plataformas web de contratação devidamente certificadas. A VORTALnext> posiciona-se como um dos líderes desta área em Portugal, oferecendo aos seus clientes, uma plataforma transversal e poderosa de contratação. Apesar da qualidade existente na plataforma VORTALnext>, existem diversas lacunas de usabilidade que são objeto de estudo nesta dissertação.\nA usabilidade apresenta-se como um ponto crítico para a aceitação da plataforma por parte dos seus clientes. Entrevistas, heurísticas, questionários e testes foram as ferramentas utilizadas para análise dos\nreais problemas dos clientes, tendo sido concluído que o dashboard é um dos fatores de aceitação mais relevantes.\nTendo como base os resultados obtidos e como foco o dashboard, foram desenhadas diversas aproximações, tanto da arquitetura de informação, como posicionamento e organização dos elementos no ecrã. Utilizaram-se cores para evidenciar as ações mais importantes, números para representar volume de trabalho/negócio, alertas visuais como aviso/lembrete, listas priorizadas e “one-click actions”, conseguindo assim minimizar as falhas de usabilidade detetadas através deste trabalho.\nAtravés desta estratégia, um novo dashboard foi desenhado para apresentar apenas o necessário, quando necessário e de uma forma verdadeiramente útil e agradável para o utilizador."
  },
  {
    "keywords": [
      "681.3",
      "658.8",
      "368"
    ],
    "titulo": "Previsão de churn em companhias de seguros",
    "autor": "Gomes, Bruno Miguel Viana",
    "data": "2011-11-22",
    "abstract": "Transversal a qualquer indústria, a retenção de clientes é um aspeto de elevada importância e a que se deve dar toda a atenção possível. O abandono de um produto ou de um serviço por parte de um cliente, situação usualmente denominada por churn, é cada vez mais um indicador a ter em atenção por parte das empresas prestadoras de serviços. Juntamente com técnicas de Customer Relationship Management (CRM), a previsão de churn, oferece às empresas uma forte vantagem competitiva, uma vez que lhes permite obter melhores resultados na fidelização dos seus clientes. Com o constante crescimento e amadurecimento dos sistemas de informação, torna-se cada vez mais viável a utilização de técnicas de Data Mining, capazes de extrair padrões de comportamento que forneçam, entre outros, informação intrínseca nos dados, com sentido e viável no domínio do negócio em questão. O trabalho desta dissertação foca-se na utilização de técnicas de Data Mining para a previsão de situação de churn dos clientes no ramo das seguradoras, tendo como o objetivo principal a previsão de casos de churn e, assim, possibilitar informação suficiente para a tomada de ações que visem prever o abandono de clientes. Nesse sentido, foi desenvolvido nesta dissertação um conjunto de modelos preditivos de churn, estes modelos foram implementados utilizando diferentes técnicas de data mining. Com esta implementação de vários modelos, pretende-se realizar uma avaliação comparativa dos mesmos, de forma a analisar qual o mais eficaz na previsão de casos churn."
  },
  {
    "keywords": [
      "Chatbots",
      "Rasa",
      "Transformers",
      "BERT",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "ChatbotWizard - o orquestrador de Chatbots",
    "autor": "Meira, Rui Miguel da Costa",
    "data": "2023-02-24",
    "abstract": "Atualmente os chatbots são usados por diversas organizações para automatizar tarefas. Os chatbots\nsão desenvolvidos para diversos casos de uso, desde ajudar os utilizadores a navegar nas aplicações até\nresolver problemas que os utilizadores encontram. No entanto, a criação de um chatbot exige recursos\nmonetários e de conhecimento. Assim, a motivação deste projeto passa é permitir a democratização de\ncriação de chatbots com o desenvolvimento da ferramenta ChatbotWizard, permitindo que um utiliza dor possa criar um chatbot sem grande conhecimento tecnológico, seja o chatbot de elevado grau de\ncomplexidade ou não.\nO ChatbotWizard usa o Rasa como sistema de diálogo, permitindo integrar vários módulos para a\ncriação de um chatbot. Os módulos disponíveis no ChatbotWizard são: módulo para a extração de entida des, módulo realizar pedidos a API, módulo de template para construir texto a partir de JSON e módulo\nde Question Answering (QA) baseado em Transformers (BERT). Estes módulos podem ser conectados\npara criar o fluxo do chatbot desejado. Do ChatbotWizard fazem parte dois componentes: o backend e\no ChatbotWizard web. O ChatbotWizard web permite a utilizadores criarem os seus chatbots integrando\ne configurando os diversos módulos. O backend tem a responsabilidade de receber o fluxo do chatbot e\ncriar um chatbot baseado no Rasa.\nCom o desenvolvimento do ChatbotWizard conseguiu-se uma aplicação que permite o utilizador criar\nchatbots e integrar os mesmos no Telegram. E por fim, foi criado um caso de estudo baseado numa API\npública."
  },
  {
    "keywords": [
      "Muon",
      "Tomography",
      "Muography",
      "Imagiology",
      "Reconstruction",
      "Muão",
      "Tomografia",
      "Muografia",
      "Imagiologia",
      "Reconstrução",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Muon tomography: application of image reconstruction algorithms within the LouMu project",
    "autor": "Duarte, Magda Mendes",
    "data": "2023-12-05",
    "abstract": "Muon tomography or muography is an imaging technique that allows non-invasive observation of the in terior of large and dense structures such as pyramids and volcanoes. Muography resorts to cosmic-ray\nmuons, subproduct particles of the interaction of cosmic rays with the Earth’s atmosphere, and to muon\ntelescopes, devices capable of detecting these particles and their trajectory. Depending on the use given\nto this instrument, the technique subdivides into scattering muography and transmission muography. The\nlatter works similarly to radiography and outputs two-dimensional projections of the structures in the field\nof view of the muon telescope. To obtain their 3D image it is necessary to apply image reconstruction\nalgorithms to the 2D projections. In 2019, LIP, the Laboratory of Instrumentation and Experimental Par ticle Physics, constructed a muon telescope and initiated the first transmission muography experiment\nin Portugal under the collaboration LouMu. As a starting point, the telescope acquired data at its con struction site, the Department of Physics of the University of Coimbra, and efforts are being made to use\nthis same data to reconstruct three-dimensional images of the building. This dissertation arrives in that\ncontext, as the work presented here concerns how the 2D projections taken at the Department of Physics\nof the University of Coimbra were obtained and how they are being used to derive 3D reconstructions of\nthe building while resorting to image reconstruction algorithms. Regarding the 2D results, it is explained\nhow simulated and experimental images muography images were obtained. It is concluded that, although\nthe two compare well on a coarse-grained scale, some disparities still need to be addressed in future\nanalysis, namely to be able to perform 3D reconstruction. In that sense, the development and testing\nof image reconstruction algorithms to retrieve 3D images from the 2D projections was performed only in\nsimulation. The applicability of two methods, the back-projection and the SART iterative algorithm, to the\ncase study of the Department of Physics of the University of Coimbra was analyzed and a third algorithm,\nanalytical inversion, is currently being developed and tested under the same conditions. Out of the first\ntwo approaches, only the iterative algorithm resulted in successful reconstructions while using the muon\ntelescope of the LouMu collaboration."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Previsão na área farmacológica: modelos estatísticos vs Deep Learning",
    "autor": "Ferreira, Raquel Marques",
    "data": "2017",
    "abstract": "Hoje em dia, a tomada de decisões de forma rápida e eficaz é essencial nas organizações de saúde. Neste sentido, surgem os Sistemas de Apoio à Decisão, as plataformas de Business Intelligence e os Sistemas de Tratamento de Dados. De forma a apoiar a decisão no âmbito farmacêutico surgem plataformas de previsão, as quais pretendem auxiliar ao máximo a tomada de decisão por parte dos prestadores de saúde. No âmbito desta dissertação, foi realizado um projeto com o objetivo de extrair conhecimento de forma automatizada a partir de informações passadas e traduzi-las de forma a desenvolver um sistema de previsão de vendas para a área farmacêutica.\nTradicionalmente, na área da previsão, é comum a utilização de modelos estatísticos, no entanto é interessante perceber se o Deep Learning consegue acompanhar os resultados obtidos através destes modelos. Para o efeito, foi elaborado um estudo comparativo entre modelos de previsibilidade, conseguidos através de modelos estatísticos e conexionistas. Para os primeiros fez-se uso de funções de modelação disponíveis em librarias da linguagem de programação R e no segundo foram aplicadas redes neuronais recorrentes, nomeadamente as Long Short Term Memory, através de bibliotecas disponíveis em Python para construção de um modelo deep learning. As metodologias desenvolvidas através dos diferentes modelos de previsibilidade foram aplicadas a três casos de estudo, cada um associado a um conjunto de dados diferente. Assim, tornou-se possível analisar o comportamento dos modelos desenvolvidos quando aplicados a conjuntos de dados distintos.\nPor último, foram apresentados os resultados obtidos para os três casos de estudo, referentes à aplicação de ambas as práticas, e feita uma comparação das mesmas. Foi verificado o sucesso da utilização de algoritmos de Deep Learning na área da previsão, obtendo melhores resultados que aqueles conseguidos através dos tradicionais modelos de previsão estatísticos. Este trabalho permitiu perceber o potencial que o deep learning apresenta, sendo no entanto necessário mais trabalho futuro para dar enfâse a esta afirmação."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desenvolvimento de um programa para comparação de curvas ROC: para amostras independentes e amostras relacionadas",
    "autor": "Moreira, Augusto Daniel Teixeira",
    "data": "2018",
    "abstract": "A análise ROC (Receiver Operating Characteristic) tem vindo a ganhar muita popularidade,\nprincipalmente na área da medicina, dado que é uma ferramenta útil para avaliar e especificar\nproblemas no desempenho de um indicador de diagnóstico.\nA área abaixo da curva ROC (AUC) é um indicador que pode ser utilizado para comparação\nde duas ou mais curvas ROC.\nEste trabalho, surgiu da necessidade de existência de softwares que permitem o cálculo\ndas medidas necessárias para comparação de sistemas com base nas curvas ROC. Existem\nvários softwares que efetuam o cálculo de medidas associadas à análise ROC, no entanto\napresentam algumas lacunas, nomeadamente no que diz respeito à comparação para\namostras independentes com diferentes dimensões e na comparação de duas curvas ROC\nquando estas se intersetam.\nNeste trabalho é apresentado uma nova aplicação que se designa por CERCUS. Esta foi\ndesenvolvida usando a linguagem de programação JAVA e destaca-se pela possibilidade de\ncomparar duas ou mais curvas ROC.\nEste programa tem como principal intuito o cálculo de várias estimativas ROC, usando\nos diferentes métodos sugeridos no desenrolar do trabalho e fazer a comparação de curvas\nROC, mesmo que haja interseção, quer para amostras independentes ou amostras emparelhadas.\nPermite ainda, a representação no plano unitário da curva ROC empírica e a área\nentre as curvas."
  },
  {
    "keywords": [
      "Machine learning",
      "Meal plan",
      "Recommendation systems",
      "Nutrition systems",
      "Planeamento de refeições",
      "Sistemas de recomendação",
      "Sistemas de nutrição",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Nutritional management and recommendations for hospital users and medical inpatients",
    "autor": "Rodrigues, Pedro Miguel de Mata",
    "data": "2022-05-14",
    "abstract": "Nutrition is fundamental to human well-being and health, especially when applied to\npatients who need special health care. In these cases, it is crucial that each patient has\nadequate nutrition to meet their needs, in order to accelerate their recovery process.\nRecommender systems make it possible to offer suggestions to users, adapted to their\npreferences and to previously obtained information about them. Food recommender systems\nare recommender systems applied to nutrition and diet. They are usually implemented\nfeeding plans recommendation platforms based on food and the person using it.\nIn this sense, the existing gap in the use of these recommendation systems applied to\nnutrition in health care is notorious. This is mainly due to the difficulty in associating the\nnutritional value of each food with the needs of patients.\nThe main objective of this project is to fill the existing void, through the development and\nimplementation of a platform that will allow the planning of meals taking into account the\nnutritional plan of the food and the specific needs associated with the users of the Vila Verde\nSocial Canteen.\nThe use of machine learning algorithms will allow us to identify how the connection\nbetween food and patient requirements can be made, making this task possible, which is\ncomplex due to the wide domain associated with it.\nThis platform will be used for the generation of kitchen meal plans, which shall be\nproduced using the algorithms developed after a bibliographic study and an investigation of\nthe existing work, in order to understand how they can be implemented and which are the\nmost adequate to the nutritional recommendations system."
  },
  {
    "keywords": [
      "Anatomical constraints",
      "C4 photosynthesis",
      "Constraint-based modelling",
      "Zea mays",
      "Restrições anatómicas",
      "Fotossíntese C4",
      "Modelação baseada em restrições",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Development and analysis of mathematical models to study metabolic constraints and capacities in different photosynthetic types",
    "autor": "Machado, Tiago Moreira",
    "data": "2022",
    "abstract": "Climate change and a growing human population necessitate improved crop adaptability and\nyield. Improving photosynthesis is one promising route to boosting plant productivity. Photosynthesis\nis hampered by the dual activity of its main CO2-fixing enzyme ribulose-1,5-bisphosphate\ncarboxylase/oxygenase (Rubisco). The enzyme side-reacts with O2, leading to the production of\na toxic byproduct, which must be expensively recycled through the photorespiratory pathway.\nRubisco’s oxygenation rate depends on the CO2 : O2 ratio and increases under high temperatures.\nIn C3 plants, which make up 90% of the known plant species, this phenomenon can\ndecrease photosynthetic efficiency by an estimated fourth. C4 plants have evolved a carbon-concentration mechanism that suppresses photorespiration by spatially separating initial carbon\nfixation and re-fixation by Rubisco. Initial carbon fixation occurs in the mesophyll cells, while\ndecarboxylation and carbon fixation by Rubisco occurs in the bundle sheath cell and releases\npyruvate or phosphoenolpyruvate which then moves back to the mesophyll cells for the next\ncycle. To successfully engineer C4 metabolism in C3 plants, it is important to obtain a quantitative\nunderstanding of both the energetics and distribution of metabolic fluxes of this metabolic\ncycle. Here, we tackle this question by analysing a large-scale metabolic model, consisting of\nmesophyll and bundle sheath cells connected through the exchange of cytosolic metabolites.\nWe parameterized the model for the main C4 crop maize (Zea mays) by using biochemical and\nanatomical constraints derived from the literature. These constraints also enable the model\nto correctly predict the appearance of the C4 cycle, different C4 subtypes and decarboxylation\nenzyme co-activity. Accounting for the volumetric ratio between the two cell types leads to\nmore accurate predictions of C2 photosynthesis, a triose phosphate-3-phosphoglycerate shuttle\nbetween the cell types, mesophyll-specific nitrate reduction, choice of decarboxylation enzyme,\nthe ratio of ATP production between the cell types, cell type-specific cyclic or linear electron\ntransport activity and biomass production. Thus, our modelling approach can guide biological\nengineering strategies to implement C4 photosynthesis into other plant systems to ultimately\nimprove crop productivity."
  },
  {
    "keywords": [
      "Ferroelectricity",
      "Non-centrosymmetric phases",
      "Non-volatile memory",
      "Thin film",
      "Fases não-centrossimétricas",
      "Ferroeletricidade",
      "Filme fino",
      "Memória não volátil",
      "Engenharia e Tecnologia::Nanotecnologia"
    ],
    "titulo": "Non-volatile memory devices based on ferroelectric oxide thin films",
    "autor": "Silva, Nuno Manuel Estrócio e",
    "data": "2023-11-30",
    "abstract": "Information storage is a paramount challenge in the current century driven by the need to scale \ndown memory cells and lower their operating voltages while ensuring high-speed and non-volatile \ncharacteristics. Over the last century, ferroelectric materials have emerged as promising candidates for \nthe development of non-volatile memories where two electrical switching states can be written and \nretained for a long time. The use of conventional ferroelectrics (perovskite oxides) for memory applications \nhas been intensively studied for decades. However, they are not CMOS compatible and are limited by the \nhigh growth temperature. Recently, the discovery of ferroelectricity in binary oxides, such as zirconium \noxide, ZrO2, or hafnium oxide, HfO2, added new advantages and functionalities in ferroelectrics-based \nmemory devices.\nThis thesis explores the potential of using ferroelectric HfO2- and ZrO2-based materials for non volatile memory applications. In the first stage, it was investigated the impact of annealing temperature \non the ferroelectric properties of (HfxZr1-x)O2 films x = 0, 0.3 and 0.5 in a Pt/(HfxZr1-x)O2/W capacitor \nstructure. It was found that an annealing at 680 ºC is the optimal choice for improving ferroelectric \nproperties in terms of thermal budget resulting in a remanent polarization (𝑃𝑟\n) of 9.2 µC/cm2\nfor the \n(Hf0.3Zr0.7)O2 composition. Moreover, the device showed a stable performance up to 1.8x106\ncycles. In \na second research study, a La0.7Sr0.3MnO3 (LSMO)/HfO2/W stack was grown on a Nb:SrTiO3 substrate. \nThe 3 nm-thick epitaxially grown HfO2 layer was found to crystallize in the polar rhombohedral phase. \nAlthough no evidence of ferroelectric properties was found, a bipolar interfacial resistive switching \nbehaviour was reported in the fabricated device. It is suggested that this RS behaviour is explained by \nphase transitions at the LSMO/HfO2 interface caused by a reversible migration of oxygen vacancies. The \ndevice showed a memory window of almost 10 and a non-volatility retention of at least 100 seconds."
  },
  {
    "keywords": [
      "681.3.06",
      "658.0"
    ],
    "titulo": "Análise e conceção de uma framework de reporting genérica e parametrizável",
    "autor": "Sá, Igor Gonçalo Gomes de",
    "data": "2012-12-12",
    "abstract": "Actualmente as aplicações PRIMAVERA incluem componentes de reporting que exigem\ndemasiado esforço de implementação e de manutenção, na medida em que todo o seu\ndesenvolvimento é manual, repetitivo e assente em tecnologia desactualizada. Estes componentes de reporting são baseados nas soluções Crystal Reports, sendo necessária a construção/desenho em tempo de desenvolvimento de todos os relatórios que são pretendidos para um determinado produto. Cada um destes relatórios tem o seu desenho próprio, a suas próprias características e configurações, não existindo qualquer forma de partilhar determinadas propriedades que possam ser comuns aos vários relatórios. Por norma pretende-se que todos os relatórios de um produto tenham um aspecto uniforme, como por\nexemplo o layout ou fonte utilizada para determinados campos (por exemplo o título do relatório). Significa isto que é necessário na construção de cada um dos relatórios replicar todas estas características que são comuns, o que exige um esforço significativo e pode ser propício ao erro quando as regras de desenho de relatórios não estão bem definidas no início do desenvolvimento. Este problema torna-se mais evidente quando por exemplo num produto com um elevado número de relatórios se pretende fazer uma alteração numa destas\ncaracterísticas comuns. A simples alteração do tipo de fonte do título do relatório acaba por ser um processo bastante dispendioso, uma vez que é necessário editar todos os relatórios individualmente. \nEsta dissertação surgiu da necessidade de desenvolver um novo componente de reporting que possa responder às limitações actuais. No âmbito do projecto PRIMAVERA ATHENA, está inserida a Framework de Reporting, cuja finalidade é dar suporte à criação, geração e apresentação de relatórios nos produtos desenvolvidos sobre a Framework ATHENA. Um dos principais objectivos da Framework de Reporting é a geração automática de relatórios a partir dos modelos das aplicações, acabando assim com todo o processo manual de criação de relatórios."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Deploying time-based sampling techniques in Software-Defined Networking",
    "autor": "Teixeira, David Rodrigues",
    "data": "2017",
    "abstract": "Today’s computer networks face demanding challenges with the proliferation of services and\napplications requiring constant access, low latency and high throughput from network infrastructures.\nThe increase in the demand for this type of services requires continuous analysis and\na network topology capable of adapting to the dynamic nature of applications, in order to\novercome challenges such as performance, security and flexibility.\nSoftware-Defined Networking (SDN) emerge as a solution to meet these challenges by\nusing a network control plane, dissociated from the data plane, able to have a global view of\nthe topology and act when required, depending on the variation in infrastructure congestion.\nDecisions involving different activities, such as network management and performance\nevaluation, rely on information about the state of the network that in traditional networks involves\na substantial amount of data. Traffic sampling is essential in order to provide valuable\nstatistical data to applications and enable appropriate control and monitoring decisions to be\nmade.\nIn this context, this work proposes the application of time-based sampling techniques in a\nSDN environment to provide network statistics at the controller level, taking into account the\nunderlying need to establish a balance between the reliability of the data collected and the\ncomputational burden involved in the sampling process.\nThe results obtained emphasize that it is possible to apply these sampling techniques by\nusing OpenFlow Group Mod messages, although packet losses can occur on the switch during\nperiods of network congestion."
  },
  {
    "keywords": [
      "Email",
      "Natural language processing",
      "Named-entity recognition",
      "Language models",
      "Database text search",
      "Application development",
      "Rissa",
      "Processamento de linguagem natural",
      "Reconhecimento de entidade mencionada",
      "Modelos de linguagem",
      "Pesquisa de texto na base de dados",
      "Desenvolvimento de aplicação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automated order processing and responses",
    "autor": "Tinoco, Daniel Jorge Barros",
    "data": "2021-08-10",
    "abstract": "This dissertation was carried out at Utilmédica - Produtos Medicos Hospitalares, Lda which\nwas born on June 9, 2004, as a result of the perception of gaps in the market for the supply of\nmedical - hospital products and equipment to health professionals. One of their main goals is\nto provide healthcare professionals with the best solutions for the noble mission of ensuring\nthe welfare of us all.\nDue to this vision and consequent growth, the company’s working methods also need to\ngrow. Therefore, the company wanted to find a solution to the growing quotation requests\nby the various messaging platforms in which they are present. Starting from this problem, a\nsystem was developed that identifies the products contained in a given message and sends\nan automatic quotation reply.\nThis system was named Rissa and to develop it, it was necessary to analyse the content of\nprevious email messages, in order to develop a NLP model that could identify the entities\npresent in future email messages. In addition to this, Rissa also contains a search system that\nfilters only the products available from the company.\nRissa had to integrate into an existing infrastructure without impacting the company’s op eration. This integration had to deal not only with external services, but also with internal\nservices and privacy policies.\nIn the end, this system was implemented in the company in a real work situation to obtain\nproduction results."
  },
  {
    "keywords": [
      "Microservices",
      "Microserviços",
      "API Gateway",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "The role of an API gateway in a microservice architecture",
    "autor": "Parente, Pedro Dias",
    "data": "2022-12-13",
    "abstract": "Nowadays, with the development of bigger and more complex applications, the architectural\nparadigm for application development is changing from a more traditional Monolithic\napproach to an architectural style called Microservices. In this more recent, and increasingly\npopular, style of developing applications, a tool that has also become increasingly more\npopular is API Gateways. In this thesis I explored these and a few other concepts on various\nexamples, recording my experience, with the intent to create a guide on how to more\nefficiently implement these tools on to your own projects, facilitating the usually long and\narduous process of researching, learning, and implementing new technologies into your\nwork."
  },
  {
    "keywords": [
      "Big five",
      "Data augmentation",
      "Data science",
      "Machine learning",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "A machine learning approach to The Big Five Personality Test",
    "autor": "Perdigão, Miguel Campos Calafate Carneiro",
    "data": "2019-12-30",
    "abstract": "One of the most accurate personality assessments available is the Goldberg’s ’The Big\nFive Personality Test’, which measures the five OCEAN dimensions: Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism. This assessment is performed by\npresenting a total of forty adjectives requesting the subject to rate each word using a scale\nof 1 to 9 indicating whether it accurately (9) describes herself or not (1). Nonetheless, scientific research has shown that this test may, accurately, suggest personality traits such as\naggressive reactions, work performance, fitness on specific expertise areas and also mental illnesses. However, one big disadvantage of this test, it simply takes too much time\nto perform, which can result on undesirable measurements. Indeed, several developments\nhave been done in order to reduce the required effort to perform this test, an example is\nThe Mini Marker Test by Saucier. This study aims to propose a viable shorter alternative to\nthis by applying machine learning techniques, i.e., although measurement precision may be\nreduced, is it possible to build a much shorter version losing as little precision as possible\nby just requiring the subject to select the adjectives that characterise him the most?\nFor this study, it was developed a platform to collect data, requesting both the subject to\nrate each adjective but also to select those he most identifies with. With this, the available\ndata contains both ratings and the selections of the words that most characterise the subject.\nThree different machine learning architectures are developed and tested. Both regression\nand classification approaches are considered. The main input for these architectures are\nthe words selected by each evaluated subject. Data collected by this work showed to be\ninsufficient, requiring the use of data augmentation techniques. For this, different versions\nare proposed, one including the use of frequent itemset mining techniques. The proposed\nmachine learning architectures shown a very high precision, with an RMSE of around 7%.\nThe results show the proposed solutions to be able to perform a shorter version of this\ntest with a minimum precision loss. It was also possible to define a list of common sets\nof selected words. Further research can be performed mainly on two different streamlines,\ni.e., strength the data collection process and develop an even shorter version of this test."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "An efficient software tool to segment slice and view electron tomograms",
    "autor": "Sousa, Paulo Rafael da Costa e",
    "data": "2017",
    "abstract": "Segmentation is a key method to extract useful information in Electron Tomography.\nManual segmentation is the most commonly used method, but it is subject to user bias and\nthe process is slow. The lack of adequate automated processes, due to the high complexity\nand to the low signal-to-noise ratio of these tomograms, provided the main challenges\nfor this dissertation: to develop a software tool to efficiently handle electron tomograms,\nincluding a novel 3D segmentation algorithm.\nTomograms can be seen as a stack of 2D images; operations on tomograms usually lead to\ncomputationally intense tasks. This is due to the large amount of involved data and to the\nstrided and random memory access patterns. These characteristics represent serious problems\non novel computing systems, which rely on complex memory hierarchy architectures\nto hide memory access latency time.\nA software tool with a user-friendly interface — TomSeg — was designed, implemented\nand tested with experimental datasets, built with sequences of Scanning Electron Microscopy\nimages obtained using a Slice and View technique. This tool lets users align, crop, segment\nand export electron tomograms, using computationally efficient processes. TomSeg takes advantage\nof the most usual architectures of modern compute servers, namely based on multicore\nand many-core CPU devices, exploring vector and parallel programming techniques;\nit also explores the available GPU-devices to speedup critical code functions. Validation\nand performance results on a compute server are presented together with the performance\nimprovements obtained during the implementation and test phases.\nTomSeg is an open-source tool for Unix and Windows that can be easily extended with\nnew algorithms to efficiently handle generic tomograms."
  },
  {
    "keywords": [
      "Formal methods",
      "Master’s dissertation",
      "Spreadsheets",
      "Typed linear algebra of programming",
      "Álgebra linear tipada da programação",
      "Dissertação de mestrado",
      "Folhas de cálculo",
      "Métodos formais",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Towards a typed linear algebra formal semantics for spreadsheets",
    "autor": "Azevedo, Rui Filipe Brito",
    "data": "2023-06-14",
    "abstract": "This master dissertation addresses the problem of spreadsheet errors by using typed linear algebra in\nspreadsheet design. The study builds on previous efforts to solve this issue and presents an approach to\nimprove the quality and reliability of spreadsheet systems.\nThe outcome of this study shows that the adoption of a typed linear algebra approach in spreadsheet\ndesign can significantly reduce the risk of errors and improve the reliability of spreadsheet-based systems.\nThe tool developed in this dissertation allows users to derive spreadsheet models in Haskell from formal\nspecifications, which are then translated into a particular spreadsheet format. This process helps to\nensure the accuracy and consistency of the generated spreadsheets, as it is based on precise and well typed specifications. Additionally, the use of typed linear algebra in the semantics of spreadsheet functions\nand constructions such as e.g. running totals provides a solid foundation for the correctness. Overall, the\nresults of this study demonstrate the effectiveness of the typed linear algebra approach in improving the\nquality and reliability of spreadsheet systems."
  },
  {
    "keywords": [
      "Dispositivos médicos",
      "Simulação",
      "Mobilidade",
      "Tecnologia",
      "Aplicações  móveis",
      "Medical devices",
      "Simulation",
      "Mobility",
      "Technology",
      "Mobile applications",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Simulação de dispositivos médicos em Android",
    "autor": "Pinto, André Miguel Bonjardim",
    "data": "2018-12-14",
    "abstract": "Hoje em dia, em qualquer unidade de saúde (hospitais, centros de saúde, clínicas, etc.), existem diversos equipamentos médicos, cada um com a sua complexidade. Alguns destes equipamentos são cruciais para o tratamento de pacientes e requerem que a interação das equipas médicas com os mesmos seja precisa e exata, correndo-se o risco de, ao mínimo erro, causar danos fatais ao paciente. Todos os dias, pelo mundo fora, ocorrem erros com dispositivos médicos e de dispositivos médicos. Os erros com dispositivos médicos são causados devido a algum erro de utilização: uma sequência errada de botões, informação inserida em campos errados, falta de atenção às unidades de medida, etc. Em alguns casos, estes erros ocorrem devido à falta de interação de um profissional com certo dispositivo. Esta interação, pode ser reduzida, muitas vezes, devido a um dispositivo ter um custo elevado que não é justificável para um dispositivo disponível somente para testes. Assim, as equipas médicas apenas interagem com o dispositivo quando um paciente precisa de cuidados médicos, o que pode ser perigoso caso um médico ou enfermeiro não esteja familiarizado(a) com o dispositivo. Por outro lado, os erros de dispositivos médicos são causados por alguma falha no próprio dispositivo, não podendo ser evitada pelas equipas médicas, e que pode dever-se a uma má programação ou construção do dispositivo. Uma solução, para a prevenção de erros na utilização de dispositivos médicos, passa pela simulação de dispositivos médicos, de modo a que os dispositivos possam ser tanto testados para prevenir eventuais erros de software na interface de utilizador que possam existir, como usados para a formação e treino de pessoal médico para diminuir os erros provocados na interação com os dispositivos. A presente proposta de dissertação, no âmbito do Mestrado Integrado em Engenharia Informática, visa tirar partido das aplicações móveis para a simulação de dispositivos médicos com que não podemos interagir todos os dias. Com a criação destas simulações será possível ter inúmeras virtualizações de dispositivos médicos presentes nas unidades de saúde (centros de saúde, enfermarias, hospitais, clínicas, etc.) ao alcance de um tablet ou smartphone. Isto irá também permitir uma maior mobilidade para simulações, permitindo aos utilizadores levar a cabo um treino diário ou semanal com um dispositivo simulado. Passa também a haver uma maior mobilidade para quem desenvolve e cria protótipos de dispositivos inovadores, sendo que podemos apresentar a aplicação ao público alvo e ter um feedback instantâneo sobre os aspetos positivos e negativos do protótipo, bem como novas ideias para futuras funcionalidades que possam ser adicionadas ao protótipo."
  },
  {
    "keywords": [
      "Redes de acesso",
      "Automatização",
      "Ansible",
      "Access networks",
      "Automation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Configuração Zero Touch",
    "autor": "Ribeiro, Pedro Alexandre Gonçalves",
    "data": "2023-01-18",
    "abstract": "A gestão de equipamentos de redes de acesso é um problema complexo e pode tomar dimensões\nque tornam o tratamento individual de cada equipamento um conceito pouco interessante quer do ponto\nde vista económico quer do ponto de vista logístico.\nNa Altice Labs foi desenvolvida uma solução para gerir os equipamentos das redes de acesso: o\nAGORA, um Network Management System. Este sistema é modular e escalável, razões pelas quais\né constituído por vários componentes que configuram o modelo FCAPS Model (FCAPS) da gestão\nde infraestruturas de rede. Este modelo define as categorias de falha, configuração, responsabilidade,\ndesempenho e segurança. [19]\nNeste contexto foi desenvolvido no AGORA um módulo de automatização. O módulo Zero Touch\nProvision (Zero Touch Provision (ZTP)) foi desenvolvido com o intuito de automatizar o aprovisionamento\nde Optical Line Terminal (OLT)’s. Este módulo faz uso de templates, baseados em playbooks Ansible, e\nque são adaptados ao cenário de cada cliente. Este módulo enquadra-se na categoria de configuração\ndo modelo FCAPS.\nO objetivo foi estender o conceito de automatização até aos Optical Network Termination (ONT)’s, que\nsão essencialmente os aparelhos que residem no cliente e terminam a ligação de fibra ótica, sendo que\nexistem ainda modelos mais recentes de ONT’s que integram já funcionalidades de routing.\nDeste modo, foi estudado neste trabalho o desenvolvimento de um módulo baseado no Zero Touch\nProvision (ZTP) que suporte a configuração automática das interfaces finais de rede, as (ONT’s). A automatização deste processo simplifica instalação de serviços no cliente, uma vez que os equipamentos\npassam a não necessitar de configurações manuais: tudo o que operador tem de fazer é correr um\nconjunto de testes que asseguram o correto funcionamento do equipamento uma vez configurado. As\ninstalações passam assim a ser mais rápidas e menos suscetíveis a erros. Para além disto, este processo permite um aprovisionamento dos equipamentos mais rápido, assim como uma maior coerência\nnas configurações dos diversos equipamentos que constituem a rede.\nEste trabalho foi focado num sub-grupo de problemas e tópicos de investigação que tiveram de ser\ncobertos antes de se avançar para a implementação final do ZTP Optical Network Unit (ONU)’s: o desenho\nde uma arquitetura escalável, o estudo da possibilidade de exploração de concorrência no processamento\ndo aprovisionamento de equipamentos, o desenvolvimento de um método de relacionar um equipamento com as regras de aprovisionamento disponíveis, o estudo das práticas de segurança a implementar de\nacordo com o estado da arte assim como o estudo das soluções possíveis para a concretização da monitorização do módulo de software."
  },
  {
    "keywords": [
      "Quadro regulatório intercontinental para a portabilidade de dados",
      "Cuidados  de saúde transfronteiriços",
      "Os Direitos dos titulares de dados",
      "Regulamento Geral sobre a  Proteção de Dados",
      "Privacidade e Proteção de Dados",
      "Intercontinental regulatory framework for data portability",
      "Cross-border  healthcare",
      "The rights of data subjects",
      "General Data Protection Regulation",
      "Privacy and Data Protection",
      "Ciências Sociais::Direito"
    ],
    "titulo": "A portabilidade de dados de saúde dentro e fora da União Europeia - desafios jurídicos e técnicos no âmbito da proteção de dados",
    "autor": "Ladeia, Yuri Rodrigues",
    "data": "2022-09-15",
    "abstract": "Constituído por três capítulos, este trabalho teve como objetivo oferecer um ponto de vista \ntécnico jurídico sobre o quadro de normalização atual do exercício do direito de portabilidade \nde dados de saúde, em um contexto de dentro e fora da União Europeia. Para o efeito, foram \nconsiderados os processos e medidas em prestação de cuidados de saúde e as respetivas \nnecessidades operacionais, extraindo das mesmas o que implicaria na necessidade de um \nintercâmbio de dados de saúde, dentro e fora da União Europeia, considerando a proteção \nespecial inerente aos mesmos e considerando alguns conceitos jurídicos diversos, mas com \nalguma intercessão, como o direito de acesso e de portabilidade. Foram observadas as medidas \nde interoperabilidade a nível nacional, continental Europeu e os desafios a nível \ntranscontinental na saúde, tendo em conta a matéria de arquitetura de segurança das redes e \nsistemas de informação relativos a dados pessoais face ao exercício do direito de portabilidade \ne o caráter normalizador global que a União Europeia tem assumido em matéria de\ninteroperabilidade para o exercício do direito de portabilidade de dados ao abrigo do RGPD.\nForam estudadas e buscadas soluções recorrendo à lógica de Soft-law, para alcançar uma \nnormalização que não enfrentasse as limitações de aplicação geográfica aplicáveis às normas \nvinculadas a um ordenamento jurídico – Hard-Law no setor da saúde, buscando perceber se um \nstandard de boas práticas para a portabilidade neste seguimento seria viável, aflorando ao final \nas soluções encontradas e as dificuldades pendentes de solução."
  },
  {
    "keywords": [
      "Aplicação",
      "Comunicação",
      "Doenças",
      "Informação",
      "Medicação",
      "Plataforma",
      "Application",
      "Communication",
      "Diseases",
      "Information",
      "Medication",
      "Platform",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Uma nova plataforma para auxiliar o processo de comunicação entre encarregados de educação, profissionais de educação e especialistas em pediatria",
    "autor": "Sousa, Sandra Teixeira Marques de",
    "data": "2021",
    "abstract": "As urgências hospitalares têm por objetivo responder a emergências que surjam, seja dentro ou fora de horas e, por ser um serviço onde qualquer tipo de caso pode surgir e, sendo cada caso um caso, é extremamente importante garantir a obtenção do melhor atendimento.\nIsto aplica-se ainda mais quando se refere à Ala de Pediatria. Estes por receberem crianças de todo o tipo de idades têm de conseguir responder às diferentes necessidades e dificuldades, mas grande parte desse trabalho passa pelos pais/encarregados de educação que têm a função de informar os enfermeiros, ou médicos, dos pré-cuidados que tenham sido administrados, ou sintomas anteriores.\nDeste modo, esta dissertação apresenta a problemática, razões e solução encontrada, ao definir objetivos e ao estabelecer metas para lá chegar. É apresentado um breve estado de arte com as pesquisas e revisão de literatura mais relevantes na área do problema e identificação de soluções comuns. Ainda são apresentadas as metodologias de investigação que serão utilizadas, são ela Design Science Research e Proof of Concept, e as tecnologias a ser utilizadas, que passam por React para o frontend, Node JS para o backend e MySQL para a Base de Dados.\nPara terminar é demonstrado os artefactos criados, desde o levantamento de requisitos, passando pela arquitetura de software e terminando no Desenho e Desenvolvimento, todos estes foram ao longo da sua execução avaliados e feitos teste de usabilidade e viabilidade."
  },
  {
    "keywords": [
      "Machine Learning",
      "Data Mining",
      "Contratação Pública",
      "Extração de Conhecimento",
      "Public Procurement",
      "Knowledge Extraction"
    ],
    "titulo": "Aplicação de técnicas de IA na deteção de irregularidades em contratos públicos",
    "autor": "Sousa, Tiago Dias de",
    "data": "2022-08-05",
    "abstract": "Com o passar dos anos, acompanhado pela evolução da tecnologia, existe um aumento acentuado no\nnúmero de dados acumulados no setor de contratação pública em Portugal. Este aumento abre caminho\npara a possibilidade de tirar proveito desses dados, com recurso à utilização de técnicas emergentes de\ninteligência artificial, de modo a melhorar o funcionamento do processo de feitura de contratos públicos\nem Portugal.\nNo panorama da contratação pública, um dos grandes problemas que afetam a qualidade dos contratos\ncelebrados é a existência de irregularidades não detetadas aquando da celebração dos contratos, essas\nirregularidades geram assim contratos que não cumprem as regras definidas para os contratos e que\npodem comprometer a qualidade dos bens e serviços providenciados pelo Estado.\nPortanto, esta dissertação visa recolher um dataset de contratos públicos a partir do portal disponi bilizado pelo governo português, processar e analisar os dados recolhidos, codificar as regras do Código\ndos Contratos Públicos num sistema de regras, investigar e utilizar técnicas de Inteligência Artificial de\nmodo a desenvolver um pipeline com a finalidade de encontrar padrões de suspeição de conluio e detetar\nirregularidades, e, por fim, conceber modelos de Machine Learning para prever os valores futuros das\ndespesas de cada entidade ou região.\nPara sustentar o trabalho desenvolvido foram analisadas e relatadas na dissertação algumas implemen tações existentes de técnicas de inteligência artificial em contratação pública, juntamente com algumas\nabordagens de deteção de fraudes, assim como foram analisados diversos paradigmas e algoritmos de\nML.\nPor fim é demonstrado de que forma os modelos de ML foram concebidos e otimizados, e é feita a\nanálise de resultados dos modelos criados.\nA investigação e experimentação realizada abre perspetivas para o futuro da aplicação de soluções\nde Inteligência Artificial na área da contratação pública."
  },
  {
    "keywords": [
      "Fidelização",
      "Lealdade de clientes",
      "Retenção de clientes",
      "Retalho",
      "Gestão comercial",
      "Consumer loyalty",
      "Fidelization",
      "Client retention",
      "Commercial management",
      "Retail",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desenvolvimento de uma plataforma de fidelização",
    "autor": "Carvalho, Hugo Filipe Duarte",
    "data": "2022",
    "abstract": "Nesta dissertação descreve-se o desenvolvimento de uma plataforma de fidelização de clientes, em colaboração com a Wintouch, que servirá como produto complementar aos já oferecidos pela empresa no mercado de gestão comercial.\nA plataforma deverá funcionar em dois níveis, empresa e consumidor, de modo a responder aos requisitos de ambos no processo da fidelização. Esta necessidade traduz-se na plataforma ser composta por dois componentes distintos mas complementares. Tratando-se de uma plataforma que tem como fim ser comercializada no mercado, esta terá que cumprir todos os requisitos de performance e funcionalidade associados, de modo a cumprir os padrões de qualidade esperados pelos parceiros comerciais da Wintouch\nO componente da empresa deverá fornecer as funcionalidades necessárias para implementar e manter campanhas de fidelização de diversos tipos e configurações, bem como permitir uma análise estatística aprofundada dos resultados das mesmas, visando permitir às empresas tomar decisões informadas e ver os efeitos concretos das campanhas nas métricas de negócio. Este componente deverá estar totalmente integrado com os restantes produtos fornecidos pela Wintouch, nomeadamente as soluções de gestão comercial e de retalho, de modo a poder ser adotado pelos parceiros sem que daí advenham mudanças radicais ao fluxo de trabalho pré-estabelecido.\nO componente do consumidor deverá permitir a este descobrir e usufruir de campanhas de fidelização de clientes, bem como servir de canal de comunicação entre empresa e consumidor, permitindo ao\nconsumidor aceder a informações relevantes sobre campanhas de fidelização."
  },
  {
    "keywords": [
      "Simulação virtual vestuário",
      "Design de moda",
      "Coleções de moda",
      "Tecnologia digital 3D",
      "3D  prototipagem virtual",
      "Virtual garment simulation",
      "Fashion design",
      "Fashion collections",
      "3D digital technology",
      "3D virtual prototyping",
      "Humanidades::Outras Humanidades"
    ],
    "titulo": "Development of fashion products using 3D digital technology to enhance the presentation of fashion collections - case studies",
    "autor": "Oleiro, Sara Luísa Ribeiro",
    "data": "2024-05-15",
    "abstract": "Esta dissertação de mestrado examina a transformação dinâmica da indústria da moda, desde as práticas \ntradicionais até a combinação de tecnologias digitais avançadas. O foco principal está na reinvenção da moda através \nda adoção de programas digitais inovadores em 3D, que são cada vez mais proeminentes no cenário da moda atual.\nEsta pesquisa apresenta uma análise abrangente no papel do design de moda, com vertente no 3D, na indústria \nda moda contemporânea, particularmente no desenvolvimento e apresentação de coleções de moda. O contraste \nentre as apresentações físicas tradicionais de moda e a abordagem moderna é orientada para a tecnologia que destaca \nos benefícios da tecnologia 3D no aumento da acessibilidade, da relação custo-benefício e da liberdade criativa.\nO estudo aborda o design 3D que integra sistemas de simulação virtual para oferecer um novo upgrade na \napresentação de coleções de moda no processo de desenvolvimento de design e na vertente comercial. A \nautenticidade da representação material e a criação de gémeos digitais realistas também são exploradas, com foco \nem plataformas que melhoram a fidelidade visual dos ativos digitais.\nA pesquisa inclui uma revisão ampliada da literatura e um estudo das marcas de moda que adotaram esta \ninovação tecnológica, selecionando 3 casos de estudos proeminentes para análise detalhada. O estudo sugere que \npara que as empresas de moda prosperem nas vendas e cativar novos clientes, é imperativo o investimento estratégico \nna tecnologia 3D e na integração digital. Esses avanços são fundamentais para promover uma indústria da moda \ninclusiva, sustentável e inovadora.\nConcluindo, esta dissertação serve como um recurso essencial para profissionais e entusiastas da moda, \noferecendo insights sobre o potencial transformador da tecnologia digital 3D no futuro das apresentações de moda."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "REM sleep: a new hypothesis for its structure and further HRV characterization",
    "autor": "Mateus, Pedro da Costa",
    "data": "2017",
    "abstract": "The understanding of the Rapid Eye Movement (REM) sleep structure is very\nlimited. The REM stage presents periods with distinct characteristics, suggesting\nthat it should be divided into sub-stages. At least two sub-stages are said to exist\nduring REM, the tonic and phasic periods, mainly separated by the presence and\nabsence of rapid eye movements.\nThe main objective of this thesis was to evaluate the existence of patterns\nduring the REM stage, based on the signals from the polysomnography. This\nevaluation focused on characterizing the phasic and tonic periods, through the development\nof algorithms. Moreover, this study was extended using unsupervised\nlearning to analyze the existence of other REM sub-stages. Finally, this investigation\nwas completed with the evaluation of the heart rate variability during the\nREM period, taking into consideration the REM sub-stages.\nThis study corroborated the characteristics described in the literature for the\nphasic and tonic period from the REM stage. The limitations showed by the initial\napproach were surpassed with the application of a clustering technique. This\nresulted in a division of the REM period in 4 sub-stages, each presenting a specific\npattern in the characteristics of the electroencephalography (EEG), electrooculography\n(EOG), and electromyography (EMG).\nThe study of the heart rate variability during REM showed specific patterns\nbetween the changes in the heart rate variability and the control of the Autonomic\nNervous System (ANS) during this period and each REM sub-stage. Moreover,\nthe structure of the REM sleep changed in a specific pattern with the advance of\nthe REM cycle, while there was a decrease of the heart rate and an increase in the\nparasympathetic activity.\nIn this thesis, we presented a new hypothesis for the REM structure, which\ncan be applied in future studies, allowing to be consolidated and possibly contributing\nto a better understanding of the REM stage. The findings emphasized\nthe need to consider the REM stage a non-homogeneous stage. As for the study\nof the heart rate variability, this investigation led to a possible explanation for the\nvariations occurring during REM sleep, bringing new knowledge that can benefit\nthe unobtrusive methods for sleep monitoring."
  },
  {
    "keywords": [
      "621.39"
    ],
    "titulo": "Impacto da utilização de técnicas de amostragem na caracterização de fluxos de tráfego",
    "autor": "Martins, David Esteves Magalhães",
    "data": "2013",
    "abstract": "The constant development of the Internet and underlying transmission technologies,\ntogether with the increasing popularity of provided services, such as multimedia\napplications and applications using P2P technologies, are contributing to the continuous\ngrowth of the network traffic in volume and diversity.\nTo be able to handle such amount of data while assuring the quality and operation\nof provided services, traffic-measuring tools are required to implement mechanisms\nthat scale and have a minimum interference on the normal network behaviour.\nOne of the most common solutions for this purpose involves the implementation\nof measurement techniques based on traffic sampling. These techniques aim to\nprovide accurate estimations of traffic behaviour and characteristics by processing\nfractions of the original network traffic.\nAnother fundamental area of traffic monitoring concerns the traffic classification\nand characterization, as it supports important tasks such as resource allocation,\nplanning and management, security and quality of service. Attending to this, added\nup to the mentioned growth in traffic volumes, it is likely that traffic classification\nand characterization will be increasingly supported by traffic sampling mechanisms.\nThis work aims to study the impact of traffic sampling mechanisms on the accuracy\nof traffic flows characterization. This was carried out through the application\nof classical and adaptive traffic sampling techniques to real traffic traces, which were\ncaptured in different real scenarios and opened to public access. The resulting sampled\ndata is then organized under the form of flow records, which are then classified\naccording to their transport and application protocols.\nThe performance of the distinct traffic sampling techniques in enabling a correct\ncharacterization of traffic flows was then assessed taking into account multiple metrics\napplied to the flow records of sampled traffic, compared to the metrics of the\nfull original traffic."
  },
  {
    "keywords": [
      "SNOMED CT",
      "Terminologias clínicas",
      "Base de dados",
      "Anatomia patológica",
      "Interoperabilidade",
      "Clinical terminologies",
      "Database",
      "Pathological anatomy",
      "Interoperability",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Aplicação de normas clínicas em anatomia patológica usando o SNOMED",
    "autor": "Domingues, Andréa",
    "data": "2019-12-16",
    "abstract": "Nos dias de hoje, os Sistemas de Informação Hospitalar assumem-se como uma ferramenta indispensável para a prestação de cuidados de saúde, uma vez que permitem o aumento da qualidade e da eficiência quer na prática clínica, quer na gestão hospitalar. A interoperabilidade emerge, assim, como uma necessidade, uma vez que a enorme diversidade de sistemas torna difícil a troca e a partilha de informação clínica. Além disso, a elevada quantidade de sistemas não articulados faz com que seja muito mais provável a existência de dados repetidos e contraditórios, razão pela qual se torna ainda mais imperativa a utilização de normas e terminologias que conduzam à uniformização do registo clínico. Neste contexto, a presente dissertação descreve a criação de um modelo relacional de dados, que serve de base a uma aplicação de classificação de termos médicos segundo o Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT), no âmbito da Anatomia Patológica. A base de dados construída assenta na estrutura original do SNOMED, e foi obtida através da criação de subsets que contêm os conceitos de interesse para o referido setor."
  },
  {
    "keywords": [
      "Avaliação de desempenho",
      "OSM",
      "NFV",
      "VNF",
      "MANO",
      "OpenStack",
      "Benchmarking",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Implantação e avaliação da plataforma Open Source MANO (OSM)",
    "autor": "Moniz, Rosana Mafalda Vieira",
    "data": "2022-12-28",
    "abstract": "A virtualização de funções de rede (NFV, Network Function Virtualization) é uma \ndas principais tecnologias impulsionadoras da quinta geração de redes móveis 5G,\ncujo objetivo é separar as funções de rede (NFs, Network Functions) do hardware. As \nNFs são virtualizadas sobre hardware comum, o que traz mais flexibilidade e \nescalabilidade às redes. Esse paradigma tem como principal desafio a integração de\ntecnologias emergentes e o apoio aos casos de uso que o 5G deverá suportar. \nNeste trabalho, fez-se a implantação e avaliação do desempenho do Open Source\nMANO (OSM) nas versões 7, 8 e 9. Em termos de métricas funcionais, foram avaliadas \nas percentagens de resource footprint da máquina virtual onde o OSM foi instalado, \nbem como as utilizadas pelas funções de rede virtual (VNFs, Virtual Network \nFunctions) no OpenStack, o gestor de rede virtual (VIM, Virtual Infrastructure \nManager), utilizado. Quanto às métricas operacionais, foi medido o atraso no processo \nde on-boarding (OPD, On-boarding Process Delay), ou seja, o tempo necessário para \nque a imagem de uma VNF inicialize, e o atraso no processo de implantação (DPD,\nDeployment Process Delay), que é o tempo necessário para que uma VNF seja \nconfigurada. Além disso, foi feita uma comparação do OSM com outras plataformas\nde gestão e orquestração (MANO, Management and Orchestration) de codigo aberto \nem relação às suas especificações técnicas.\nPara a implantação da plataforma OSM foi criado um ambiente virtualizado\nadequado à realização do experimento, onde foram instaladas as três versões do OSM\n(7, 8 e 9) e o OpenStack, versão MicroStack, sendo que cada versão do OSM foi \nintegrada com o OpenStack para realização do experimento operacional. Foi possível \nrealizar os experimentos para todas as versões. Dos experimentos para a medição das \nmétricas operacionais OPD e DPD, verificou-se, de forma geral, que as versões 7 e 8 \ndo OSM apresentam desempenho semelhante e melhor que o da versão 9. Também\nse verificou que o valor do OPD tem uma tendência crescente com a complexidade daVNF e com o incremento sucessivo de VNFs na infraestrutura de virtualização de redes\n(NFVI, Network Functions Virtualization Infrastructure)."
  },
  {
    "keywords": [
      "Quantum computation",
      "Sparse sampling",
      "Quantum reinforcement learning",
      "Computação quântica",
      "Amostragem esparsa",
      "Aprendizagem por reforço quântica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Quantum-enhanced reinforcement learning",
    "autor": "Sequeira, André Manuel Resende",
    "data": "2021-01-14",
    "abstract": "The field of Artificial Intelligence has lately witnessed extraordinary results. The ability to\ndesign a system capable of beating the world champion of Go, an ancient Chinese game\nknown as the holy grail of AI, caused a spark worldwide, making people believe that some thing revolutionary is about to happen. A different flavor of learning called Reinforcement\nLearning is at the core of this revolution. In parallel, we are witnessing the emergence of a\nnew field, that of Quantum Machine Learning which has already shown promising results in\nsupervised/unsupervised learning. In this dissertation, we reach for the interplay between\nQuantum Computing and Reinforcement Learning.\nThis learning by interaction was made possible in the quantum setting using the con cept of oraculization of task environments suggested by Dunjko in 2015. In this dissertation,\nwe extended the oracular instances previously suggested to work in more general stochastic\nenvironments. On top of this quantum agent-environment paradigm we developed a novel\nquantum algorithm for near-optimal decision-making based on the Reinforcement Learn ing paradigm known as Sparse Sampling, obtaining a quantum speedup compared to the\nclassical counterpart. The achievement was a quantum algorithm that exhibits a complexity\nindependent on the number of states of the environment. This independence guarantees its\nsuitability for dealing with large state spaces where planning may be inapplicable.\nThe most important open questions remain whether it is possible to improve the orac ular instances of task environments to deal with even more general environments, especially\nthe ability to represent negative rewards as a natural mechanism for negative feedback\ninstead of some normalization of the reward and the extension of the algorithm to perform\nan informed tree-based search instead of the uninformed search proposed. Improvements\non this result would allow the comparison between the algorithm and more recent classical\nReinforcement Learning algorithms."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "MINHA: avalição realista de aplicações distribuídas num ambiente centralizado",
    "autor": "Bordalo, João",
    "data": "2011-11-07",
    "abstract": "Nos últimos anos os sistemas distribuídos têm sofrido um crescimento exponencial. Estes sistemas, normalmente implementados na plataforma Java, são compostos por um vasto conjunto de componentes de middleware, os quais desempenham várias tarefas de comunicação e de coordenação. Esta tendência influencia a modelação e a arquitetura de novas aplicações cada vez mais complexas obrigando a um enorme esforço e a um custo elevado na avaliação do seu desempenho. A concorrência e a sua distribuição, bem como o facto de muitos problemas só se manifestarem pela grande escala em si, não permite que a sua avaliação seja feita com recurso a simples ferramentas que não tenham em conta estas características. Avaliação realista e controlada de aplicações distribuídas é ainda hoje muito difícil de alcançar, especialmente em cenários de larga escala. Modelos de simulação pura podem ser uma solução para este problema, mas criar modelos abstratos a partir de implementações reais nem sempre é possível ou mesmo desejável, sobretudo na fase de desenvolvimento na qual ainda podem não existir todos os componentes ou a sua funcionalidade estar incompleta. Para colmatar esta falha, nesta dissertação é apresentada o Minha, uma plataforma que permite uma avaliação realista das aplicações através da combinação de modelos abstratos de simulação e implementações reais num ambiente centralizado. Esta plataforma combina a execução de código real sob análise, com modelos de simulação do ambiente envolvente, isto é, da rede e da aplicação. Este sistema permite reproduzir as condições de um sistema em grande escala e através da manipulação de bytecode Java, suporta componentes de middleware inalterados. A utilidade deste sistema é demonstrada aplicando-o ao WS4D, uma pilha que cumpre a especificação Device Profile for Web Services."
  },
  {
    "keywords": [
      "XAML",
      "WPF",
      "SVG",
      "SCADA",
      "EFACEC",
      "XAML",
      "WPF",
      "SVG",
      "SCADA",
      "EFACEC"
    ],
    "titulo": "Suporte à interoperabilidade entre o Automation Studio e Sistemas SCADA : tradução de sinópticos de XAML para SVG",
    "autor": "Silva, Nuno Miguel Milhases da",
    "data": "2012-12-20",
    "abstract": "Em 2008 é posto em marcha um projecto de inovação e desenvolvimento denominado por InPact entre a EFACEC, a EDP Distribuição e a Universidade do Minho. O objectivo deste projecto é fornecer um conjunto de ferramentas de engenharia para programação dos sistemas de protecção, automação e controlo de sistemas de energia. Pretendia-se que as ferramentas suportassem a gestão completa dos sistemas da EFACEC, baseadas nas normas internacionais IEC 61850, IEC 61131-3, IEC 61499 e IEC 60870-5. Nesse âmbito, a EFACEC criou o Automation Studio, um ambiente de desenvolvimento integrado, desenvolvido em linguagem C# da framework .Net da Microsoft, sendo as ferramentas integradas nesse ambiente como plugins. Entre as ferramentas desenvolvidas, conta-se um editor de sinópticos. Tendo este sido desenvolvido em C# .Net, utiliza XAML para a descrição dos diagramas. No entanto, dos equipamentos produzidos pela EFACEC, mais concretamente, a plataforma para automação e controlo de sistemas de energia e gestor de sistemas SCADA UC 500, utiliza SVG para a visualização e interacção com os sinópticos. Assim, embora o editor permita criar os sinópticos para a plataforma UC 500, as linguagens utilizadas não são compatíveis. Para ultrapassar estes problemas de interoperabilidade, entre o editor e a plataforma, surgiu a necessidade de desenvolver um compilador XAML para SVG.\nO objectivo do trabalho, desenvolvido no âmbito desta dissertação, foi então o desenvolvimento do referido compilador de XAML para SVG. Este deveria ser integrável no ambiente de edição do Automation Studio para, desta forma, permitir a configuração de diversos equipamentos da EFACEC, em particular da plataforma UC 500, a partir desse ambiente de desenvolvimento integrado. Após várias fases de testes e de melhoramentos, o compilador foi definitivamente integrado no editor Automation Studio na sua versão 2.0 e seguintes. O resultado positivo deste projecto é visível pela utilização actual em dois exemplos reais, um na subestação de Ermesinde e outro no Bahrain, ambos apresentados neste documento."
  },
  {
    "keywords": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Development of a scoring system to assess potential biomarkers for atrial fibrillation",
    "autor": "Magalhães, Beatriz Teixeira de",
    "data": "2018",
    "abstract": "Atrial fibrillation affects millions of individuals worldwide, posing a major threat to\npublic health due to the variety of comorbidities that constitute by-products of the disease.\nIn light of this epidemic, new means of diagnosis, prognosis and therapy are pressing.\nBiomarkers, particularly protein markers, are important tools in this process but lack\nvalidation, which is essential before clinical translation. Several appraisal benchmarks\nhave been developed to determine the relative potential of biomarkers, but these present\nmultiple limitations.\nWe developed a bioinformatic-oriented scoring function aimed at weighing the\nimportance of proteins and mitigating the limitations of the currently known scores. After\ntaking an extensive literature search and mining a massive volume of reports, data was\norganized into several subsets, according to the sample major characteristic and atrial\nfibrillation type. A mathematical scoring function was proposed, based on the consensus\nof studies supporting the protein-disease association (incoherence), median of the\nreported fold-changes and importance of each study according to the number of diseased\nindividuals, and applied to each subset in the form of an algorithm implemented in Python\n3.5.\nThe developed ranking method performed well regarding both the degree of alteration\nand the inconsistency parameters. Our results portray a set of proteins with the highest\nbiomarker potential (highest scores) for atrial fibrillation. We also selected the top five\npotential biomarkers for atrial fibrillation in general and for each type of disease. The\nmain biological functions in which they are involved were retrieved for comparison with\nthe state of the art. Alterations in the expression levels of proteins involved in either of\nthese functions seem to agree with AF’s pathophysiology and clinical presentation,\nshowing the effectiveness of the developed algorithm.\nOverall, the developed pipeline seems to improve the processes of biomarker ranking\nand selection for a target disease, allowing a leap towards clinical translation."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Analysis of the effects of the radiation therapy in cancer treatment by medical image processing",
    "autor": "Catarina, César Fernando Vivo Ferreira de",
    "data": "2017",
    "abstract": "This dissertation begins with a brief introduction, where the main objective,\nsegmentation of an organ/structure into sub-segments and performing\na radiation analysis after a radiotherapy treatment. Then, there is a short\nintroduction to what cancer is, as well as some of the most relevant to this\ndissertation. After, the implementation of the various interfaces created, for\nthe identification of the contours of the structures and for the final objective.\nFinally, the results obtained from these interfaces, where its possible\nto observe the fulfillment of the proposed objectives. Finally the conclusions\nobtained and the validation by a Doctor of the field, and proposals for future\nwork, namely the segmentation of more complex structures, such as rectum\nand sigma-colon."
  },
  {
    "keywords": [
      "Similarity analysis",
      "Case-based reasoning",
      "Intelligent systems",
      "Degree-of-confidence",
      "Análise de similaridade",
      "Raciocínio baseado em casos",
      "Sistemas inteligentes",
      "Grau-de-confiança",
      "681.3"
    ],
    "titulo": "Handle default data with case-based reasoning: an approach to solve problem reports",
    "autor": "Fernandes, Bruno Filipe Martins",
    "data": "2014-11-24",
    "abstract": "On a business context, it is responsibility of the Software Product Support Team analyze and solve, if necessary, problems that may arise on software products. Sometimes, the reported problems are not a real defect, i.e., sometimes the client does not have a full understanding about all features of the software product. The team must evaluate and analyze all the Problem Reports that arrive every day. As products are spread across different customers, it is normal to have Problem Reports that are very similar to others that have already been solved for other clients and/or by another member of the Support Team. This dissertation proposes the development of a system that is able to analyze a Problem Report and then provide past problems that are similar to the one being analyzed. An artificial intelligence technique, named Case-Based Reasoning, will be used to achieve such goals. Existent Case-Based Reasoning systems are neither complete nor adaptable to specific domains since the effort to adapt either the reasoning process or the knowledge representation mechanism, to a new domain, is too high. To address such drawbacks, a generic reasoning component will be designed and developed. This dissertation introduces a new approach to the typical Case-Based Reasoning cycle where is possible to handle default, unknown and incomplete data."
  },
  {
    "keywords": [
      "Deep learning",
      "Semi-supervised learning",
      "Computer vision",
      "Object detection",
      "Image classification",
      "Fruit detection",
      "Image synthesis",
      "Aprendizagem profunda",
      "Aprendizagem semi-supervisionada",
      "Deteção de objetos",
      "Classificação de imagem",
      "Deteção de fruta",
      "Síntese de imagem",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Semi-supervised object detection: a pipeline workflow proof of concept",
    "autor": "Sousa, Bruno Alexandre Dias Novais de",
    "data": "2023-12-15",
    "abstract": "Object detection in computer vision plays a pivotal role in applications such as autonomous vehicles, surveillance\nsystems and medical imaging. However, a consistent challenge faced by the field is the scarcity of labeled data\nrequired for training robust object detection models. The manual annotation process for objects within images\nis labor-intensive and expensive, constraining access to large-scale annotated datasets, which in turn hinders\nprogress in object detection research and applications.\nThis work aims to respond to the pressing issue of data scarcity by exploring a series of automated learning\nmechanisms, harnessing pre-existing image classification datasets to craft custom-labeled datasets tailored for\nobject detection. In its foundation, is the streamlining of the transformation of image classification datasets into\naccurately annotated object detection datasets by generating bounding box annotations and object labels. This\ndata synthesis strategy serves the subsequent stages of model training, where object detection models are trained\non the newly generated datasets without the need for labor-intensive manual annotations. The significance of\nthis work resides in its potential to reduce the time and financial costs associated with manual labeling. This\ncost-effectiveness holds immense importance for organizations reliant on the practical application of object\ndetection, particular so in more niche areas, potentially breaking down entry barriers for smaller enterprises.\nThe validity of the methodology is put to the test within the domain of fruit detection, where annotated data\nnotably sparse. Performance is assessed using common metrics, including mean average precision, precision,\nand recall, with pre-existing annotations from the DeepFruits dataset.\nIn this work, we start by exploring a fully autonomous semi-supervised pipeline-based workflow which allowed\nto replicate the labeling of 45% of a subset of DeepFruits. This strategy’s performance, which provided an object\ndetection precision of approximately 69%, was then improved by introducing a manual filtering step for the removal\nof false positive detections, thus necessitating some amount of human interaction. However this, in turn, increased\nthe new pipeline’s precision to 88%, allowing it to correctly recognize 60% of DeepFruits’ objects. Finally, we\nexplored the possibility of converting this manual filtering stage to an automatic verification layer supported by an\nunderlying convolutional neural network that partitions the detected objects into desirable or undesirable objects.\nHere, we evaluated how the application of such image classifier would have improved the previous pipeline, which\nnecessitated manual filtering. Applying it would have allowed for the removal of 81% of all accumulated false\npositives."
  },
  {
    "keywords": [
      "SDN",
      "Encaminhamento",
      "Tráfego",
      "Dados",
      "Protocolos",
      "Networking",
      "Gestão de fluxo",
      "Infraestrutura",
      "Forwarding",
      "Traffic",
      "Data",
      "Protocols",
      "Networking",
      "Flow management",
      "Infrastructure",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Encaminhamento de tráfego em redes SDN (Software-Defined Networking)",
    "autor": "Pereira, Luís Gonçalo Epifânio",
    "data": "2019-12-23",
    "abstract": "Through the exponential growth of the internet service usage comes the inevitability of\nthe creation of a tool that can both be useful and versatile in the management of all of the\ncreated traffic volume.\nFrom this necessity arises the concept of Software-Defined Networking (SDN), that aims\nto offer a set of protocols and technologies capable of easing the management and the\nefficiency of maintenance of the various network infrastructures that require its usage.\nThis work aims for the initial comparison of the multiple capacities of several SDN controllers existent in the market, specifying their programming languages and characteristics.\nAfter such comparison follows the selection of one of the SDN controllers, in order for\na prototype of a traffic forwarding solution to be developed, as to get the most out of the\nchosen controller’s characteristics and of the global SDN approaches.\nThat same developed solution should allow for multiple characteristics, highlighting the\npossibility of its capacity to deal with traffic forwarding under specific parameters. Initially,\nthe solution should incorporate the Dijkstra Algorithm to calculate the shortest paths and\ninject its results into the network, as it also should immediately converge after link failure.\nThen, it should also be conducive to events that occur in real time, it should allow for link or\nrouter protection in the infrastructure, effectively react to link failure, react to different load\nlevels in the network, flow traffic forwarding through specific routes, topology multiplexing\nthrough different virtual networks, among others. That way, the developed prototype may\ncome up as an alternative SDN approach to certain well-known interior routing protocols,\nsuch as Routing Information Protocol (RIP) and Open Shortest Path First (OSPF)."
  },
  {
    "keywords": [
      "Mineração de dados",
      "Aprendizagem automática",
      "Acidente Vascular Cerebral",
      "Previsão",
      "Data mining",
      "Machine learning",
      "Stroke",
      "Prediction",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Algoritmos de aprendizagem automática para previsão de AVC",
    "autor": "Costa, Eduardo João Gomes Teixeira da",
    "data": "2022-12-13",
    "abstract": "O Acidente Vascular Cerebral (AVC) foi, em 2020, a segunda principal causa de morte no mundo e \nprimeira no que toca a incapacidade. Com a motivação de contribuir para ajudar a reduzir os números \nque são alarmantes e continuam a crescer, surge este projeto, do qual se pretende que resultem modelos \nque possam tentar prever se um indivíduo irá, ou não, ser vítima deste problema e descobrir quais as\nsuas características ou dados clínicos que mais influenciam esta previsão, pois, segundo a Sociedade \nPortuguesa de Medicina Interna (SPMI), 80% dos casos podem ser prevenidos[1]. \nPara o efeito, o projeto a desenvolver incluirá uma recolha e tratamento de datasets que organizem \ndados clínicos de vários pacientes e a incidência desta problemática, um estudo acerca das técnicas e \nalgoritmos de Machine Learning mais adequados aos modelos a desenvolver, sendo depois aplicados \natravés de modelos de Data Mining (DM), dando uso a ferramentas como Weka e RapidMiner, para \nindução dos modelos de previsão, assim como algoritmos em linguagens como Python e R, conjugando, \nassim, os factos de que \"o setor da saúde é rico em informação, e o Data Mining está a tornar-se uma \nnecessidade\"[2]. Finalmente, estes modelos serão testados, validados e comparados, do qual resulta \nesta dissertação."
  },
  {
    "keywords": [
      "Reinforcement learning",
      "Actor-critic methods",
      "Online environments",
      "Black box system",
      "Aprendizagem por reforço",
      "Métodos ator-crítico",
      "Ambientes online",
      "Black box vision",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Autonomous optimization for a transactional middleware",
    "autor": "Marques, Susana Vitória Sá Silva",
    "data": "2022-12-05",
    "abstract": "In the last few years, data management engines have become increasingly modular, separating some of its\nmain layers, such as data storage and transactional management. The exposure of the transactional manage ment component brings new challenges, in particular its correct configuration and tuning when running different\nworkloads. In this sense, this dissertation focuses on the autonomous optimization of a particular transactional\nmiddleware, pH1, while keeping in mind the tuning of other similar systems.\nIt is becoming more and more important to develop algorithms that can automatically optimize these systems\nwhose performance is heavily dependent on a proper configuration. The use of machine learning techniques for\nsimilar problems (database knob tuning) has become common in the literature [1, 2, 3, 4], especially in a black\nbox perspective where it does not have visibility over particular details of the system.\nUsually, these systems are located in realistic online environments, where workloads can change at different\ntimes. Even though there are numerous research projects for automatic knob tuning, these projects have not\nentirely addressed this problem and are mostly developed for offline training when the workloads remain static.\nWe propose OPAL as the component that when executing transactional workloads is able to dynamically adjust\nits configurations in an online environment with a continuous space. Our approach allows for online changes and\nuses reinforcement learning as a starting point taking into consideration tuning algorithms in continuous spaces,\nas is the case of DDPG [5]."
  },
  {
    "keywords": [
      "Interstitial lung diseases",
      "Pneumonia",
      "Chest radiographs",
      "Artificial intelligence",
      "Deep learning",
      "Convolutional neural networks",
      "Doenças intersticiais pulmonares",
      "Radiografias torácicas",
      "Inteligência artificial",
      "Redes neuronais convolucionais",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Development of a deep learning-based algorithm to predict pneumonia cases fram chest X-ray images",
    "autor": "Carlos, Nuno Rafael Boto",
    "data": "2020-05-11",
    "abstract": "Interstitial lung diseases (ILD) are defined as a set of more than 200 pulmonary disorders. Among these, the ones broadly termed as pneumonia represent a major cause of morbidity and mortality in the world. The chest radiograph (CXR) was the first x-ray based lung imaging technique to emerge and is still widely used as a diagnostic method for pneumonia and other lung diseases. However, correct interpretation of CXR requires analysis by experts and stays vulnerable to errors and observer-related variation. To counteract these problems, artificial intelligence (Al) methods have been applied for the automated analysis of CXR and other medical images. The deep learning (DL) branch of AI and in the particular the methods based on convolutional neural networks (CNN), recently obtained impressive results in these tasks. \nThis dissertation presents a DL approach to classify pneumonia from medical CXR image datasets. Two different models based on the development of CNN were trained from a preprocessed dataset of CXR images obtained from 8562 individuals classified as normal (n=7214) or with pneumonia (n=1348) (Dataset XP1’). Model 1 applied a normal cross entropy loss function, and model 2 an alternative loss function aiming at counteracting the unbalance in normal/pneumonia class frequency. For performance enhancing both models underwent a hyper optimization procedure. The optimized model 1 and 2 were tested on a test set from PI'. To better understand the predictability and generalization potential we then tested both models on an unrelated test set of 624 images (Dataset XP2).\n Interestingly, model 1 obtained better performance when tested on XP2 than in XP1', scoring an accuracy of 85%, recall of 93% and precision of 85% for the detection of the pneumonia class. The higher homogeneity present on dataset XP2 compared with dataset XP1' could be a plausible justification. As for model 2, it correctly predicted more pneumonia cases an test set XP1' than model 1. However, on test set XP2 the results were poor, predicting most cases as pneumonia and scoring a recall value of only 26% for the pneumonia class. Testing the DL models on unseen data is a relevant but not always performed validation. Overall, the higher accuracy, recall and precision levels of model 1 in XP2 suggests it has a higher potential to be applied for real-word application although its performance should be further improved and evaluated. This work opened promising new lines of research for the future development of a high-performance CNN-based automated method to classify CXR and assist in the diagnostic of pneumonia."
  },
  {
    "keywords": [
      "Sistemas de avaliação de conhecimento",
      "Motor de inferência",
      "Tutor artificial",
      "Processo de avaliação",
      "Mecanismo de raciocínio",
      "Regra de produção",
      "Base de conhecimento",
      "Knowledge evaluation systems",
      "Inference engine",
      "Artificial tutor",
      "Assessment process",
      "Reasoning mechanism",
      "Production rule",
      "Knowledge base",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Mecanismos de raciocínio para sistemas de avaliação de conhecimento",
    "autor": "Coelho, João da Cunha",
    "data": "2019-12-23",
    "abstract": "Sendo certo que o recurso à tecnologia no ensino é cada vez mais notório, a utilização de sistemas \ninformáticos de tutoria continua aquém do seu potencial, ainda que seja um tema abordado há já \nalgumas décadas. Assim, surgiu a iniciativa Leonardo e o respetivo desenvolvimento de uma \nferramenta computacional para sistemas de avaliação de conhecimento, com vista a ser aplicada, \npelo menos, no suporte de processos de avaliação de alunos, na Universidade do Minho. De entre \nos módulos que caracterizam estes agentes de software, no contexto desta dissertação, destacam se a base de conhecimento, o mecanismo de raciocínio e o modelo do estudante. Dado que o \nesforço maior recai em habilitar os tutores artificiais à adaptação, em tempo real, da avaliação ao \nnível de conhecimento atual dos alunos, surge a necessidade de desenvolvimento de um \nmecanismo de raciocínio, que seja capaz de determinar, criteriosamente, o que deve ser \napresentado de seguida num dado momento avaliativo. O trabalho desta dissertação focou-se na \nconceção e implementação de um sistema de avaliação baseado em conhecimento para o sistema \nLeonardo, com a capacidade de ajustar de forma dinâmica, à medida da perícia e conhecimento \ndos estudantes alvos do processo de avaliação, o seu comportamento, acompanhando de perto a \nevolução do processo de aprendizagem dos estudantes. Essencialmente, neste trabalho \nimplementou-se a “máquina” de raciocínio para o sistema Leonardo poder sustentar de forma \nefetiva a avaliação de estudantes ao longo do tempo, numa ou mais áreas do conhecimento."
  },
  {
    "keywords": [
      "Assinaturas digitais",
      "PDF",
      "PAdES",
      "Digital signatures",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "PAdES server signer",
    "autor": "Araújo, Francisco Fernando Vilela",
    "data": "2022",
    "abstract": "Em todas as organizações, públicas ou privadas, as soluções de desmaterialização são cada vez mais\nusadas, começando a existir pressão para a assinatura eletrónica (em formato PAdES) dos documentos\nPDF. Com a introdução de tecnologias de assinatura qualificada remota, cada vez menos se justifica que\nos documentos sejam assinados de forma manuscrita e depois digitalizados para serem integrados no\nsistema documental da organização, sendo natural a evolução para a integração de serviços de assinatura\nno próprio sistema documental da organização.\nNeste sentido, esta dissertação de mestrado tem como objetivo desenvolver uma plataforma (que\ndesignaremos por “PAdES Server Signer”) que permita efetuar uma assinatura eletrónica (em formato\nPAdES) de documentos PDF, com base nos mecanismos de assinatura eletrónica: CMD (Chave Móvel\nDigital) e CC (Cartão de Cidadão). A plataforma rege-se pelas rigorosas especificações do Regulamento\neIDAS da União Europeia para assinaturas eletrónicas. Perante o aumento da utilização das assinaturas\nrealizadas remotamente, visa-se garantir que a plataforma esteja em conformidade com a especificação\ndo Cloud Signature Consurtium para a realização de assinatura baseadas em chave privada e hardware\ncriptográfico remoto, utilizado sob controlo do titular da mesma. Em paralelo, foi realizada uma Aplicação\nWeb que realize a interação com o utilizador, permitindo que este possa assinar um documento PDF,\natravés do PAdES Server Signer."
  },
  {
    "keywords": [
      "Aplicação móvel",
      "Segurança",
      "Posicionamento indoor",
      "Navegação indoor",
      "Leitura do ambiente rádio",
      "Mapa de rádio",
      "Fingerprints",
      "Mobile application",
      "Security",
      "Indoor positioning",
      "Indoor navigation",
      "Radio environment scan",
      "Radio map",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Where@UM: onde é a sala da minha próxima aula?: o problema do posicionamento",
    "autor": "Pinheiro, Lázaro Donato Martins",
    "data": "2022",
    "abstract": "Com a evolução natural da humanidade há uma preocupação crescente com o aumento das infraestruturas, por forma a acompanhar o aumento populacional. Essas infraestruturas assumem dimensões\nde grande escala, o que torna a tarefa de navegação e posicionamento no seu interior complexa.\nA presente investigação, enquadrada no projeto Where@UM, procura desenvolver uma aplicação\nmóvel que auxilie a navegação dos pedestres, no interior dos campi da Universidade do Minho.\nEsta dissertação procura responder aos domínios da segurança e posicionamento indoor. Para tal\nfoi desenvolvido um sistema de segurança robusto, garantindo questões de privacidade e de controlo de\nacesso a alguns serviços. Foi concebida uma aplicação móvel que faz uso deste sistema de segurança,\na qual disponibiliza estratégias para que, de forma colaborativa, os utilizadores desta possam ter um\ncontributo ativo na criação e manutenção do mapa de rádio do sistema.\nCom objetivo de testagem da aplicação desenvolveu-se uma campanha de recolha de dados com 8\nparticipantes, por forma a testar a credibilidade e funcionalidade do sistema concebido. Foram encontrados dados que suportam a qualidade do sistema desenvolvido, com base na análise quantitativa dos dados recolhidos bem como por meio da opinião de satisfação dos participantes."
  },
  {
    "keywords": [
      "Injeção de faltas",
      "Perda de dados",
      "Reprodutibilidade",
      "Sistema de ficheiros",
      "Data loss",
      "Fault injection",
      "File system",
      "Reproducibility",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "LazyFS: a file system for assessing applications data durability",
    "autor": "Azevedo, João Pedro Rodrigues",
    "data": "2022-12-15",
    "abstract": "A atual era digital depende de dados numa perspetiva de grande escala e as organizações requerem\nsistemas de armazenamento que funcionem corretamente sob falhas. Por exemplo, falhas de energia\npodem levar à perda de dados em aplicações cujos ficheiros ainda estão armazenados em memória,\nisto é, num meio volátil. Evitar estes cenários de perda de dados constitui um grande desafio, uma\nvez que exige que os programadores apliquem primitivas de sincronização (fsync()) que garantem a\ndurabilidade dos dados, a custo de uma potencial diminuição do desempenho das aplicações.\nAs ferramentas de injeção de faltas permitem ajudar os programadores com testes automáticos e\nconsequente validação das suas políticas de durabilidade de dados. No entanto, as abordagens atuais\npara sistemas de ficheiros focam-se: (1) na manipulação direta de hardware; ou (2) em erros internos\nde implementação do sistema de ficheiros, e não na interação da aplicação com o mesmo. Além disso,\nestas ferramentas são limitadas quanto à informação disponibilizada ao programador, de forma a este\ncompreender a causa efetiva que levou à perda de dados reportada.\nPara resolver estes desafios, esta dissertação propõe o LazyFS, um sistema de ficheiros que simula\na perda de dados utilizando uma abordagem de injeção de faltas em software reprodutível e automática.\nEste sistema tem uma cache dedicada que gere os dados das aplicações e a sua sincronização para\numa camada persistente. A pedido, o LazyFS pode limpar todos os dados que não foram previamente\nsincronizados, fornecendo também aos programadores informações relevantes sobre os dados em risco\nde serem perdidos com potenciais falhas de energia.\nO desempenho e validação da correção do protótipo demonstra que a nossa solução consegue avaliar\na durabilidade dos dados de aplicações, sem adicionar uma sobrecarga significativa à sua execução\nnormal. Foram também reproduzidas quatro anomalias em diferentes bases de dados e o protótipo já se\nencontra integrado na ferramenta de injeção de faltas Jepsen. Atualmente, o LazyFS está a ser usado,\njuntamente com o Jepsen, para avaliar sistemas de bases de dados em produção, como o Percona\nMySQL Server, MongoDB e o etcd. Adicionalmente, foi descoberta uma possível violação de coerência\nno etcd, que está a ser estudada pela sua equipa de desenvolvimento."
  },
  {
    "keywords": [
      "Inteligência Artificial",
      "Algoritmos",
      "Aprendizagem automática",
      "Aprendizagem profunda",
      "Redes neuronais",
      "Artificial Intelligence",
      "Machine learning",
      "Deep learning",
      "Algorithms",
      "Damage detection",
      "Neural networks",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "A deep-learning approach to detect scratches in vehicles",
    "autor": "Soares, André Rodrigues",
    "data": "2023-12-15",
    "abstract": "A deteção de danos na estrutura externa de veículos representa um desafio para os fornecedores\nde serviços de viaturas de aluguer, especialmente os serviços mais recentes de mobilidade em que a\ninspeção de danos não é realizada no final de cada aluguer. À medida que estes serviços se tornam mais\npopulares e que mais pessoas deixam de ter necessidade de possuir um carro pessoal, espera-se que\nas empresas procurem formas de facilitar este tipo de inspeção de danos. Como tal, o objetivo principal\ndesta dissertação é desenvolver uma solução que os fabricantes de automóveis poderiam potencialmente\nadotar. Esta solução envolve a criação de um algoritmo para detetar riscos em carros usando dados de\náudio obtidos a partir de microfones.\nEste estudo explora a utilização de Aprendizagem Profunda na análise de imagens, por exemplo\nespectrogramas, que são representativas de eventos de riscos. Para tal serão estudados e implementados\nmétodos de transformação do sinal de áudio em imagens referentes a uma representação espectral do\náudio e avaliar a capacidade de um algoritmo de Aprendizagem Profunda aprender a identificar este tipo\nde eventos. O algoritmo é treinado tendo em conta a enorme variedade de sons que podem ser captados\npelo microfone, considerando os infinitos ambientes possíveis a que um carro pode estar sujeito, devido\na uma condução diária."
  },
  {
    "keywords": [
      "Sistemas de eLearning",
      "Sistemas de avaliação",
      "Raciocínio baseado em casos",
      "Raciocínio baseado em regras",
      "Motor de raciocínio",
      "eLearning Systems",
      "Evaluation systems",
      "Case-based reasoning",
      "Rule-based reasoning",
      "Reasoning engine",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Integração de mecanismos de raciocínio adaptativo em sistemas de avaliação",
    "autor": "Leite, Jaime Ricardo Faria",
    "data": "2022-03-21",
    "abstract": "Um sistema de raciocínio pode ser caracterizado como um agregado de componentes de software que realizam em conjunto processos de tomada de decisão complexos. Este tipo de sistema está bastante ligado a uma\ndas áreas de trabalho mais mediáticas atualmente, a Inteligência Artificial. Algumas iniciativas de desenvolvimento dentro desta área tendem a incorporar este tipo de ferramenta em sistemas de avaliação, mais concretamente em tutores inteligentes, com o intuito de ajudar os estudantes no seu processo de aprendizagem. Nesta dissertação apresenta-se a conceção e a implementação de um conjunto de mecanismos de raciocínio baseado em casos e baseado em regras. Estes dois tipos de mecanismos foram idealizados para integrar o atual módulo de avaliação do sistema Leonardo, uma plataforma que complementa o estudo presencial dos alunos da Universidade do Minho. Os novos mecanismos, em particular os de raciocínio baseados em casos, complementam o processo de avaliação do sistema Leonardo aumentando as suas capacidades de raciocínio aquando da realização dos processos de avaliação estendendo as sessões de Quizz. Quanto aos mecanismos baseados em regras, estes representam uma importante camada entre o módulo de avaliação e a interface do tutor do sistema, visto que não permite apresentar questões de escolha múltipla na interface que não estejam de acordo com critérios estabelecidos por peritos. Nesta dissertação veremos como tais mecanismos foram fundamentados, desenvolvidos e integrados no sistema Leonardo."
  },
  {
    "keywords": [
      "Reinforcement learning",
      "Bayesian networks",
      "Quantum computing",
      "Quantum decision-making",
      "Aprendizagem por reforço",
      "Redes Bayesianas",
      "Computação quântica",
      "Tomada de decisão quântica",
      "Ciências Naturais::Ciências Físicas"
    ],
    "titulo": "Quantum bayesian reinforcement learning",
    "autor": "Cunha, Gilberto Rui Nogueira",
    "data": "2023-01-10",
    "abstract": "Reinforcement learning has had many recent achievements and is becoming increasingly more relevant\nin the scientific community. As such, this work uses quantum computing to find potential advantages over\nclassical reinforcement learning algorithms, using Bayesian networks to model the considered decision making environments. For this purpose, this work makes use of quantum rejection sampling, a quantum\napproximate inference algorithm for Bayesian networks proposed by Low et al. [2014] with a quadratic\nspeedup over its classical counterpart for sparse networks. It is shown that this algorithm can only provide\nquantum speedups for partially observable environments, and a quantum-classical hybrid lookahead al gorithm is presented to solve these kinds of problems. Moreover, this work also includes both sample and\ncomputational complexity analysis of both this quantum lookahead algorithm and its classical alternative.\nWhile the sample complexity is shown to be identical for both algorithms, the quantum approach provides\nup to a quadratic speedup in computational complexity. Finally, the potential advantages of this new algo rithm are experimentally tested in different small experiments. The results show that this speedup can be\nleveraged either to improve the rational decision-making skills of agents or to reduce their decision-making\ntime due to the reduction in computational complexity."
  },
  {
    "keywords": [
      "Energy-split",
      "Intramolecular energy",
      "Development framework",
      "HPC",
      "Energia intramolecular",
      "Framework de desenvolvimento",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Improving the efficiency of the energy-split tool to compute the energy of very large molecular systems",
    "autor": "Pereira, Sara Alexandra da Silva",
    "data": "2021-08-10",
    "abstract": "The Energy-Split tool receives as input pieces of a very large molecular system and computes\nall intra and inter-molecular energies, separately calculating the energies of each fragment\nand then the total energy of the molecule. It takes into account the connectivity information\namong atoms in a molecule to compute (i) the energy of all terms involving atoms covalently\nbonded, namely bonds, angles, dihedral angles, and improper angles, and (ii) Coulomb\nand the Van der Waals energies, that are independent of the atom’s connections, which\nhave to be computed for every atom in the system. The required operations to obtain the\ntotal energy of a large molecule are computationally intensive, which require an efficient\nhigh-performance computing approach to obtain results in an acceptable time slot.\nThe original Energy-Split Tcl code was thoroughly analyzed to be ported to a parallel and\nmore efficient C++ version. New data structures were defined with data locality features, to\ntake advantage of the advanced features present in current laptop or server systems. These\ninclude the vector extensions to the scalar processors, an efficient on-chip memory hierarchy,\nand the inherent parallelism in multicore devices. To improve the Energy-Split’s sequential\nvariant a parallel version was developed using auxiliary libraries. Both implementations\nwere tested on different multicore devices and optimized to take the most advantage of the\nfeatures in high performance computing.\nSignificant results by applying professional performance engineering approaches, namely\n(i) by identifying the data values that can be represented as Boolean variables (such as\nvariables used in auxiliar data structures on the traversal algorithm that computes the\nEuclidean distance between atoms), leading to significant performance improvements due to\nthe reduced memory bottleneck (over 10 times faster), and (ii) using an adequate compress\nformat (CSR) to represent and operate on sparse matrices (namely matrices with Euclidean\ndistances between atoms pairs, since all distances further the cut-off distance (user defined)\nare considered as zero, and these are the majority of values).\nAfter the first code optimizations, the performance of the sequential version was improved\nby around 100 times when compared to the original version on a dual-socket server. The\nparallel version improved up to 24 times, depending on the molecules tested, on the same\nserver. The overall picture shows that the Energy-Split code is highly scalable, obtaining\nbetter results with larger molecule files, even when the atom’s arrangement influences the\nalgorithm’s performance."
  },
  {
    "keywords": [
      "Data augmentation",
      "Imbalanced data",
      "Machine learning",
      "Dados desbalanceados",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Conception and evaluation of data augmentation techniques for tabular data",
    "autor": "Machado, Pedro Filipe Costa",
    "data": "2022",
    "abstract": "Imbalanced learning and small-sized datasets are present in Machine Learning problems, even with the increased data availability provided by recent developments. The performance of learning algorithms\nin the presence of unbalanced data and significant class distribution skews is known as the imbalanced\nlearning problem. The models’ performance on such problems can drastically decrease for certain classes\nwith an uneven distribution, because the models do not learn the distributive features of the data and\npresent accuracy too favorable for a specific set of classes of data. This can have negative consequences\nwhen talking about cancer detection, for example, since the model may identify poorly unhealthy patients.\nHence, Data Augmentation techniques are usually conceived to evaluate how models would behave in nondata-\nscarce environments, generating synthetic data similar to real data. By applying those techniques,\nthe amount of available data can be increased, balancing the class distributions. However, there is no\nstandardized Data Augmentation process that can be applied to every domain of tabular data. Therefore,\nthis dissertation aims to identify which characteristics of a dataset provide a better performance when\nsynthesizing samples by a data augmentation technique in a tabular data environment. Moreover, if the\ndata augmentation algorithm synthesizes more real samples, it is expected to increase the classifier’s\nperformance as well. Our results demonstrate that datasets whose features are mainly categorical have\nan associated difficulty in increasing the classifier results by adding new samples. Furthermore, the\ntechnique that adapted best to those kinds of datasets was the more classical one, SMOTE. As for the\ndatasets with more continuous features, the variations of Variational Autoencoder, principally the VAE with\nK-means and decay, as well as GAN, demonstrated an increased capability when augmenting those kinds of\ndatasets. This dissertation demonstrated that more categorical datasets could achieve better performance\nby including 25% synthetic samples, whereas continuous datasets could only do so by including minority\nsamples."
  },
  {
    "keywords": [
      "Hypatiamat",
      "Matemática",
      "Jogos",
      "Multijogador online",
      "Math",
      "Games",
      "Online multiplayer",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Hypatiamat: a funcionalidade de multijogador em jogos online",
    "autor": "Costa, João da Cunha e",
    "data": "2022-12-13",
    "abstract": "O desempenho escolar na área da Matemática é uma preocupação crescente junto\nda comunidade educativa, tendo em conta o elevado insucesso escolar e correspondente\nabandono escolar precoce. Os professores tentam perceber qual a melhor forma para captar\na atenção dos alunos ou incutir um maior interesse nestes relativamente a esta disciplina.\nPara além das técnicas utilizadas no ensino da matéria, os professores começam a aderir cada\nvez mais às novas tecnologias que auxiliam o ensino, uma vez que, estas permitem captar\numa maior atenção por parte das crianças e jovens que, desde cedo, estão familiarizadas\ncom as tecnologias atuais.\nO Hypatiamat é um projeto, orientado mais em específico para alunos do 1º ao 9º ano,\ncujo principal objetivo passa por despertar, junto dos alunos, o gosto pela Matemática e\nconsequentemente aumentar o aproveitamento escolar nesta disciplina.\nPara este efeito, esta plataforma fornece diversos conteúdos hipermédia como explicações,\nresumos, aplicações, jogos, etc. Para além disso, esta plataforma realiza periodicamente\ndiversos campeonatos com alguns dos seus jogos para milhares de alunos. No entanto, estes\npecam por não possuírem a opção de multijogador online, o que dificulta a sua realização e\nimpossibilita que os utilizadores possam jogar entre si os diversos jogos da plataforma de\nforma remota.\nDeste modo, esta dissertação centra-se na implementação deste modo multijogador online\nnum dos jogos do Hypatiamat, para futuramente servir como modelo de um guião a ser\nimplementado nos restantes jogos que a plataforma Hypatiamat possui."
  },
  {
    "keywords": [
      "DevOps",
      "Software development",
      "Application deployment",
      "Monitoring",
      "Continuous integration",
      "Continuous delivery",
      "Virtualization",
      "Container orchestration",
      "Desenvolvimento de software",
      "Deployment de aplicações",
      "Monitorização",
      "Integração contínua",
      "Entrega contínua",
      "Virtualização",
      "Orquestradores de containers",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Software defined applications: a DevOps approach to monitoring",
    "autor": "Alves, Luís Miguel Andrade",
    "data": "2022",
    "abstract": "DevOps presents a mix of agile methodologies that allow an application’s release cycle to be shortened. This\ntranslates into a faster delivery of value to the stakeholders.\nHowever, the value creation chain does not finish at the end of that cycle. It is necessary to monitor the artifacts\nproduced at a system level, and at the application level, in order to ensure the compliance of the functional and\nnon functional requirements.\nToday, there seems to be a clear separation between the monitoring process and the application development\nprocess. As the development and operations processes have merged in DevOps, this dissertation pretends to\ninvestigate how to integrate several aspects of monitoring into the regular lifecycle of an application’s development.\nThe inclusion of external services further emphasizes the need to include an observability component into an\ninfrastructure.\nThe main goal of this dissertation is to develop a solution for the deployment of an infrastructure using stateof-\nthe-art technologies and frameworks, while also providing observability to the system and to the applications\nrunning on it.\nTo do so, it required the investigation of the methodologies and concepts that are the base of the software\ndevelopment lifecycle, focusing on the latter stages of that process: the deployment, and monitoring phases.\nThese methodologies and concepts were complemented with the study of state-of-the-art technologies and\nframeworks that aim to ease the burden of setting up an infrastructure quickly and with the necessary tools to\nevolve it after the initial setup and with each new software release. Furthermore, it also involved the research of\ntools that enable the collection of metrics from applications, as well as processing such data and displaying it in\nuseful ways for operators and stakeholders.\nIn this context, this dissertation aims to provide a solution for the deployment of MobileID applications at INESC\nTEC, using the Mobile Driving Licence as the primary case study. The proposed design and implementation\nwith a container orchestration framework and CI/CD pipelines, enables faster development of different MobileID\napplications, while also providing continuous monitoring to the deployments.\nWith this implementation, it was possible to assess how container orchestration frameworks provide greater\nflexibility to applications, and how this observability can be augmented with the use of dedicated monitoring\nsystems."
  },
  {
    "keywords": [
      "Anonymization",
      "DSL",
      "PPDP",
      "Privacy",
      "Repositories",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Privas: assuring the privacy in database exploring systems",
    "autor": "Miguel, Joana Margarida",
    "data": "2020",
    "abstract": "Currently, given the technological evolution, data and information are increasingly valuable in\nthe most diverse areas for the most various purposes. Although the information and knowledge\ndiscovered by the exploration and use of data can be very valuable in many applications,\npeople have been increasingly concerned about the other side, that is, the privacy threats that\nthese processes bring.\nThis document follows an user-role approach within the data exploration process. These\nusers are: Data Provider (provides the data), Data Collector (collects and stores the data\nprovided), Data Publisher (transforms data and publishes it to be explored) and Data Explorer\n(retrieves information from data). All of them have privacy concerns and can address them\nwith appropriate methods and techniques.\nIn this Master thesis we built a system named Privas that aids the Data Publisher in its\npublishing process. Currently he can assure the data privacy by adopting, manually choosing\nand then applying the privacy-preserving data publishing techniques (PPDP). Privas accepts\na repository with its description (written in a DSL) and creates a copy maintaining the\ninformation to be explored but assuring that involved individuals/organizations cannot be\nidentified by applying PPDP techniques. Privas automatically chooses the privacy models to\napply according with the description, and applies the transformation. In the end of the process\nmetrics about the privacy loss are reported. The Domain Specific Language (DSL) – called\nPrivasL – was developed to easily allow the original repository description, the identification of\nthe data entities that one wants to explore and the definition of the privacy level to be assured.\nTo visually help end-users to describe their repositories, a web platform was developed – where\nafter describing the repository, the correspondent PrivasL description is generated.\nIn the end, an analysis on different kind of repositories, with different information using the\nPrivas tool, was made – conclusions were drawn about transformations and in privacy loss."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Classificação e monitorização escalável de serviços de vídeo",
    "autor": "Cunha, João Marcelo da Silva",
    "data": "2016-12-21",
    "abstract": "Face ao crescimento do volume de tráfego de vídeo na Internet, a monitorização eficiente de serviços de vídeo apresenta-se como um desafio de grande importância para a gestão das redes atuais e de próxima geração, onde múltiplos serviços, protocolos e tecnologias de acesso coexistem e competem por recursos. \nA monitorização eficiente de serviços envolvem a medição, ao precisa de parâmetros de interesse e o menor impacto possível na operação normal da rede. Neste sentido as técnicas de amostragem de tráfego procuram obter informações sobre todo o tráfego considerando apenas um subconjunto dos pacotes em trânsito na rede, apresentando-se como uma solução escalável para os desafios impostos pela monitorização de serviços de vídeo. Neste contexto, o presente trabalho de mestrado tem como principal objetivo analisar o desempenho da monitorização baseada em amostragem de tráfego na correta classificação e caracterização de serviços de vídeo."
  },
  {
    "keywords": [
      "Brain-computer interface (BCI)",
      "Electroencephalogram (EEG)",
      "Motor intention decoding",
      "Signal processing",
      "Interface cérebro-computador (ICC)",
      "Eletroencefalografia (EEG)",
      "Decodificação de intenção motora",
      "Processamento do sinal",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Decoding human motion intentions from brain signals",
    "autor": "Gil, Ana Catarina Cardoso",
    "data": "2022",
    "abstract": "Gait function can be affected by neurological disorders such as spinal cord injury (SCI), stroke, or traumatic\nbrain injury (TBI). These limitations have significant negative effects on the affected people’s independence\nand quality of life. Brain-computer interfaces (BCIs) have the potencial to create solutions that may overcome\nirreversible disabilities. Several studies in recent years have shown that electroencephalographic\n(EEG) signals can be used to develop BCIs for the rehabilitation of human limbs through lower-limbs robotic\ndevices and exoskeletons. Therefore, their effectiveness and safety depend on how successfully they can\ndetect and react to movement.\nThis dissertation aims at developing and validating an EEG-based motor intent decoding framework\nto accurately classify human intent regarding five daily performed locomotor tasks. This framework will\ncontribute on the developing of BCI to recover the mobility of neurologically impaired subjects. For this, a\nprovided multi-channel dataset will be used.\nThe implementation of this solution was divided into two phases. The first is about how signals are\nprocessed to obtain the features that best characterize each of the locomotion modes under analysis.\nAs a result, three distinct studies that differ in the number of channels used were created. Through the\napplication of the ICA method, it has been determined that the more channels are used in a study, the\nmore likely it is that these channels may be corrupted, affecting the ICA method’s effectiveness.\nThe second section discusses the classification methodology. Three different Deep Learning algorithms,\nCNN, LSTM, and their combination, C-LSTM, were studied here. Additionally, three different features used\nas the input for the models were compared for each of them and for each of the studies.\nThe features that were selected showed a higher impact on the results than the actual classification\nalgorithm, with ERPs being the features that produced the best results. On the other hand, across classifiers,\nall three provided high performance, demonstrating reduced differences between them. The study with\nhigher accuracy as the study 3 with the most reliable channel selection."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "TOM Framework: uma ferramenta de testes baseados em modelos para interfaces gráficas web",
    "autor": "Pinto, Luís Miguel Carvalho",
    "data": "2017",
    "abstract": "As técnicas de teste baseados em modelos (do inglês, Model Based Testing (MBT)) comparam\no comportamento do sistema sob teste com o comportamento do modelo do sistema\n(o oráculo). A aplicação de MBT às interfaces gráficas do utilizador (do inglês, Graphical\nUser Interface (GUI)) permite uma avaliação mais exaustiva e contínua do sistema, através\nda simulação de ações do utilizador com a interface gráfica. Desta forma, é possível reduzir\nsignificativamente o custo de avaliação do sistema, e identificar, eventualmente, erros de\nimplementação através da GUI, sem o envolvimento de utilizadores externos. Este processo\ndecorre através da execução dos casos de teste, gerados a partir do modelo do sistema, na\naplicação sobre teste. São estes casos de teste que verificam se a implementação está de\nacordo com o modelo, assegurando assim uma melhoria da qualidade do sistema desenvolvido.\nEsta dissertação descreve uma ferramenta de MBT para aplicações web, a TOM Framework.\nParte da framework (TOM Generator) aproveita trabalho anteriormente desenvolvido,\na outra (TOM Editor) é aqui apresentada. Os objetivos principais da framework\npassam por automatizar e facilitar a criação de modelos do sistema que, posteriormente,\nsão utilizados para gerar automaticamente casos de teste executáveis na interface gráfica\nsobre teste. A captura e interpretação da interação do utilizador com a aplicação web sobre\nteste foi um dos desafios ultrapassados no desenvolvimento desta dissertação. No final da\nmesma, encontra-se uma aplicação da framework a um caso de estudo."
  },
  {
    "keywords": [
      "Administração de sistemas",
      "Email",
      "Segurança",
      "Redes",
      "Internet",
      "Formação",
      "Plataformas",
      "681.3"
    ],
    "titulo": "Relatório de atividade profissional",
    "autor": "Oliveira, Ricardo Manuel Ferreira Moreira de Magalhães",
    "data": "2013-01-07",
    "abstract": "Este relatório resume o meu percurso profissional centrado em funções de Direcção Técnica, Gestão de Projectos, Consultoria e Formação em Tecnologias e Sistemas de Informação, apresentado para efeitos de obtenção do Mestrado\nem Engenharia Informática, de acordo com o Despacho RT-38/2011, de 21 de Junho.\nA minha experiência centrou-se nas a áreas de infra-estrutura, tecnologia e plataformas de TI, com um grande ênfase na prestação de serviçs de desenho, implementação, operação e manutenção de serviços Internet e Intranet. Entre as entidades empregadoras (directa ou indirectamente) encontram-se Universidades, Empresas Tecnológicas e de Marketing, Bancos, Instituições Governamentais, entre outras.\nA nível pessoal, tive ainda a oportunidade de publicar duas obras técnicas, sobre serviços de TI open-source."
  },
  {
    "keywords": [
      "Microarrays",
      "Genes",
      "Área abaixo da curva ROC",
      "Coeficiente de sobreposição",
      "Arrow plot",
      "Area under the ROC curve",
      "Overlapping coefficient",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Seleção de genes diferencialmente expressos baseada em metodologia ROC (Receiver Operating Characteristic)",
    "autor": "Lemos, Catarina Isabel Ferreira Miranda",
    "data": "2017",
    "abstract": "A análise da expressão genética é essencial para uma identificação da função dos genes\ne para a identificação destes quando relacionados com doenças. Para a realização de um\nestudo em larga escala de mudanças na expressão genética é necessário encontrar um\nmétodo que o faça com precisão e exatidão. Desta forma, foi aqui incluída, uma análise\npela tecnologia de microarrays, uma ferramenta importante no diagnóstico de doenças.\nA execução de um método que identificasse genes com regulação negativa e positiva e\ngenes diferencialmente expressos simultaneamente, tornou-se, a principal motivação deste\ntrabalho.\nDe entre as diferentes técnicas estatísticas, a metodologia ROC (Receiver Operating\nCharacteristic) foi a escolhida para o efeito.\nQuando se associa a metodologia ROC com a análise de dados de microarrays é possível\nver que uma das principais aplicações é a identificação de grupos de genes associados ao\ndesenvolvimento de qualquer patologia cancerígena. Para a análise deste último parâmetro\né utilizado o arrow plot com a representação do OVL (Overlapping Coefficient) e da AUC\n(Area Under the Curve) para cada gene, numa experiência de microarays e comparar a sua\neficácia com outros métodos existentes para o mesmo propósito.\nAtravés da análise de um conjunto de dados de pacientes afetados pelo adenocarcinoma\ndo pâncreas foi possível identificar os genes diferencialmente expressos, sendo este o\nprincipal objetivo do trabalho em questão."
  },
  {
    "keywords": [
      "Mashups",
      "Message brokers",
      "Meteorology",
      "Scalability",
      "Software architectures",
      "Arquiteturas de software",
      "Escalabilidade",
      "Meteorologia",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Mashup de serviços de meteorologia",
    "autor": "Vieira, João Pedro Ferreira",
    "data": "2021-10-01",
    "abstract": "Nowadays, there are a lot of information services, many of them making their data available through APIs\n(some free, others not) so that users can use the data at their own way. Services related to the weather informa tion area are an example of this type of available services, which makes it possible to query weather information\n(such as temperature, humidity, rain, wind, etc.) in real time from the moment when the information provided by\nmeasuring stations (domestic or professional) can be accessed.\nThe aim of this project is to research and work on aspects of application integration of this kind of information\nresorting to the development of an application that allows users to access information (obtained from various\nsources) about, for example, the meteorology of a given location of choice, by combining official sources with\nother sources, starting with the use of the infrastructure provided by Netatmo as a test bed example.\nGiven the context of using multiple data sources, it is critical to study and develop an architecture that meets\nthe expectations associated with service-based architectural environment.\nIn addition, to establish this proof of concept, the system should be scalable and comply with a set of Quality\nof Service (QoS) parameters established initially, such as the use of message brokers and caching."
  },
  {
    "keywords": [
      "Serviço de validação",
      "Assinatura eletrónica qualificada",
      "Selo eletrónico qualificado",
      "Listas de confiança",
      "União Europeia",
      "eIDAS",
      "ETSI",
      "Confiança",
      "Identificação digital",
      "Políticas de validação",
      "Validation service",
      "Qualified electronic signature",
      "Qualified electronic seal",
      "Trust lists",
      "European Union",
      "Trust",
      "Digital identification",
      "Validation policies",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "eIDAS Qualified services: serviço de validação",
    "autor": "Oliveira, Miguel Rúben Silva de",
    "data": "2024-05-20",
    "abstract": "A transformação digital é cada vez mais visível na nossa sociedade, e as transações realizadas entre diferentes\nentidades, sejam eles algum tipo de empresa ou organização, ou até mesmo cidadãos comuns, não escapam\na esta nova era digital. Para viabilizar este novo paradigma de identificação digital é necessário estabelecer\numa base de confiança, de modo a que todos se sintam confortáveis para migrarem, com alguma segurança,\npara esta nova realidade. É em seguimento desta premissa que surge o regulamento eIDAS. Este regulamento\nemitido pelo parlamento europeu estabelece as condições e conceitos que configuram as bases de confiança\nnecessárias para concretização prática dos serviços de identificação eletrónica reconhecidos por todos os\ndiferentes estados membros. Com vista à definição de normas sob uma perspetiva mais técnica, surge o\norganismo ETSI, o qual apresenta normas relativas a todos os elementos e serviços de confiança que integram\ne servem de fundamento para todo este ecossistema de identificação digital, como é o caso das assinaturas e\nselos eletrónicos qualificados(as). No que diz respeito à validação, foram emitidos standards com vista à\nnormalização dos mais variados elementos, desde as próprias assinatura eletrónicas (AdES), e alternativas de\nimplementação prática para as mesmas (CAdES, XAdES, etc.), aos mais ínfimos detalhes tais como, o próprio\nprocedimento a aplicar por parte da entidade prestadora do serviço de validação, assim como o relatório de\nvalidação emitido, protocolos de comunicação, políticas para validação, entre outros.\nNo contexto desta dissertação, é efetuada uma implementação prática de uma prova de conceito do serviço\nqualificado de validação de assinaturas e selos eletrónicos qualificados, a par de uma aplicação cliente\ncapaz de demonstrar as suas funcionalidades para alguns casos de uso. Essa implementação foi precedida por\num levantamento de requisitos a serem cumpridos pelo serviço de validação a implementar, tendo por base a\ndocumentação emitida pelo ETSI nesse sentido. A justificação para esse processo de seleção prende-se com a\nexistência de uma quantidade considerável de requisitos que incidem sobre questões fora do âmbito de uma\nprova de conceito (p.e. deploy, etc.) e que por isso, não lhe são aplicáveis. Adicionalmente, foram implementados\nalguns serviços e funcionalidades secundárias com o intuito de proporcionar uma melhor experiência de\nutilização da aplicação, como é o caso do serviço de gestão de políticas de validação, que viabiliza a criação,\npesquisa e obtenção de políticas de validação aplicáveis, em qualquer momento, à validação, e ainda a\npossibilidade de personalização da base de certificados de confiança a serem usados pelo processo de\nvalidação."
  },
  {
    "keywords": [
      "Simulação",
      "Discos",
      "Sistemas distribuídos",
      "Simulation",
      "Disks",
      "Distributed systems",
      "681.3"
    ],
    "titulo": "Modelo de simulação de discos",
    "autor": "Gomes, João Miguel Barbosa",
    "data": "2013",
    "abstract": "A simulação de componentes é uma importante ferramenta para o auxílio no desenvolvimento\nde sistemas, realização de testes e uma melhor compreensão acerca desses\nmesmos componentes por parte de investigadores e desenvolvedores. Esta pode ser realizada\nutilizando diferentes abordagens, mas tem de permitir uma reprodução fiável do\nambiente.\nA presente dissertação assenta sobre uma plataforma já existente, o Minha. Esta plataforma\npermite simular sistemas distribuídos e é capaz de simular todas as interações\nentre várias máquinas ao nível da rede. Embora a plataforma seja capaz de realizar a\nsimulação ao nível da rede, esta não era capaz de realizar qualquer simulação ao nível\ndos discos das máquinas simuladas, até à realização do dissertação. É este o problema\nque a presente dissertação se propõe resolver, criando um módulo que realize a intercepção\ndas operações sobre o disco e que trate as mesmas de forma a simular a existência\nde um disco independente para cada uma das máquinas simuladas.\nEsta dissertação tem como objetivo dotar a plataforma de um novo módulo que permita\nque a mesma consiga simular sistemas que necessitem de recursos do disco, como bases\nde dados. Até à realização da dissertação a plataforma não fazia qualquer controlo sobre\nos recurso requeridos do disco, o que provocava resultados de simulação inconsistentes\ndevido à partilha não controlada do disco da máquina onde a simulação era realizada.\nO modelo de simulação apresentado é validado experimentalmente com um microbenchmark\ne com TPC-B sobre a base de dados HyperSQL.\nDe realçar que o resultado da dissertação em questão já se encontra integrado na plataforma\ne disponível no repositório oficial da plataforma Minha que se encontra alojado\nem http://code.google.com/p/minha/."
  },
  {
    "keywords": [
      "Pensamento computacional",
      "Recurso educacionais",
      "Realidade aumentada",
      "Programação de computadores",
      "Estruturas de dados",
      "Computational thinking",
      "Learning resources",
      "Augmented reality",
      "Computer programming",
      "Data structures",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Imperative programs visualization with augmented reality",
    "autor": "Martins, Luis Carlos da Costa Salazar",
    "data": "2023-12-15",
    "abstract": "Este documento relata um Projeto de Mestrado, do segundo ano do Mestrado em Engenharia\nInformática da Universidade do Minho, em Braga, Portugal. O projeto consiste no desenvolvimento de uma Aplicação Mobile de Realidade Aumentada a ser utilizada como Recurso Educacional para apoiar o ensino introdutório de Programação de Computadores, com o objetivo de aumentar a motivação e a perseverança dos estudantes, ajudando-os a superar certas dificuldades que muitos estudantes enfrentam na abstração e compreensão de diferentes tipos de estruturas de dados. Para atingir os objetivos do Projeto de Mestrado, foi necessário, no início, definir uma linguagem de programação imperativa com várias estruturas de dados e operações para as manipular e usar, para depois utilizar a Realidade Aumentada em telefones móveis para criar um sistema que ajude a visualizar e entender as referidas estruturas de dados. Nesta dissertação, a linguagem de programação referida é especificada e ilustrada com a ajuda de alguns exemplos. A arquitetura do sistema é proposto, e o seu desenvolvimento é descrito em detalhe, abordando os detalhes mais importantes. O sistema desenvolvido é apresentado, e os resultados obtidos a partir de um experimento envolvendo estudantes em uma sala de aula real são analisados."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Evolutionary engineering of lactic acid bacteria",
    "autor": "Gomes, José Fernando Santos",
    "data": "2016",
    "abstract": "Biotechnology plays an essential role in the modern industry and in guaranteeing sustainable future for humankind. Advances of metabolic engineering and systems biology allow the adaption of complex cellular networks for the production or uptake of certain molecules, with great economical interest, enabling the creation of cell factories. Among the potential microorganisms that fit this role is the well-known group, due to their role in food fermentation and, in particular, their use in dairy industry, known as Lactic acid bacteria (LAB). Their metabolism is known for its relative simplicity and lack of biosynthesis capacity, creating a potential application as a cell factory in transformation processes.\nThe purpose of this work is to develop through evolutionary engineering a strain of LAB capable of utilizing mannitol as the sole carbon source and identify mutations in the evolved strain, with the objective of associate these mutations with the mannitol consuming phenotype.\nThrough the usage of adaptive laboratory evolution (ALE), several strains of LAB were evolved and a selected evolved strain of Lactococcus lactis subsp cremoris, capable of consuming mannitol as the sole carbon source successfully, was sequenced using next-generation sequencing.\nFrom the analysis of this genomic data using several bioinformatics tools available, 3 mutations affecting the genes pta, adhA and mtlF were identified as likely having an impact in the new phenotype presented by the evolved strain.\nThis work provides an initial inquiry into a potential application of brown algae, which accumulate mannitol, as a new feedstock for biofuel production using LAB as cell factories."
  },
  {
    "keywords": [
      "Spectre attacks",
      "Speculative execution",
      "Ataques spectre",
      "Jasmin",
      "Type system",
      "Blade",
      "Hash Blake2b",
      "oo7",
      "Execução especulativa",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Speculative execution resilient cryptography",
    "autor": "Carneiro, Ana Luísa Lira Tomé",
    "data": "2023-12-05",
    "abstract": "Spectre attacks pose a significant threat to modern computer systems, exploiting speculative execution\nto leak sensitive information from a program. Since speculative execution is present in modern CPU it is\nof high priority to protect programs against these spectre attacks without reducing performance.\nThis study presents a comprehensive comparison of mitigation strategies employed by different tools\nto counteract the effects of spectre attacks. However, this paper focuses its analyses on the type system\nfrom the Jasmin framework and on the Blade tool. The type system uses three main primitives that work\ntogether to protect vulnerable variables from leaking. On the other hand, the Blade tool allows for an\nautomatic mitigation strategy that implements a Min-Cut algorithm to a graph representing the different\noutputs of a program in order to find the minimal cut points that stop speculative execution from leaking\nvulnerable variables.\nAside from these strategies, this study also presents in detail the oo7 tool that identifies spectre\nvulnerable patterns which are used to easily identify if a program is vulnerable to spectre attacks.\nThrough an in-depth analysis of their techniques, performance implications, and applicability, this\nresearch evaluates the suitability of both strategies to protect cryptography functions against spectre attacks\nby protecting the Blake2b hash function. In the end, this comparative analysis between these two mitigation\nstrategies determines in which scenario or purpose which technique should be used."
  },
  {
    "keywords": [
      "SGBD",
      "Sql2unicage",
      "Interrogação",
      "SQL",
      "Unicage",
      "Linha de comandos",
      "Ficheiros",
      "DBMS",
      "Query",
      "Command line",
      "Files",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Interpretação e execução de SQL sobre ficheiros",
    "autor": "Dias, Bruno Filipe de Sousa",
    "data": "2024-05-20",
    "abstract": "Para processar dados utilizando SQL, em geral, é necessário carregá-los previamente para uma base de\ndados. A Unicage propõe uma solução personalizada para processamento de dados que permite fazer a\ngestão de todo o ciclo de vida dos dados sem que seja necessário efetuar o seu carregamento prévio. Esta\nsolução oferece um conjunto de comandos baseados numa filosofia Unix, sendo a sua sintaxe bastante\ndiferente de uma interrogação SQL convencional. A conversão de uma interrogação SQL para comandos\nUnicage é um processo manual, custoso e demorado, que requer a intervenção de um perito. Torna-se,\nentão, pertinente a construção de uma plataforma que efetue a tradução e otimização de uma interrogação\nSQL para operações de acordo com a filosofia Unicage, passíveis de serem aplicadas diretamente sobre\nficheiros de texto.\nO objetivo desta dissertação é a implementação ou adaptação de um motor de interrogações para\nreceber uma interrogação SQL e a transformar em operações sobre ficheiros de texto, utilizando os co mandos da shell e comandos Unicage. Em particular, utiliza-se o otimizador do motor de interrogação\npara gerar um plano de execução otimizado, tendo em conta as especificidades da execução sobre fi cheiros alterando, por exemplo, a ordem de execução de comandos de forma a minimizar o tempo de\nresposta.\nA avaliação mostra que as regras e estratégias tradicionais de otimização de interrogações SQL conti nuam a ter um impacto positivo apesar da conversão de SQL para Unicage, diminuindo o custo cumulativo\ndas interrogações e o tempo de execução das mesmas. A avaliação de desempenho, mostra ainda que a\naplicação desenvolvida se revela vantajosa comparativamente ao SGBD PostgreSQL, quando comparados\nos tempos de execução de uma mesma interrogação sobre os mesmos dados."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Implementation and test of transactional primitives over Cassandra",
    "autor": "Coelho, Fábio André Castanheira Luís",
    "data": "2013",
    "abstract": "NoSQL databases opt not to offer important abstractions traditionally\nfound in relational databases in order to achieve high levels of scalability and\navailability: transactional guarantees and strong data consistency. These\nlimitations bring considerable complexity to the development of client applications\nand are therefore an obstacle to the broader adoption of the technology.\nIn this work we propose a middleware layer over NoSQL databases that\noffers transactional guarantees with Snapshot Isolation. The proposed solution\nis achieved in a non-intrusive manner, providing to the clients the same\ninterface as a NoSQL database, simply adding the transactional context. The\ntransactional context is the focus of our contribution and is modularly based\non a Non Persistent Version Store that holds several versions of elements\nand interacts with an external transaction certifier.\nIn this work, we present an implementation of our system over Apache\nCassandra and by using two representative benchmarks, YCSB and TPC-C,\nwe measure the cost of adding transactional support with ACID guarantees."
  },
  {
    "keywords": [
      "Biofilmes",
      "Morfologia de colonia",
      "Processamento de imagens",
      "Mineração de dados",
      "Biofilms",
      "Colony morphology",
      "Image processing",
      "Data mining",
      "681.3:57",
      "57:681.3"
    ],
    "titulo": "Mining images of microbial communities for morphological characteristics in a support of clinical decision making",
    "autor": "Domingues, Ana Catarina de Jesus",
    "data": "2013",
    "abstract": "Um biofilme é uma comunidade de microrganismos envoltos por uma matriz extracelular\nproduzida pelos próprios, que lhes garante proteção. Os biofilmes representam um problema para\na saúde pública pois facilmente encontram-se em dispositivos médicos, podendo causar\nproblemas graves para os pacientes.\nEstudos prévios indicam algumas alterações observáveis do aspeto físico e bioquímico das\ncomunidades microbianas na resposta à resistência e à virulência. Assim a morfologia da\ncomunidade pode ser um indicativo da reação regulatória associada com fenómenos de\npatogenicidade microbiana.\nO objetivo deste trabalho é por um lado a criação de um novo sistema de classificação de\nmorfologia de colonia com medidas extraídas de softwares de imagem por outro lado, o estudo da\nclassificação morfológica existente e do novo sistema de classificação, através de técnicas de\nmineração de dados com o objetivo de ajudar nestas classificações. Apresentamos vários\nsoftwares como solução que vão desde a caraterização da estrutura dos biofilmes até a\ncaraterização de morfologia de colonia"
  },
  {
    "keywords": [
      "Xilose redutase",
      "Xilitol desidrogenase",
      "D-xilose",
      "Xilitol",
      "Docking",
      "Volume",
      "Xylitol",
      "Xylose reductase",
      "Xylitol dehydrogenase",
      "D-xylose",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Melhorar a taxa de fermentação da xilose em etanol em Saccharomyces cerevisiae através da incorporação de informações estruturais de proteínas",
    "autor": "Nóvoa, Cláudio Filipe Ferreira",
    "data": "2019",
    "abstract": "O consumo crescente do petróleo constitui um grave problema ambiental e económico para a sociedade atual. Para solucionar este problema, o uso de biocombustíveis apresenta-se como uma viável alternativa. Através do aproveitamento da cana-de-açúcar e do milho, utilizando microrganismos, é possível obter etanol, o biocombustível mais utilizado atualmente. Porém, nem todos os açúcares provenientes destas matérias-primas são fermentados de forma economicamente eficiente, sendo que a D-xilose, o segundo açúcar mais abundante, continua a ter um aproveitamento ineficiente por parte de microrganismos como a Saccharomyce cerevisiae, apesar das alternativas já exploradas. A via de fermentação da xilose é constituída por diversas enzimas, sendo que o problema da fraca taxa de conversão de xilose em etanol se deva essencialmente às primeiras duas: a xilose redutase e a xilitol desidrogenase.\nMuitas das abordagens para solucionar o ineficiente consumo da D-xilose têm por base a engenharia metabólica, ou como tornar a xilose redutase ou a xilitol desidrogenase específica para o mesmo cofator. Este trabalho tem o objetivo de descobrir caraterísticas estruturais, na xilose redutase e na xilitol desidrogenase, que possam ter influência na afinidade da D-xilose e no seu modo de ligação. Para tal foram usadas e testadas abordagens in silico tendo em consideração a estrutura das proteínas e os açúcares que estas utilizam como substrato, deixando para segundo plano os cofatores e o metabolismo. Foram recolhidas diversas sequências de xilose redutases e xilitol desidrogenases de diversos organismos e as suas estruturas tridimensionais modeladas. Para esses modelos foi medido o volume dos seus centros ativos e realizado o docking molecular da D-xilose, xilitol e outros substratos. Os resultados do docking e dos volumes foram comparados com os KM dos diferentes substratos.\nPraticamente todos os volumes da xilose redutase foram corretamente medidos, o que não se verificou no caso da xilitol desidrogenase. Relativamente ao docking molecular, foram analisados os resíduos envolvidos nos processos catalíticos destas duas enzimas bem como os scores resultantes. A Candida tenuis e Candida boidiini foram as que apresentaram uma maior afinidade no docking da xilose com um valor de 5,1. No docking do xilitol nas xilitol desidrogenases a Rhizomucor pusillu foi a que teve melhor score com 5,1.\nEste trabalho evidenciou que os volumes do centro ativo e o KM aparentam não estar diretamente relacionados. Os resíduos N e H do centro catalítico da xilose redutase estão conservados em todas as xilose redutases e estão envolvidos nas interações polares com a xilose e outros açúcares."
  },
  {
    "keywords": [
      "Parkinson’s Disease",
      "Deep learning",
      "Human activity recognition",
      "Doença de Parkinson’s",
      "Reconhecimento de atividade humana",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automatic detection of daily living activities in people with Parkinson’s disease using kinematic-driven data",
    "autor": "Abreu, Luís Filipe Simões de",
    "data": "2022-12-21",
    "abstract": "Parkinson’s Disease (PD) is a neurodegenerative disorder of the central nervous system. Resting \ntremor, akinesia, and bradykinesia (slow movements), rigidity, shuffling walking, and postural instability\nare some of the symptoms that not only negatively impacts patients’ life, but also the life of people around\nthem.\nCurrent approaches for monitoring patients’ motor autonomy are limited to the observer and self reported methods. The observer-based examinations, patients perform a set of standard PD examinations.\nThe self-reported method relies on patients’ daily activities diaries. These approaches are commonly used,\nbut are limited to a few sessions per year, they do not address common motor daily tasks, and their \nresults are object of subjective interpretation by the clinical expert.\nBy combining kinematic-driven data from wearable sensor with AI, the main goal of this \ndissertation is to develop an automatic software for recognition of human activities (e.g., walking, \nstanding, turning, sitting, and lying) in PD to assist the clinical experts with objective and concrete data.\nA data collection protocol was developed and captured, resulting in a database comprised of data \ncollected from eighteen PD patients who performed three trials of six different daily activities: walk; 180º \nturning; sit on chair; get up from chair; lay on bed and get up from bed.\nA Deep Learning (DL) framework based on Convolutional Neural Network capable of recognizing \ndaily activities was developed and attained a performance of F1 Score equal to 0.90892.\nAs a complementary goal an automatic software for human walk initial contact (IC) and final \ncontact (FC) recognition using kinematic data was also developed. IC and FC are tremendously important \nto provide patient on-demand motor assistance and estimation of walking-associated metrics.\nA Deep Learning framework based on Bidirectional Long Short-Term Memory Neural Network\ncapable of walking IC/FC events detection was developed and attained a performance of MCC Score \nequal to 0.538386.\nPromising results were attained for both DL frameworks, however, this dissertation suggests that \nthere is still room for further improvements. Enriching the dataset with more data from different patient, \ndata balancing and feature extraction techniques, experimenting new models’ architectures should be \nconsidered in future works."
  },
  {
    "keywords": [
      "681.3:78",
      "78:681.3"
    ],
    "titulo": "A toolkit for music processing and analysis",
    "autor": "Azevedo, Bruno Miguel Correia",
    "data": "2013-09-27",
    "abstract": "Presentemente, plataformas cooperativas para edição de partituras musicais, como a Wiki::Score que utiliza a notação abc, não têm à sua disposição utilitários de avaliação e deteção de erros, nem ferramentas que auxiliem a musicologia. Esta carência impede os utilizadores de tirarem o melhor partido dessas plataformas e proporciona um sentimento de limitação na composição e transcrição de partituras.\n\nPara colmatar estas falhas, e adotando a filosofia utilizada pelo sistema operativo Unix, criar-se-á um toolkit, em que cada ferramenta trata um problema individualmente, como a deteção e correção de erros sintáticos, léxicos, entre outros. Para que estas ferramentas tenham uma componente musicológica como a análise tonal e deteção de padrões, é necessária a construção de corpora de obras musicais, onde, após análise, é possível extrair conhecimento que será integrado nas ferramentas criadas ou exibido ao utilizador num formato específico."
  },
  {
    "keywords": [
      "Algorithm",
      "Timed regular expression",
      "Brzozowski's method",
      "Software tool",
      "Algoritmo",
      "Expressões regulares temporais",
      "Método de Brzozowski",
      "Ferramenta de software",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Regular expressions for timed automata",
    "autor": "Ribeiro, Paulo",
    "data": "2021-10-21",
    "abstract": "Finite automata are valuable models for various types of software, such as system verifica-tion software and text search software, etc. A widely acclaimed result in computer science is the proof that finite automata can be represented through a notation structure called a regular expression. This means that regular expressions succeed in describing the same patterns that can be represented by a finite automaton. We know that it is difficult to prove that two finite automata are equivalent. Still, by converting them to a regular expression, you can determine relatively easily whether they represent the same language. Timed automata are an extension of finite automata with a finite set of clocks. Timed automata are widely used in model checking and also in real-time systems such as commu-nication and security protocols. In recent years there is an increasing demand for this type of software. As in classical automata, timed automata can also be represented by regular expressions. However, so far, there is no conversion software developed. There are two main methods for converting: the state elimination method; and the Brzozowski's method. These methods refer to the classic automaton model. The goal of this work is the study and development of an algorithm that converts a timed automaton into a timed regular expression. For this, i) we developed a conversion al-gorithm based on Brzozowski's method for timed automata adding to the classical case several changes, in particular, the incorporation of clocks and transition restrictions, among other, 2) we developed a software tool that converts a timed regular expression into a timed automaton, and depicting both of them."
  },
  {
    "keywords": [
      "Classificação de expressões faciais",
      "Deteção facial",
      "Machine learning",
      "Visão por computador",
      "Parametrização facial",
      "Expressões faciais em jogos",
      "Facial expression Classification",
      "Facial detection",
      "Machine learning",
      "Computer vision",
      "Facial parameterization",
      "Facial expressions in games",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Reconhecimento e uso de emoções em jogos",
    "autor": "Carvalhais, Tiago João Lopes",
    "data": "2018",
    "abstract": "O reconhecimento automático de expressões faciais tem sido um tópico bastante ativo desde\no início dos anos noventa. Nos últimos anos, têm ocorrido bastantes avanços nas temáticas\ndo tracking e da deteção de faces, nos mecanismos de extração de características faciais e\nnas técnicas de classificação das emoções.\nNeste último campo, existem algumas propostas de jogos em que, através da deteção de\nfaces, é possível detetar quem está a jogar e desse modo carregar automaticamente o perfil\ndo jogador. Igualmente, o reconhecimento das expressões faciais dos jogadores tem sido\nusado para alterar a expressão dos seus avatars.\nCom a realização desta dissertação, pretenderam-se estudar várias formas de usar o reconhecimento\nda expressão facial de um jogador para alterar alguns parâmetros de um jogo,\ncomo a diminuição da sua dificuldade, por exemplo, quando se detetar que o jogador está\nstressado ou, em casos extremos, até sugerir a suspensão do jogo.\nO maior desafio que se encontra, neste caso, é o da deteção e classificação da expressão\nfacial do jogador em tempo real, assim como a inclusão desta funcionalidade dentro do jogo.\nDurante o período de desenvolvimento, foram estudados diversos métodos de deteção e\nclassificação das emoções de um indivíduo e procurou-se a melhor forma de as conciliar\ncom a construção do jogo, que foi desenvolvido para uma plataforma desktop.\nA abordagem tomada para a classificação das expressões faciais de um jogador passou pelo\ntreino de modelos de aprendizagem, com pontos que representam as características da face.\nIsso ocorreu com a ajuda de uma base de dados de fotografias tiradas a determinados sujeitos\nde teste, tendo sido determinados os pontos faciais para cada imagem e, de seguida,\nalimentados aos modelos de aprendizagem. Após este processo, passa a ser possível a\nclassificação em tempo real de uma expressão facial, bastando para tal fornecer-se ao modelo\npré-treinado o conjunto dos pontos faciais do jogador, obtidos a cada frame.\nO passo seguinte consistiu na construção de um jogo que implementasse estas funcionalidades.\nCom o modelo de classificação de expressões faciais a correr em paralelo ao módulo\ndo jogo, foi possível determinar qual o estado emocional sentido pelo jogador durante certos intervalos de tempo. Com esta informação, foram alterados determinados parâmetros\ndo jogo, nomeadamente a dificuldade do mesmo.\nNo que toca à classificação de expressões faciais, verificou-se uma eficácia elevada dos\nmodelos de aprendizagem, através da utilização deste método. Contudo, é preciso ter em\nconta que mesmo que seja possível definir um determinado conjunto de emoções, o que\né certo é que a grande variedade de estados emocionais e a sua instabilidade ao longo do\ntempo acarretam um grande número de dificuldades.\nOs parâmetros do jogo foram alterados de forma bastante satisfatória, embora ainda se\npossa proceder a uma exploração mais profunda sobre este aspeto, na medida em que as\nemoções expressas por um jogador dependem sempre de um grande número de fatores,\nnomeadamente do género do jogo."
  },
  {
    "keywords": [
      "Aplicação de software",
      "Cibersegurança",
      "Eventos",
      "Visualização",
      "Cybersecurity",
      "Events",
      "Software application",
      "Visualization",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Visualização de eventos de cibersegurança",
    "autor": "Sousa, Daniel Filipe Santos",
    "data": "2023-11-22",
    "abstract": "Na atual era digital, a cibersegurança surgiu como um aspeto essencial da tecnologia da informação que\ntem uma importância significativa a nível mundial. A crescente dependência das plataformas digitais e o\nvolume cada vez maior de dados sensíveis trocados através da internet fizeram com que a cibersegurança\ndeixasse de ser um elemento opcional e passasse a ser uma necessidade fundamental.\nA importância da cibersegurança vai para além das medidas de proteção. Desempenha um papel\ncrucial para garantir a continuidade das operações comerciais, proteger a privacidade das pessoas e\nmanter a integridade dos sistemas e dos dados.\nUm dos problemas atualmente é o excesso de informação de segurança que se tem de analisar. Além\ndisso, nem sempre a relação entre eventos é direta e, por vezes, apenas é possível detetar algo malicioso\nquando são observados eventos que à partida parecem não relacionados.\nEm resposta a este desafio, o tema desta dissertação é o desenvolvimento de um gestor de eventos\nno contexto da cibersegurança. Este gestor tem como objetivo simplificar o processo de identificação e\ncompreensão das relações entre vários eventos de segurança, mesmo aqueles que podem parecer não\nrelacionados à primeira vista.\nO sistema proposto fornecerá uma vasta quantidade de dados, permitindo aos utilizadores identificar\npadrões, anomalias e correlações que possam significar potenciais ameaças à segurança. A interface pro posta não é apenas uma ferramenta para interpretar dados de segurança, mas uma solução abrangente\nque aumenta a eficiência das operações de cibersegurança.\nEsta tese tem como objetivo demonstrar a implementação deste tipo de gestor de eventos, explorando\nos seus potenciais benefícios e implicações para as práticas de cibersegurança."
  },
  {
    "keywords": [
      "Incontinence",
      "Alarm",
      "Bluetooth",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desenvolvimento de sistema de alarmes para incontinentes",
    "autor": "Soares, Roberto da Silva",
    "data": "2015-07-13",
    "abstract": "A incontinência urinária, é um problema de saúde com múltiplas repercussões, que interferem negativamente na qualidade de vida dos doentes. Aliado a este facto, está a dificuldade em realizar diagnósticos corretos acerca do grau de incontinência. Sem um diagnóstico correto o tratamento, que já por si é complicado, incorre em dificuldades adicionais. Assim, tem-se assistido ao desenvolvimento de vários produtos para incontinência, cujo objetivo é melhorar a qualidade de vida dos pacientes, face aos danos causados pela doença. Tratam-se de sistemas concebidos para aumentar a autoestima das pessoas na sua presença em público, face ao receio de algum evento de incontinência. Nesse sentido, o principal objetivo deste trabalho, é a criação de um sistema de alarme para incontinentes, que é constituído por uma componente de deteção de líquido, incorporada em roupa interior específica para incontinentes e uma componente de geração de alarme, que corresponde a um dispositivo android . A comunicação entre os dois dispositivos é estabelecida, através do protocolo Bluetooth. A componente de deteção de líquido, inclui a implementação de um sensor de líquido baseado em materiais fibrosos, integrado no substrato têxtil específico para incontinentes. É também responsável por, processar a informação proveniente do sensor e por comunicar com o dispositivo android . Os resultados provenientes do sensor consistem na resistência elétrica produzida, relacionada com a quantidade de líquido libertado. Por sua vez, a componente de geração de alarme, é uma aplicação móvel, concretizada com o propósito de emitir alarmes, quando recebe sinais provenientes do dispositivo. Além disso, fornece uma interface de configuração do dispositivo de deteção, para o nível de alarme desejado. Permite ainda registar eventos de incontinência numa base de dados local. O trabalho desenvolvido tornou possível a conceção e implementação de um sistema de alarme para incontinentes, sendo que o nível de alarme pode ser configurado em função da quantidade de líquidos. A existência duma base de dados com registo de eventos permite a monitorização em modo offline por períodos mais longos."
  },
  {
    "keywords": [
      "Education",
      "Computational thinking",
      "Web platform",
      "Programming",
      "Ontology",
      "Educação",
      "Pensamento computacional",
      "Plataforma web",
      "Programação",
      "Ontologia",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Micas, a web platform to support teachers of computing at school",
    "autor": "Azevedo, Ana Cristina Branco",
    "data": "2020-02-12",
    "abstract": "This document presents the motivation, development and results of a Masters Thesis work\nin Informatics focused on Computational Thinking Education, that was accomplished at\nUniversidade do Minho in Braga, Portugal. This thesis is based on a big ontology that\ndescribes in detail the concepts ’Computational Thinking’ and ’Programming’, which maps\nthose concepts to different education levels, starting with the first year. The main goal is\nthe development of a Web Platform that, on one hand, helps on collecting in a repository\nand classifying any kind of resources to be used by teachers on computing classes and, on\nthe other hand, helps on the retrieval from that repository of the most adequate resources\nto teach a specific subject to a specific level. The classification and the intelligent search\nmechanism will follow the knowledge description defined by the ontology."
  },
  {
    "keywords": [
      "Cidades inteligentes",
      "Transporte inteligente",
      "Monitorização do tráfego",
      "Estado do pavimento",
      "Acelerómetros",
      "Internet das coisas",
      "Sensores",
      "Smart Cities",
      "Smart transportation",
      "Traffic monitoring",
      "Pavement condition",
      "Accelerometers",
      "Internet of things",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Monitorização do estado do pavimento e da congestão das vias em cidades inteligentes",
    "autor": "Oliveira, Carolina Castro de",
    "data": "2023-12-15",
    "abstract": "Nos últimos anos, cada vez mais pessoas que anteriormente viviam em zonas rurais migram para centros urbanos à procura de novas oportunidades. Face a este movimento, vários problemas e adversidades foram-se agravando, nomeadamente, o aumento do fluxo rodoviário, que cria problemas de trânsito, o aumento dos níveis de poluição, o acesso à saúde, entre outros. Desta forma, torna-se imperativo gerir de forma eficaz e sustentável os recursos, com a finalidade de melhorar a qualidade de vida dos habitantes destas cidades.\nNeste contexto, juntamente com os avanços tecnológicos que se tem observado, surge o conceito de Cidades Inteligentes, que recorrendo a redes de sensores recolhem todos os dados necessários para ”virtualizar” as cidades. Desse modo, a informação coletada está centralizada, para que assim seja possível gerir os recursos disponíveis de forma informada, responsável e eficiente, para que seja possível responder às necessidades da população. Com este trabalho, pretende-se estudar dois problemas concretos no âmbito das Cidades Inteligentes, nomeadamente na área do Transporte Inteligente, recorrendo à simulação de redes de sensores, constituídas por sensores de aceleração instalados na rede de transporte públicos da cidade, a partir da qual vão ser recolhidos dados. O primeiro problema que se tenciona solucionar está relacionado com a monitorização do estado do pavimento. Com os dados provenientes dos acelerómetros, espera-se ser possível estimar o estado de conservação das vias rodoviárias e, desta forma, as entidades responsáveis\npassam a ser capazes de realizar decisões informadas e apropriadas face ao estado de determinada estrada, procedendo assim à sua restauração caso necessário. Uma segunda vertente que se pretende explorar foca a monitorização da congestão das vias rodoviárias em que, com base na mesma rede de transportes, se projeta ser possível determinar os níveis de fluxo rodoviário. Por fim, é ainda expectável que beneficiando dos transportes públicos dos quais já se está a tirar proveito, seja plausível medir os níveis de poluição aérea."
  },
  {
    "keywords": [
      "Cancer",
      "Metabolic rate",
      "Mitochondrial proteins",
      "Differential gene expression",
      "Clustering",
      "Cancro",
      "Taxa metabólica",
      "Proteínas mitocondriais",
      "Expressão genética diferencial",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "MitoProfiles: cancer mitochondrial profiles in high metabolic rate organs",
    "autor": "Ferreira, Catarina Gomes",
    "data": "2023-12-21",
    "abstract": "Metabolic reprogramming is recognized as a critical hallmark of cancer, influencing cancer initiation and progression. Emerging evidence suggests that the metabolism of non-cancer cells within the tumor microenvironment plays a pivotal role in modulating tumor development, underscoring the importance of metabolic variables for better understanding cancer. The main goal of this study is to identify genes exhibiting differential expression in cancer, with a specific emphasis on distinguishing between organs with high metabolic rates (brain, liver, and kidneys) and organs with low metabolic rates (bladder, colon, and skin), particularly focusing on genes encoding\nmitochondrial proteins. For this, we used two databases containing RNA-seq samples from normal and cancer tissues, obtained from the Genotype-Tissue Expression (GTEx) and The Cancer Genome Atlas (TCGA) projects, respectively. General Linear Models (GLMs) were applied for differential expression analysis, and hierarchical clustering e soft fuzzy clustering to identify distinct gene expression profiles. Our research showed that many of the differentially expressed mitochondrial genes, such as ACSM1\nand ACSM5, and PRODH, represent potential adaptations of cancer cells to metabolic and micro environmental stress. Additionally, FDX2, a crucial player in iron-sulfur protein biogenesis, and ACSM2B, responsible for catalyzing the activation of free fatty acids (FFAs) to CoA, showed substantial expression differences, highlighting the importance of these two pathways for the oncogenic process. The most sub stantial genetic expression differences were observed between normal and cancer tissues, rather than between high and low metabolic rate organs, suggesting that the signal from the metabolic rate could be masked by the pronounced changes that cancer induces in cells. Despite the unequal sample sizes and the usage of two different data sources, our findings provide valuable insights into the complex interplay between metabolism and gene expression in cancer."
  },
  {
    "keywords": [
      "DevOps",
      "Dynamic analysis",
      "Infrastructure as code",
      "KICS",
      "Static analysis",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Infrastructure as code: analysis of misconfiguration and non-compliance problems",
    "autor": "Silva, Rafaela Maria Soares da",
    "data": "2022",
    "abstract": "Infrastructure as Code (IaC) is an innovative DevOps approach to infrastructure configuration and management.\nInstead of using traditional interactive tools — such as command line — or cloud provider web\ninterfaces, it automates several tasks through extensive use of scripting languages and tools.\nBeing a relatively new field, with a fast-paced developing set of tools, it is of crucial importance to assist\nits users and its developers to tackle security concerns that might affect the environments these tools are\nmeant to manage. Some of those security concerns must always be handled within an actual live, running\nenvironment. This is the case, for example, of checking for service availability. Issues like this are already\nbeing addressed by existing dynamic analysis tools. Others should be handled using a static analysis\napproach, which, in turn, should prevent those security concerns from ever becoming a live security issue.\nIn this dissertation, we focus on trying to bridge the gap between the set of security checks currently\nbeing addressed by tools that follow these approaches. We identify 150 security checks currently being\nperformed only by dynamic analysis tools, and we implement 23% of them in KICS, a Checkmarx-backed,\nopen source, static code analysis tool for IaC solutions.\nThe new checks we contribute to KICS address misconfiguration and non-compliance problems that can\nbe prevented using static analysis, mainly focusing on access control, but also on network security. Overall,\nthis dissertation addresses 34 security checks, effectively bridging the gap between static and dynamic\nanalysis for IaC in the KICS context.\nAlthough not always possible, we strive to make available each security check to Ansible, CloudFormation,\nand Terraform. These new security checks and the necessary changes to KICS were submitted to the GitHub\nproject’s repository, were approved by the KICS team, and are now into its master branch. This means that\nnew KICS releases will make available these security checks to its current users and to a broader audience,\nand, hopefully, will foster the development of community-based extensions and enhancements, such as\nsupport for other IaC platforms and security domains that we were unable to tackle due to time constraints."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Creating intelligible metrics road traffic analysis",
    "autor": "Quintas, Artur Filipe Freitas",
    "data": "2016",
    "abstract": "The increasing pervasiveness and lower cost of electronic devices equipped with sensors\nis leading to a greater and cheaper availability of localized information. The advent of\nthe internet has brought phenomena such as crowd-sourced maps and related data. The\ncombination of the availability of mobile information, community built maps, with the\nadded convenience of retrieving information over the internet creates the opportunity to\ncontextualize data in new ways.\nThis work takes that opportunity and attempts to generalize the detection of driving\nevents which are deemed problematic as a function of contextual factors, such as neighbouring\nbuildings, areas, amenities, the weather, and the time of day, week or month.\nIn order to research the problem at hand, the issue is first contextualized properly, providing\nan overview of important factors, namely Smart Cities, Data Fusion, and Machine\nLearning.\nThat is followed by a chapter concerning the state of the art, that showcases related\nprojects and how the various facets of road traffic expression are being approached.\nThe focus is then turned to creating a solution. At first this consists in aggregating data\nso as to create a richer context than would be present otherwise, this includes the retrieval\nfrom different services, as well as the composition of a unique view of the same driving\nsituation with new dimensions added to it. And then Models were created using different\nMachine Learning methods, and a comparison of results according to selected and justified\nevaluation metrics was made. The compared Methods are Decision Tree, Naive Bayes, and\nSupport Vector Machine.\nThe different types of information were evaluated on their own as potential classifiers and\nthen were evaluated together, leading to the conclusion that the various types combined\nallow for the creation of better models capable of finding problems with more confidence\nin such results.\nAccording to the tests performed the chosen approach can improve the performance\nover a baseline approach and point out problematic situations with a precision of over 90%.\nAs expected by not using factors concerning the driver state or acceleration the scope of\nproblems which are detected is limited in domain."
  },
  {
    "keywords": [
      "Anonymity",
      "Privacy",
      "Security",
      "Tor",
      "Traffic classification",
      "Anonimato",
      "Classificação de tráfego",
      "Privacidade",
      "Segurança",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Detection of anonymized traffic: Tor as case study",
    "autor": "Dantas, Bruno Rafael Lamas Corredoura",
    "data": "2019-12-23",
    "abstract": "This master thesis studies Tor, an anonymous overlay network used to browse the Internet.\nIt is an open-source project that has gain popularity mainly because it does not hide its\nimplementation. In this way, researchers and security experts can examine and confirm its\nsecurity requirements.\nIts ease of use has attracted all kinds of people, including ordinary citizens who want\nto avoid being profiled for targeted advertisements or circumvent censorship, corporations\nwho do not want to reveal information to their competitors, and government intelligence\nagencies who need to do operations on the Internet without being noticed. In opposite, an\nanonymous system like this represents a good testbed for attackers, because their actions\nare naturally untraceable.\nTraffic characteristics are studied in detail, which can be used to detect Tor. Further,\na detection mechanism was developed to prevent users from reaching the Tor network.\nFinally, some changes are proposed so that Tor can better disguise its traffic with traditional\nweb browsing traffic to overcome any intention of blocking it."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Estimativa de funções de probabilidade cumulativa em redes de larga escala",
    "autor": "Silva, Miguel Ângelo Borges da",
    "data": "2011-11-30",
    "abstract": "A capacidade de agregar dados é uma característica fundamental na conceção de sistemas de informação escaláveis, que permite a determinação de propriedades globais importantes de forma descentralizada, para a coordenação de aplicações distribuídas, ou para fins de monitorização.\n\nAgregados simples como mínimos/ máximos, contagens, somas e médias foram já extensivamente estudados no passado. No entanto, este tipo de agregados pode não ser suficiente para caracterizar distribuições de dados enviesadas e na presença de valores atípicos (outliers), tornando-se então relevante a determinação de uma estimativa dos valores na rede (e.g. histograma, função de distribuição cumulativa), dado que métricas como médias ou desvio padrão escondem em muitos casos alterações na propriedade monitorizada que são relevantes para decisão de controlo.\n\nSão ainda relativamente escassos os trabalhos que se focam sobre a agregação de métricas mais expressivas. Uma proposta recente nesse domínio [SNSP10] refere atingir uma precisão nas estimativas superior à atingida em abordagens anteriores. Trata-se de um algoritmo para a determinação de funções cumulativas de distribuições.\n\nApesar do contributo, essa proposta mostra limitações na tolerância a faltas e no suporte à monitorização contínua de propriedades, dado que para acompanhar alterações dos valores amostrados, a estratégia usada exige que o protocolo seja reiniciado periodicamente. Para além disso, os pressupostos dessa abordagem não admitem a perda de mensagens nem a sua duplicação.\n\nAssim, e tomando como ponto de partida o actual estado da arte, é apresentado nesta tese um algoritmo distribuído para a determinação de funções cumulativas de probabilidade em redes de larga escala. As suas principais vantagens são a imunidade à perda de mensagens, a velocidade de convergência e a precisão que se obtém na aproximação à distribuição original. É simultaneamente adaptável a alterações no valor amostrado e resiliente a dinamismo no número de nodos na rede. Usa também um mecanismo de quiesciência dos nodos assim que a variação local da estimativa é inferior a um determinado limiar. Nessa circunstância, o nodo deixa de transmitir. Isto leva à diminuição do número de mensagens trocadas entre nodos.\n\nAs distribuições determinadas em todos os nodos permitem a tomada de decisões que tirem partido do facto de se estar a agregar uma função probabilística. Assim o nodo pode excluir outliers ou observar determinados quantis da propriedade. Para além disso, cada nodo da rede possui uma estimativa global sobre o estado geral da propriedade distribuída, o que lhe permite também a tomada de decisões com base em conhecimento local.\n\nSão apresentados nesta tese resultados de simulação que confirmam a validade da abordagem seguida. É também apresentada uma revisão da literatura relacionada cujo âmbito incluiu as técnicas mais representativas da agregação de dados para métricas escalares e as técnicas de agregação de dados para métricas complexas."
  },
  {
    "keywords": [
      "Master thesis",
      "GPU Computing",
      "Physics",
      "HPC",
      "Mathematics",
      "Dissertação de mestrado",
      "Computação em GPU",
      "Física",
      "Matemática",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Parallelization of the ADI method exploring vector computing in GPUs",
    "autor": "Silva, Filipe Pereira da",
    "data": "2021-10-27",
    "abstract": "The 2D convection-diffusion is a well-known problem in scientific simulation that often uses\na direct method to solve a system of N linear equations, which requires N3 operations.\nThis problem can be solved using a more efficient computational method, known as the\nalternating direction implicit (ADI). It solves a system of N linear equations in 2N times with\nN operations each, implemented in two steps, one to solve row by row, the other column by\ncolumn. Each N operation is fully independent in each step, which opens an opportunity to\nan embarrassingly parallel solution. This method also explores the way matrices are stored in\ncomputer memory, either in row-major or column-major, by splitting each iteration in two.\nThe major bottleneck of this method is solving the system of linear equations. These\nsystems of linear equations can be described as tridiagonal matrices since the elements are\nalways stored on the three main diagonals of the matrices. Algorithms tailored for tridiagonal\nmatrices, can significantly improve the performance. These can be sequential (i.e. the Thomas\nalgorithm) or parallel (i.e. the cyclic reduction CR, and the parallel cyclic reduction PCR).\nCurrent vector extensions in conventional scalar processing units, such as x86-64 and\nARM devices, require the vector elements to be in contiguous memory locations to avoid\nperformance penalties. To overcome these limitations in dot products several approaches\nare proposed and evaluated in this work, both in general-purpose processing units and in\nspecific accelerators, namely NVidia GPUs.\nProfiling the code execution on a server based on x86-64 devices showed that the ADI\nmethod needs a combination of CPU computation power and memory transfer speed. This\nis best showed on a server based on the Intel manycore device, KNL, where the algorithm\nscales until the memory bandwidth is no longer enough to feed all 64 computing cores. A\ndual-socket server based on 16-core Xeon Skylakes, with AVX-512 vector support, proved to\nbe a better choice: the algorithm executes in less time and scales better.\nThe introduction of GPU computing to further improve the execution performance (and\nalso using other optimisation techniques, namely a different thread scheme and shared\nmemory to speed up the process) showed better results for larger grid sizes (above 32Ki x\n32Ki). The CUDA development environment also showed a better performance than using\nOpenCL, in most cases. The largest difference was using a hybrid CR-PCR, where the OpenCL\ncode displayed a major performance improvement when compared to CUDA. But even with\nthis speedup, the better average time for the ADI method on all tested configurations on a\nNVidia GPU was using CUDA on an available updated GPU (with a Pascal architecture) and\nthe CR as the auxiliary method."
  },
  {
    "keywords": [
      "Data encryption",
      "Multi-cloud",
      "Crypto wallet",
      "Encriptação de dados",
      "Multi-nuvem",
      "Carteira digital",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Implementation of practical and secure methods for storage of cryptographic keys in applications",
    "autor": "Lopes, João Nuno Alves",
    "data": "2022-04-01",
    "abstract": "Encryption has been essential to protect modern systems and services. It became the security foundation of\ndatabases, payment systems, cloud services, and others. Cryptography enabled the creation and validation of\ndigital signatures, where the protection of the private key is very important to prevent false signatures. Cryptocur rencies rely on this mechanism.\nCrypto wallets hold private keys used to sign transactions and prove ownership of a digital asset. These have\nto keep the private key secure, but accessible to its owner, as it may be needed frequently. With the increasing\nnumber of decentralized web applications that interact with a blockchain, this subject has become more prevalent,\nas they usually require frequent signatures from the user.\nThe mass adoption of cryptocurrencies by non-technical users urged the creation of crypto wallets that are\nsecure but prioritize usability. Some of these are hosted services that store the private keys in their servers\nand others are non-hosted, where the user is responsible for storing it. When implemented as a browser plugin,\nthese wallets allow the user to seamlessly interact with a web application. The rise of cloud technology brought\nforth multi-signature on the cloud, by combining different cloud services owned by the user. These give the user\ncontrol of his private key and are less vulnerable to cyber attacks.\nIn this work, it is presented a comprehensive analysis of existing crypto wallet approaches in usability and\nsecurity to understand the existing problems. The next step was to propose multiple possible solutions to those\nproblems and produce their implementations. These take advantage of previously studied multi-cloud technology\nand are used to attempt to improve usability and security. To evaluate the proposed solutions and to compare\nthem to the existing ones, we have developed a framework that consisted of various objective tests based on\nprevious work, which have the goal of evaluating security and usability.\nFinally, the proposed and existing solutions were compared using the proposed framework."
  },
  {
    "keywords": [
      "Artificial intelligence",
      "Neural network",
      "Deep",
      "Convolutional network",
      "Recurring network",
      "YOLO",
      "Web application",
      "Inteligência artificial",
      "Rede neuronal",
      "Profunda",
      "Convolucional",
      "Recorrentes",
      "Aplicação web",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Conversão de esboços de páginas Web para HTML usando aprendizagem automática",
    "autor": "Bouças, Tiago André Alves",
    "data": "2020-01-09",
    "abstract": "In the last decades, there has been an exponential development in the area of computing,\nwhich includes Artificial Intelligence (AI). The development of AI translates into the emergence of programs that replicate the ability to make decisions, perceive and solve problems\nin a similar way to humans. Today, artificial intelligence is already part of various areas of\nsociety, such as security, health, or virtual assistants.\nThis dissertation aimed to develop a Web application that converts graphical interface\nsketches, elaborated with the Balsamiq Mockups application, into HTML, CSS and Bootstrap code. Converting a Web page sketch into code is a task that developers typically\nperform. Due to the time consuming of this task, it becomes impossible to devote more\ntime to the application logic. On the other hand, it is a repetitive and tedious task.\nTwo deep neural network models were built, divided into two distinct approaches. The\nfirst approach, presenting poor results, uses a convolutional network and two recurring\nnetworks, according to an encoder-decoder architecture, similar to image captioning. It also\nuses a DSL language and a compiler that transforms DSL into code. The second approach\nis completely different and it is more focused on the spatial component of the addressed\ntask. It uses YOLO model and a layout algorithm that converts the output of YOLO into\ncode.\nIn the same test set, the first approach achieves 71.30% accuracy, while in the second\napproach it yields 88.28% accuracy.\nThe Web application, which allows the user to upload images and automatically generate\nHTML, CSS and Bootstrap code, is supported by the YOLO based model as it gives better\nresults."
  },
  {
    "keywords": [
      "Geração de código",
      "Padrões de concepção",
      "Code generation",
      "Design patterns",
      "681.3.06",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Geração automática de código para padrões de conceção",
    "autor": "Neto, Jaime Emanuel Vieira dos Santos Moura",
    "data": "2011-12-06",
    "abstract": "O recurso a ferramentas de geração automática de código permite economizar tempo quando se desenvolvem soluções de software, factor importante em questões de produtividade.\n\nExiste um conjunto de padrões de conceção [Gamma et al., 1995] que representam soluções genéricas para problemas relativos ao desenvolvimento de aplicações de software, numa perspetiva orientada aos objetos. Para cada um deles pode ser vista a sua estrutura de classes, métodos e relacionamentos, bem como as situações mais adequadas para a sua utilização. Bastará consultar o catálogo de padrões de conceção [Gamma et al., 1995] e utilizar aquele que mais se adequar à resolução de determinado problema que surja no desenvolvimento de um novo programa.\n\nA existência de uma aplicação de software capaz de fazer a geração automática do código associado aos padrões de conceção, agiliza o desenvolvimento de novas aplicações, porque fornece de imediato o respetivo código.\n\nO que se propõe com o desenvolvimento desta dissertação é uma solução de software, capaz de efetuar a geração automática de código para os padrões de conceção catalogados em [Gamma et al., 1995]. Juntamente com o programa desenvolvido, é também apresentado um levantamento do estado da arte sobre os padrões de conceção, considerando também situações atuais\nda sua aplicabilidade. Em seguida, é descrita a especificação da aplicação elaborada, bem como o seu processo de desenvolvimento, acompanhado de um exemplo de utilização. Por fim, encontram-se dois casos de estudo, servindo para provar que o programa elaborado pode ser utilizado em contextos reais."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Bluetooth hotspots for smart spaces interaction",
    "autor": "Almeida, Miguel Craveiro Martins de",
    "data": "2011-11-19",
    "abstract": "Tanto na literatura científica como na indústria, especificamente na área da Computação Disseminada e Espaços Inteligentes (Pervasive Computing and Smart Spaces), são encontradas muitas aplicações e sistemas baseados em interações (implícitas e explícitas) com recursos físicos no ambiente, tais como pontos Wi-Fi, recetores GPS, componentes Bluetooth, leitores RFID, telefones móveis ou câmaras. Bluetooth é uma tecnologia sem fios de curto alcance, que não requer configurações e que está presente num elevado número de dispositivos móveis. Tornou-se, assim, numa poderosa ferramenta para interação com ambientes físicos, sendo consecutivamente adotada por diferentes aplicações e sistemas como uma tecnologia privilegiada de interação. Atualmente, estas aplicações e sistemas implementam os seus próprios componentes Bluetooth, capazes de executar tarefas especificamente relacionadas com as aplicações em causa, tais como a obtenção de informação sobre os dispositivos dos utilizadores ou trocas de ficheiros com estes.\n\nNesta dissertação argumenta-se que os componentes Bluetooth podem ser tratados como recursos de interação do espaço físico, com a possibilidade de serem partilhados e reutilizados por diferentes aplicações e sistemas. Desta forma liberta-se os seus programadores de questões relacionados com a implementação e gestão específicas da tecnologia Bluetooth, incentivando-os a focarem-se nos objetivos da aplicação. Além disso, a nossa abordagem poderá também contribuir para a sustentação da indústria da Computação Disseminada sob uma perspetiva ambiental, dado que irá permitir a partilha e reutilização de recursos físicos, reduzindo o número de dispositivos computacionais.\n\nNeste trabalho estudam-se projetos e aplicações relevantes para a indústria da Computação Disseminada e Espaços Inteligentes que recorrem à tecnologia Bluetooth como uma forma de interação com os seus utilizadores. São identificadas as características comuns destes projetos, tratando-se de um passo importante para a sistematização destas interações. Este trabalho constitui a base do desenho de um novo recurso Bluetooth. Foram desenvolvidos e instalados protótipos em vários cenários reais, a fim de se validar não apenas a viabilidade de tal componente em Espaços Inteligentes mas, também, o modelo de integração com as aplicações. Tal validação é apresentada neste documento, funcionando como uma prova de conceito desta componente."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Improving the performance of liquid surfaces modelling in multicore devices",
    "autor": "Ribeiro, José Ricardo Cunha da Silva",
    "data": "2015-12-18",
    "abstract": "When assembling bottom terminated components in printed circuit boards, connectivity is\nextended through metallized terminals. To minimize thermal fatigue failure of the welds, software\ntools have been developed to model liquid surfaces shaped by various forces and constraints.\nSurface Evolver (SE) is the software tool used by Bosch in their media entertainment products to\nmodel liquid surfaces through the analysis of discrete parts of that surface. However, depending\non the level of detail, this process may have long execution times, which is not consistent with\nthe demand of industry and mainly in an interactive software where users expect the results to\nbe obtained quickly.\nThis dissertation aims to improve the efficiency of SE, through the optimization of the total\nenergy computation, taking advantage of vectorization, parallel computing and other high\nperformance techniques.\nThe analysis and profile of the current SE version were crucial to support the decisions taken\nto improve the computational performance of the software. Scalability tests, taking into account\nthe Amdahl’s law, call graphs and other profiling analysis helped to identify bottlenecks, where\nan effort should be invested to improve the software. One of the heaviest computations identified\nin SE is the computation of the total energy of the configuration.\nSE was identified to be a memory-bounded software, mainly due to its current mesh data\nstructure, implemented with linked lists, which limits the use of the vectorization features on\ncurrent CPU cores and also does not support data parallelization techniques and data locality.\nA new data structure was proposed to overcome these performance constraints, which led to a\nfaster execution of SE.\nThe results showed an improvement on the total energy computation, an increase of vectorizable\noperations, software prefetching techniques and scheduling optimizations which, alongside\nthe alternative data structure, increased the performance of the SE."
  },
  {
    "keywords": [
      "Anotação de textos automática",
      "Processamento de linguagem natural",
      "Extração de informação clínica",
      "Registo médico eletrónico",
      "Automatic annotation of text",
      "Natural language processing",
      "Clinical information extraction",
      "Electronic medical record",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Anotação automática de informação clínica",
    "autor": "Caçador, Ricardo Filipe Sousa",
    "data": "2023-04-11",
    "abstract": "A proximidade entre a Informática e a Saúde é cada vez maior a cada dia que passa. Nos dias que\ncorrem é comum os hospitais guardarem eletronicamente todo o historial e relatórios clínicos dos utentes.\nO armazenamento digital destes dados traz vantagens aos sistemas de saúde como a acessibilidade,\na otimização de recursos e redução de custos, a diminuição do erro médico e o auxílio nas tomadas de\ndecisões. Grande parte desses dados está em formato de texto livre, ou seja, são dados não estruturados.\nPara os sistemas computacionais, este tipo de dados representa um maior desafio quer na análise, quer\nno seu processamento. Sendo que, para este tipo de informação ser processada automaticamente é necessário recorrer ao Processamento de Linguagem Natural, uma subárea da Inteligência Artificial. Tarefas\ncomo classificação ou reconhecimento de entidades em textos requerem quase sempre textos anotados.\nO processo de anotação dos textos é demorado e pouco atrativo para o ser humano levando a que\na quantidade disponível de dados anotados não seja em grande volume e consequentemente a que a\naplicação de modelos de Machine Learning não seja a mais eficiente, resultado em problemas de over fitting e não generalizando como seria de desejar. Devido a isto, a procura por uma solução de anotação\nautomática dos dados em massa é necessária e extremamente útil.\nA principal contribuição desta dissertação é o desenvolvimento de uma aplicação para a anotação\nautomática de informação clínica. Esta aplicação permitirá a anotação de grandes quantidades de dados\nde forma automática comparativamente a outras ferramentas e abordagens existentes."
  },
  {
    "keywords": [
      "Kubernetes",
      "Microsserviços",
      "Sistemas monolíticos",
      "Deployment",
      "Agile",
      "Estratégias de deployment",
      "Estratégias de deployment em Kubernetes",
      "Microservices",
      "Monolithic",
      "Deployment strategies",
      "Kubernetes deployment strategies",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Estratégias de deployment em arquitetura de microsserviços",
    "autor": "Pinto, Francisco Felícia Correia",
    "data": "2023-12-28",
    "abstract": "Com a chegada das metodologias Agile, passou a ser possível a entrega e mudanças mais rápidas do software (1), aumentando o ritmo de desenvolvimento de toda a indústria de software, que trouxe como resultado inevitável o aumento do número de deployment de software desenvolvido de forma a satisfazer a necessidade de entregas ao cliente. Estas metodologias provocaram também a mudança de paradigma de sistemas monolíticos para a utilização de microsserviços, pois o crescente ritmo de desenvolvimento tornou a gestão de sistemas monolíticos insustentável, sendo vantajosa a utilização de microsserviços pela sua manutenção, reusabilidade, escalabilidade e disponibilidade ser facilitada (2). No entanto, o potencial dos microsserviços é maximizado aquando da utilização de um sistema de orquestração que permita a simplificação e gestão dos deployment’s, especialmente em sistemas de alta complexidade (3) que necessitem de manter a alta disponibilidade, gerir a sua escalabilidade e reagir rapidamente a falhas (4). Assim, esta dissertação pretende explorar diversas estratégias de deployment na tecnologia Kubernetes (5), com o objetivo de verificar quais os seus impactos nos casos de estudo utilizados no que diz\nrespeito à qualidade de software e prevenção de erros do software entregue, sejam eles, erros de disponibilidade durante e/ou após o deployment ou erros reportados pela monitorização posterior dos containers (6)."
  },
  {
    "keywords": [
      "681.3.02",
      "681.586"
    ],
    "titulo": "Plataforma integradora de serviços em tempo real baseada em tecnologia OSGi",
    "autor": "Brandão, João Pedro Almeida",
    "data": "2013",
    "abstract": "Este projeto tem o objetivo principal de melhorar a qualidade de vida de pessoas idosas, com\ndeficiências físicas ou psicológicas. Quando ficam em casa sozinhas são privadas de uma vida\nautónoma e ativa o que leva à necessidade de uma monitorização continua. Tendo em consideração\na crescente disponibilidade de dispositivos interativos num ambiente doméstico abre-se\numa porta para necessidade de sistemas de integração e fusão de sensores. Uma plataforma\ninteligente de monitorização capaz de comunicar com os vários dispositivos prova a sua utilidade\nao ser capaz de tornar as pessoas mais autónomas proporcionando aos familiares e amigos\nmecanismos de monitorização ajustados conforme o perfil do utilizador em caso de ausência.\nPretende-se desenvolver um sistema que possa ser implementado num dispositivo central que\nestabeleça a comunicação entre os diferentes dispositivos e sistemas electrónicos, seja capaz de\nintegrar serviços em tempo real e registe o seu estado em determinado momento. Tem o intuito\nde aproveitar todos os aparelhos e serviços que as pessoas já possuem, evitando assim um gasto\nmonetário exagerado tendo em conta o estado económico existente e a capacidade monetária\ndos utilizadores. Estes sistemas electrónicos ou dispositivos podem ser, por exemplo, um sistema\nde ar condicionado, capaz de adequar a temperatura à preferível pelo utilizador, ou um sistema\nde iluminação com capacidade de regular a intensidade da luz e assim reduzir nas despesas.\nEstes sistemas implicam uma melhoria em termos de qualidade de vida do utilizador ao providenciar\nautomatismos inteligentes no seu dia-a-dia."
  },
  {
    "keywords": [
      "Smart home",
      "Machine Learning",
      "Automação residencial",
      "Deep Learning",
      "IoT",
      "HAR",
      "Home automation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Estudo da aplicabilidade de ML no âmbito de smart homes",
    "autor": "Rosa, Alexandre da Silva",
    "data": "2023-11-29",
    "abstract": "As técnicas de aprendizagem automática são amplamente empregadas em Smart Homes, em combinação com dispositivos IoT. A combinação entre ambas as tecnologias permite a coleta e análise de dados\nde sensores e outros dispositivos para a aprendizagem e automatização de tarefas domésticas, como regulação de temperatura, controle de iluminação e segurança. Algumas técnicas comuns de ML aplicadas\nincluem aprendizagem por reforço, aprendizagem supervisionada e aprendizagem não supervisionada.\nEssas tecnologias permitem que os dispositivos IoT aprendam com o comportamento do utilizador e melhorem as suas capacidades de automatizar tarefas, tornando a vida dos utilizadores mais conveniente e\neficiente.\nA presente dissertação tem como objetivo principal a análise das principais técnicas de ML utilizadas\nno âmbito das Smart Homes. Neste sentido, é feito um levantamento sobre a automação residencial\nantes da popularização das técnicas de ML. Analisam-se as técnicas do período anterior ao atual, quando\nnão havia um poder computacional que permitisse a implementação de técnicas de ML em ambientes\nresidenciais. Além disso, é abordada a relação entre as técnicas de ML e a criação de ambientes domésticos mais sustentáveis, onde a racionalização de recursos se torna uma realidade e a sua implementação\nnão causa alterações significativas na qualidade de vida dos utilizadores deste tipo de residência. Por\nfim, é apresentada a implementação de uma prova de conceito relacionada com o reconhecimento de\natividades humanas em ambiente doméstico."
  },
  {
    "keywords": [
      "Medical image segmentation",
      "Brain tumor",
      "Deep learning",
      "U-Net",
      "Tiramisu",
      "Loss function",
      "Segmentação de imagens médicas",
      "Tumor cerebral",
      "Aprendizagem profunda",
      "Função de perda",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Semantic segmentation of medical images with deep learning",
    "autor": "Tabrizi, Mohammad Reza",
    "data": "2023-08-03",
    "abstract": "The use of deep learning techniques in medical image analysis has been a subject of growing\ninterest in recent years. One of the most important applications of these techniques is the\ndetection and segmentation of tumors in histological images. This dissertation focused on\ninvestigating the use of deep learning models to segment tumors, with the aim of providing\nmedical specialists with a tool that can help them make more precise diagnoses.\nTumor growth patterns are an important histological characteristic that can provide\ninformation about the aggressiveness and degree of malignancy of a tumor. Specifically,\nthe epithelial-mesenchymal transition on the tumor front is a pattern that has been shown\nto confer high aggressiveness and a great capacity to invade tissues and cause metastases,\nleading to a poor prognosis regarding the evolution of the tumor. Therefore, detecting\nand segmenting tumors in histological images can be a critical step in the diagnosis and\ntreatment of tumors.\nThe research process involved several steps, including preprocessing the images to prepare\nthem for deep learning models. This step involved developing methods to enhance the\nquality of the images and make them suitable for training deep learning models. Two types\nof deep learning architectures, the U-Net and Tiramisu, were trained in a supervised way,\nand different types of loss functions were experimented with to measure their efficiency in\ncontrolling the training process. Additionally, different types of hyperparameters were tried,\nand the best value was chosen for each hyperparameter.\nFinally, the effectiveness of the models was evaluated and compared both qualitatively\nand quantitatively based on their performance in image segmentation. The results obtained\nshow that deep learning models surpassed the initially predicted values and reached a value\nabove 94% based on the training data. for the Interception over the Union metric. This result\ndemonstrates the potential of deep learning techniques to detect and segment tumors in\nhistological images and reinforces the importance of continuing to investigate this topic. The\nbest results of the present work were achieved with total loss, as explained on page 89."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Analysis and visualization of dynamic social networks",
    "autor": "Caldas, Jorge",
    "data": "2017",
    "abstract": "This document represents the study developed under the master’s thesis Analysis of Visualization\nof Social Networks, that overlaps two main scientific fields, sociology (more\nconcisely social networks) and computer science, aiming at the design and implementation\nof a system for social network analysis.\nNowadays we face an age of massive Internet usage, with Online Social Networks we\npractically live this parallel reality where everything we do and everyone we met is exposed\nand shared through these online ”worlds”. Today, being able to study and understand\nhow information flows and how relationships are built within these online networks\nis of paramount importance for various reasons, these can be social, educational, political\nor economical. This master work studied sociology, social network analysis, and computer\nscience to employ the researched material aiming at building a tool that allows users to\nexplore their social structure in order to derive sophisticated conclusions, that wouldn’t\nnormally come up when they are browsing through their online feeds, because we provide\nto the end user a personalized, macroscopic and objective perspective of their social\nnetwork."
  },
  {
    "keywords": [
      "Sistemas in memory data grid",
      "Software open-source",
      "Sistemas de transações distribuídas",
      "Avaliação de soluções IMDG",
      "In memory data grid systems",
      "Open-source software",
      "Distributed transactions systems",
      "IMDG solutions evaluation",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Estudo de alternativas open source para soluções IMDG",
    "autor": "Gomes,  Hugo André Esteves",
    "data": "2016",
    "abstract": "Conseguir satisfazer os clientes em mercados altamente competitivos depende diretamente da\nqualidade e desempenho das aplicações que lhes são direcionadas. Alguns segundos de atraso\npodem fazer a diferença entre o sucesso e o fracasso de uma empresa. A incapacidade de\nprocessar, aceder, analisar e integrar dados rapidamente num dado sistema é bastante\nproblemática para organizações que têm de processar uma grande quantidade e variedade de\ndados. Os sistemas In Memory Data Grids (IMDG) operam essencialmente com os seus dados em\nmemória, podendo, porém, ser suportados por vários servidores incorporados num sistema\ndistribuído. Estes sistemas são recomendados para aplicações que exijam a manipulação de\ngrandes volumes de dados, uma vez que são facilmente escaláveis e de fácil implementação. Além\ndisso, em termos técnicos, os sistemas IMDG são claramente vantajosos em processos que\nrequeiram rápidas tomadas de decisão, exijam elevados níveis de produtividade e solicitem um\natendimento de alta qualidade aos seus sistemas e utilizadores clientes. Neste trabalho de\ndissertação foram estudas, de forma detalhada, várias alternativas IMDG open source existentes\nna atualidade, tendo como base de trabalho um conjunto de condições funcionais e estruturais\ndefinidas por uma empresa de telecomunicações, com o objetivo de viabilizar a utilização de uma\nsolução IMDG open source em substituição de uma solução dita comercial. Adicionalmente,\nidealizou-se um pequeno conjunto de casos de estudo que foram utilizados como base para o\nprocesso de criação de duas aplicações práticas reais utilizando duas soluções IMDG open source\ndistintas, nomeadamente, o Hazelcast e o Infinispan. No processo de elaboração destes casos de\nestudo tomou-se em consideração alguns cenários de aplicação bastante típicos em sistemas de\ntelecomunicações, bem como, nas fases de implementação das aplicações, as funcionalidades mais\nrelevantes que se podem encontrar em sistemas distribuídos deste género, em particular a\nexecução local de dados em ambiente distribuído, a afinidade de dados em casos de\nparticionamento, a capacidade de replicação de cache em cenários topológicos com mais de um\ncluster e, por fim, a integração de Java Persistence API (JPA) e Java Transaction API (JTA) como\nmecanismos para controlo e gestão de persistência e das transações distribuídas."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Relatório de atividade profissional",
    "autor": "Marques, Pedro Miguel Lorga Monteiro",
    "data": "2016",
    "abstract": "O presente documento constitui o relatório de atividade profissional com vista à obtenção do grau de Mestre ao abrigo do despacho RT-38/2011, regulamentado pela circular EEUM-CC-02/2012. Tendo realizado a minha licenciatura na Universidade do Minho em Engenharia de Sistemas e Informática com especialização em de Comunicações e Redes, tendo trabalhando e complementada a formação desde então na referida área, qualificam-me para requerer a equivalência ao Mestrado em Engenharia de Redes e Serviços Telemáticos.\nO presente documento irá centrar-se nas funções desempenhadas durante os anos de trabalho na FCCN (Fundação para a Computação Científica Nacional) enquanto engenheiro de redes e de gestor da respetiva área. Um primeiro capítulo introdutório descreve brevemente o meu percurso profissional assim como o académico, que servirá de enquadramento para o restante relatório.\nTal como especificado na circular, a parte seguinte do relatório descreve algumas atividades desenvolvidas no âmbito do trabalho, sendo enquadradas dentro de um contexto científico num tema em que a FCCN foi pioneira – a implementação do IPv6 na rede académica. Serão ainda apresentados alguns outros pequenos projetos desenvolvidos como ilustradores do que é a área da engenharia de redes.\nO terceiro capítulo descreve um grande projeto em que fui o principal responsável pelo seu desenvolvimento e implementação. Trata-se da criação de uma rede de fibra ótica trazendo o estado da arte para a rede académica portuguesa. Foi um processo longo e complexo que englobou diversas áreas da engenharia, requerendo conhecimentos profundos de telecomunicações, controlo financeiro, gestão de projetos e domínio de aspetos jurídicos associados às obrigações legais decorrentes do financiamento.\nNo âmbito do relatório é ainda solicitada a apresentação de eventuais trabalhos de natureza científica. Neste aspeto há a ressaltar que, durante o último ano em que trabalhei na FCCN, integrei um projeto de investigação europeu – Joint Research Activity. A temática era Future Transport Networks e visava estudar os mais recentes desenvolvimentos na área das arquiteturas de rede. A componente em que trabalhei foi na Evolution Beyond 100G, nomeadamente no estudo das diversas codificações para débitos de 100G e superiores.\nNo capítulo final é apresentada a conclusão e nos apêndices são indicados estágios e ações de formação realizados ao longo dos anos."
  },
  {
    "keywords": [
      "61:681.3",
      "681.3:61",
      "616.12-073.97",
      "Ciências Médicas::Biotecnologia Médica"
    ],
    "titulo": "Monitorização de ECG de pacientes em mobilidade",
    "autor": "Valente, João Henrique Ribeiro",
    "data": "2014",
    "abstract": "Devido à actual conjunctura económica e social, é necessário adoptar\nmedidas ao nível das instituições de saúde que visem uma redução\nde custos. Estas medidas devem fazer-se acompanhar de uma\npolítica de qualidade na prestação de cuidados de saúde. Tem de\nse verificar o cumprimento dos requisitos necessários para um tratamento\nadequado dos pacientes e uma diminuição dos erros cometidos\npelos profissionais de saúde.\nDeslocações regulares às Unidades de Saúde traduzem-se num maior\ncongestionamento dos serviços das várias unidades, conduzindo a\numa menor Qualidade do Serviço prestado ao paciente.\nOs últimos desenvolvimentos nos campos da tecnologia de sensores\nbiomédicos e dispositivos móveis, combinados com maiores capacidades\ndas comunicações sem fios, têm tornado possível o surgimento\nde novos paradigmas de monitorização de saúde. Este tipo de\nparadigmas visa uma melhoria da qualidade de vida dos pacientes e\numa diminuição da afluência às Unidades de Saúde.\nAs funcionalidades cada vez maiores dos dispositivos móveis como\nos Personal Digital Assistants (PDAs) e os smartphones fazem deles um\ncomponente vital em sistemas de monitorização de saúde.\nNeste trabalho, define-se uma arquitectura de um sistema de monitorização\nremota do sinal de Electrocardiograma (ECG) de pacientes\nem mobilidade.\nPara este projecto recorre-se ao BITalino como plataforma de aquisição\nde sinais vitais.\nA utilização das tecnologias no contexto da saúde implica o cumprimento\nde elevados padrões de segurança.\nDeve ser possível a transmissão de informação dos vários utentes\nsem que esta esteja sujeita a possíveis ataques informáticos (ataques\ndo tipo man in the middle e DOS (Denial of Service) ou DDOS (Distributed\nDenial of Service) ao servidor)."
  },
  {
    "keywords": [
      "e-Portfolios",
      "Microservices",
      "WebAssembly",
      "Rust",
      "Web engineering",
      "Microsserviços",
      "Engenharia Web",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of an e-portfolio social network using emerging web technologies",
    "autor": "Martins, Paulo Jorge Pereira",
    "data": "2022-03-17",
    "abstract": "Digital portfolios (also known as e-Portfolios) can be described as digital collections of artifacts, being both a\nproduct (a digital collection of artifacts) and a process (reflecting on those artifacts and what they represent). It\nis an extension of the traditional Curriculum Vitae, which tells the educational and professional milestones of\nsomeone, while the portfolio proves and qualifies them (e.g.: annually thousands of students finish a Master\ndegree on Informatics, but only one has built Vue, Twitter or Facebook – the Portfolio goes beyond the CV\nmilestones by specifying the person’s output throughout life and distinguishing them). e-Portfolios augment this\nby introducing new digital representations and workflows, exposed to a community, being both a product and\na process. This approach can be useful for individual self-reflection, education or even job markets, where\ncompanies seek talented individuals, because it expands the traditional CV concept and empowers individual\nmerit. There have been many studies, theories, and methodologies related with e-Portfolios, but transpositions\nto web applications have been unsuccessful, untuitive and too complex (in opposition to the CV format, which\nhad success in various applications, for example LinkedIn).\nThis project aims to study new approaches and develop an exploratory web/mobile application of this method ology, by exploring the potential of social networks to promote them, augmented by emergent web technologies.\nIts main output is the prototype of a new product (a social network of e-Portfolio) and its design decisions, with\nnew theoretical approaches applied to web development. By the end of this project, we will have idealized a web\ninfrastructure for interacting with networks of users, their skills, and communities seeking them.\nThe approach to the development of this platform will be to integrate emerging technologies like WebAssembly\nand Rust in its development cycle and document our findings. At the end of this project, in addition to the\nprototype of a new product, we hope to have contributed to the State of the Art of Web Engineering and to be\nable to answer questions regarding new emerging web development ecosystems."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Real-time MIDI device data analysis and context aware music generation",
    "autor": "Ribeiro, Miguel Angelo Ferreira Dias",
    "data": "2017",
    "abstract": "Computer music generation is an active research field encompassing a wide range of approaches.\nAs we involve more and more technology in our creative endeavors, it becomes\nvital that we provide our systems with the capability to understand and represent art concepts\ninternally, as well as understand and predict artistic intent and emotion. In respect to\nmusic, being able to work with this information opens up fantastic possibilities for artistmachine\nsynergetic collaborations as well as machine creativity endeavors. In this spirit,\nthis dissertation explores a system capable of analyzing in real-time a performance piece\nplayed on a MIDI (Musical Instrument Digital Interface) capable instrument and produce a\nmusically coherent piece of accompaniment music. This system comprises of two major subsystems:\none responsible for analyzing and extracting features from the live performance\nand one that takes these features and generates MIDI tracks to be played in conjunction\nwith the live audio, in a way that blends in with the performance."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Sistema de recomendação de rotas em cidades inteligentes",
    "autor": "Fernandes, Stéphane Alexandre Alves",
    "data": "2018",
    "abstract": "Hoje em dia as zonas urbanas sofrem de sobrepopulação, por isso, é essencial que os seus\nrecursos sejam geridos da melhor forma com o intuito de proporcionar uma melhor qualidade\nde vida aos seus habitantes bem como, um desenvolvimento sustentável. Neste\ncontexto, surge o conceito de Cidades Inteligentes que é focado no uso de novas tecnologias\nquer ao nível dos transportes, da saúde ou mesmo relacionadas com o meio ambiente,\ndando a oportunidade de melhorar a qualidade de vida dos habitantes das zonas urbanas.\nCom este trabalho, pretende-se desenvolver um sistema de recomendação de rotas para\nauxiliar cidadãos com base numa função objetivo específica, como a definição de um percurso\nde atividade desportiva. Um protótipo do sistema foi construído e testado usando\numa ferramenta de simulação chamada CupCarbon. Assim, como caso de estudo, considerase\numa zona urbana onde é definida uma rede de sensores sem fios para monitorizar parâmetros\nambientais como luminosidade ou cobertura Wi-Fi, com o intuito de auxiliar o\nplaneamento de rotas de acordo com os parâmetros pretendidos pelo utilizador."
  },
  {
    "keywords": [
      "SAP",
      "ITIL",
      "ERP",
      "IT",
      "681.3:658.0",
      "658.0:681.3",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Gestão de serviços de tecnologias de informação num ERP: caso prático SAP",
    "autor": "Lisboa, Nuno Jorge Rebelo Teixeira",
    "data": "2014-12-18",
    "abstract": "Este trabalho pretende analisar, em simultâneo, um projeto de implementação de um software\nde gestão de serviços de tecnologias de informação desde o seu início á sua conclusão numa\nempresa, assim como a própria aplicabilidade do software de gestão escolhido SAP ITSM (IT\nService Management).\nPretende deixar um contributo à forma e conteúdo da aplicação SAP ITSM e lançar as bases na\nempresa, em que o estudo se insere, para aproximação dos serviços prestados às boas práticas\nITIL, com a certificação ISO /IEC 20000 no horizonte.\nPara cumprir esse objetivo, exploram-se os conceitos e a literatura adjacente ao tema,\nidentificando os principais problemas e preocupações a ter em consideração na aplicação das\nboas práticas ITIL.\nMetodologicamente segue-se o estudo de caso, beneficiando da presença do autor no seio do\ntrabalho da equipa de implementação, levando a cabo, para além da observação, a que se junta\ndocumentação para análise.\nPor fim apresentam-se os resultados, discutindo o seu significado e concluindo quanto ao\nproblema de investigação identificado."
  },
  {
    "keywords": [
      "Ontologia",
      "Genealogia",
      "Preservação",
      "Sistema de ficheiros",
      "DSL",
      "Python",
      "Linux",
      "Jinja2",
      "Ontology",
      "Genealogy",
      "Preservation",
      "File-system",
      "Domain specific languages",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Caderno de antepassados em suporte sistema de ficheiros",
    "autor": "Oliveira, Duarte Manuel Vilar de",
    "data": "2024-01-19",
    "abstract": "O Ancestors Notebook é uma ferramenta de apoio à gestão e organização de documentos e informações\nsobre a história e herança familiar. Tem como intuito oferecer diferentes potencialidades que facilitem todo\no processo de registo e construção de um legado relativamente a uma ou mais genealogias específicas.\nO Ancestors Notebook funciona sobre o file-system Linux utilizando um conjunto de convenções,\ncomandos e Domain Specific Languages (DSLs) para nomear e organizar diretorias e com um foco\nespecial em documentos com um formato específico - DGU - criados especialmente este toolkit. O seu\npropósito é trazer um controlo organizacional personalizado ao utilizador, contribuindo assim para um\nfluxo coerente de ideias sempre correlacionado com o aglomerar de dados de cariz genealógico.\nA organização dos dados passa pela definição de entidades representativas de vários elementos, para\naglutinar distintos formatos num só, de maneira a ter um maior segmento organizacional no sistema de\nficheiros. Definiu-se, também, a geração de templates genéricos para uma visualização mais agradável\ne familiar, exportável em formato PDF, denominados Caderno de Antepassados. Estes consideram uma\nfuncionalidade de agregação e organização de documentos por entidades, contribuindo assim para um\nmaior leque de alternativas na definição dinâmica de opções de visualização.\nO Ancestors Notebook toolkit dispõe de um sistema de controlo de versões, cujo funcionamento está\ndependente de um sistema de representação de conhecimento sob forma de ontologia e um projection\neditor que permite visualizar e manipular a estrutura genealógica como está representada no sistema de\nficheiros.\nO toolkit é definido na linguagem de programação Python com definição de comandos disponíveis no\nsistema de ficheiros. Utilização de diferentes módulos Python para a definição de views para o utilizador.\nA criação de templates é feita usando o motor de geração de templatesJinja2. O toolkit é definido como\npackage instalável através do pip."
  },
  {
    "keywords": [
      "Machine Learning",
      "Dementia classification",
      "Magnetic resonance imaging",
      "Radiomics",
      "Brain morphological features",
      "Neurodegenerative diseases",
      "ADNI",
      "Classificação de demências",
      "Imagens ressonância magnética",
      "Radiomics",
      "Features morfológicas cerebrais",
      "Doenças neurodegenerativas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Classification of dementias based on brain radiomics features",
    "autor": "Carvalho, Sofia Manuela Gomes de",
    "data": "2021-12-02",
    "abstract": "Neurodegenerative diseases impair the functioning of the brain and are characterized by alterations in \nthe morphology of specific brain regions. Some of the main disorders include Alzheimer's, Parkinson's, \nand Huntington's diseases, and the number of cases increases exponentially since ageing is one of the \nmain risk factors. Trying to identify the areas in which this type of disease appears is something that \ncan have a very positive impact in this area of Medicine and can guarantee a more appropriate \ntreatment or allow the improvement of the quality of life of patients. With the current technological \nadvances, computer tools are capable of performing a structural or functional analysis of neuroimaging \ndata from Magnetic Resonance Images(MRI). Therefore, Medical Informatics uses these techniques to \ncreate and manage medical neuroimaging data to improve the diagnosis and management of these \npatients. MRI is the image type used in the analysis of the brain area and points to a promising and \nreliable diagnostic tool since it allows high-quality images in various planes or strategies and MRI \nmethods are fundamental diagnostic tools in clinical practice, allowing the diagnosis of pathologic \nprocesses such as stroke or brain tumours. However, structural MRI has limitations for the diagnosis of \nneurodegenerative disorders since it mainly identifies atrophy of brain regions.\nCurrently, there is increased interest in informatics applications capable of monitoring and quantifying \nhuman brain imaging alterations, with potential for neurodegenerative disorders diagnosis and \nmonitoring. One of these applications is Radiomics, which corresponds to a methodolog ythat allows the \nextraction of features from images of a given region of the brain. Specific quantitative metrics from MRI \nare acquired by this tool, and they correspond to a set of features, including texture, shape, among \nothers. To standardize Radiomics application, specific libraries have been proposed to be used by the \nbioinformatics and biomedical communities, such as PyRadiomics, which corresponds to an open source Python package for extracting Radiomics of MRIs.\nTherefore, this dissertation was developed based on magnetic resonance images and the study of Deep\nLearning (DL) techniques to assist researchers and neuroradiologists in the diagnosis and prediction of \nneurodegenerative disease development. Two different main tasks were made: first, a segmentation, \nusing FreeSurfer, of different regions of the brain and then, a model was build from radiomic features \nextracted from each part of the brain and interpreted for knowledge extraction."
  },
  {
    "keywords": [
      "Linkage disequilibrium",
      "Single-nucleotide polymorphism",
      "Recombination",
      "Phylogenetics",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Estimating recombination frequency throughout the human genome using a phylogenetic-based method",
    "autor": "Silva, Raquel dos Santos",
    "data": "2016",
    "abstract": "Recombination rate is an essential parameter for most studies on human variation. Linkage\ndisequilibrium (LD) measures the association between two variants in the same chromosome.\nWhen a new variant arises by mutation in a germinal line, that variant will be in\ncomplete linkage with the variants in the chromosomic background where it arises. Recombination\nthrough time (occurring during meiosis) will decrease the association decreasing\nthe LD. Understanding how recombination occurs throughout the genome is the basis to\ninterpret various association studies (search from causal variants for a given disease) and\ncharacterization of selective events. In this project the aim is to establish a novel methodology\nto estimate rate of recombination along a chromosome using a phylogenetic method.\nFor this to be done, each chromosome will be divided into small overlapping windows of\nvariation containing 20/30 variants. For each of these windows a phylogenetic network will\nbe calculated using the reduced-median algorithm. Highly recombining regions will show\na higher rate of cycles or reticulations in the network. A linkage map will be constructed for\neach chromosome using this novel methodology, compare the results with methods already\navailable, locate region of low recombination of possible use for phylogenetic analysis and\nalso explore some properties of the method for evaluation of selection."
  },
  {
    "keywords": [
      "Object tracking",
      "Self-driving vehicles",
      "LiDAR",
      "Deep learning",
      "Tracking de objetos 3D",
      "Condução autonóma"
    ],
    "titulo": "LiDAR based 3D object tracking for autonomous driving",
    "autor": "Figueiredo, André Sousa",
    "data": "2022-12-13",
    "abstract": "Technology has become essential for society’s every-day-life and with the recent increase in artificial intelligence’s interest, this area has gained more and more relevance for both people (e.g., due to the increasing number of users of personal assistants, such as Siri, Alexa and Google Assistant) and service providers (e.g., Google search engine and social networks’ recommendation algorithms to keep users busy and active on their platforms - Facebook, Youtube, TikTok, etc.). Nevertheless, artificial intelligence has been applied to many other areas, such as targeted advertising to specific users, cybersecurity, medicine, and the automobile industry. Although artificial intelligence has not been the perfect solution in the aforementioned applications, it has been responsible for several significant improvements in the last decade. For example, in the automobile industry, there are more and more companies offering solutions for autonomous vehicles, being Tesla the most notorious. This evolution was driven by several factors, including need and interest in improving road safety, growing traffic problems that exist due to the increase of vehicles circulating, more reliable sensors, and recent advances in various areas of artificial intelligence, such as object detection, semantic segmentation, and object tracking. These three areas are interconnected. However, they have different purposes - the first two (detection and segmentation) more related to static frame analysis (e.g., image based analysis), while object tracking is usually applied in dynamic environments (e.g., sequence of frames, such as a video) where its input is processed in order to track objects over time, allowing an intelligent system to be “aware” of its environment. That said, this dissertation aims to study and explore the applicability and feasibility, as well as to develop and implement an object tracker in the context of autonomous driving. Furthermore, it is also intended to make a benchmark with state-of-the-art approaches and identify their main limitations. The input data will be focused on Light Detection and Ranging (LiDAR) based 3D point cloud, as there are several datasets available, in particular KITTI [1], which, in addition to being widely used in the state-of-the-art, has also achieved positive results, even in real-time execution situations. However, these solutions usually require a lot of computational resources and, which can be a hurdle for its application in real-life settings."
  },
  {
    "keywords": [
      "616-079",
      "616.8",
      "61:681.3",
      "681.3:61"
    ],
    "titulo": "Comparação de análises estruturais (Volumetria, Espessura Cortical, e Voxel based Morphometry) em Neuroimagem",
    "autor": "Magalhães, André Nogueira",
    "data": "2013",
    "abstract": "Os estudos relacionados com a neuroimagem têm assumido nos últimos anos grande\nimportância por parte da comunidade médica e científica. O aumento da esperança\nmédia de vida faz com que se registem cada vez mais doenças do cérebro, das quais\nas demências e as doenças degenerativas têm assumido especial importância. Na\ntentativa de perceber quais as alterações anatómicas registadas no cérebro com a\nidade e aquando do surgimento destas patologias começaram a ser realizados estudos\nestruturais a este onde a Ressonância Magnética tem-se demonstrado como principal\nferramenta para este estudo.\nAtualmente existem diversas técnicas que possibilitam o estudo estrutural e\nanatómico do cérebro todavia ainda não existe nenhuma técnica que possibilite o\nestudo integral de todas as características estruturais do cérebro; no entanto a\nmedição da espessura cortical, volumetria e morfometria baseada em vóxel têm\nassumido especial preponderância no estudo destas características.\nO objetivo principal do presente trabalho consistiu em efetuar uma análise por regiões\ne por vóxeis de forma a perceber quais as regiões cerebrais que eram mais afetadas\ncom a idade no estudo da volumetria, espessura cortical e da área, para isto foram\nutilizados um método convencional, e o GLMfit para a análise por regiões e o QDEC\ne o SPM8 para o estudo por vóxeis.\nPara se poder efetuar os estudos referidos anteriormente foi necessário pré-processar\ntodos os dados em estudo através da utilização da aplicação Freesurfer que\npossibilitou a correção de pequenos erros originados durante a aquisição das imagens.\nCom esta dissertação conclui-se que os métodos utilizados para a deteção do\ncomportamento das variáveis em estudo nas análises por regiões se demonstram\ncoerentes entre si e entre os dados bibliográficos consultados, todavia na análise por\nvóxeis as conclusões não foram tão lineares sendo mesmo impossível efetuar uma\ncomparação entre esses métodos, pois os resultados obtidos foram totalmente distintos."
  },
  {
    "keywords": [
      "CLAV",
      "DGLAB",
      "AE",
      "Workflow",
      "LC",
      "TS",
      "RADA",
      "PGD",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "CLAV - processamento dos autos de eliminação",
    "autor": "Santos, Filipa Alves dos",
    "data": "2022-12-19",
    "abstract": "O CLAV é um projeto nacional que tem como objetivo a classificação e avaliação de todos\nos documentos da administração pública portuguesa, com base em normas e orientações\ndelineadas pela Direção Geral do Livro, dos Arquivos e das Bibliotecas (DGLAB). Esta iniciativa\nsurgiu como solução à grande perda de informação causada pelo o uso de papel como\nmétodo principal de conservação, bem como à incapacidade deste método de processar a\nenorme quantidade de informação digital produzida atualmente.\nUma parte essencial para o bom funcionamento da plataforma inclui a eliminação documental de informação arquivística, que permite uma gestão mais eficiente, assim como uma\nredução de custos de armazenamentos, melhorando a eficiência e eficácia na recuperação de\ninformação.\nCom isto, para proceder à eliminação de um documento, é necessário que as entidades\nque o desejem fazer enviem através da plataforma do Classificação e Avaliação de Processos da\nAdministração Pública (CLAV) um pedido de autorização para essa eliminação. Este pedido\ntem o nome de Auto de Eliminação (AE). Para ser autorizado, terá de ser validado e analisado\ntendo em conta prazos de conservação administrativa, destinos finais, quem são os donos\ndo processo a quem respeita a documentação, etc.\nNeste sentido, esta dissertação tem como desígnio a continuação do desenvolvimento do\ntratamento dos AE, a serem incluídos no CLAV, e do seu armazenamento, para que seja\npossível garantir transparência da ação administrativa, bem como da capacidade do Estado\nno cumprimento da sua missão (Lourenço and Gomes, 2019)."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "GreenSource: repository tailored for green software analysis",
    "autor": "Rua, Rui António Ramada",
    "data": "2018",
    "abstract": "Both energy consumption analysis and energy-aware development have gained the attention\nof both developers and researchers over the past years. The interest is more notorious\ndue to the proliferation of mobile devices, where energy is a key concern.\nThere is a gap identified in terms of tools and information to detect and identify anomalous\nenergy consumption in Android applications. A large part of the existing tools are\nbased on external hardware (costly solutions in terms of setup-time), through predictive\nmodels (requiring previous hardware calibration) or static code analysis methods. We could\nnot identify so far a tool capable of monitor all relevant system resources and components\nthat an application uses and appoint its energy consumption, while being easily integrated\nwith the application and/or with its development environment. Due to the lack of a tool\ncapable of gathering all this information, a natural consequence is the lack of information\nabout the energy consumption of applications and factors that can influence it.\nThis dissertation aims to carry out a study on the energy consumption of applications and\nmobile devices in the Android platform, having developed in this scope the GreenSource\ninfrastructure, a repository containing the source code, representative metadata and metrics\nrelatively to a large number of applications (and respective execution in physical devices).\nIn order to gather the results, an auxiliary tool has been developed to automatize the\nprocess of testing and collect the respective results for each one of the applications. This tool\nis a software-based solution, allowing to obtain results in terms of consumption through\nexecutions made directly on a physical device running the Android platform.\nThe developed framework, the AnaDroid, has the capability to perform static and dynamic\nanalysis of an application, being able to monitor power consumption and usage of\nresources for each application through tests execution. This is done following a whitebox\ntesting approach, in order to test applications at source code level. It invokes calls to\nthe TrepnLib library at strategic locations of the application code (through instrumentation\ntechniques) to gain control over relevant portions of the source code, like methods and unit\ntests. In this way the programmer can have results about the use, state and consumption of\nresources such as energy, CPU, GPU, memory, sensor usage and complexity of developed\ntest cases.\nThe information gathered through the use of the AnaDroid over a large set of applications\nwas stored in GreenSource backend. With the collected results, we expect to be able to\ncharacterize and classify applications, as well the tests developed for it. It is intended that\nthis will be made publicly available and serve as a reference for future works and studies."
  },
  {
    "keywords": [
      "614:681.3",
      "681.3:614",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Auditoria aos sistemas de informação hospitalares",
    "autor": "Alpuim, Ana Isabel Castro da Guia",
    "data": "2014",
    "abstract": "Ao longo dos anos, tem aumentando o número de Tecnologias da Informação e Comunicação(TIC) em todas as organizações e também nas organizações de saúde, como é o caso do Registo de Saúde Eletrónico (RSE). Este\nsistema integra dados provenientes de diferentes fontes, reunindo registos clínicos dos utentes, informação que suporta a decisão clínica, repositórios de\ndados, aplicações de gestão hospitalar. Permite ainda o acesso a aplicações\npara a realização de processos operacionais, como é o caso da prescrição\neletrónica de exames e de medicamentos.\nAs organizações de saúde, têm como principal objetivo, uma boa prestação de cuidados de saúde aos utentes, garantindo a prestação de serviços de\nqualidade. Para isto, a tomada de decisão tem que ser rápida e e caz. Assim,\no conceito de Business Intelligence (BI) vem contribuir para a melhoria da\nprestação de cuidados de saúde na medida em que se baseia na organização\ne análise de dados, de forma a disponibilizar informação útil para formar\nconhecimento, o que consequentemente, facilita a tomada de decisão.\nO objetivo deste projeto é estudar e analisar, através de ferramentas de\nBI, o tempo de prescrição de Meios Complementares de Diagnóstico e Terapêutica (MCDT). Tal estudo é feito através da criação de uma plataforma de\nauditoria, com a ferramenta de BI Pentaho Community que, de forma simples\ne atrativa mostra informação relativa ao tempo que os pro ssionais de saúde\nlevam a preencher os formulários de prescrição. Esta auditoria é necessária,\npara obter informação útil, formar conhecimento sobre a prescrição e sobre\nos médicos e enfermeiros, do Centro Hospitalar do Porto (CHP) e implementar\nmudanças. Este sistema permite avaliar os tempos de prescrição, tanto\npor médico bem como por especialidade, com o intuito de, posteriormente,\nmelhorar todo o processo de Prescrição Eletrónica Médica (PEM).\nVeri cou-se que é a especialidade de radiologia que executa mais pedidos\ne são as tarefas de gastrenterologia que, em média demoram mais tempo a\nserem executadas."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desenvolvimento de ferramentas para análise de sentimentos em redes sociais: a rede social Twitter",
    "autor": "Brito, Luís",
    "data": "2018-12-20",
    "abstract": "Social networks have allowed, over the past few years, the appearance of new ways to share\nopinions and ideas in texts, providing a basis for studying opinions on a large scale. The tools for the retrieval and analysis of sentiments, contained in this information, are still under\ndevelopment, limited by access restrictions and technical difficulties in the development of\nnew methods, which involve natural language processing and text mining. This work aims\nto develop tools to recover and analyze sentiments present in social networking texts.\nA case study using Twitter will be used for validation. During this process, data obtained from this social network was stored in a document oriented database, Elasticsearch,\norganized by topics helping its use in the following phases. Then, the data went through\na set of pre-processing steps, to maximize the value of their content, seeking to improve\nthe chances of obtaining a correct classification. Finally, the processed textual data were\nsubmitted to the algorithm chosen for classification, Naive Bayes. The results obtained over\ntwo different datasets show that the pre-treatment of data is very important regarding to\nthe classification of sentiments in texts.\nOverall, a computational architecture has been developed that can foster sentiment analysis applications over social network data from Twitter"
  },
  {
    "keywords": [
      "Optimization",
      "Inventory routing problem",
      "Approximate methods",
      "Exacts methods",
      "Metaheuristics",
      "Otimização",
      "Métodos aproximados",
      "Métodos exatos",
      "Metaheurísticas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Optimization of the inventory routing problem with Artificial Intelligence",
    "autor": "Silva, João Miguel Pimenta da",
    "data": "2024-01-08",
    "abstract": "Nowadays the success of a business is dependent on the ability to effectively integrate in an intricate\nnetwork of entities that are connected by material and information flows, inventory management being\none of the main concerns. These flows are characterized by decision-making processes that will vary\ndepending on the environment, entities and business models in the network. So, these networks need\na decision-making system capable of providing solutions that dictate the optimal way the network and its\nentities provide and collect inventory in order to reduce costs and maximize profit. In the context of this\ndissertation, the problem arises when there is a stock disruption in the network and outside entities can no\nlonger answer the stores’ supply requests and these stores become the entities responsible for requesting and delivering products to each other. This problem is modeled as an Inventory Routing problem,\nsince it encompasses inventory management and routing decisions. The main goal of the system can be\ndescribed as maximizing the collection of products per travel distance, without causing stock-outs at any\nsupplier, for the entire network. The problem at hand is an optimization problem.\nIn order to solve this optimization problem, first, the structural characteristics and key aspects were identified and studied, followed by the mathematical conceptualization, which involved the definition of the\nobjective function and the corresponding set of constraints. The mathematical formulation allows the\nproblem to be translated into a specific and precise mathematical language, making it possible to evaluate\nsolutions, by means of a fitness function, and apply optimization algorithms to solve the problem. These\noptimization algorithms can be approximate or exact methods and their suitability to the problem depends\non many factors such as the size, structure and complexity of the problem. So, the choice of the optimization algorithms must be preceded by a careful analysis of the problem at hand and its characteristics.\nAfter this, in the implementation phase, two adaptations of the genetic algorithm, two adaptions of the\nsimulated annealing algorithm, two adaptations of the tabu search algorithm were developed. Additionaly,\nanother algorithm responsible for generating reference solutions was also developed (Dynamic2). In order to test and compare the developed optimization algorithms, three different sized scenarios were generated.\nEach of these scenarios has a different amount of data associated with it, whether it be in the number of\nstores, types of products or number of requests. As to compare the results of the different instances of\nthe algorithms fairly in each scenario, these were made to generate roughly the same number of solutions.\nIn scenarios 1 and 3, all the optimization algorithms developed were successful in finding solutions with\nhigher fitness values than the baseline Dynamic2 solution. In scenario 2, due to time constraints and\ncomputational complexities, only the Genetic algorithm and the Genetic algorithm with Elitism using an\ninitial population consisting of solutions generated by the Dynamic2 algorithm, managed to find solutions\nwith higher fitness value than the baseline solution. In this scenario, the developed optimization algorithms\nwere also tested using feasible solutions generated through random mechanisms as initial solutions. These\ninstances also achieve solutions with improved fitness values when compared to their respective initial solutions or populations.\nFurthermore, in scenarios 1 and 3, the Genetic algorithm with an initial population consisting of feasible\nsolutions generated through random mechanisms was the optimization algorithm that found the best solutions, with these solutions having fitness values 56.14% and 92.07% greater than the baseline Dynamic2\nsolution’s, respectively. In scenario 2, the Genetic algorithm with Elitism, utilizing an initial population consisting of solutions generated by the mentioned Dynamic2 algorithm, found the solution with the highest\nfitness value, being approximately 1.00% higher than the baseline solution."
  },
  {
    "keywords": [
      "Sensor fusion",
      "Impact detection",
      "Machine learning",
      "Signal processing",
      "Fusão sensorial",
      "Deteção de impactos",
      "Processamento de sinal",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sensor fusion for detection and classification of vehicle impacts",
    "autor": "Santos, João Gabriel Lopes dos",
    "data": "2022-04-21",
    "abstract": "This thesis was developed as part of a curricular internship at Bosch Car Multimédia SA, in collaboration with the University of Minho, More specifically, an exploratory research thesis aligned with an R&D project that is being developed internally and whose objective is to detect impacts on vehicles that cause damage based on data obtained through sensors, The usefulness of the work developed in this thesis and the project in which it is inserted, in a real context, would be to help vehicle rental companies and car-sharing services to better monitor the conditions of vehicles in their fleets, This would be achieved by placing a device in vehicles that continuously monitored their status, reducing the need for validation and human interaction after use, The main focus of this thesis was to explore how the fusion of information from different sensors could improve the decision-rnaking capabilities of a system whose purpose is to determine whether impacts on the exterior of a vehicle, captured with a set of sensors, resulted in damage, This conjugation of sensory information is known as sensor fusion. ft is a process of combining information from different homogeneous and heterogeneous sensors to obtain a better representation of what is being observed, The approach chosen to achieve this goal consisted of training a set of Machine Learning (ML) algorithms with two distinct datasets, one based only on one data source and the other multiple sources combined. Each pair of models was further evaluated on unseen data, and their performances were compared based on the va lues obtained, Based on the results obtained, it can be said that the application of sensor fusion allowed for better learning by the models, which led to greater robustness in data never seen before. Of the four chosen algorithms, XGBoost, Random Forest (RF), Support Vector Machine (SVM), and Artificial Neural Network (ANN), all had at least one of the evaluation metrics, the Matthews Correlation Coefficient (MCC) and number of False Positive (FP)s in the test set, superior in model-based fused data. Of these, XGBoost and ANN stand out where the results were significantly better in both metrics,"
  },
  {
    "keywords": [
      "Blockchain",
      "Smart contracts",
      "Iot",
      "Security",
      "Compliance",
      "Contratos inteligentes",
      "Segurança"
    ],
    "titulo": "A secure IoT Communication system for smart contracts",
    "autor": "Leite, Nuno André Lopes",
    "data": "2021-03-09",
    "abstract": "The need to ensure the confidentiality and integrity of data generated in industrial systems\nand applications has been increasingly highlighted over the years, due to the clear and\nurgent requirements of not disclosing sensible proprietary information and ensuring that\ndata is kept immutable since it is generated until it is permanently stored.\nIt is from these two main ideas that this dissertation is created, framed in a project that is\nbeing developed at the Digital Transformation CoLab with Bilanciai and Cachapuz. These are the\nindustrial partners and key stakeholders of this project, having identified the requirements\nfor the weight measurement process that occurs in the weighing stations that are placed in\ntheir customers. This dissertation essentially consists on the definition of a secure Internet of\nThings (IoT) communication system between the devices that operate on the weighing stations\nof the customers and on top of that, develop a smart contract application using blockchain\ntechnology capable of: i) automating the process of verifying the correct application of\nweighing guidelines; and ii) registering and storing ”receipts” of weighings that take place\nin the customers’ weighing stations.\nIn this dissertation, a revision of the state of the art is made with the goal to perceive\nthe most secure and current technologies capable of providing the required functionalities,\nwhich are the fuel for the identification of the problems and challenges that such a project\nmight face, ultimately leading to the design of a solution that can both: i) mitigate the\naforementioned problems and challenges; and ii) comply with the goals defined for the\ndissertation. Additionally, in this document, the development of such a solution is also\nexplored by providing clear insights into the decisions that were made and the reasoning\nbehind them and by implementing components that are able to provide registration and rich\nquerying of weighing tickets (receipts), weighing ticket building and secure communication\nas well as the enforcing of a blockchain network structure that fosters data confidentiality.\nUltimately, results are shown, collected from a proof of concept, which essentially provide\nevidence on the functional correctness of the system that was built, i.e., its ability to grant\nthe retainment of weighing ticket characteristics and the capabilities of the communication\nsystem, which demonstrates to be able to securely build and transmit weighing tickets, with\nfault tolerance.\nThe outcomes of this project can be integrated into existing systems of the industrial\npartners to increase efficiency, security and business innovation."
  },
  {
    "keywords": [
      "Distributed consensus",
      "Raft algorithm",
      "Gossip",
      "Acordo distribuído",
      "Raft",
      "Propagação epidémica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Driftwood: decentralized Raft consensus",
    "autor": "Gonçalves, André Silva",
    "data": "2024-01-08",
    "abstract": "The Raft consensus algorithm allows multiple state machine replicas to behave as a single and robust system, capable of tolerating various faults, offering more resilience when compared to a monolithic system. Raft is known for its ease of understanding and practical implementation, due to its utilization of strong leadership and division into three relatively independent sub-problems: log replication, leader election and safety. The combination of its simplicity, strong consistency, and fault tolerance guarantees in data replication has solidified Raft as one of the most popular algorithms since its creation a decade ago. It has been widely adopted in production by various systems, including Etcd, CockroachDB, MongoDB, and Neo4j. The leader in a Raft cluster has the active role of replicating its log entries on a process-by-process basis. In contrast, the other processes play a passive role, waiting for the leader’s requests and responding accordingly. This approach limits Raft in terms of scalability and performance: the larger number\nof processes in the system, the higher the leader’s replication workload, causing the leader to become the system bottleneck. Therefore, the overall performance of a Raft cluster is heavily dependent on the efficiency of the leader process. In terms of fault tolerance, a Raft cluster with 2f+1 processes can tolerate f processes to crash, requiring only a majority of correct processes to ensure availability. Furthermore, Raft allows for messages to be delayed or lost and can tolerate total network partitions. However, it assumes that messages are eventually delivered, as the leader needs to maintain constant communication with all processes to retain its leadership status. If a process does not receive any communication from the leader within a specified timeout period, it will assume that the leader failed and initiate an election. Therefore, Raft needs a transitive network to operate efficiently. If there are processes that do not receive messages from the leader,\nit increases the likelihood of elections being triggered constantly, leading to a loss of system performance. A consequence of such an occurrence was the outage experienced by Cloudflare on November 2, 2020, which lasted for six and a half hours [30].\nIn this thesis, we present Driftwood, a novel algorithm that expands Raft by incorporating gossip mechanisms to decentralize the leader replication effort. Gossip protocols allow a process to deliver a message to all processes, even in non-transitive networks, with a high probability. The Driftwood’s leader, instead of sending its log entries one-by-one, will initiate gossip rounds with a message containing uncommitted entries so that the processes propagate these entries among themselves and replicate the leader’s log. Furthermore, this message serves as the leader’s heartbeat when first received, avoiding unnecessary elections in non-transitive networks. However, the leader still has to receive replication confirmations from the processes in order to commit entries. So, we also propose new replicated data structures, shared in the gossip rounds, to discover new committed log entries. Driftwood is experimentally evaluated and compared with Raft. To this end, we implemented three algorithms in the Paxi framework: Raft, which serves as the baseline comparison, and two versions of Driftwood, one that uses the new replicated data structures while the other doesn’t. Through our evaluation, we determine the differences in performance, scalability, and distributed resource usage between Raft and Driftwood. We also analyse the behaviour of these algorithms in simulated non-transitive network scenarios."
  },
  {
    "keywords": [
      "Data analysis",
      "Data visualisation",
      "Knowledge exploration",
      "Health care",
      "Análise de dados",
      "Visualização de dados",
      "Exploração de conhecimento",
      "Cuidados de saúde"
    ],
    "titulo": "Biometric analysis of behaviours in serious games",
    "autor": "Pinho, Rafaela Pinto de",
    "data": "2022-04-05",
    "abstract": "This document is a Master’s Dissertation, within the scope of data analysis for visualisation\nand exploration of knowledge in the health area.\nThe thesis herein described is part of the second year of the Master’s in Informatics\nEngineering, and it was held at the University of Minho in Braga, Portugal.\nThe main objective of this project is to combine the data measuring the performance of a\npatient when he is playing a specific game with the data collected at the same time from\nsome biometric sensors so that it is possible to apply data analysis algorithms in order to\ndiscover new relationships between the data. It is also intended that, through adequate\nvisualisation, knowledge can be created. Data collection was carried out in partnership with\nthe Centro Neurosensorial de Braga.\nFor data collection, initially, it was used an emotion recognition activity, a test that\nmeasures the processing speed (quick naming) and two tests to better characterise the child\nin terms of memory and attention capacity.\nInitially, a small analysis was made of the data that were extracted through the platform\nprovided by the Centro Neurosensorial de Braga. After a mass data collection, its analysis was\ncarried out.\nIt was possible to verify, analytically, that as the memory deficit increases, among others,\nthe number of fixations, the number of regressions, the time taken to perform the rapid\nnaming test increases. Regarding the emotions expressed during the rapid naming test, it\nwas possible to verify that respondents who expressed happiness during the test show a\nbetter memory capacity, while children who expressed emotions of surprise or sadness are\nsubject to memory deficit."
  },
  {
    "keywords": [
      "Alloy",
      "Category theory",
      "Quantitative formal methods",
      "Relational algebra",
      "Typed linear algebra",
      "Álgebra relacional",
      "Métodos formais quantitativos",
      "Teoria das categorias",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Towards a quantitative alloy",
    "autor": "Silva, Pedro Faria Durães da",
    "data": "2021-07-26",
    "abstract": "When one comes across a new problem that needs to be solved, by abstracting from its associated details\nin a simple and concise way through the use of formal methods, one is able to better understand the matter\nat hand. Alloy (Jackson, 2012), a declarative specification language based on relational logic, is an example\nof an effective modelling tool, allowing high-level specification of potentially very complex systems. However,\nalong with the irrelevant information, measurable data of the system is often lost in the abstraction as well,\nmaking it not as adequate for certain situations.\nThe Alloy Analyzer represents the relations under analysis by Boolean matrices. By extending this type of\nstructure to:\n• numeric matrices, over N0\n, one is able to work with multirelations, i.e. relations whose arcs are\nweighted; each tuple is thus associated with a natural number, which allows reasoning in a similar\nfashion as in optimization problems and integer programming techniques;\n• left-Stochastic matrices, one is able to model faulty behaviour and other forms of quantitative\ninformation about software systems in a probabilistic way; in particular, this introduces the notion of\na probabilistic contract in software design.\nSuch an increase in Alloy’s capabilities strengthens its position in the area of formal methods for software\ndesign, in particular towards becoming a quantitative formal method.\nThis dissertation explores the motivation and importance behind quantitative analysis by studying and\nestablishing theoretical foundations through categorial approaches to accomplish such reasoning in Alloy.\nThis starts by reviewing the required tools to support such groundwork and proceeds to the design and\nimplementation of such a quantitative Alloy extension.\nThis project aims to promote the evolution of quantitative formal methods by successfully achieving\nquantitative abstractions in Alloy, extending its support to these concepts and implementing them in the\nAlloy Analyzer."
  },
  {
    "keywords": [
      "Blockchain",
      "Consensus",
      "Optimization",
      "Machine learning",
      "Acordo distribuído",
      "Otimização",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Adaptive consensus for the blockchain",
    "autor": "Pereira, Ricardo António Gonçalves",
    "data": "2019-12-23",
    "abstract": "Consensus is essential to the Blockchain as it enables participants to share a consistent view\nof the underlying distributed ledger. Currently existing protocols either rely on Proof-of Work or similar economic incentive schemes, with high transaction latency but that can\nhandle thousands of participants or on classical byzantine fault tolerant consensus protocols, with low transaction latency but that do not scale well with the number of participants.\nIn this work, one goal is to look at classical consensus protocols and assess the impact that\nprotocol parameters can have on the behaviour of the system, considering different settings\n(e.g. network), scale (participants), load and trust assumptions, for example. Furthermore,\nwe propose an adaptive consensus protocol for the Blockchain, using an optimization mechanism that configures the protocol automatically."
  },
  {
    "keywords": [
      "Computação afetiva",
      "Sustentabilidade",
      "Inteligência ambiente",
      "Psicologia",
      "Affective computing",
      "Sustainability",
      "Ambient intelligence",
      "Psychology",
      "681.3",
      "159.942",
      "504.06"
    ],
    "titulo": "Social simulations of human behavior in virtual agents for sustainability management platforms",
    "autor": "Felgueiras, Gilberto Martins",
    "data": "2013",
    "abstract": "impacto negativo que o ser humano tem provocado no meio ambiente precisa de ser contrariado,\nassim, são necessárias soluções para atuar nesse sentido, soluções estas que precisam de assegurar a\nsustentabilidade entre questões ambientais, económicas, e sociais. Esta dissertação aborda este tópico no\ncontexto de sistemas de informação, mais concretamente, na interacção social de uma pessoa com o meio\nem que está inserida. A ideia surgiu para colmatar uma lacuna existente nas soluções computacionais para\ngestão de sustentabilidade, pois estas centravam o problema nas questões externas ao utilizador, descartando\no seu grau de conforto. Para isso, foi implementado um sistema de simulação de emoções humanas,\nbaseado nas características psicológicas dos utilizadores, com o propósito de integrar este sistema numa\nplataforma informática de apoio à sustentabilidade. Esta simulação pediu emprestado os conceitos de\nComputação Afectiva, área da informática que tem ganho importância nos últimos anos, e é responsável\npor fundir os trabalhos em psicologia e computação. Assim, o trabalho presente nesta tese apresenta\ndados sobre a relação psicologia, sustentabilidade, computação afectiva e como estes campos se podem\ninterligar. Foi desenvolvida uma plataforma utilizando tecnologia de agentes virtuais que recolhessem informações\nsobre o ambiente e procedessem ao cálculo de um estado emocional. Os resultados obtidos\ncom o trabalho desenvolvido foram animadores, e revelam que este tipo de simulação poderá ser uma\nvantagem, para que um sistema de suporte à sustentabilidade, baseado em inteligência ambiente, possa\ntomar decisões e atuar sobre um meio, tendo informação prévia ou uma hipótese do estado emocional dos\nseus utilizadores."
  },
  {
    "keywords": [
      "Post-pandemic",
      "Remote work",
      "Software development",
      "Questionnaire",
      "Portugal",
      "Pos-pandemia",
      "Trabalho remoto",
      "Desenvolvimento de software",
      "Questionário",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Remote work adoption among software development teams in Portugal after the COVID-19 pandemic: an empirical analysis",
    "autor": "Costa, Filipe Barbosa Soares da",
    "data": "2023-12-11",
    "abstract": "Remote working is not a new concept, having become a more viable option with the advent of personal\ncomputers and high-speed Internet connections. Even so, it is safe to say that the percentage of profession als remotely working reached unprecedented proportions during the COVID-19 pandemic. Consequently,\nfor many, this peculiar virus containment period meant their first contact with teleworking.\nHowever, the obligation to work from home eventually came to an end, meaning that employers and\nemployees regained the autonomy to decide together whether or not to invest in teleworking. Now, with a\nnotable difference: both, with a few exceptions, are already familiarized with teleworking, its advantages\nand challenges, and the team dynamics adapted to allow for virtual communication.\nIt is within this post-pandemic context that this dissertation provides a comprehensive view of the\nadoption of remote working among Portuguese software development teams. Therefore, it intends to\nstudy the current prevalence of teleworking, the challenges posed by the coexistence of remote and in office work, and how to make this symbiosis more effective and productive.\nTo attain this, 175 valid testimonials were collected through a questionnaire distributed between March\nand June of 2023. Analyzing the responses, it is possible to observe a significant migration from face-to face to remote work between the period before and after the pandemic. Avoiding daily commuting and\nhaving more time for family and leisure activities were some of the primary motivations for this migration.\nIt can be asserted that the coexistence of remote and face-to-face professionals induces a slight negative\nimpact on team dynamics. Lastly, and with the intention of optimizing the dynamics of teams that accept\nremote work, a set of recommendations is presented based on the participants’ testimonies."
  },
  {
    "keywords": [
      "BPMN",
      "Workflow",
      "CLAV",
      "Runtime",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "CLAV: especificação dos workflows (BPMN)",
    "autor": "Ramos, Luís Miguel Arieira",
    "data": "2022-12-13",
    "abstract": "A plataforma web CLAV (https://clav.dglab.gov.pt/) visa na disponibilização de\ninformação e ferramentas que facilitem e normalizem as práticas de classificação e avaliação\nde toda a documentação circulante na administração pública portuguesa. Um dos seus\nprincipais objetivos é a redução da utilização dos documentos físicos que armazenam\nessa informação, substituindo o uso dos mesmos pelo formato digital. Assim sendo, dá-se uma redução do uso do papel, significando uma menor perda de informação e uma\nmaior facilidade no tratamento dos processos, não havendo necessidade de transitar esses\ndocumentos físicos.\nÀ data de início desta dissertação, o CLAV já continha alguns workflows especificados\nem linguagem BPMN, sendo que estes ainda não eram automatizados. Nesta dissertação,\npretendeu-se dar continuidade ao trabalho anteriormente realizado no CLAV, ou seja,\npartindo das especificações BPMN já existentes, seria necessário revê-las e alterá-las, caso se\njustificasse, e acrescentar os workflows ainda em falta (Araújo, 2021).\nCom o principal objetivo de tornar o CLAV mais inteligente e automático, pretende-se investir na automatização dos workflows especificados anteriormente, de modo que as\ninterfaces fossem capazes de reagir a esses workflows e às mudanças que estes pudessem\nsofrer.\nPor último, existia a necessidade de acrescentar à plataforma CLAV a possibilidade\nda edição gráfica das especificações BPMN. Assim sendo, seria possível que o utilizador\nadministrador conseguisse melhorar e alterar os fluxos de trabalho, através da edição gráfica\ndos modelos BPMN na plataforma, com a possibilidade de, no futuro, executar essa edição\nem runtime."
  },
  {
    "keywords": [
      "Metabolic network",
      "Lactobacillus helveticus",
      "Metabolic Models Reconstruction using Genome-Scale Information (merlin)",
      "Computational biology",
      "Enzymes",
      "Transporters",
      "TRIAGE",
      "COBRA",
      "OptFlux",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Reconstructing the metabolic network of Lactobacillus helveticus on a genome-wide scale",
    "autor": "Dias, José Miguel Gonçalves",
    "data": "2017",
    "abstract": "The constant growth of high-throughput data generation and omics approaches require\ninformatics support and (semi) automated processes to be developed. With increasing number\nof sequenced genomes available, metabolic engineering processes will allow a rational alteration\nof the genetic architecture to achieve specific phenotypes. These alterations will allow\nto generate and optimize features of some organisms with economic and health interest.\nLactobacillus helveticus is an important industrial lactic-acid bacterium being used in\nthe production of several types of cheese. The metabolic activities of the bacterium contribute\nto the cheese flavour and reduce bitterness. Lb. helveticus is a growing body of literature on\nthe health-promoting properties of its various strains and generally accepted as probiotic for\nits anti-mutagenic, immunomodulatory and anti-diarrheal effects.\nThe aim of this project was to reconstruct a genome-scale metabolic network of Lb. helveticus\nCNRZ32, based on its genome sequence annotation as well as known biochemical and\nphysiological characteristics. The generated model contained 790 reactions, 894 metabolites\nand 1687 genes. The growth rate predicted by the model on sugar was comparable to the\nreported in literature.\nThis model provides the basis for a constraint-based mathematical model capable of\nsimulating the phenotype of the organism under different growth conditions and guiding indepth\nphysiological studies and hypothesis generation."
  },
  {
    "keywords": [
      "681.3",
      "613.98"
    ],
    "titulo": "Mobile intelligent sensoring system",
    "autor": "Ribeiro, Tiago José Fernandes de Sá de Araújo",
    "data": "2012-07-19",
    "abstract": "Tendencialmente as pessoas idosas têm problemas cognitivos e físicos que potencializam a queda. Estudos revelam que 30% das pessoas com idades à volta dos 65 anos cai pelo menos uma vez por ano, piorando o cenário para 50% a partir dos 80 anos de idade. Quando a assistência à pessoa que caiu tarda em chegar surge evidentemente o agravamento do estado de saúde e das condições de cura e recuperação. Recorrendo a técnicas dos Sistemas Inteligentes, neste trabalho pretende-se estudar e analisar formas computacionais de deteção da queda. Para que a deteção seja correcta é necessária a utilização de sensores, e software que interprete a informação proveniente dos mesmos. Será ainda necessário estudar diferentes abordagens na utilização dos dados dos sensores para optimizar a eficácia do sistema. Neste trabalho dá-se ênfase ao uso de dispositivos de fácil aquisição e uso, e a uma abordagem não invasiva. Especificamente, estuda-se a utilização dos acelerómetros de plataformas móveis Android para a aquisição de dados sobre os padrões de movimento dos utilizadores por forma a detectar quedas e minimizar o tempo decorrido entre a queda e a chegada de ajuda."
  },
  {
    "keywords": [
      "Deep learning",
      "Machine learning",
      "Rede neuronal",
      "Verificação formal",
      "Why3",
      "Formal verification",
      "Neural network"
    ],
    "titulo": "Formalization of deep learning techniques  with the Why3 proof platform",
    "autor": "Sousa, Márcio Alexandre Mota",
    "data": "2022-04-05",
    "abstract": "Machine Learning como um campo, parte integrante da área de Inteligência Artificial, tem crescido\nexponencialmente, principalmente nesta última década, onde passou de quase desconhecido pelo público\nem geral para a existência de carros autónomos e até robôs humanóides como o robô Sophia da Arábia\nSaudita. A maioria de nós agora lida com Inteligência Artificial todos os dias, em anúncios direcionados\npor exemplo, o que é agora a norma.\nDeep Learning, um ramo específico de Machine Learning de onde originaram as Redes Neuronais,\né vastamente utilizado no desenvolvimento de sistemas autónomos de alta complexidade. Alguns destes\nsistemas em particular podem ser classificados como sistemas críticos, o que traz a necessidade de\nfornecer alguma forma de garantia de que estes sistemas vão sempre funcionar como é suposto, uma vez\nque qualquer falha em sistemas desta categoria pode ter consequências graves. Isto naturalmente levanta\npreocupações relativas à segurança, levando a comunidade a procurar uma forma de obter tais garantias,\neventualmente levando-os aos métodos de Verificação Formal para atingir os níveis de confiabilidade\nnecessários para a adoção pública de tais sistemas.\nTem havido um interesse crescente quanto a este assunto, uma vez que as aplicações de Redes\nNeuronais estão em constante expansão, e muitas ferramentas de software já resultaram deste trabalho,\nsendo algumas dessas ferramentas analisadas nesta dissertação. Este estudo vem contribuir para esse\nesforço, e tem como objetivo principal a avaliação do Why3, a fim de compreender se esta ferramenta\npossui as características necessárias que lhe permitam juntar-se a estas ferramentas já existentes como\num novo meio de verificação da correção de Redes Neuronais. Para atingir este objetivo, primeiramente\ncriamos um proof-of-concept a fim de analisar se o Why3 fornece o suporte necessário para esta tarefa.\nEm seguida, damos um passo em frente e formalizamos uma Rede Neuronal à escala de uma aplicação\nreal no Why3, de onde tiraremos as nossas conclusões.\nDurante o trabalho sobre a formalização de Redes Neuronais, pretendemos também compilar um\nguia abrangente sobre Why3, desde as funcionalidades que oferece, até exemplos de como pode ser\naplicado explicados passo a passo, com o objetivo de oferecer uma base de conhecimento compreensiva\npara qualquer pessoa interessada em explorar o Why3, contribuindo ao mesmo tempo para a escassa\ndocumentação existente sobre o Why3."
  },
  {
    "keywords": [
      "Gestão de permissões",
      "Blockchain",
      "Hyperledger Fabric",
      "Privacidade",
      "Confidencialidade",
      "Permission management",
      "Privacy",
      "Confidentiality",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Gestão de permissões e acesso a dados para Hyperledger Fabric",
    "autor": "Parente, João Pedro Araújo",
    "data": "2022-12-21",
    "abstract": "A gestão de acesso e permissões afeta várias áreas como a da saúde, mais concretamente os registos de\nsaúde eletrónicos, que se espera que nos próximo anos cresça exponencialmente e alcancem um valor\nno mercado de $39.7 biliões no ano 2022. A utilização de blockchain aparece como uma solução para\nestes cenários onde existem diversos domínios em que os dados são potencialmente sensíveis, quer como\ndados pessoais, quer como dados que podem revelar segredos de negócio.\nAlgumas tecnologias, como o HyperLedger Fabric, já prometem resolver estes problemas, mas sempre com uma granularidade baixa, com bastantes limitações ao nível da definição de políticas de acesso\ne de transformações dos dados. No contexto de HyperLedger Fabric vamos implementar um mecanismo\nde gestão de permissões flexível, que além de diferenciar o acesso com base na identidade de quem\nfaz o pedido permite considerar um conjunto de atributos configurável. Adicionalmente, além de decisões binárias sobre o acesso, o sistema que implementamos permite implementar políticas que definem\ntransformações a ser aplicadas aos dados acedidos."
  },
  {
    "keywords": [
      "35:681.324",
      "681.324:35"
    ],
    "titulo": "Serviços + perto : prestação de serviços ao cidadão sobre dispositivos móveis",
    "autor": "Silva, António Pedro Cunha Martins da",
    "data": "2012",
    "abstract": "Numa era de grandes evoluções tecnológicas e de fácil acesso a todo o tipo de informação\na nível global, gerou-se uma oportunidade há muito idealizada para que fossem\nlibertados dados adquiridos pelos governos para uso livre, pela e para a comunidade\nem geral. No entanto, o acesso aos dados por parte de um cidadão por si só pode\nnão ter grande impacto social. Posto isto, a utilização livre dos dados transformandoos\nem informação, seja de forma aberta ou comercial, por parte de indivíduos ou equipas\nde desenvolvimento especializadas poderá fornecer ao cidadão todo um leque de serviços até então inexistentes. Assim, o principal objetivo deste trabalho passa por disponibilizar uma aplicação\nmóvel que se torne uma mais-valia para os cidadãos, auxiliando-os na resposta a necessidades\ninerentes aos serviços públicos e privados mais procurados.\nDe forma a concretizar o objetivo proposto, foi efetuado um levantamento do funcionamento\nreal da estrutura a representar. Posto isto, verificou-se a necessidade de\ncriação de um back-office para gestão e centralização da informação associada ao modelo\noriginado.\nComo resultado, foi criada uma aplicação para Android que servirá como primeira\nferramenta de medida dos benefícios que a solução pretendida poderá levar à vida dos\ncidadãos. Espera-se ainda que a aplicação sirva de incentivo à criação de soluções do\nmesmo âmbito.\nOs objetivos foram alcançados e brevemente os cidadãos poderão usar a aplicação\ncriada, assim como terão acesso a todo o código fonte para melhorias que possam ser\nacrescentadas ao sistema."
  },
  {
    "keywords": [
      "FPA",
      "AAB",
      "Atletismo",
      "Plataforma",
      "Live-results",
      "Gestão de competição",
      "Rankings",
      "Perfil de atleta",
      "Confirmações online",
      "Inscrições",
      "Athletics",
      "Platform",
      "Competition management",
      "Athlete profile",
      "Online confirmations",
      "Registration",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Desenvolvimento da plataforma “Portugal Athletics FPA”",
    "autor": "Pacheco, Alexandre de Freitas Ferreira",
    "data": "2022",
    "abstract": "Nesta dissertação propõe-se a realização de uma plataforma e ferramentas capaz de elevar informaticamente a modalidade em questão (atletismo). O objetivo desta dissertação é criar um ponto comum, capaz\nde coordenar diversas ferramentas existentes, com a inovação de aprimoramento de algumas destas.\nEste projeto encontra-se com o apoio da FPA (Federação Portuguesa de Atletismo), utilizando a AAB\n(Associação de Atletismo de Braga), como os primeiros utilizadores na fase de testes, extendendo gradualmente o número de intervenientes, até o sistema ser implementado a nível nacional.\nSerão desenvolvidas 2 ferramentas distintas:\n• Plataforma web\n• Aplicação para Concursos/Provas Fora da Pista\nA primeira ferramenta será uma plataforma web, que terá a função de ser o ponto de controlo, terá\nacesso à base dados que contêm os atletas federados na FPA. Esta deverá ser capaz de criar e tolerar\ntodas as necessidades para a gestão de competição, desde a possibilidade para os clubes inscreverem\nos seus atletas nas competições até a geração dos comunicados de resultados e estatísticas das competições. Numa fase final do projeto também deverá efetuar rankings dos melhores atletas por prova, assim\npossibilitando os atletas de visualizar o seu perfil, visualizando assim os seus resultados e melhores marcas.\nA segunda será uma aplicação desktop, com o objetivo de gerir os diversos concursos a ocorrer numa\nprova de pista ( Lançamentos, Saltos Verticais e Saltos Horizontais), assim como provas de Corta-mato e\nEstrada fora da pista. Esta aplicação também deverá ter funcionalidades específicas para que seja possível\na utilização simultânea de placares electrónicos no local da prova e esteja integrada para possibilitar liveresults com a primeira ferramenta."
  },
  {
    "keywords": [
      "681.3:614",
      "614:681.3",
      "613.63",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Plataforma de Business Intelligence para o estudo de infeção nosocomial",
    "autor": "Silva, Eva",
    "data": "2014",
    "abstract": "O controlo e a prevenção de infeções nosocomiais são essenciais para a\nredução de custos, bem como para a melhoria dos cuidados prestados numa\ninstituição de saúde. Por outro lado, o tratamento de dados que permitam\ncompreender, caracterizar e monitorizar as infeções possibilita um controlo\ne uma prevenção mais eficaz das mesmas. Sendo um método automatizado\ne eficiente para o tratamento de dados, a tecnologia de Business Intelligence\npermite a extração de informação importante para gerar conhecimento que\npode auxiliar o processo de tomada de decisão dos profissionais de saúde.\nO principal objetivo deste trabalho é o desenvolvimento e implementação\nde uma plataforma de Business Intelligence que permita o estudo da incidência\nde infeção nosocomial nas Unidades de Medicina do Centro Hospitalar\ndo Porto. Este estudo é feito através da apresentação de um conjunto de\nindicadores clínicos (informações importantes extraídas dos dados referentes\na infeções nosocomiais) que ajudam a analisar e caracterizar estas infeções.\nPor conseguinte, depois de identificados os indicadores relevantes, torna-se\npertinente desenvolver um sistema que permita tratar os dados, extrair os\nindicadores destes e apresentá-los, de forma atrativa, na plataforma. Por\nsua vez, a plataforma facilita a análise das informações que disponibiliza,\napoiando a tomada de decisões, nomeadamente através da identificação dos\nprincipais fatores de risco. Assim, o sistema atua como um Sistema de Apoio\nà Decisão Clínica, podendo auxiliar no controlo e prevenção destas infeções.\nPretende-se ainda estudar a aplicabilidade da tecnologia de Data Mining\nna criação de modelos de classificação capazes de prever a ocorrência de\ninfeções nosocomiais, na presença de determinados fatores de risco.\nO conhecimento obtido com a análise dos indicadores e as previsões efetuadas\npode possibilitar a diminuição da incidência de infeção nosocomial\ne, consequentemente, a redução dos custos associados à sua ocorrência, bem\ncomo o aumento da segurança e do bem-estar dos doentes, ao permitir a tomada\nde decisões mais fundamentadas. A aplicação de Business Intelligence\nna área da saúde contribui para melhorar não só o  fluxo de trabalho diário\nnas unidades de saúde, como também a qualidade dos cuidados prestados."
  },
  {
    "keywords": [
      "663.15",
      "681.323"
    ],
    "titulo": "Desenvolvimento de ferramentas computacionais para a optimização de processos de fermentação em Biotecnologia",
    "autor": "Rocha, Orlando",
    "data": "2009-11-20",
    "abstract": "Actualmente, uma larga variedade de produtos tais como antibióticos,\nproteínas, vacinas e outros compostos químicos são produzidos através de processos\nfermentativos. Devido à subida dos preços do petróleo e aos fortes incentivos por\nparte das instituições para substituir os produtos derivados de petróleo por “produtos\nverdes”, muitos dos processos tradicionais têm vindo a ser substituídos por\nbioprocessos. Consequentemente, tem existido um esforço para melhorar a\nprodutividade dos processos biológicos. A optimização destes processos pode ser\nrealizada em duas etapas: primeiramente, faz-se uma selecção e uma melhoria\ngenética do microrganismo e num segundo passo são identificadas as melhores\ncondições para realizar o processo fermentativo. Nesta etapa, normalmente são\nrealizados estudos experimentais através de tentativa-erro para obter as condições\nambientais que propiciem o melhor crescimento e produtividade do microrganismo,\nmanipulando as concentrações iniciais dos nutrientes, os perfis de alimentação de\nsubstrato ao reactor, os modos de operação, bem como a temperatura e o pH.\nNos últimos anos, têm sido desenvolvidas várias ferramentas informáticas para\nsimulação e optimização de bioprocessos. Porém, a maioria destas ferramentas está\ndireccionada para estudar as vias metabólicas de um microrganismo de modo a\noptimizar a produtividade de determinado produto. Numa fase posterior, é efectuada\numa optimização genética do microrganismo. Apesar de existir uma grande variedade\nde ferramentas informáticas verifica-se que nenhuma delas está desenhada\nespecificamente para a optimização e simulação de processos fermentativos. Assim, o\nobjectivo deste trabalho foi desenvolver de raiz uma ferramenta direccionada para\nsimulação, optimização e estimação de parâmetros de processos fermentativos.\nA aplicação OptFerm foi desenvolvida sobre uma plataforma denominada\nAIBench, tendo-se utilizado a linguagem Java como linguagem de programação. O\nOptFerm foi então desenvolvido de modo a ser uma ferramenta de fácil uso, extensível\ne que pudesse funcionar em qualquer sistema operativo, estando disponível como\nsoftware livre em http://darwin.di.uminho.pt/optferm/. A aplicação foi desenhada de\nmodo a que o utilizador pudesse realizar várias tarefas de simulação, optimização e\nestimação de parâmetros com diferentes condições no que se refere a variáveis de\nestado, parâmetros, perfis de alimentação, etc.. As tarefas de optimização foram\nfocadas na determinação do melhor perfil de alimentação de uma corrente de\nsubstrato a alimentar ao reactor, dos melhores valores das variáveis de estado para\niniciar uma fermentação e do tempo óptimo de duração para uma fermentação.\nForam realizados alguns estudos de optimização e estimação de parâmetros\ncom o objectivo de verificar se a aplicação era suficientemente robusta. Os estudos\nforam baseados na repetição das experiências por 30 vezes para obter significância\nestatística. Após os estudos, verificou-se que as operações foram realizadas levando a\nresultados coerentes, não tendo sido detectados erros relevantes."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "A survival prediction model for colorectal cancer patients",
    "autor": "Silva, Ana Paula Pinto da",
    "data": "2016",
    "abstract": "The importance of making predictions in health is mainly linked to the decision-making\nprocess. Make survival predictions accurately is a very difficult task for healthcare professionals\nand a major concern for patients. On the one hand, it can help physicians decide\nbetween palliative care or other medical practice for a patient. On the other hand, the notion\nof remaining lifetime could help patients in the realization of dreams. However, the\nprediction of survivability is directly related to the experience of health professionals and\ntheir ability to memorize.\nMost decisions are made based on probability and statistics, but these are based on large\ngroups of people and may not be suitable to predict what will happen in particular cases.\nConsequently, the use of machine learning techniques have been explored in healthcare. Their\nability to help solve diagnostic and prognosis problems has been increasingly exploited.\nThe main contribution of this work is a prediction tool of survival of patients with cancer\nof the colon and/or rectum, after treatment and a few years after treatment. The characteristics\nthat distinguishes it is the balance between the number of required inputs and their\nperformance in terms of prediction. The tool is compatible with mobile devices, includes\na online learning component that allows for automatic recalculation and flexibly of the\nprediction models, by adding new cases.\nThe tool aims to facilitate the access of healthcare professionals for instruments that\nenrich their practice and improve their results. This increases the productivity of healthcare\nprofessionals, enabling them to make decisions faster and with a lower error rate."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Desenvolvimento e prospeção de ferramentas colaborativas nos cuidados de neonatologia e pediatria",
    "autor": "Martins, Bia Soraia da Silva",
    "data": "2017",
    "abstract": "É cada vez mais importante que os Sistemas de Informação Hospitalar garantam uma\nmelhoria na segurança e na qualidade dos cuidados médicos. Os pacientes neonatais e\npediátricos são mais vulneráveis que os pacientes adultos tornando essencial orientar as\nTecnologias de Informação para as suas necessidades.\nErros na administração de medicamentos são os erros mais comuns e potencialmente\nmais nocivos nas instalações hospitalares, sendo a sua taxa de incidência maior na população\npediátrica. Neste sentido, torna-se essencial melhorar a segurança do paciente. Para além\ndisto, é essencial ao profissional de saúde uma interligação da informação do paciente pelos\ndiferentes Sistemas de Informação que ele possui ao seu dispor.\nEsta dissertação tem como principal objetivo a finalização e implementação de uma plataforma\nde apoio à decisão médica através do desenvolvimento de diversas ferramentas, que\nauxiliem os médicos nas suas atividades diárias e que contribuam para a diminuição da\ntaxa de ocorrência de erro médico. Este desenvolvimento foi acompanhado por um médico\npediatra do Centro Hospitalar do Porto.\nO sistema desenvolvido permite colmatar falhas existentes nos sistemas utilizados atualmente\nem algumas unidades hospitalares e, deste modo, obter um sistema que permitisse\numa troca de informação e uma comunicação entre os serviços de pediatria e neonatologia\ne os serviços de farmácia. Assim, é possível facilitar o trabalho diários dos profissionais\nde saúde e, ainda, provocar uma diminuição nos efeitos adversos causados por erros de\nmedicação.\nSendo um processo acompanhado, a plataforma foi testada e melhorada ao longo do\ntempo de forma a obter um sistema final satisfatório. Por fim, é lançada uma avaliação aos\ntestes realizados na aplicação."
  },
  {
    "keywords": [
      "Microserviços",
      "Estudo empírico",
      "Padrões arquiteturais",
      "Arquitetura de software",
      "Microservices",
      "Empirical study",
      "Architectural patterns",
      "Software architectures",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Estudo empírico sobre parâmetros de qualidade na adoção de microsserviços",
    "autor": "Fernandes, Luís Filipe Silva",
    "data": "2023-12-28",
    "abstract": "O desenvolvimento de aplicações tem sido alvo de recentes alterações, procurando cada vez mais rapidamente entregar as aplicações aos clientes. Aliado a isto, a procura de uma integração mais eficaz com as várias equipas de desenvolvimento leva à procura por alternativas ao que era feito anteriormente. Indo um pouco ao encontro do que é pretendido, acaba por surgir a arquitetura utilizando microserviços, que apresenta várias vantagens, sendo que muitas vezes é apresentada como a alternativa perfeita à arquitetura monolítica. Por estas razões, em conjunção com a adoção desta arquitetura por grandes empresas, regista um grande crescimento e aceitação nos últimos anos, tanto no mercado, como em contextos académicos. A verdade é que com pesquisas profundas em vários artigos é possível verificar que esta arquitetura também apresenta vários inconvenientes, principalmente relacionados com a sua natureza distribuída, que muitas das vezes acabam por passar despercebidos devido às suas prometidas vantagens e por serem vagamente referidos na maioria dos trabalhos na comunidade cientifica. Este paradigma distribuído acaba por levantar todo um novo conjunto de desafios e associado com o facto de ser uma arquitetura recente, muitas das equipas de desenvolvimento não estão preparadas para fazer a sua implementação corretamente. Como resultado, as aplicações, tem dificuldades em cumprir os objetivos pretendidos. Para mitigar estas questões, começaram a ser desenvolvidos vários padrões para problemas bastante comuns, para a grande maioria das aplicações. Atualmente existem vários padrões para esta arquitetura já desenvolvidos, sendo que para cada problema, podem existir vários padrões desenvolvidos,\ncomo o problema de leitura de dados distribuídos em vários serviços. Cada um destes padrões tem as sociados vários compromissos e por esse motivo é importante identificar cada um, de modo a escolher o padrão que melhor se adeque à aplicação que se aspira desenvolver. Para a realização deste estudo empírico, foi definida uma aplicação referência que servirá como base. A seguir, são definidos vários casos de estudo onde são desenvolvidas as diversas aplicações com os padrões em questão. No final, foi feita uma comparação e uma análise dos compromissos relacionados com vários atributos de qualidade."
  },
  {
    "keywords": [
      "DNA",
      "DNA sequence classification",
      "Machine Learning",
      "Deep Learning",
      "Classificação de sequências de ADN"
    ],
    "titulo": "Development of DNA sequence classifiers based on deep learning",
    "autor": "Abreu, João Nuno Cardoso Gonçalves de",
    "data": "2022-12-19",
    "abstract": "Deoxyribonucleic acid (DNA) is a biological macromolecule whose primary function is to store an individual’s\ngenetic information. Because of breakthroughs in sequencing technology, the number of DNA sequences\nis now growing at an exponential rate. The assignment of a function to these sequences is a great obstacle\nin Bioinformatics, and current methods rely on homologies, a solution that is slow and less accurate.\nMachine learning (ML) has been widely employed as it is a relevant tool for processing huge amounts of\ndata by learning on its own without explicit programming. Using ML, it is now possible to speed up and\nautomatically classify DNA sequences into existing categories with the objective of learning their functions.\nHowever, building a machine learning classifier of biological sequences is a tough challenge due to the\nlack of numerical properties in the sequence that the model requires. Therefore, it is still necessary to apply\nsome pre-processing techniques so that the sequences are properly represented for the model. These\ntechniques include feature extraction and feature selection, and they are the most difficult components\nbecause sequences lack explicit features. Deep learning models have recently been developed that not\nonly extract features from input automatically, but also improve the prediction and classification of DNA\nsequences.\nThe main goal of this project is to create a tool that can automatically classify DNA sequences using\nmachine and deep learning models and algorithms, followed by its integration into ProPythia, a Python\npackage developed by the host group. Automated ML classifiers will also be developed to integrate in\nOmniumAI software platforms. Transcription factor annotation and essential gene determination will be\nused as case studies for the platform validation. With this study, it is intended to encourage the use of\nsuch technologies to develop new tools that can manage vast volumes of biological data, thus boosting\nDNA prediction understanding."
  },
  {
    "keywords": [
      "Certification",
      "General Data Protection Regulation",
      "Software",
      "Programming",
      "Data protection officer",
      "Certificação",
      "Regulamento Geral de Proteção de Dados",
      "Programação",
      "Responsável pela proteção de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Regulamento Geral de Protecção de Dados: uma plataforma de apoio à certificação",
    "autor": "Baptista, Sandra Clemente",
    "data": "2022-03-11",
    "abstract": "The number of personal data circulating through computer applications and web today is\nquite large, leading the European Parliament to propose and approve a regulation aimed at\nprotecting this data.\nThe General Data Protection Regulation (GDPR) is a new European legal framework that\ncame into force on May 25, 2018 that focuses on the protection, collection and management\nof personal data, i.e. data about individuals. This regulation applies to all companies and\norganizations in the European Union\nThis Master’s thesis in Computer Engineering focused on creating a solution that has as\nits main objective to facilitate the work of the people who are responsible for monitoring\nand ensuring that the regulation is being complied with.\nThis solution emerged in a work context from a sharing of ideas between me and the\nIdealMais entity, which proposed my integration in the architecture and development team\nof the solution.\nThe developed backend has as main features the management of measures, customers\nand compliance reports, i.e. reports intended to indicate the measures, their status and the\nactions that should be taken, serving as support for certification."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "A guide for functional magnetic resonance imaging",
    "autor": "Sousa, Alexandre Vieira de",
    "data": "2016",
    "abstract": "The brain is the most amazingly powerful and complex organ in the human\nbody. Constituted by approximately 86 billion of highly interconnected neurons, it\nallows us to have unique cognitive capabilities, such as language production and\ncomprehension, memory, judgement and problem solving, or even experience feelings.\nThe full understanding of the organization and functioning of the human brain is\nreceiving increasing attention but remains an exciting challenge for all neuroscientists.\nTo assist in this quest, fMRI stands as a safe and powerful non-invasive\nneuroimaging tool providing high visualization quality of the location of activity in the\nbrain resulting from, for example, sensory stimulation, cognitive or motor function, or\neven resting state fluctuations, being then widely used for mapping the human brain. It\nallows the study of how the healthy brain works, how it is affected by different diseases,\nhow it recovers from damage and how drugs can modulate activity or post-lesion\nrecovery. Starting purely as a research tool, fMRI was quickly adopted for clinical\npurposes and has now a growing role in clinical neuroimaging.\nConstantly gaining increased popularity among clinicians and researchers, fMRI\nis presently a promising tool for studying the brain function in living humans. However,\nit has a complex workflow that implicates knowledge of paradigm design, imaging\nartifacts, complex MRI protocol definition, a multitude of preprocessing and analysis\nmethods in several software packages, statistical analyzes, and in results interpretation.\nIn addition, fMRI data can be analyzed with a large quantity of commonly used tools,\nwith minor consensuses on how, when, or whether to apply each one.\nThis dissertation aims to compile a practical guide of crucial information and\nessential references to consider in setting up fMRI studies, optimizing data quality, and\ninterpreting results. All the major stages are covered with the aim to ultimately help the\nfMRI beginner researcher, clinician to consider and overcome the most significant\ndifficulties along the process and expand the use of this imaging technique. To validate\nthis guide two examples of fMRI studies were analyzed, with real data, obtaining results\naccording to similar studies literature."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Molecular dynamics simulation in hybrid systems",
    "autor": "Silva, João Tiago Araújo da",
    "data": "2016-03-20",
    "abstract": "The molecular dynamics simulation is a topic fairly investigated because it solves countless problems\nof physics, chemistry, or biology. From the computer engineering point of view it is an\ninteresting case study because it is a computationally complex problem. The complexity arises\nwhen there are a high number of particles, thereby resulting in a high number of iterations to\ncompute on each iteration. Presently there are systems with millions of particles that need to\nbe simulated in the shortest time possible. This led to the development of molecular dynamics\npackages that attempt to use all the resources available to improve the execution of simulations.\nThe main goal of this thesis is to run efficiently molecular dynamics simulations on hybrid systems.\nInstead of starting a molecular dynamics implementation from scratch, it was used the\nMOIL package. Then it was developed an implementation based on MOIL with optimizations\nthat allow the code to be automatically vectorized by the compiler. These optimizations focused\non the calculation of forces and the data structures. New data structures were introduced to decompose\nthe simulation domain into cells. The vectorization was used both in sequential and\nparallel implementations. In both cases, vectorization allowed a higher performance when used\nwith cells. In order to achieve the best possible performance, the optimized code has been parallelized\nusing different strategies, including shared memory, distributed memory, and a hybrid\nsolution. In the execution of the parallel code several combinations of processes and threads\nwere tested. Among all the developed versions, the one that achieved the best performance was\nthe hybrid version. All implementations were compared to Gromacs, the reference in terms of\nperformance of the molecular dynamics simulation."
  },
  {
    "keywords": [
      "681.324",
      "613"
    ],
    "titulo": "Ambient intelligence for monitoring weight and physical activity",
    "autor": "Ferreira, João Manuel Rodrigues",
    "data": "2013-10-09",
    "abstract": "We have an increasingly sedentary population without the concern about a healthy diet.\nTherefore, it becomes necessary to give the population the opportunity, despite living a\nvery busy and tiring life, to have control over important aspects of their health. This work\naims to analyze and evaluate the impact of an ambient intelligence system on weight control\nand physical activity in active individuals. To accomplish this objective we have developed\na mobile application that allows users to monitor their weight over a period of time,\nidentify the amount of food they consume and the amount of exercise they practice. University\nstudents will be invited and selected, in a first stage, to participate in this study. All\nof the students must be considered “active students”, according to our selection criteria.\nStudents with physical disabilities will be excluded from the study. This mobile application\ngives information to the users about dietary and physical activity guidelines in order to improve\ntheir lifestyles. It is expected that students improve their lifestyles."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Convergent types for shared memory",
    "autor": "Brandão, José Pedro Maia",
    "data": "2019",
    "abstract": "It is well-known that consistency in shared memory concurrent programming comes with\nthe price of degrading performance and scalability. Some of the existing solutions to this\nproblem end up with high-level complexity and are not programmer friendly.\nWe present a simple and well-defined approach to obtain relevant results for shared memory\nenvironments through relaxing synchronization. For that, we will look into Mergeable\nData Types, data structures analogous to Conflict-Free Replicated Data Types but designed to\nperform in shared memory.\nCRDTs were the first formal approach engaging a solid theoretical study about eventual\nconsistency on distributed systems, answering the CAP Theorem problem and providing\nhigh-availability. With CRDTs, updates are unsynchronized, and replicas eventually converge\nto a correct common state. However, CRDTs are not designed to perform in shared\nmemory. In large-scale distributed systems the merge cost is negligible when compared to\nnetwork mediated synchronization. Therefore, we have migrated the concept by developing\nthe already existent Mergeable Data Types through formally defining a programming\nmodel that we named Global-Local View. Furthermore, we have created a portfolio of MDTs\nand demonstrated that in the appropriated scenarios we can largely benefit from the model."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Ruby on Rails versus Java WEB: estudo comparativo arquitetural e metodológico",
    "autor": "Maia, Paulo Jorge Silva",
    "data": "2016",
    "abstract": "O desenvolvimento metodológico de aplicações Web multi-camada, satisfazendo o modelo\nMVC e contendo uma camada de persistência de dados suportada por bases de dados\nrelacionais, tem sido suportada fundamentalmente por duas grandes plataformas de desenvolvimento:\nJava Web e .NET. O aparecimento do open source Framework Ruby on\nRails, num momento em que a criação de aplicações Web é uma área fundamental da\nEngenharia de Software, obriga a uma análise detalhada das características desta terceira\nplataforma. Este trabalho tem por objetivo fundamental estudar de forma comparativa as\narquiteturas de software e tecnologias propostas por Ruby on Rails versus as tecnologias\nJava Web (i.e Apache Struts2) e, ainda, comparar as metodologias e tecnologias de suporte\nao desenvolvimento subjacentes a ambos os ambientes."
  },
  {
    "keywords": [
      "Vehicular networks",
      "5G system",
      "Wi-Fi",
      "Machine learning",
      "Throughput prediction",
      "Redes veiculares",
      "Sistemas de transporte inteligente",
      "Sistema 5G",
      "Aprendizagem automática",
      "Estimação de taxa de transmissão",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Opportunistic Wi-Fi network selection in heterogeneous vehicular wireless networks for detecting VRUs through edge computing",
    "autor": "Teixeira, Daniel Filipe da Rocha",
    "data": "2022-12-28",
    "abstract": "The future of vehicles is for them to become smarter: able to perceive the its surroundings, detect dangerous\nsituations, and act accordingly. To realize this vision, vehicles must collect information and share it with others,\nallowing them to have shared knowledge of an event that their sensors could not yet detect. However, some\nvehicles may not have enough computational resources to process the information and comply with low-delay\nrequirements, and may need to offload the data to edge computing platforms to extend their own processing\ncapabilities.\nWhile offloading, a vehicle may have multiple access networks at their disposal and by choosing the best\nnetwork it may maximize the amount of data it can offload. These access networks include the mobile 5G\nnetwork and Wi-Fi access points, which the operator can integrate within the 5G system to take advantage of\nunlicensed spectrum in the 2.4, 5 and 60 GHz ranges. In the current work, we focused on leveraging Wi-Fi and\nallowing vehicles to decide which access network to use among three Wi-Fi 802.11n/ac/ad networks.\nWe developed a Wi-Fi performance monitoring and decision-making system (WiPerf) that can: (i) collect\nthroughput measurements and channel state information for multiple Wi-Fi networks; (ii) estimate throughput\nusing passive measurements; (iii) predict the next 40 seconds of throughput; (iv) decide which network to use,\nbased on the throughput forecasts and on the time it takes to switch to another network. The system was\nimplemented in a real-world setup with two vehicles and two TP-Link Talon AD7200 access points. We performed\nan initial set of experiments to collect a dataset to develop estimation and forecasting models, and a second set\nof experiments to validate our decision-making system.\nFor throughput estimation, we developed the UKF-SR model, a novel approach that combines Symbolic Re gression with a non-linear recursive Bayes filter. Results showed that our solution was superior to NN, DT, and\nRF models, with these having higher RMSE values by at least 4.94 %, 38.09 %, and 9.59 % for 802.11n/ac/ad,\nrespectively. Considering forecasting, we adapted previously developed spatial-clustering models, that forecast\nthroughput based on a set of similar historic samples, and compared them with a time-series approach, us ing ARIMA and VAR models. VAR showed the best results among the time-series models, but they were still\noutclassed by spatial-clustering, considering both MAE and MASE values.\nWe integrated the estimation and forecasting models with a decision-making algorithm. The algorithm sched ules which networks the vehicle should use considering the time if takes to switch networks, to maximize the\namount of data it can offload. Compared with the optimal solution, based on the real throughput measurements\nand without forecasting (perfect prediction), the results show that our approach has near-optimal performance\nwith an average throughput of only 4.43 % less than the optimal one."
  },
  {
    "keywords": [
      "681.3",
      "613.98"
    ],
    "titulo": "iLeisure : an intelligent free time scheduler based on user profiling",
    "autor": "Marques, Vítor",
    "data": "2012-07-19",
    "abstract": "O envelhecimento populacional é uma realidade dos nossos tempos. O envelhecimento provoca muitas vezes o problema do sedentarismo, que é associado à obesidade e doenças cardiovasculares. O sedentarismo é por isso um problema preocupante que para além de afetar as idades avançadas afeta todas as outras faixas etárias. É nos tempos livres que as pessoas são mais sedentárias.\nA prática de atividades físicas combate o sedentarismo e é benéfica em termos psicológicos, sociais e económicos. É necessário incentivar as pessoas a praticá-la e uma das formas de o conseguir é através de um sistema que recomende tais atividades. A recomendação deverá contudo ter em consideração as preferências das pessoas pelas atividades, para que estas tendam a aceitar as atividades que são recomendadas. As preferências de uma pessoa são associadas ao conceito de perfil e é através deste, que um sistema de recomendação infere sobre que atividades recomendar. É por isso imperativo que o perfil esteja constantemente atualizado e consistente com a pessoa que representa. Garantir estas propriedades num perfil não é computacionalmente claro, devido às propriedades naturais de um perfil e devido à capacidade das pessoas em evoluir e variar nas suas preferências.\nEste trabalho tem como objetivo desenvolver um sistema que recomende às pessoas atividades que não sejam sedentárias e que incentivem o convívio social. Para que o sistema seja preciso nas atividades que recomenda, é também objetivo deste trabalho desenvolver um mecanismo para obtenção, representação e modulação de perfis que garanta a sua dinamicidade, que garanta que os perfis sejam eficientemente adaptáveis e atualizáveis.\nRealizaram-se observações acerca da taxa de aceitação de utilizadores, em relação às atividades recomendadas pelo sistema desenvolvido. Estas observações assistiram na avaliação da precisão do sistema em recomendar e a sua capacidade em acompanhar os utilizadores e aprender acerca das suas preferências. Consultando periodicamente a quantidade de atividades recomendadas aceites ou rejeitadas, verificou-se que o número de atividades aceites tende a aumentar e as rejeitadas a diminuir. Estes resultados demonstram que o sistema possui a capacidade de aprender e aperfeiçoar o perfil dos seus utilizadores, tornando-se mais preciso nas atividades que recomenda. Acredita-se desta forma que o mecanismo desenvolvido para a obtenção e modulação de perfis dinâmicos é eficiente e robusto."
  },
  {
    "keywords": [
      "Spreadsheet",
      "Model-Driven Engineering",
      "Embedded domain specific languages",
      "Bidirectional transformations",
      "Prototype",
      "681.3.06",
      "519.863"
    ],
    "titulo": "Evolution of model-driven spreadsheets",
    "autor": "Mendes, Jorge Cunha",
    "data": "2012-09-18",
    "abstract": "Spreadsheets are the most used programming environment, mostly because they are very flexible. This is due to the lack of restrictions imposed on them which can lead to lots of errors. A first approach to Model-Driven Engineering was already suggested to improve spreadsheets, providing them with specifications and checking tools. However, users have to learn how to use these tools on top of their existing spreadsheet host system.\n\nTo remove that difficulty, the work for this thesis describes an embedding of spreadsheet models within spreadsheet themselves. This embedding enables users to create models in the same environment that they use for spreadsheet development and that they are familiar with.\n\nMoreover, a set of operations that can be performed on these models and respective instances is defined. This way, users interact with models and spreadsheets in the same environment with the objective to improve work performance and reduce errors.\n\nResulting from this work, a prototype was created and is also discussed in this dissertation. This prototype can be used to validate the approach taken in this thesis and to provide a base framework for future developments."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desenvolvimento e optimização de back-end de compilador criptográfico para plataformas ARM",
    "autor": "Boas, Rui Pedro Araújo Vilas",
    "data": "2017",
    "abstract": "Devido à elevada proliferação tecnológica, existe software criptográfico implementado numa miríade de\nplataformas. Plataformas essas que podem ter características de computação bastante diferentes. No\nentanto, o software criptográfico deverá ser imperceptível ao utilizador final, uma vez que deve funcionar\ncomo uma camada de protecção às tarefas que o utilizador possa estar a realizar e não aumentar a sua\npegada computacional. Essa exigência leva muitas vezes a que o software criptográfico tenha que ser\nre-implementado de acordo com as capacidades computacionais da plataforma/dispositivo-alvo (sendo até\ncomum recorrer-se a assembly para atingir tal objectivo). O desenvolvimento de software criptográfico\ntambém exige que o programador seja versado em diversas áreas da ciência, devendo assim ser realizado\npor criptógrafos especializados.\nO CAO é uma DSL imperativa para a área da criptografia. Munido de um compilador, interpretador e\numa ferramenta de verificação formal, o CAO permite a passagem de conhecimento criptográfico para um\nprogramador com menos experiência na área da criptografia através da automatização da validação formal\ndas implementações. O compilador CAO possui um back-end altamente configurável que permite geração\nde executáveis dedicados às plataformas destino.\nCom a grande utilização actual de processadores ARM, torna-se interessante estudar formas de explorar\nas características destes processadores e desenvolver um back-end que as implemente com o objectivo de\nobter melhor desempenho do software criptográfico. Nesta dissertação explorou-se a utilização dos tipos\nde dados vectoriais e instruções do co-processador NEON de forma a obter paralelismo ao nível dos dados.\nTambém foi estudada a possibilidade de inclusão de paralelismo nativo ao nível das tarefas no CAO, de\nforma a tirar partido das arquitecturas multi-core."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Combined MRI with non-image clinical data for brain tumor classification: a CNN/DL approach",
    "autor": "Espanha, Raphael Alves",
    "data": "2017",
    "abstract": "Prognosis and patient stratification for brain tumors is an important and clinically relevant task and a precise treatment outcome prediction would allow to choose an adequate therapy strategy and schedule the most appropriate follow-up examinations. Magnetic Resonance Imaging (MRI) is an already know imaging technique to assess these tumors. Next to medical imaging, other clinical information is important for patient management, e.g. genetic markers like O6-Methyl-Guanine-Methyl-Transferase (MGMT) methylation is a well-known prognostic marker in Glioblastoma (GBM) tumors.\nTherefore, the main goal of this thesis was to study Deep Learning (DL) approaches to combine MRI with non-image clinical data in two different classification scenarios: brain tumor segmentation and patient outcome prediction. There are studies that combine these two types of data, however, in two steps: extracting MRI features and then combining them with relevant non-image data. Here, end-to-end DL architectures with two input layers are presented, as well as an infrastructure that allows the easy development of future Machine Learning (ML) /DL models that consumes these two types of data in a clinical context. In this way, the classification in both scenarios is done in a single step, where Convolution Layers perform the feature extraction in MRI input.\nIn brain tumor segmentation, the model with combined data achieved a slightly better Dice Similarity Coefficient (DSC) (0.894 ± 0.025) over image only model (0.882 ± 0.025). As for patient outcome prediction, when trying to predict the Progression-Free Survival (PFS) class (“bad”,” medium” and “good” outcomes), the combined model didn't improve when compared with the model where only MRI was used. Both models, however, outperformed models where only non-image data was used.\nThe segmentation results point to a positive influence when adding the clinical information to MRI. Nevertheless, there is a lot more to investigate in this field, not only in the model architecture, but also in selecting relevant clinical information. In same way, more tests should be run for patient outcome prediction, especially using Overall Survival (OS) information."
  },
  {
    "keywords": [
      "Domótica",
      "SNMP",
      "SNMPv2",
      "Agente SNMP",
      "MIB",
      "Light SNMP",
      "Light MIB",
      "Dispositivos domóticos",
      "Home automation",
      "SNMP Agent",
      "Home automation devices",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Arquitetura integradora com SNMP para gestão de edifícios",
    "autor": "Nogueira, Gonçalo Pinto",
    "data": "2023-12-28",
    "abstract": "Ao longo das últimas duas décadas a utilização de sistemas domóticos, num mundo cada vez mais\nconectado e tecnológico, tem vindo a revelar-se cada vez mais atrativo e com maior aceitação do público\nem geral. Novos produtos suportados por novos protocolos e tecnologias estão constantemente a ser\nintroduzidos no mercado. No entanto, este desenvolvimento foi quase sempre efetuado sem grande\npreocupação em definir regras e normas para que fosse possível a interoperacionalidade entre produtos\nde diferentes fabricantes, originando soluções pouco modulares, de elevado custo e forçando os clientes a\nescolher um ecossistema de um mesmo fabricante sem ser possível de forma rápida e facilitada integrar\ntecnologias de vários fabricantes num mesmo sistema.\nO principal objetivo desta dissertação foi a definição de uma arquitetura integrada para sistemas\ndomóticos baseada no protocolo de gestão SNMP e que permitisse ultrapassar algumas das mais importantes limitações das soluções atuais para este tipo de sistema. Nesse sentido foi criada uma nova\nMIB domótica para implementação num agente SNMP integrador. Além disso, foi desenvolvido um\nnovo protocolo de gestão para dispositivos domóticos, mais simples que o SNMP e mais adequado para\ngestão de pequenos equipamentos sensores ou atuadores utilizados em sistemas domóticos. Este protocolo, designado por Light SNMP (L-SNMP), será utilizado na comunicação entre o agente SNMP\ne os dispositivos domóticos que implementam uma Light MIB (L-MIB) de domótica. No decorrer do\nprojeto foi criado um sistema protótipo com dispositivos domóticos implementando a L-MIB de domótica,\num agente SNMPv2 implementando a MIB domótica e uma aplicação gestora SNMP que contém um\nsimples interface com o utilizador. As experiências realizadas com este protótipo permitiram confirmar a\ncorreção funcional da solução e a sua viabilidade como alternativa tecnológica válida, potencialmente de\nbaixo custo e com elevados níveis de interoperabilidade."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desempenho de uma aplicação web para câmaras ONVIF e notificação de eventos",
    "autor": "Varela, José Luís Cerqueira",
    "data": "2016-04-08",
    "abstract": "O recurso a sistemas de videovigilância tem-se tornado cada vez mais popular. No entanto,\ncada fabricante deste tipo de equipamentos desenvolvia os seus próprios protocolos de comunicação,\nnão existindo compatibilidade entre diversos sistemas de videovigilância. Este\ncenário era economicamente prejudicial para os consumidores, e dificultava o desenvolvimento\nde sistemas que integrem equipamentos diferentes ou de diferentes fabricantes. Foi\nentão necessário criar um protocolo comum a todos eles.\nNesse contexto surgiu o Open Network Video Interface Forum (ONVIF), uma organiza-\nção sem fins lucrativos composta pelas principais companhias deste ramo que tem como\nobjetivo desenvolver normas para estes dispositivos. A norma ONVIF baseia-se em serviços\nweb Simple Object Access Control (SOAP) e também em protocolos que já estão padronizados\ncomo o HyperText Transfer Protocol (HTTP) ou Real-time Transfer Protocol (RTP). Os\ndispositivos ONVIF são divididos em Network Video Transmitter (NVT), Network Video\nDisplay (NVD), Network Video Storage (NVS) e Network Video Analytics (NVA).\nO HyperText Markup Language (HTML) era, inicialmente, utilizado para definir a estrutura\nde documentos. No entanto, devido à sua baixa complexidade de utilização tornou-se\nrapidamente a linguagem de marcação mais utilizada para a construção de páginas web.\nHoje em dia, a mesma está na quinta versão a qual permite maior flexibilidade na utiliza-\nção de conteúdo multimédia. Estas páginas juntamente com plugins ou com a linguagem de\nprogramação JavaScript são capazes de constituir as Rich Internet Application (RIA), aplicações\nque são executadas em ambiente web. Devido à falta de segurança e instabilidade\ncausados pelos plugins, hoje em dia começa a ser utilizado apenas o JavaScript.\nDesta forma, foi desenvolvida uma aplicação web que consiste num cliente que faz a\ncomunicação com um Web Service (WS) Representational State Transfer (REST). Este por\nsua vez, encontra-se alojado num servidor HyperText Transfer Protocol (HTTP) Apache\ne está implementado como um Fast Common Gateway Interface (FastCGI). Este FastCGI\nutiliza a biblioteca UMOC para transferir dados com dispositivos NVT (câmaras Internet\nProtocol (IP)).\nO objetivo deste projeto é aumentar o desempenho desta aplicação existente, tanto no\nservidor como no cliente e ainda implementar novas funcionalidades do ONVIF.\nForam desenvolvidas soluções para o lado do cliente que permitem que a aplicação seja\nexecutada com maior velocidade e com menor consumo de recursos e foram também implementadas\nnovas funcionalidades. De entre as contribuições técnicas destacam-se a utiliza-\nção da Web Storage em vez da Indexed DB, a transformação da Application Programming Interface (API) de comunicação com as câmaras mais percetível e mais eficaz e a apresentação\ndos dados de forma dinâmica. Em termos de funcionalidades, foi adicionado\no suporte à receção dos eventos da câmara utilizando Server Sent Events (SSE). No que\ntoca ao lado do servidor, foi realizado o estudo experimental dos servidores HTTP mais\nconhecidos pela sua eficiência e implementação do WS-Notification através da ferramenta\ngSOAP."
  },
  {
    "keywords": [
      "Artificial Intelligence",
      "Chatbot",
      "Virtual agent",
      "ServiceNow",
      "Inteligência Artificial",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Ally - o Chatbot ao serviço da Deloitte",
    "autor": "Rocha, Diogo Miguel Alves",
    "data": "2023-10-19",
    "abstract": "From the beginning, humans have sought to develop tools that facilitate their work. From the first tools designed\nfor hunting or agriculture, to the industrial revolution and the use of computers in the context of work or even\npersonal life, one of the goals has been to improve the quality of life regarding the impact of work. In various areas,\nfrom banking, commerce, to health and customer support, it is quite common to see the presence of Chatbots to\nprovide assistance in various functions. Whether it is for help with navigation, problem resolution, or even for the\nsale of a product, sometimes we don’t even notice it, but it’s there. The most common forms are an embedded\nChatbot on a website or in a support chat.\nUsing a tool like a Chatbot can be very useful in assisting the end customer, but not only that. In assisting with\nrepetitive tasks that can be automated and due to its total availability, a Chatbot can allow for a decrease in the\nworkload of employees in tasks that would have had to be manually performed by them in the past. With these\ntasks being performed automatically, employees can focus on something that truly requires their participation.\nThis implementation will allow companies to save resources, particularly time and money, which can be applied to\nless automation-prone areas.\nWith this goal in mind, Deloitte decided to support this dissertation by creating a project in the form of a proof of\nconcept to obtain answers about whether a Chatbot with these functionalities would be useful within the company\nand if its integration into one of the preferred platforms, ServiceNow, would be feasible. Thus, it was proposed to\ndevelop the Chatbot integrated into ServiceNow, referred to by the platform as the Virtual Agent and as Ally on\nthis project."
  },
  {
    "keywords": [
      "Fermentation",
      "Metabolism",
      "Modelling",
      "S. kudriavzevii",
      "Fermentação",
      "Metabolismo",
      "Modelação",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Dynamic genome-scale modelling of the Saccharomyces non-cerevisiae yeasts metabolism in wine fermentation",
    "autor": "Santos, David Miguel Ferreira dos",
    "data": "2021",
    "abstract": "The wine industry is facing challenging times due, mostly, to climate change and changing\nconsumer demands. The urge to innovate stimulates R&D of new fermentation processes\nusing non-conventional yeast species (e.g. non-cerevisiae Saccharomyces species).\nWhile recent research approached the physiology of diverse non-conventional yeast\nspecies, little is known about their metabolism in different environmental conditions.\nIn this work, a previously developed dynamic genome-scale model was adapted to\nstudy the metabolism of Saccharomyces kudriavzevii in wine fermentation at two temperatures,\n25ºC and 12ºC. Adjustments included the addition of metabolic pathways and\ndynamic constraints. Goodness-of-fit of the model to measurements of the extracellular\ncompounds was satisfactory, i.e. the median values of R2 are 0.95 and 0.87 for 25ºC and\n12ºC, respectively.\nThe model was then used to explore the differences in the dynamics of metabolism\nbetween temperatures. The most significant differences appeared in the stationary\nphase: 1) the strain produces more mevalonate and succinate at 25ºC, probably due\nto a late response to stress and the maintenance of redox balance via the GABA shunt,\nrespectively, 2) erythritol flux is higher at 12ºC, probably due to the conditions of formation\nlasting longer and 3) the production of higher alcohols, mostly de novo, is higher at 12ºC,\ndue to the longer viability of the cells.\nThe proposed model provided a comprehensive picture of the main steps occurring\ninside the cell during wine fermentation. Model predictions are consistent with experimental\ndata and previous findings, but it also brought novel results, such as the role of\nthe GABA shunt or the production of mevalonate in the metabolism of S. kudriavzevii,\nworth being explored further."
  },
  {
    "keywords": [
      "Deep learning",
      "Machine learning",
      "Medical imaging",
      "MRI",
      "X-rays",
      "Imagens ressonância magnética",
      "Imagiologia médica",
      "Raios-X",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "Deep learning applied to medical imaging",
    "autor": "Ramos, Ana Filipa de Oliveira",
    "data": "2019-11-08",
    "abstract": "Medical examinations in general and medical imaging in particular play an important role in disease \ndetection and patient monitoring. Most of the medical images produced are based on X-rays and are used \nby almost all medical specialties. They provide sensitive results and are relatively inexpensive compared \nto other techniques such as Magnetic Resonance Imaging (MRI). However, MRI has proven to be a great \ndiagnostic tool as it enables high-quality images in multiple planes or directions.\nProgressively more researchers are exploring the use of Deep Learning (DL) algorithms to process and \nanalyse medical images. The main goal of this work is to study and evaluate DL methods applied to two \ndifferent types of medical images: 2D X-rays images and 3D MRI scans. Both of these types are often \nused for early detection, diagnosis, and treatment of diseases. Existing DL architectures and image pre processing techniques have also been studied and developed for segmentation of various brain regions \n(using MRIs) and classification of chest diseases (using X-rays).\nConvolution Neural Network (CNN) architectures have aroused interest in health domain because of their \npowerful learning ability. These networks use multiple convolutions, pooling and fully connected layers to \nlearn the features. This work investigated several CNN architectures and their application in classification \nof chest X-rays. The use of transfer learning, a deep learning technique, was also studied. This technique \nenabled the initialization of the CNN with weights from a pre-trained model, whereby the knowledge was \ntransferred from another model. Fully Convolutional Networks (FCN), have been successfully used in the \nsegmentation of biomedical images. In this work, an FCN based on the well-known U-net architecture was \nused to perform the segmentation of brain MRI scans. U-net uses convolution, max pooling, concatenation \nand up sampling layers to extract the features. For the segmentation task the best result obtained with \nbrain MRI using Dice Similarity Coefficient was 95.97%. For the classification task using chest X-rays, the \nbest result using Area Under the ROC Curve was 96.40%."
  },
  {
    "keywords": [
      "Quantum computing",
      "Quantum walks",
      "Python",
      "Qiskit",
      "Computação quântica",
      "Caminhadas quânticas",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Quantum random walks: simulations and physical realizations",
    "autor": "Santos, Jaime Pereira",
    "data": "2021-09-24",
    "abstract": "Quantum computing is an emergent field that brings together Quantum Mechanics,\nComputer Science and Information Theory, which promises improvements to classical\nalgorithms such as simulation of quantum systems, cryptography, data base searching and\nmany others. Among these algorithms, quantum walks may provide a quadratic speed up\nwhen compared to their classical counterparts, allowing improvements to applications such\nas element distinctness, searching problems, matrix product verification and hitting times\nin graphs. The present work offers a general theoretical overview, simulation and circuit\nimplementation of the coined, staggered and continuous-time quantum walk models. The\nfirst two chapters of this thesis are dedicated to the definition of the theoretical framework,\nsimulation in Python and comparison of the aforementioned quantum walk models for the\nsimple case of the dynamics in a line graph and for the search algorithm in a complete\ngraph. This is then used as a benchmark for the final chapter, devoted to building and\ntesting the circuits corresponding to models mentioned above in IBM’s Qiskit. A main\ncontribution of this dissertation concerns the circulant graph approach to diagonal operators\nfor continuous-time quantum walks."
  },
  {
    "keywords": [
      "Application-layer Traffic Optimization",
      "Content distribution networks",
      "Network optimization",
      "Peer-to-peer",
      "Traffic engineering",
      "Engenharia de tráfego",
      "Otimização de rede"
    ],
    "titulo": "Development of a system compliant with the Application-Layer Traffic Optimization Protocol",
    "autor": "Caldas, Paulo Edgar Mendes",
    "data": "2021-10-27",
    "abstract": "With the ever-increasing Internet usage that is following the start of the new decade,\nthe need to optimize this world-scale network of computers becomes a big priority\nin the technological sphere that has the number of users rising, as are the Quality of\nService (QoS) demands by applications in domains such as media streaming or virtual\nreality.\nIn the face of rising traffic and stricter application demands, a better understand ing of how Internet Service Providers (ISPs) should manage their assets is needed. An\nimportant concern regards to how applications utilize the underlying network infras tructure over which they reside. Most of these applications act with little regard for\nISP preferences, as exemplified by their lack of care in achieving traffic locality during\ntheir operation, which would be a preferable feature for network administrators, and\nthat could also improve application performance. However, even a best-effort attempt\nby applications to cooperate will hardly succeed if ISP policies aren’t clearly commu nicated to them. Therefore, a system to bridge layer interests has much potential in\nhelping achieve a mutually beneficial scenario.\nThe main focus of this thesis is the Application-Layer Traffic Optimization (ALTO) work ing group, which was formed by the Internet Engineering Task Force (IETF) to explore\nstandardizations for network information retrieval. This group specified a request response protocol where authoritative entities provide resources containing network\nstatus information and administrative preferences. Sharing of infrastructural insight\nis done with the intent of enabling a cooperative environment, between the network\noverlay and underlay, during application operations, to obtain better infrastructural re sourcefulness and the consequential minimization of the associated operational costs.\nThis work gives an overview of the historical network tussle between applications\nand service providers, presents the ALTO working group’s project as a solution, im plements an extended system built upon their ideas, and finally verifies the developed\nsystem’s efficiency, in a simulation, when compared to classical alternatives."
  },
  {
    "keywords": [
      "Modularidade em java",
      "Metodologias de desenvolvimento",
      "Arquiteturas modulares",
      "Programação estruturada",
      "Jigsaw",
      "Modularity in java",
      "Development methodologies",
      "Modular architectures",
      "Structured programming",
      "681.3.062"
    ],
    "titulo": "Modularidade em Java : o impacto do projeto Jigsaw",
    "autor": "Santos, Luís Fernando Rodrigues Loureiro dos",
    "data": "2011-10-24",
    "abstract": "A modularidade é um conceito importante na implementação de sistemas\nsuportados por software. A linguagem Java é uma das linguagens utilizadas para\nimplementar este tipo de sistemas.\nEsta dissertação apresenta um estudo sobre os conceitos de modularidade que o\nprojeto Jigsaw propõe para a linguagem Java, demonstrando como se comparam\ncom o estado de arte de modularidade em ambientes de desenvolvimento Java,\nas melhorias para a linguagem Java e para os sistemas de software\ndesenvolvidos em Java, nomeadamente sistemas baseados em servidores\naplicacionais.\nO projeto, através do conceito de modularidade proposto, introduz alterações\nimportantes na linguagem e plataforma Java, na forma de desenvolvimento e\ndistribuição de aplicações e esta dissertação pretende, através de análise e\ndemonstração, mostrar a importância da metodologia apresentada e de que\nforma pode melhorar e substituir as várias metodologias de modularidade em\nJava atualmente existentes.\nNo âmbito desta dissertação, é apresentada uma aplicação informática, na forma\nde prova de conceito, desenvolvida utilizando a linguagem Java, que procura\nautomatizar processos associados à aplicação da metodologia Jigsaw no\ndesenvolvimento de aplicações.\nAs conclusões deste estudo permitem perceber que o Jigsaw apresenta melhorias\nsignificativas que devem ser incorporadas no Java mas, permitem também\nperceber a existência de limitações que devem ser corrigidas por forma a tornar o\nconceito mais abrangente para ser utilizado nos mais variados cenários,\nnomeadamente na implementação de aplicações complexas, como é o caso de\nservidores aplicacionais. A plataforma Java encontra-se numa fase de evolução sensível, onde decisões que estão a ser tomadas pelas várias entidades que\ndeterminam o futuro da plataforma podem implicar o sucesso ou fracasso da\nplataforma, sendo o Jigsaw um ponto em aberto nesses processos de decisão."
  },
  {
    "keywords": [
      "Atmospheric scattering",
      "Night sky",
      "Stars",
      "Real time simulation",
      "Dispersão atmosférica",
      "Céu noturno",
      "Estrelas",
      "Simulação em tempo real",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Rendering the Sky",
    "autor": "Fernandes, Pedro Miguel Machado",
    "data": "2023-10-24",
    "abstract": "The appearance of the sky is defined by the position of the elements we can see in it. With that in mind, it’s\ninteresting to comprehensively understand the procedures needed to accurately compute the position of\nsaid elements in the sky, given the latitude, longitude of the observer and the date. Alongside the accurate\nposition of these elements, we can take into consideration the atmospheric scattering and attenuation, as\nwell as taking advantage of our virtual environment to implement non-realistic features that enhance the\nvisualization experience.\nTwo applications, one for desktop and for Android, were developed using multiple free publicly available\ntools and resources. The 3 most relevant resources are a star catalogue (Nash), giving us the positions\nof over 140.000 stars, the set of equations presented in Paul Schlyter’s website (Schlyter) which compute\nthe positions of the planets of the Solar System, and the SAMPA algorithm developed by NREL (2012).\nThe latter both computes the position of the Sun and Moon and allows us to translate the position of\ncelestial elements from a geocentric coordinate system to a topocentric one. This thesis mostly focuses on\ndescribing how to use these resources to accurately compute the position of and display the stars, planets,\nMoon and Sun.\nBesides the accurate positioning of these elements, atmospheric scattering is also taken into account,\nduring the day, which gives our sky its familiar color, as well as atmospheric attenuation, during the night,\nwhich color shifts the planets, Moon and stars depending on their position on the sky. For ease of visualiza tion, non-realistic features were added that increase the visibility of usually imperceptible elements, such\nas increasing the scale of the planets, increasing the brightness of the dimmest stars and overlaying the\nartwork of the 88 constellations over the night sky."
  },
  {
    "keywords": [
      "Administração Pública",
      "CLAV",
      "Web Semântica",
      "Workflow",
      "Public Administration",
      "Semantic Web",
      "Workflow"
    ],
    "titulo": "Especificação e implementação de um workflow numa plataforma para classificação e avaliação documental",
    "autor": "Araújo, Rui Filipe Ferreira",
    "data": "2021-03-15",
    "abstract": "A substituição do papel pelo formato digital nas instituições e empresas tornou-se uma\nprática comum, sendo que algumas já extinguiram a utilização de formatos analógicos.\nAs políticas europeias incentivam a que sejam adotadas medidas para a redução de papel.\nDesta forma, a administração pública tem abandonado a utilização de suportes analógicos,\nsubstituindo-os pelo formato digital, tendo as entidades públicas passado a prestar os seus\nserviços e a disponibilizar a documentação de forma eletrónica.\nA Plataforma “CLAV - Classificação e avaliação da informação pública” desenvolvida pela\nDGLAB pretende disponibilizar instrumentos, tal como a Lista Consolidada; a mediação des materializada da produção de tabelas de seleção; e a prestação de um serviço automatizado\nde controlo da eliminação da informação pública. A adoção de esquemas de metainformação\npara a interoperabilidade permite a disponibilização de uma linguagem comum aos vários\norganismos da Administração existente na Lista Consolidada, através da plataforma CLAV,\npermitindo ainda a integração com sistemas de informação organizacionais e a troca de\ninformação entre entidades.\nEm concordância com os objetivos delineados para o desenvolvimento do projeto, realizou se um estudo sobre abordagens idênticas ao CLAV noutros países, onde foram identificadas\nas formas de classificação desenvolvidas e as tecnologias utilizadas para essa implemen tação, fazendo uma comparação com a abordagem de Portugal. O plano de trabalho do\nprojeto dividiu-se então em três fases. Inicialmente realizou-se um estudo teórico acerca da\nabordagem de Andaluzia (Espanha) e a de Portugal. Seguiu-se com o desenvolvimento de\num modelo gráfico acompanhando as especificações do Business Process Model and Notation e,\npor fim, a implementação e o desenvolvimento desse modelo na plataforma CLAV.\nVerificou-se que, após a implementação do workflow para a gestão dos pedidos, este tornou\ntodo o processo mais simples e rápido para os responsáveis pelo tratamento destes, uma\nvez que todo o processo é realizado através da plataforma, não havendo a necessidade de\ntransitar documentos físicos."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Dados nomeados em redes móveis Ad Hoc",
    "autor": "Pereira, Ana Filipa Fernandes",
    "data": "2016-06-09",
    "abstract": "As redes móveis Ad hoc são compostas por nós móveis, que têm a capacidade de, autonomamente, criar uma rede de comunicações entre eles, sem assistência de um ponto de acesso, contrariamente às redes de infraestrutura. A comunicação é sem fios e ocorre diretamente entre os nós vizinhos, localizados no mesmo raio de alcance. No entanto, neste tipo de redes, a topologia da rede é muito dinâmica, devido à mobilidade dos nós, à entrada e saída de nós na rede e às quebras de ligações constantes, que provocam alterações inesperadas nas rotas. Além disso, o meio de comunicação é sem fios e partilhado entre os nós vizinhos.\nO paradigma dos dados nomeados (NDN - Named Data Networks) pode contribuir para minimizar alguns dos problemas das redes Ad hoc. As NDNs constituem uma alternativa às redes IP, apontada como uma das mais promissoras arquiteturas de rede da Internet do futuro e baseiam-se no paradigma receiver driven. Os utilizadores (consumidores de informação) apenas têm que saber qual o conteúdo (nome) pelo qual têm interesse, não sendo necessário conhecer a sua localização (endereço). Para obter a informação que deseja, o consumidor propaga um pacote de interesse através da rede.\nQuando este atinge um nó produtor ou um nó intermédio, que possua o conteúdo em cache, os dados são devolvidos pelo caminho inverso. Nesta dissertação propôs-se uma nova estratégia de reenvio de Interesses e Dados, designada por MPRstrategy (MultiPoint Relay), numa rede de dados nomeados Ad hoc. O objectivo é diminuir o envio redundante de pacotes e melhorar o desempenho da rede. Os pacotes de interesses são enviados apenas por um subconjunto de nós vizinhos, escolhidos de modo a minimizar as retransmissões. Além disso, o reenvio é atrasado de modo a poder reduzir as transmissões redundantes. Os pacotes de dados seguem o percurso inverso, se possível. Senão são reenviados da mesma forma. A estratégia foi implementada e avaliada no simulador ndnSIM, mostrando melhorias de desempenho, quando comparada com outras alternativas."
  },
  {
    "keywords": [
      "616.8:681.324",
      "681.324:616.8"
    ],
    "titulo": "Sistema de geo-localização referencial para pessoas com perdas cognitivas",
    "autor": "Ramos, João Ricardo Martins",
    "data": "2012",
    "abstract": "A tecnologia tem revelado avanços significativos ao longo dos anos. Porém os desenvolvimentos\nnão se focaram apenas na área da computação, expandido-se para outras áreas como a saúde,\npermitindo o desenvolvimento de novas técnicas de diagnóstico ou o aperfeiçoamento das existentes.\nDesta forma a qualidade de vida e os cuidados de saúde disponibilizados pelas instituições\nàs pessoas foram melhorados. No entanto, estes progressos não permitem a total supressão de\npatologias, existindo algumas sem uma cura efetiva como é o caso das perdas cognitivas.\nO diagnóstico de perdas cognitivas pode potenciar algumas modificações na vida do paciente\ncomo, por exemplo, a presença de uma pessoa prestadora de cuidados, provocando a perda de\nindependência do doente. De forma a diminuir a invasão de privacidade, vários investigadores\ndesenvolveram projetos para permitir a orientação destas pessoas, porém exigiam algum esforço\nmental que pontualmente se poderia tornar demasiado complexo.\nO projeto desenvolvido pretende orientar as pessoas com perdas cognitivas, permitindo que\nestas tenham uma vida mais independente. Por outro lado, a pessoa prestadora de cuidados\npode desenvolver outro tipo de atividade sem descurar o tipo de serviço que presta através de\naplicações que lhe permitem conhecer a posição atual da pessoa a seu cargo.\nCom este projeto pretende-se que a pessoa com perdas cognitivas tenha uma maior independência\ntanto em trajetos comuns como em percursos de lazer. Este grau de independência é\npossível através da utilização de uma aplicação informática para dispositivos móveis que permite\na seleção de diversos pontos de interesse e posterior orientação até estes. Este projeto utiliza\nconceitos de realidade aumentada e sinal GPS para aquisição da localização atual, indicando o\ncaminho através de símbolos simples.\nNuma outra aplicação para dispositivos móveis o percurso pode ser visualizado em tempo\nreal possibilitando uma constante monitorização da pessoa com perdas cognitivas. No caso de\nocorrer alguma eventualidade é possível consultar os últimos pontos frequentados por esta."
  },
  {
    "keywords": [
      "Automatic assessment",
      "Assessment software",
      "Programming exercises",
      "Dynamic analysis",
      "Static analysis",
      "Education technology",
      "Avaliação automática",
      "Programa de avaliação",
      "Exercícios de programação",
      "Análise dinâmica",
      "Análise estática",
      "Tecnologia educacional",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "ACE grader automatic grading of programming exercises",
    "autor": "Santos, Sofia Guilherme Rodrigues dos",
    "data": "2023-09-06",
    "abstract": "Despite their rising usage in classrooms, most automatic grading tools for programming\nexercises are quite simple, using only output comparison or unit tests to evaluate a solution,\nin contrast with manual grading methods used by teachers, which also look at the code\nitself, even if it doesn’t produce a correct solution. Static analysis methods for code have\nbeen around for a while, but largely ignored in assessment software.\nThe Master’s project here reported proposes an automatic grading method for pro gramming exercises that, in addition to dynamic analysis, uses static analysis to evaluate\nsubmissions. This method benefits both teachers and students, since, by scoring solutions\nthat produce the wrong output, it provides a more comprehensive evaluation of student submitted programs while also making it easier to see exactly what needs to be improved.\nMoreover, it makes evaluation more rigorous, by requiring more than just a program that\nsolely produces the correct result. A prototype application called ACE Grader was created\nto demonstrate the efficacy of this grading strategy.\nThis dissertation describes a bibliographic review of existing automatic grading tools,\nproposes and introduces ACE Grader through an overview of its architecture and its\ndevelopment process. As an initial version of the application was deployed in the middle\nof the second semester of university classes, experiments with students in a real classroom\nsetting are also presented and discussed."
  },
  {
    "keywords": [
      "Conflict-Free Replicated Data Types",
      "Probabilistic representation of sets",
      "Bloom filters",
      "Eventual consistency",
      "Representação probabilística de conjuntos",
      "Consistencia eventual",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Probabilistic data types",
    "autor": "Fernandes, Pedro Henrique Moreira Gomes",
    "data": "2021-12-03",
    "abstract": "Conflict-Free Replicated Data Types (CRDTs) provide deterministic outcomes from concurrent\nexecutions. The conflict resolution mechanism uses information on the ordering of the last\noperations performed, which indicates if a given operation is known by a replica, typically\nusing some variant of version vectors. This thesis will explore the construction of CRDTs\nthat use a novel stochastic mechanism that can track with high accuracy knowledge of the\noccurrence of recently performed operations and with less accuracy for older operations.\nThe aim is to obtain better scaling properties and avoid the use of metadata that is linear on\nthe number of replicas."
  },
  {
    "keywords": [
      "Stress",
      "Conteúdos multimédia",
      "Sistemas de recomendação",
      "Televisão interativa",
      "Multimedia content",
      "Recommender systems",
      "Interactive television",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Televisão interativa: recomendação de conteúdos multimédia",
    "autor": "Machado, Luís Duarte Dias",
    "data": "2016",
    "abstract": "A televisão de hoje em dia disponibiliza um enorme lote de conteúdos. Quando existe um\nleque de conteúdos de grande dimensão é difícil optar pela solução que mais nos agrada ou que\nmelhor satisfaz as exigências dos espetadores. O tempo consumido na seleção pode mesmo esgotar\no tempo que o espetador dispõe para visualizar o conteúdo selecionado. Este facto influencia\no nível de stresse nas pessoas. Para tentar responder a este problema, recorreu-se a técnicas de\ninteligência artificial para a criação de um sistema inteligente que seja capaz de proporcionar ao\nutilizador uma melhor experiência televisiva, contribuindo para o seu bem estar e diminuição dos\nseus níveis de stress, reduzindo o tempo gasto."
  },
  {
    "keywords": [
      "Avaliação de solvabilidade",
      "Crédito ao consumo",
      "Dados",
      "Decisões automatizadas",
      "Inteligência Artificial",
      "Métodos preditivos",
      "Scoring",
      "Artificial Intelligence",
      "Automated decision-making",
      "Consumer credit",
      "Creditworthiness",
      "Data",
      "Predictive methods",
      "Scoring",
      "Ciências Sociais::Direito"
    ],
    "titulo": "Decisões automatizadas e scoring no crédito ao consumo",
    "autor": "Rebelo, Diogo José Morgado",
    "data": "2022-10-26",
    "abstract": "Em plena revolução digital, seria inconcebível que o crescendo na procura de crédito ao consumo \nnão fosse acompanhado pela maior preponderância que mecanismos como o scoring baseado em téc nicas de Inteligência Artificial representa para uma mais e melhor avaliação da solvabilidade em sede de \ndecisões automatizadas. Vislumbra-se o começo de um mundo novo em que a aceitação ou rejeição no \npedido de empréstimo já não é tão-só conatural a decisões de puro julgamento dos analistas. E se, por \num lado, a possível integração de megadados de crédito em modelos analíticos pode, até certo ponto, \nminorar o esforço das análises preditivas, em contramão, maiores serão os riscos de preterição do rango \nde tutela do bem jurídico proteção de dados, tal-qual prescrito no (ou pelo) Direito dos Livros. De facto, \ntópicas como esta exibem uma complexidade tal que é impossível olvidar debruçar-se sobre as mesmas \ntendo por base uma única lente jurígena, sem se ampliar o pendor multidisciplinar que a uma investiga ção desta índole deve subjazer. Como tal, principiara-se o presente ensaio com o discorrer da temática \nem contexto histórico e sociotécnico. E porque a concessão de crédito ao consumo é, no atual estado de \narte scoring, não só comandada pelos modelos de exploração e aprendizagem automática, mas antes, \ne sobretudo, pela qualidade dos conjuntos de dados que os alimentam, ainda em contexto prolegómeno, \nepitomaram--se os elementos-chave que concorrerão para um scoring de crédito mais exato e, espera- \n-se, cada vez mais transparente. Foi, portanto, a partir destas premissas técnico-científicas que se abor dou a figura jurídica dos contratos de crédito ao consumo, com especial enfoque na contratação rápida \nem linha e nas diligências pré-contratuais concernentes à injuntividade da avaliação de solvabilidade. Em \núltima instância, porquanto o objeto de estudo suscita uma energética (ou antes, idiossincrática) tutela \nsobre a proteção de dados pessoais, foi à luz do emaranhado axiológico-normativo do regime das deci sões individuais e exclusivamente automatizadas, incluindo a definição de perfis, bem como das quimé ricas salvaguardas adequadas (ou antes, ilusórias) consagradas e sugeridas pelo legislador europeu,\ntanto no Regulamento Geral sobre Proteção de Dados Pessoais, como na Proposta de Diretiva, relativa \naos créditos aos consumidores, de 30 de junho de 2021, que se conclui a necessária adoção de políticas \nsetoriais que primem por uma lhana infoliteracia financeira dos (ciber)consumidores."
  },
  {
    "keywords": [
      "Fuzzy parsing",
      "IDE construction",
      "Program Comprehension",
      "Automata",
      "Grammars",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Unfuzzying fuzzy parsing",
    "autor": "Carvalho, Pedro Miguel de Jesus Ventura",
    "data": "2015-02-20",
    "abstract": "Recognizing sentences of a language in an efficient and precise manner has always been a strong\nsubject within computer science. Many theories, algorithms and techniques have been proposed along\ncomputing history, but at the end it all comes down to performing lexical and syntactic analysis of the\nsource, originating a parse tree as the result.\nSometimes there is no need for full precision or even a full parse tree. A good example of one of\nthese cases is architecture extraction from source code. In this case only a small portion of the code\nis of interest. Another good example is recognizing handwritten expressions, because it is entirely\nimpossible to predict the kind of calligraphy that will be analyzed, it is also impossible to perform an\none hundred percent precise recognition. This need for tolerant parsing lead to the development of\nmany forms of tolerant parsing along the years.\nThis master work will focus on one form of tolerant parsing in particular, Fuzzy Parsing. From\nthis work it is expected the emergence of a new Fuzzy Parsing technique based on automata, where\nautomata states would represent context and edges would represent potential matches inside that context.\nThe hypothesis of this work is that such an approach reduces uncertainty and recognition time.\nIt is also expected the creation of a tool suit that facilitates the process of developing fuzzy parsers.\nWe believe that such a tool will be a great addition to areas such as Program Comprehension or IDE\nconstruction."
  },
  {
    "keywords": [
      "Urban evolution",
      "Urban research",
      "Urban morphology",
      "Ontology",
      "XML",
      "Evolução urbana",
      "Investigação urbana",
      "Morfologia urbana",
      "Ontologia",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Urban evolution of Fafe in the XIX and XX centuries",
    "autor": "Lameiras,  João Filipe Campos",
    "data": "2019-12-30",
    "abstract": "The movement of people from dispersed living to concentration in urban environments\nis a large change both for human civilization and for the environment. Urbanization is\nthe process of changing from natural habitats to dense grey space made up primarily of\nbuildings, roads, and accessory infrastructure accompanied by dense human populations.\nWhile many cities are well established, humans continue to build new cities or expand cities\noutward in a network of suburban environments. And urbanization is not simply about a\ntransition from green to grey space, other abiotic changes such as changes in light regimens\ndue to artificial lighting, increased pollution, and increased impervious surfaces leading\nto runoff are found in urban areas. The study of Urban Evolution of Fafe in the XIX and\nXX Centuries is an interesting theme not only because of the lack of works in this area but\nalso because of the possibility of understanding the organization of the current city. The\nmain problem that we faced it was that as the years go, the mapping and buildings of cities\nchange. And the information of these changes is stored in texts, records, maps, etc. This\nfact made the study of urban evolution difficult because the information is widespread and\nhard to gather. So, in order to study Fafe urban evolution we needed to recover and gather\ninformation of the changes and new buildings in the city during the XIX and XX centuries.\nGiven the inexistence of an exhaustive investigation of an urban history we had to seek to\ninterpret from the present formation the successive processes of urbanization and respective\nextensions, juxtapositions and overlaps. More important is the diverse set of sources that\nallowed to characterize the urbanism of the city of Fafe. With that said it was important\nto create an integrated repository in digital format to enable its analysis and search of\ninformation, and visual exploration through a map. For that purpose, it was necessary\nto create ontologies related to urban evolution that allowed us to develop web-supported\ntools derived from these ontologies for the acquisition of the state and the location of the\nbuildings and in order to analyze the changes as the years go by. The web-supported tools\nare available in http://www4.di.uminho.pt/∼gepl/UEF/."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Safety critical interactive computing systems' modelling",
    "autor": "Sousa, Manuel António Freitas de",
    "data": "2012-12-14",
    "abstract": "Typically, testing an interactive system involves manually testing their possible interactions. Since this is a manual process, it becomes very costly to check all possible interactions. In safety critical interactive systems this task is essential. One way to overcome this problem is to use tools for systematic analysis. IVY Workbench is one of these tools. We plan to apply it to perform verification of Safety Critical Interactive Systems. The objectives for this dissertation are: development of a set of models of safety critical interactive systems; verification of relevant properties of the models; critical assessment of the modelling process and suggestion of improvements to the tool and language."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "OML - Ontology Manipulation Language",
    "autor": "Carvalho, Nuno Alexandre Ramos",
    "data": "2008",
    "abstract": "Ontologies are a common approach used in nowadays for formal representation of concepts in a structured way. Natural language processing, translation tasks, or building blocks for the new web 2.0 (social networks for example) are instances of areas where the adoption of this approach is emerging and quickly growing.\nOntologies are easy to store and can be easily build from other data structures. Due to their structural nature, data processing can be automated into simple operations. Also new knowledge can be quickly infered, many\ntimes based on simple mathematics properties. All these qualities brought together make ontologies a strong candidate for knowledge representation.\nTo perform all of these tasks over ontologies most of the times custom made tools are developed, that can be hard to adapt for future uses.\nThe purpose of the work presented in this dissertation is to study and implement tools that can be used to manipulate and maintain ontologies in a abstract and intuitive way. We specify a expressive and powerful, yet simple, domain specific language created to perform actions on ontologies. We will use this actions to manipulate knowledge in ontologies, infer new relations or concepts and also maintain the existing ones valid. We developed a set of tools and engines to implement this language in order to be able to use it. We illustrate the use of this technology with some simple case studies."
  },
  {
    "keywords": [
      "Network simulators",
      "Software defined networking",
      "Energy models",
      "OpenFlow",
      "Simuladores de rede",
      "Modelos de energia",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Green communications: an environment to support energy-aware networks developments",
    "autor": "Monteiro, Rui Pedro da Cunha",
    "data": "2023-02-24",
    "abstract": "The continuous joint growth of the communication networks is causing a simultaneous increase in\nthe energy consumption of these infrastructures. To help fight the consequent environmental impact is\nrequired that new traffic engineering techniques are developed to help mitigate this energy consumption.\nThis work proposes the Flexcomm Simulator 1\n, a simulation environment with the intent of creating a\nplatform that helps to develop new routing algorithms, that combine Software Defined Networking (SDN)\nand Energy Flexibility techniques to optimize the energy consumption of large scale wired communication\nnetworks. The environment allows simulating real infrastructures conditions, so policies can be developed\nclose to real scenarios, facilitating their deployment in real environments.\nThe tool’s abilities have been demonstrated, by generating data that allows the evaluation of different\nrouting strategies. Moreover, the Flexcomm Simulator has made possible the developments on early work\nof new algorithms that demonstrate the ability to achieve a more balanced energy consumption. These\nnew techniques are capable of adapting to changes in energy availability and relocating network flows\nacross different regions of a network, respecting flexibility imposed by electrical grids while maintaining a\nminimum Quality of Service (QoS)."
  },
  {
    "keywords": [
      "Deep Learning",
      "Explicabilidade",
      "Machine Learning",
      "Transparência",
      "Séries temporais",
      "Explainability",
      "Time series",
      "Transparency",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "IA explicável para problemas de séries temporais",
    "autor": "Franco, Francisco Correia",
    "data": "2024-05-20",
    "abstract": "Ao longo dos últimos anos, a aplicação de modelos de Machine Learning (ML) tem desempenhado um papel fundamental na nossa sociedade, ajudando a obter conhecimentos sobre grandes quantidades de dados. No entanto, existe uma preocupação quanto à clareza da forma como estes modelos chegam às suas conclusões. Por outras palavras, existe uma ”blackbox”sobre a forma como estes modelos chegam às suas conclusões, quer sejam problemas de classificação ou de regressão. Atualmente, a explicabilidade tornou-se uma peça fundamental no desenvolvimento e aplicação de modelos de previsão para séries temporais. Em contextos onde a tomada de decisões é sensível, requerem fundamentação sólida e\nos dados são maioritariamente séries temporais, modelos com explicabilidade oferecem vantagens significativas. A transparência nas previsões não só ajuda a identificar possíveis falhas, mas também facilita a adoção desses modelos em ambientes críticos, promovendo uma integração mais eficaz da Inteligência Artificial (IA) no dia a dia.\nDeste modo, esta dissertação visa a implementação de técnicas de explicabilidade num modelo LongShort Term Memory (LSTM), com o objetivo de aumentar a interpretabilidade deste modelo de blackbox. Os dados utilizados para treinar o modelo, e efetuar o estudo da explicabilidade, são relativos a emissões de vários poluentes na cidade do Porto. Desta forma, cada feature é uma série temporal que contribui para prever as próximas 24 horas do Dióxido de Nitrogénio (NO2). O modelo foi treinado inicialmente sozinho e depois foi inserida uma camada de attention no início. Desta forma, foi possível extrair explicabilidade ante-hoc da camada de attention e ainda aplicar estratégias post-hoc, como o Shapley Additive Explanations (SHAP) e feature permutation, ao modelo com e sem attention. Através dos resultados obtidos, é possível perceber que o modelo foi capaz de identificar dependências temporais e relações não lineares nos dados que foram cruciais para o ”raciocínio”e tomada de decisão destes. Estas relações apenas foram possíveis de observar graças aos métodos de explicabilidade implementados, que, pela sua diversidade e convergência nos resultados, transmitem segurança e confiança na sua veracidade."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Development of web-based tools for spectral data analysis and mining",
    "autor": "Afonso, Telma Adriana Pereira",
    "data": "2017",
    "abstract": "The recent advances in different analytical techniques able to produce spectral data, including\nRaman, Infrared (IR) or Ultraviolet-Visible (UV-vis) spectroscopies, have provided novel\napproaches for many research issues in the biological and chemical fields. Indeed, they have\nallowed to address tasks in functional genomics, sample characterization and classification,\nor drug discovery. To take full advantage of these data, advanced bioinformatics methods\nare required for data analysis and mining.\nA number of methods and tools for spectral data analysis have been put forward recently,\nbeing one of the major limitations still faced the lack of integrated frameworks for extracting\nrelevant knowledge from these data and being able to integrate these data with previous\nbiochemical knowledge. Also, the lack of reproducibility in many data analysis or data\nmining processes is a strong obstacle for biological discovery, being common the lack of\ndata and data analysis pipelines in the published work.\nIn recent work from the host group, specmine, a metabolomics and spectral data analysis/\nmining framework, in the form of a package for the R system, has been developed to\naddress some of these issues. In this thesis, the main aim was to design and develop an\nintegrated web-based platform for spectral data analysis and mining, based on the specmine\npackage, providing an easier and more user friendly interface, but also addressing some of\nthe package’s current limitations.\nThe developed platform contains features that cover the main steps of the metabolomics\ndata analysis workflow, with modules for data reading and dataset creation, data preprocessing\nand a variety of analysis types. It includes an authentication system, allowing\nthe user to have his own personal workspace where projects can be stored and accessed\nlater, with the option to share projects with other users. The different modules were validated\nusing real data from previously published studies in the host group, related to the\nanalysis of the characteristics and potential of natural products, addressing as well the\nexploration and integration of data from distinct experimental techniques, attesting the\nplatform’s robustness and utility."
  },
  {
    "keywords": [
      "Fingerprinting",
      "Clustering",
      "RSSI Averaged Positioning Error (APE)",
      "RSSI APE"
    ],
    "titulo": "Two-level fingerprint-based indoor positioning using advanced Machine Learning",
    "autor": "Ramires, Moisés Manuel Borba Roriz",
    "data": "2022-12-15",
    "abstract": "These days, positioning systems are Global Navigation Satellite System (GNSS) based – such as Global Positioning System (GPS) or the European Galileo – have been deployed worldwide, due to their efficiency, reliability, and need. Today using GPS for navigation or localization is quite common, this technology shaped our world and it is now part of our life.\nHowever, these satellite-based positioning systems fail to provide good results inside infrastructures. If someone is inside a building, walls and other objects inside will attenuate the signals, making them unreliable for obtaining a position. For example, an Indoor Positioning System (IPS) offering localization services inside a hospital could bring a lot of benefits, namely patient orientation, locating doctors and nurses for emergency responses, or immediately locating critical instrumentation, among others. In the case of a warehouse, it can be used for better logistics, optimization of resources and autonomous vehicle driving. Other related contexts can be found in airports, museums and shopping malls, where IPSs can be used to support indoor navigation.\nThere is a large number of solutions created for this challenge, using technologies such as Bluetooth Low\nEnergy (BLE), Wi-Fi, Ultra-Wideband (UWB), Light Detection and Ranging (LIDAR), and Infrared, among many others. These are usually associated with techniques such as proximity, trilateration, triangulation, and fingerprinting. This work will focus on using Wi-Fi technology using the fingerprinting technique, i.e., Wi-Fi Fingerprinting for large indoor enviroments."
  },
  {
    "keywords": [
      "Robotics",
      "ROS2",
      "SROS2",
      "Security properties",
      "Observational determinism",
      "Software verification",
      "Alloy",
      "Robótica",
      "Propriedades de segurança",
      "Determinismo observacional",
      "Verificação de software",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Formalizing ROS2 security configuration with Alloy",
    "autor": "Ribeiro, Luís Mário Macedo",
    "data": "2022-12-19",
    "abstract": "Industrial manufacturing is becoming highly reliant on automation developments, as they bring more\nefficient and accurate processes, with lower associated costs. Consequently, robots are increasingly being\ndeployed in a wide range of scenarios, especially where safety is demanded. In such cases, it is critical\nto employ appropriate procedures to verify both the system’s quality and safety.\nFollowing the current growth of cyber-physical systems, as well as their usage in various technology\ndomains, the development of software applications is becoming more demanding due to the complexity\nbehind the integration of complementing services, beyond those provided by the operating system.\nOne of the most popular open-source software platforms for building robotic systems is the Robot Operating System (ROS) [53] middleware, where highly configurable robots are usually built by composing\nthird-party modules. Robot Operating System 2 (ROS2) is implemented using the Data Distribution\nService (DDS) [49] communication protocol. ROS2 implicitly makes use of the DDS-Security artefacts\nthrough the Secure Robot Operating System 2 (SROS2) security toolset.\nThe present study focus on detecting security problems in ROS2 networks, in which it is intended to\nverify, through formal techniques, security properties. However, security is a very broad subject, so this\nstudy focuses on a particular security property to show the viability of the proposed technique, namely\nObservational Determinism (OD).\nThis dissertation introduces a software tool, named Security Verification in ROS (svROS), that\nprovides multiple functionalities to support this type of security analysis using Alloy [32], a formal specification language and analysis tool."
  },
  {
    "keywords": [
      "Computational thinking",
      "Learning resources",
      "Ontology",
      "Adult learning",
      "Gamebased learning",
      "OntoDL",
      "Prolog",
      "Pensamento computacional",
      "Recursos educativos",
      "Ontologias",
      "Ensino de adultos",
      "Ensino baseado em jogos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Adequacy analysis of learning resources in adult education",
    "autor": "Barbosa, Diana Ribeiro",
    "data": "2021-10-27",
    "abstract": "The present document identifies and details the research and development held under the scope\nof a MSc Thesis pertaining to the scientific area of pedagogic tools for teaching support, ontolo gies and learning resources. This masters thesis in Informatics Engineering was developed in the\nUniversity of Minho, Braga.\nThe purpose of the project is to study the learning process of adults and how it connects to Learn ing Resources (LRs) in order to understand if a learning resource used to teach Computational\nThinking (CT) to children, is suitable for adult learners. This approach ought to take into account\nadult learning theory to set its requirements, as well as CT principles and learning resources\nclassification.\nTo this end, an approach to the Adequacy of Learning Resources in Adult Education was created\nwhich comprises the ontology OntoAL that describes in detail the domain of Adult Learning\n(AL) including the theory of AL and a classification of both the adult learner and the learning\nresources. This ontology was developed in OntoDL and Prolog. In addition, we analyze the\nexperiment conducted as part of the validation of this approach and the OntoAL ontology.\nTherefore, in this document, it is presented the state of the art pertaining to this field, exploring\nthe concepts of learning resources, computational thinking, ontologies and adult learning and\neducation. Furthermore, it is rendered an introduction of the subject and the project, detailing\nthe context of the problem, the objectives to be accomplished and the research hypothesis of\nsaid thesis. Next, it is presented the state of the art regarding Computational Thinking, Adult\nLearning and Education, Ontologies and Learning Resources. Thereafter, it is put forward the\nwork proposal. Then it is introduced the OntoAL ontology in both OntoDL and Prolog (detailing\nthe process of its development and the choices made), the questionnaires that were created as\nwell as the analysis of the responses that we obtained. Lastly, there are listed the conclusions and\nthe future work."
  },
  {
    "keywords": [
      "Redes de computadores",
      "Análise de tráfego",
      "Machine mearning",
      "Computer networks",
      "Traffic analysis",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Caracterização de tráfego de serviços de streaming em dispositivos móveis",
    "autor": "Martins, Gabriela Sá",
    "data": "2021",
    "abstract": "No contexto atual, o contínuo desenvolvimento tecnológico permite um acesso fácil e rápido a variadíssimos\nserviços e plataformas, por meio de dispositivos móveis. Por este motivo, o volume e diversidade de tráfego tem\ncrescido de forma exponencial também.\nO conhecimento do tráfego que circula nas redes atuais torna-se indispensável, seja para ajudar a melhorar\na gestão e configuração dos elementos e serviços de rede, seja para os utilizadores terem a oportunidade de\ngerir melhor os recursos na utilização dos seus dispositivos móveis e aplicações.\nAssim, este trabalho pretende aprofundar o estudo das características do tráfego gerado por dispositivos móveis,\nno acesso a determinadas plataformas/serviços. Também se espera incluir uma componente de Machine\nLearning para previsão da experiência do utilizador. Além disso, pretende-se obter base de comparação entre\nas versões web e aplicacional de um mesmo serviço."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Distributed databases synchronization in named data delay tolerant networks",
    "autor": "Liu, Chong",
    "data": "2016-10-06",
    "abstract": "Delay Tolerant Network (DTN) is a small regional network designed to provide better communications when the end-to-end connection is not always possible. DTN is well known for intermittent connections and long delays. Nodes store data packets in the buffers and forward later when the connection is restored. Recently, Named Data Networking (NDN) has been drawing wide attention as a Future Internet architecture. This architecture\nshifts the emphasis from host to content and pays little attention to where is the content. Routing in NDN is based on the name of the content.\nNamed Data-Delay Tolerant (ND-DT) network is an integration of DTN and NDN. It takes the advantages of both architectures by applying named data approach in DTN scenarios. In ND-DT network, distributed databases are maintained by a group of fixed or moving nodes. Data inconsistency always exists because of the intermittent connections and long delays.\nHowever, data synchronization solutions can minimize this inconsistency, helping to reduce the data access delay. ChronoSync is a well-known NDN state synchronization protocol. Data synchronization in ND-DT networks are challenging because of the intermittent connections and the nodes’ mobility. Moreover, the connection between nodes is not assured, which may make synchronization to fail. In this work, it is assumed that there is at least one path between each pair of database nodes. The aim of this work is to improve the recovery process of ChronoSync in order to enhance its adaptability to ND-DT network scenarios. For this\ngoal, ChronoSync and our improved solution were implemented and tested on an ND-DT network simulator.\nThe results show that our improved ChronoSync is more adaptable to ND-DT networks. The improved ChronoSync consumes less time to finish synchronization tasks in all the scenarios. What’s more, in three database scenarios, IChronoSync decreasing about 83% of the synchronization time while Chronosync decreases 62% when changed from sparse network to dense network. What’s more, improved ChronoSync generates 27% fewer data packets, which can increase the probability of other network nodes getting connected."
  },
  {
    "keywords": [
      "Sistemas de data warehousing",
      "Processamento analítico de dados",
      "OLAP",
      "Sessões OLAP",
      "Cadeias de markov",
      "Classes de equivalência",
      "Assinaturas OLAP",
      "Data warehousing systems",
      "On-line analytical processing",
      "OLAP Sessions",
      "Markov chains",
      "Equivalence classes",
      "OLAP signatures",
      "681.3:658.0",
      "658.0:681.3"
    ],
    "titulo": "Materialização à medida de vistas multidimensionais de dados",
    "autor": "Duarte, Ana Sofia da Silva",
    "data": "2012-11-22",
    "abstract": "Com o emergir da era da informação foram muitas as empresas que recorreram a data warehouses para armazenar a crescente quantidade de dados que dispõem sobre os seus negócios. Com essa evolução dos volumes de dados surge também a necessidade da sua melhor exploração para que sejam úteis de alguma forma nas avaliações e decisões sobre o negócio. Os sistemas de processamento analítico (ou OLAP – On-Line Analytical Processing) vêm dar resposta a essas necessidades de auxiliar o analista de negócio na exploração e avaliação dos dados, dotando-o de autonomia de exploração, disponibilizando-lhe uma estrutura multiperspetiva e de rápida resposta. Contudo para que o acesso a essa informação seja rápido existe a necessidade de fazer a materialização de estruturas multidimensionais com esses dados já pré-calculados, reduzindo o tempo de interrogação ao tempo de leitura da resposta e evitando o tempo de processamento de cada query. A materialização completa dos dados necessários torna-se na prática impraticável dada a volumetria de dados a que os sistemas estão sujeitos e ao tempo de processamento necessário para calcular todas as combinações possíveis. Dado que o analista do negócio é o elemento diferenciador na utilização efetiva das estruturas, ou pelo menos aquele que seleciona os dados que são consultados nessas estruturas, este trabalho propõe um conjunto de técnicas que estudam o comportamento do utilizador, de forma a perceber o seu comportamento sazonal e as vistas alvo das suas explorações, para que seja possível fazer a definição de novas estruturas contendo as vistas mais apropriadas à materialização e assim melhor satisfaçam as necessidades de exploração dos seus utilizadores.\n\nNesta dissertação são definidas estruturas que acolhem os registos de consultas dos utilizadores e com esses dados são aplicadas técnicas de identificação de perfis de utilização e padrões de utilização, nomeadamente a definição de sessões OLAP, a aplicação de cadeias de Markov e a determinação de classes de equivalência de atributos consultados. No final deste estudo propomos a definição de uma assinatura OLAP capaz de definir o comportamento OLAP do utilizador com os elementos identificados nas técnicas estudadas e, assim, possibilitar ao administrador de sistema uma definição de reestruturação das estruturas multidimensionais “à medida” da utilização feita pelos analistas."
  },
  {
    "keywords": [
      "Marketing Automation",
      "Software architecture",
      "Web services",
      "Representational state transfer (REST)",
      "e-commerce",
      "Cloud computing",
      "Marketing Automático",
      "Arquitetura de software",
      "Serviços web",
      "e-comércio",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Development of a marketing automation platform to integrate online e-commerce services",
    "autor": "Gonçalves, Diogo Alexandre Domingues",
    "data": "2022-04-06",
    "abstract": "E-commerce is continuously expanding which increases market competitiveness. With the increase of plat-forms arrives a need to stand out from the competition, thus creating the necessity to improve marketing strate-gies. Marketing strategies, such as creating personalized birthday emails or registration welcome-emails cannot be done in the traditional way. This idea of creating custom services like sending user-specific emails creates the need for a marketing automation solution. Following this need, its proposed the development of a marketing automation platform with integration with a machine learning engine. This system will be hosted on a cloud and will automate marketing campaigns and provide dents with results from machine learning models."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Exploração de paralelismo massivo em algoritmos evolucionários",
    "autor": "Martins, Tiago Augusto Simões",
    "data": "2016",
    "abstract": "Esta dissertação está centrada na paralelização massiva da biblioteca Java Evolutionary Cornputation Library), JECoLi, que se foca no desenvolvimento de meta-heurísticas de \notimização(e.g. Algoritmos Evolucionários (AEs)) na linguagem Java. Os AEs são um \nparadigma da Computação Evolucionária (CE) utilizados para resolver problemas complexos \natravés de um método iterativo que evolui um conjunto de soluções (população) tendo em \nconta os princípios da teoria de evolução por seleção natural apresentada por Charles Darwin. \nEstes algoritmos estão divididos em duas categorias, AEs não estruturados e AEs estruturados. Os AEs não estruturados são caracterizados por uma população centralizada onde existe apenas um conjunto de soluções ao qual é aplicado o processo evolutivo. Por outro lado, os AEs estruturados contêm várias populações onde os processos evolutivos são conduzidos de forma independente, embora existindo troca de informação. \nOs algoritmos de ambas as categorias podem ser paralelizados de diferentes maneiras. \nNesta dissertação, foram implementadas quatro versões paralelas da plataforma JECoLi de \nforma o menos invasiva possível, tendo em conta modelos paralelos já formulados: um modelo \nde paralelismo global; um modelo de ilhas em ambiente de memória partilhada; um modelo \nde ilhas em ambiente de memória distribuída; e um modelo híbrido. \nEstas implementações paralelas foram executadas no cluster Services and Advanced Research Cornputing with HTC/HPC clusters (SeARCH) utilizando o máximo de recursos computacionais possíveis de modo a realizar uma posterior análise dos resultados obtidos. Foram utilizados dois casos de estudo reais para validar as implementações paralelas, um problema de otimização de um bioprocesso de fermentação fed-batch e outro de otimização dos pesos de um protocolo de encaminhamento (OSP F). \nCada uma das implementações paralelas foi testada nos dois casos de estudo, aplicando \no máximo de paralelismo possível tendo em conta as limitações de cada caso de estudo, dos \nmodelos paralelos e dos recursos disponíveis. \nCom estes testes concluí-se uma boa escalabilidade destes algoritmos, onde se destacam \nas implementações relativas ao modelo de ilhas em memória distribuída e ao modelo híbrido. \nContudo, algumas configurações que originam maiores ganhos foram descartadas pois não \nproduzem valores de aptidão aceitáveis."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Translating alloy specification to the point-free style",
    "autor": "Macedo, Nuno",
    "data": "2010-10-28",
    "abstract": "Every program starts from a model, an abstraction, which is iteratively re ned until we reach the\n nal result, the implementation. However, at the end, one must ask: does the  nal program resemble\nin anyway the original model? Was the original idea correct to begin with? Formal methods\nguarantee that those questions are answered positively, resorting to mathematical techniques. In\nparticular, in this thesis we are interested on the second factor: veri cation of formal models.\nA trend of formal methods defends that they should be lightweight, resulting in a reduced\ncomplexity of the speci cation, and automated analysis. Alloy was proposed as a solution for this\nproblem. In Alloy, the structures are described using a simple mathematical notation: relational\nlogic. A tool for model checking, automatic veri cation within a given scope, is also provided.\nHowever, sometimes model checking is not enough and the need arises to perform unbounded\nveri cations. The only way to do this is to mathematically prove that the speci cations are correct.\nAs such, there is the need to  nd a mathematical logic expressive enough to be able to represent\nthe speci cations, while still being su ciently understandable.\nWe see the point-free style, a style where there are no variables or quanti cations, as a kind\nof Laplace transform, where complex problems are made simple. Being Alloy completely relational,\nwe believe that a point-free relational logic is the natural framework to reason about Alloy\nspeci cations.\nOur goal is to present a translation from Alloy speci cations to a point-free relational calculus,\nwhich can then be mathematically proven, either resorting to proof assistants or to manual proving.\nSince our motivation for the use of point-free is simplicity, we will focus on obtaining expressions\nthat are simple enough for manipulation and proofs about them."
  },
  {
    "keywords": [
      "Cloud computing",
      "Google Docs suite",
      "Microsoft Online suite",
      "Network behavior",
      "Quality of  experience",
      "QUIC",
      "SaaS performance",
      "Traffic profile",
      "Comportamento da rede",
      "Computação em nuvem",
      "Desempenho SaaS",
      "Perfil  de tráfego",
      "Qualidade de experiência",
      "Suite da Google Docs",
      "Suite da Microsoft Online"
    ],
    "titulo": "Network and cross-platform SaaS performance: a case study",
    "autor": "Carreira, Joana Lourenço",
    "data": "2020-06-18",
    "abstract": "The traditional role of a personal computer is dramatically changing with the shift towards cloud-based \nservices. Cloud computing and storage provides end-users with universal access to their data across \nvarious devices, coherent application and service experience, and substantially decreases hardware \nrequirements for end-user clients (personal computers). However, this cloud-oriented paradigm requires \nthe redesign of applications and services, as well as a serious analysis of the current situation and proper \nscaling of access networks since this new paradigm changes everyday habits of end-users. \nThis work is focused on the impact of cloud-based applications (using the paradigm of Software as a \nService (SaaS)) on access networks, analyzes the network and the application behavior, while also \naddressing application usability and Quality of Experience (QoE) in different scenarios. A detailed study of \nthe impact on access networks imposed by such cloud-based services (and vice versa) is currently \nmissing, especially in the case of bandwidth-constrained, high-latency mobile access networks.\nFurthermore, this work involves analysis of various cloud-based applications, namely office tasks (text, \npresentation, and spreadsheet editing), in different combinations and executed on different hardware and \nsoftware platforms with different levels of integration with cloud-based services. Network traffic analysis \nwill be executed, including collecting Wireshark traces of the generated and received traffic, correlated \nwith specific executed tasks. The impact of network congestion and latency is also examined in the QoE focused section. The work discussion is broken down into individual hypotheses, reflecting expectations\nregarding behavior of SaaS applications, data volume of the network, and QoE of the end user. Different \nend-user experience metrics are used in combination with network-based monitoring (including peak and \naverage bandwidth measurements, latency, packet loss, etc.)."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "JxAppDev framework for hybrid applications",
    "autor": "Ferreira, Ricardo Ribeiro",
    "data": "2018",
    "abstract": "In modern days, it becomes more and more common for software solutions to focus on\nmobile and web technologies, therefore the current desktop market has been shrinking.\nDue to the big impact that web technologies are having on the market and user’s daily\nbasis it has become impossible for developers to neglect this evolution.\nNevertheless, in some cases it is difficult to justify the development of some web applications\nsince the benefits are to small and the costs to high.\nDue to this problem and some other small inconveniences, there are some emerging\ntechnologies that try to close the gap between desktop and web applications by trying to\ncombine the best of both worlds.\nThere are some well-known technologies such as Java Applets, which are mainly Java\napplications that can be executed on the browser. Even though these technologies are very\ninteresting and in some specific cases very useful, companies avoid taking this path since\nthis kind of software applications raise some problems, which are making sure that these\nnew technologies are abandoned and forgotten (e.g. some security problems with regard\nto plugin installation).\nWith this project, we intend to create a Framework, which main goal is to ease the hybrid\napplication development.\nThis framework allows users to develop native Java SE applications, that can be accessed\nas normal Desktop applications, but at the same time it is possible to access the same\ncontent through a regular Web Browser, using common well-known technologies such as\nHTML, JavaScript and CSS.\nWith this solution, it is possible to avoid high costs on Web application development, and\navoid other small problems such as security problems when installing plugins that can be\nfound in the current existing solutions.\nThis way it is possible to develop a single Desktop application that is reusable on the\nBrowser if needed.\nThe idea is not to allow the user to create a new application that can be accessed on both\nplatforms, but on the contrary it aims Java applications that have already been developed\nor that will be developed with no intention of making them accessible on the Web, but at\nsome point the urge to port the application appears and the user won’t need to rebuild\neverything from scratch, but he will simply need to invest some time developing the new\nUser Interface for the Web version that he wants to provide."
  },
  {
    "keywords": [
      "Automatic speech recognition",
      "European Portuguese",
      "End-to-end learning",
      "Data collection",
      "Reconhecimento automático de fala",
      "Português Europeu",
      "Aprendizagem ponta a ponta",
      "Recolha de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automatic speech recognition for European Portuguese",
    "autor": "Campinho, Adriano Vaz de Carvalho",
    "data": "2021",
    "abstract": "The process of Automatic Speech Recognition (ASR) opens doors to a vast amount of possible\nimprovements in customer experience. The use of this type of technology has increased\nsignificantly in recent years, this change being the result of the recent evolution in ASR\nsystems. The opportunities to use ASR are vast, covering several areas, such as medical,\nindustrial, business, among others. We must emphasize the use of these voice recognition\nsystems in telecommunications companies, namely, in the automation of consumer assistance\noperators, allowing the service to be routed to specialized operators automatically through\nthe detection of matters to be dealt with through recognition of the spoken utterances. In\nrecent years, we have seen big technological breakthrough in ASR, achieving unprecedented\naccuracy results that are comparable to humans. We are also seeing a move from what\nis known as the Traditional approach of ASR systems, based on Hidden Markov Models\n(HMM), to the newer End-to-End ASR systems that obtain benefits from the use of deep\nneural networks (DNNs), large amounts of data and process parallelization.\nThe literature review showed us that the focus of this previous work was almost exclusively\nfor the English and Chinese languages, with little effort being made in the development of\nother languages, as it is the case with Portuguese. In the research carried out, we did not\nfind a model for the European Portuguese (EP) dialect that is freely available for general\nuse. Focused on this problem, this work describes the development of a End-to-End ASR\nsystem for EP. To achieve this goal, a set of procedures was followed that allowed us to\npresent the concepts, characteristics and all the steps inherent to the construction of these\ntypes of systems. Furthermore, since the transcribed speech needed to accomplish our goal\nis very limited for EP, we also describe the process of collecting and formatting data from a\nvariety of different sources, most of them freely available to the public. To further try and\nimprove our results, a variety of different data augmentation techniques were implemented\nand tested. The obtained models are based on a PyTorch implementation of the Deep Speech\n2 model.\nOur best model achieved an Word Error Rate (WER) of 40.5%, in our main test corpus,\nachieving slightly better results to those obtained by commercial systems on the same data.\nAround 150 hours of transcribed EP was collected, so that it can be used to train other ASR\nsystems or models in different areas of investigation. We gathered a series of interesting\nresults on the use of different batch size values as well as the improvements provided by\nthe use of a large variety of data augmentation techniques. Nevertheless, the ASR theme is vast and there is still a variety of different methods and interesting concepts that we could\nresearch in order to seek an improvement of the achieved results."
  },
  {
    "keywords": [
      "Alloy",
      "Electrum",
      "TLA",
      "Formal specification language",
      "Dynamic systems",
      "Rich configurations",
      "Model-Checking",
      "Parallel verification",
      "Linguagem de especificação formal",
      "Sistemas dinâmicos",
      "Configurações ricas",
      "Verificação de modelos",
      "Verificação paralela",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Parallel verification of dynamic systems with rich configurations",
    "autor": "Pessoa, Eduardo José Dias",
    "data": "2016-12-10",
    "abstract": "Model checking is a technique used to automatically verify a model which represents the specification\nof some system. To ensure the correctness of the system the verification of both static and dynamic\nproperties is often needed.\nThe specification of a system is made through modeling languages, while the respective verification\nis made by its model-checker. Most modeling frameworks are not ready to verify models rich in both\nkind of properties thereby limiting the verification of dynamic systems with rich configurations.\nElectrum is a modeling language which mixes the best of the Alloy and TLA specification languages,\nwith the capability of handling the problem mentioned above. This language is supported by\ntwo model-checking techniques – one bounded and one unbounded.\nNonetheless, the Electrum’s bounded model-checker has limitations, thus, this dissertation aims to\novercome them with the purpose of improving the analysis procedure of Electrum models, in particular,\nthe definition of a new Electrum’s semantics through a translation into Kodkod as well as the\ncreation of a novel procedure of verifying Electrum models in parallel. Hence, in order to achieve\nthese goals, a temporal extension to the Kodkod constraint solver was implemented."
  },
  {
    "keywords": [
      "Intrusion detection system",
      "Security in industrial networks",
      "Denial of service",
      "Brute force",
      "Sistema de detecção de intrusão",
      "Segurança em redes industriais",
      "Negação de serviço",
      "Força bruta",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Anomaly-based intrusion detection systems for industrial networks",
    "autor": "Costa, Afonso Fernando da",
    "data": "2023-09-29",
    "abstract": "The combination of operational technology of industrial networks with information technologies has en abled the increase of attacks on industrial networks, causing professionals and researchers linked to the\nsecurity area to study tools, mechanisms and techniques capable of detecting and blocking malicious\nactivities in industrial network environments.\nThis dissertation sets out to comprehensively explore and analyze Anomaly-based Intrusion Detection Sys tems (IDS) as a central focal point. The primary ambition is to meticulously investigate the efficacy of\nsuch systems in identifying, monitoring, and recording abnormal behaviors, detecting malicious activities,\nand pinpointing potential attacks that exploit remote services and orchestrate denial of service incidents.\nThrough an in-depth examination and critical evaluation, this study aims to contribute to the existing body\nof knowledge in the realm of cybersecurity, advancing our understanding of IDS capabilities and their sig nificance in safeguarding digital and industrial environments against emerging threats.\nIn this way, it is intended to create a model relating the attack techniques, their indicators, and the fields,\nlogs, and events generated by the IDS, in order to help detect such attacks, in a way, to evaluate the\nefficiency of the IDS in detecting malicious activities."
  },
  {
    "keywords": [
      "Smartwatch",
      "Digital",
      "ISO",
      "Identity",
      "Service",
      "Autenticidade",
      "Identidade",
      "Serviço",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Suporte wearable para Mobile Documents (mDoc) ISO/IEC 18013-5",
    "autor": "Matias, Hugo Fernandes",
    "data": "2024-01-15",
    "abstract": "The popularization of mobile devices and the characteristic pervasiveness of internet access have led to\nincreased interest in developing potentially relevant services for citizens. The various solutions for identifying people\nusing digital devices are an example of this. Among these solutions is the ISO/IEC DIS 18013-5 standard. This\nstandard published by the International Organization of Standardization (ISO) defines the technical requirements\nnecessary for secure transmission, i.e. with guarantees of data integrity and authenticity, of identification attributes\nrelated to a driver’s license. The interest to implement the standard ISO/IEC DIS 18013-5 has risen due to the\nincrease in development and public interest in wearable devices, and the recent effort by the European Union\nto encourage the use of digital identification documents. Implementation of this standard is made easier by the\nexistence of cross-development tools, e.g., Xamarin, Flutter, that allow code sharing between different wearable\nplatforms. These tools serve as a catalyst for implementation while maintaining the functionality of the code."
  },
  {
    "keywords": [
      "Xylella fastidiosa subsp. pauca De Donno",
      "Phytopathogen",
      "Systems biology",
      "Genome-scale metabolic model",
      "Ciências Agrárias::Biotecnologia Agrária e Alimentar"
    ],
    "titulo": "Genome-scale metabolic modelling of the pathogen Xylella fastidiosa",
    "autor": "Silva, Miguel Ângelo Fernandes da",
    "data": "2019-12-31",
    "abstract": "Xylella fastidiosa is a phytopathogenic bacteria that causes disease in hundreds of differ ent plant species. Increased reports of plants infected by these xylem-limited bacteria are\nalarming as this pathogen continues to attack crops of relevant economic power such as\ncitrus, grapes, olives and almonds, with considerable economical losses for the producers.\nThe current employed strategy to contain this epidemic is radical in action as it destroys\nthe infected plant and surrounding area. For this reason, it became urgent to develop new\nways to eliminate these bacteria with therapeutics that are more pathogen oriented.\nGenome-Scale Metabolic (GSM) models contain genomic and metabolic information of a\ngiven organism and can be used to discover new potential drug targets. Thus, a GSM\nmodel of X. fastidiosa may unveil new ways to control these bacteria.\nIn this work, we developed a high-quality GSM model for X. fastidiosa subsp. pauca De\nDonno, using the user-friendly software Metabolic Models Reconstruction Using Genome Scale Information (merlin). This strain was chosen for its importance in the national econ omy, as it causes Olive Quick Decline Syndrome. The reconstructed model of X. fastidiosa\ncomprises a set of 1280 reactions and 524 genes.\nThe genome of X. fastidiosa subsp. pauca De Donno was functionally annotated in order\nto identify the metabolic potential of the phytopathogenic organism. Metabolic functions\nidentified in the genome were used to assemble the initial draft metabolic network. Manual\ncuration procedures were made in order to correctly represent organism’s capabilities and\nbiomass related reactions, based on literature and experimental data, were added to model.\nThe reconstructed model was then validated using experimental data, regarding the aerobic\nmetabolism, carbon flux pattern, carbon usage, amino acid auxotrophies and growth in\nseveral media developed for X. fastidiosa.\nIn silico simulations revealed interesting metabolic properties of X. fastidiosa. The usage\nof carbon through the Entner-Doudoroff pathway seems to be a way of generating redox\npotential for defensive mechanism against the host plant. An absence of auxotrophies and\nthe presence of catabolic routes for amino acids shows the metabolic and adaptive potential\nof the organism, as expected since it grows on nutrient-limited environments.\nThis reconstructed model can be used to explore the metabolism and provide information\nfor potential drug targets for the agricultural-destructive phytopathogen X. fastidiosa."
  },
  {
    "keywords": [
      "Discalculia",
      "Crianças",
      "Aplicação móvel",
      "Jogo",
      "Plataforma web",
      "Base de dados",
      "Interface  de programação de aplicações",
      "Aplicação web",
      "Análise de dados",
      "Modelação",
      "Dyscalculia",
      "Children",
      "Mobile application",
      "Game",
      "Web platform",
      "Database",
      "Application programming interface",
      "Web application",
      "Data analytics",
      "Modeling",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Children dyscalculia: a web system for diagnosis and treatment",
    "autor": "Cachulo, César Augusto Lourenço",
    "data": "2020-06-16",
    "abstract": "A discalculia é uma desordem neurológica que dificulta a aprendizagem de diversos conceitos \nmatemáticos. Sendo uma desordem inerente à pessoa, pois nasce com ela, é crucial dedicar-lhe atenção \ndesde cedo. Uma forma de atenuar os seus efeitos é o uso de tecnologia moderna como aplicações \nmóveis, nomeadamente jogos com foco na prática e ensino de diversos conceitos relacionados com \nmatemática, nos primeiros anos de vida escolar em pessoas que sofrem de discalculia. \nEste projeto tem como foco a criação de uma plataforma cujo objetivo é complementar estes jogos \nregistando diversos parâmetros relacionados com o desempenho dos alunos em diversas tarefas e \nretirando conclusões baseadas nestes parâmetros de forma a detetar a discalculia e os seus sintomas, \nassim como acompanhar o progresso do aluno no que toca ao tratamento desta desordem.\nA criação desta plataforma permitiria a existência de uma nova e útil ferramenta no que toca à discalculia \ne ao tratamento dos seus sintomas o mais cedo possível na vida de uma pessoa."
  },
  {
    "keywords": [
      "Serviços 3play",
      "Infraestruturas de comunicação",
      "Cablagem de rede",
      "Cobre",
      "Fibra",
      "3play services",
      "Communications infrastructure",
      "Network cabling",
      "Copper",
      "Fiber",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Instalação e manutenção de redes de acesso e distribuição em operadores 3play",
    "autor": "Ferreira, Miguel Carreiro Gomes",
    "data": "2016",
    "abstract": "No âmbito da Dissertação, incluída no plano curricular do Mestrado em Engenharia de Redes e Serviços Telemáticos, foi proposta a realização de um período de estudo a um prestador de serviços de um ISP (Internet Service PrQv/del) que fornece tecnologias de acesso de voz, Internet e televisão (TV) em cobre e fibra. Este prestador de serviços é responsável por toda a construção e implementação das infraestruturas de redes, tanto de cliente como de acesso público.\nTodo este trabalho em backgroundé o que suporta e dá vida às redes de nova geração. Sem ele, não seria possível construir esta gigante rede a que se chama \"Internet\", que permite o acesso facilitado à comunicação e troca de informações.\nCom este documento pretende apresentar-se a metodologia para a criação de infraestruturas de rede cio operador, de modo a fazer chegar os serviços 3play (voz, TV e Internet) à casa do cliente. O caso de estudo apresentado visa analisar a implementação de uma rede de fibra ótica na Ribeira Grande, ilha de São Miguel, Açores, tendo em consideração as especificidades insulares, como é o caso da distância ao continente e os perigos geológicos associados. São exemplos destes perigos os sismos e os movimentos de vertentes."
  },
  {
    "keywords": [],
    "titulo": "Improving the response time of emergency vehicles: using V2X communications",
    "autor": "Pereira, Bruno Martins",
    "data": "2020-01-09",
    "abstract": "Emergency vehicles are bound to lose unnecessary time on their response. Furthermore,\nresearch shows that emergency vehicles are prone to fatal crashes while on an emergency\ncall. Emergency vehicle’s response time is commonly affected by factors unbeknownst to it,\nsuch as traffic, traffic lights, road conditions, and accidents.\nThe overall goal of this dissertation project was to improve the response time of an emergency vehicle, while improving its safety along the route. Thus, V2X communications was\nused to obtain knowledge from the environment, so that the emergency vehicle can choose\nthe best decisions to reduce the response time. The emergency vehicle issued alerts to all\nof the entities of the road system so that these can adapt their behaviour collaboratively. In\nthis context, V2X communications may occur between vehicles and every other entity of the\nsystem, such as infrastructures, pedestrians, vehicles, and the network.\nThe present document describes the research and development work that aimed to establish a comparison between two scenarios: with and without support on a vehicular network\nsimulator, namely Veins, incorporating the American standard WAVE. The V2X scenario\nbenefited from use cases as emergency avoidance, dynamic routing, intersection and traffic\nlight’s logic. Alternatively, the non-V2X communications setup did not have access to communications, simulating a current context for urban road environments. The evaluation of\nthe V2X benefits was made by analysing the most relevant criteria like response time, stop\ntime, distance covered by the emergency vehicle.\nThe performance of the system prototype based on these metrics showed a clear improvement when using V2X communications. In particular, the response time was reduced by\n41% using V2X, guaranteeing that the use of V2X is a step forward in this context. Since\nthe results from the tests using a full simulated setup were positive, a pair of additional\nexperiments integrating real Bosch V2X communication boards were planned: one with the\nintegration of these boards into the already simulated environment and another with the integration of these boards on real Bosch prototype vehicles. While this later experiment was\nnot possible to realize during this dissertation work time, the former was conducted with\nsuccesses proving that the it is already possible to develop RD projects that use real V2X\ncommunications boards, which is a step closer to test these new technologies on real environments or, at least, on controlled environments that better emulate real environments."
  },
  {
    "keywords": [
      "Avaliação documental",
      "Classificação arquivística",
      "Lista consolidada",
      "M51-CLAV",
      "Tabelas de seleção",
      "Archivist classification",
      "Documental evaluation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "CLAV: avaliação da informação pública",
    "autor": "Gomes, Alfredo Miguel Macedo",
    "data": "2019-12-23",
    "abstract": "Existe uma preocupação cada vez maior em relação a quantidade de informação gerada \ne recebida por diversas instituições não só no que concerne ao consumo excessivo de papel, \ncomo também à gestão de grandes quantidades de informação. \nCom o intuito de simplificar esta gestão documental, foi criado o “Projeto M51-CLAV-Arquivo digital: Plataforma modular de classificação e avaliação da informação pública” que visa a classificação e a avaliação de toda a documentação que circula na Administração Publica portuguesa, utilizando um referencial comum, a Lista Consolidada (LC), que permite o desenvolvimento de instrumentos de natureza transversal a aplicar em contexto organizacional.\nEstando o projeto já numa fase avançada, a LC foi modelada como uma ontologia OWL \ne encontra-se disponível para consulta na plataforma do CLAV, sendo que estão em desenvolvimento outras funcionalidades com o fim de tornar esta plataforma capaz da gestão\ndocumental referida.\nUma dessas funcionalidades pretendidas e a criação de instrumentos de classificação e \navaliação, chamados de Tabelas de Seleção (TS), que é o objetivo principal desta dissertação. \nPretende-se então tornar a plataforma capaz do processo de criação de uma TS de uma \nforma simples, rápida e intuitiva, e que se prende com o preenchimento de um formulário\ncom vários passos. Após este preenchimento, é gerado um pedido de submissão de uma \nnova TS, pedido esse que terá de passar por uma tramitação para que seja validado e, caso \ntal aconteça, a TS passara a ser um instrumento de extrema importância para avaliar e \nclassificar os documentos produzidos pela(s) entidade(s) em causa, ou seja, permite uma\nmelhor gestão documental."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "titulo": "AIDA-MCDT: nova abordagem à visualização de meios complementares de diagnóstico e terapia",
    "autor": "Neto, Cristiana Marisa Pereira",
    "data": "2018-10-01",
    "abstract": "Over the last years, the implementation and evolution of computer resources has been improving both the financial and temporal efficiency of clinical processes, as well as the security in the transmission and maintenance of their data, also ensuring the reduction of clinical risk. Currently, the importance of all the information flowing in health\ninstitutions is unquestionable. In this way, it is essential that institutions, more specifi cally hospital institutions, have a good Hospital Information System (HIS) in order to collect and analyze information, also helping to support decision making. The most common application of these type of systems is the Electronic Health Record (EHR),\nwhich, despite bringing many benefits, is still associated with a low level of usability.\nHowever, the different systems present in hospitals are distributed and heterogeneous. Since the interaction between these systems is crucial these days, there is the \nAgency for Integration, Diffusion and Archive (AIDA) implemented in some Portuguese hospitals. AIDA is a platform developed to enable the dissemination and inte gration of information generated in a health environment by different systems, inclu ding for example information on Complementary Diagnostic and Therapeutic Means (MCDT).\nPrevious research has shown that health professionals often do not analyze and act\naccurately and appropriately on test results. Preventing errors during the access to\nMCDT is essential as this is a crucial step in the diagnostic process, thus avoiding\nnegative consequences for the patient.\nIn this sense, a new MCDT visualization platform (AIDA-MCDT) was implemented\nin this project, specifically in the Hospital Center of Porto (CHP), with several new\nfunctionalities in order to make this process faster, intuitive and efficient, always guaranteeing the confidentiality and protection of patients’ personal data and significantly\nimproving the usability of the system, leading to a better delivery of health care."
  },
  {
    "keywords": [
      "CMDB",
      "CI",
      "Automatic discovery",
      "Mapping",
      "API",
      "Descoberta automática",
      "Mapeamento",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Construção automática de CMDB",
    "autor": "Pereira, Joana Catarina Maciel",
    "data": "2021-08-10",
    "abstract": "Computing infrastructure management is increasingly demanding and has to comply with\nregulatory requirements. To comply with these requirements, the existence of a Configuration\nManagement Database (CMDB) is fundamental. One of the challenges that any team has when\nstarting IT Service Management (ITSM) is to create the organization’s CMDB.\nCMDB is a database that stores information about the components, usually called Configu ration Items (CIs), of the infrastructure and the relationships between them. Thus, the CMDB\ncreation implies discovering information about the infrastructure, saving it in the CMDB\nchosen by the organization.\nThis dissertation presents a tool for the automatic creation of a CMDB that uses automatic\ndiscovery, mapping, and population mechanisms, to find information about the infrastructure\ncomponents and store these results in the CMDB. It was also necessary to adapt the populate\noperation according to the database structure of the selected CMDB.\nThis tool uses several discovery mechanisms to explore different types of Configuration\nItems (CIs), discovering information about them and their dependencies. It also uses an\nautomatic mapping mechanism to adapt the types of discovered data with the CMDB\nstructure where they will be stored. Finally, it populates the CMDB using its Application\nProgramming Interface (API) to create the CIs and relationships."
  },
  {
    "keywords": [
      "Online corpus",
      "Ukraine war",
      "Rebuild the war through the news",
      "Social network analysis",
      "Web scraping",
      "Natural language processing",
      "Corpus online",
      "Guerra da Ucrânia",
      "Reconstrução da guerra através das notícias",
      "Análise de redes sociais",
      "Web scraping",
      "Processamento de linguagem natural",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Ukraine War: an online corpus to analyze the impact of the war in Ukraine",
    "autor": "Rosendo, Ana Rita Miranda",
    "data": "2024-07-25",
    "abstract": "This document reports a Master’s work, the final project of the 5th year of the Integrated\nMaster’s in Informatics Engineering, that was accomplished at Universidade do Minho in\nBraga, Portugal.\nOn February 24, 2022, a conflict between two countries, Ukraine and Russia, began. The\nwar between two countries is devastating and affects many people, both residents of the\ncountries directly involved and neighboring countries. As a highly significant event, it\ngathers coverage from many sources globally, including traditional print newspapers, online\nnews platforms, social networks, blogs, television programs, and more. However, all of this\ninformation is scattered across different websites and social networks. If researchers (in the\nareas of Linguistics, History, Humanities, etc.) and curious people want to analyze this data,\ntheir work will be very difficult. Therefore, it is essential to gather the information on a\nsingle platform.\nThis work aims to create an online corpus in the Portuguese language regarding the\nUkraine War, based on Portuguese online newspapers’ news as well as comments on social\nmedia.\nTo fulfill the goal of this work, initially, a variety of news sources were considered, and\nthe Portuguese online newspapers “Público” and “Jornal de Negócios” were selected, as\nwell as the platform “Reddit”. To extract the required information, the technique of Web\nScraping was used. Therefore, for each source, an extractor was developed that extracted\nthe necessary information and saved it in a JSON file. Following that, Natural Language\nProcessing Techniques were used to process the gathered information. Afterward, the\nextracted information was stored in a non-relational database, MongoDB. Finally, a website\ncalled GUCO was designed and implemented, providing users with the capability to navigate\nand explore the created corpus.\nThe GUCO website is available at the address: https://guco.epl.di.uminho.pt/."
  },
  {
    "keywords": [
      "Data sharing",
      "Medical imaging",
      "Neuroinformatics",
      "Neuroimaging",
      "XNAT",
      "Partilha de dados",
      "Imagiologia médica",
      "Neuro-informática",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Building an imaging-based research platform for experiments with brain connectivity data",
    "autor": "Moreira, Rogério Gomes Lopes",
    "data": "2019-12-23",
    "abstract": "Within the past decade, not only societies in general but also medicine and healthcare, in particular, have \nchanged tremendously. In large part because of the rapid dissemination of computers and digital \ncommunications which lead to the appearance of new medical disciplines, such as Medical Informatics. \nNowadays, one of the most prominent field in Medical Informatics is Medical Imaging, as it is implied, it \nis a collection of methodologies and techniques used in order to visually and spatially represent parts of \nthe brain for diagnostic and research purposes.\nIn the research ecosystem, Neuroimaging is an increasing popular field, with applications in neurology \nand psychiatry. However, due to the difficulties to handle Neuroimaging data, since data has its own \nspecificities, researchers have encountered problems to correctly handling this data. This can be a crucial \nissue specially with large volumes of Neuroimaging data and all the research materials associated. \nThis work aims to architect and build a research platform to correctly archive Medical Imaging data and \nall the associated research materials, where researchers can exchange imaging data and collaborate in \nNeuroimaging research projects. The platform offers a correct way to collect and store all imaging data, \narchiving all of patient exams with the correspondent information, making available the correspondent \ninformation to researchers in a confidential, secure and efficient way.\nThe two main outcomes of this work are an architecture of a platform that manages all imaging data and \nassociated research materials, plus an open-source Python package to easily interact with that platform."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Computação segura sobre sistemas de dados em ambientes cloud",
    "autor": "Ferreira, João Carlos Carvalho",
    "data": "2017",
    "abstract": "O desenvolvimento dos serviços cloud gerou uma migração massiva de dados para os mesmos.\nO facto da cloud fornecer um serviço acessível a partir de qualquer dispositivo (computador\npessoal, telemóvel, tablet, etc.) através do acesso à Internet fez com que a sua\nutilização aumentasse. Estes serviços trazem bastantes vantagens para os utilizadores. A\nfacilidade em aceder e partilhar os dados pelos diferentes utilizadores combinada com\na maleabilidade e escalabilidade oferecidas pela cloud, faz com que este serviço se torne\nnuma plataforma bastante pretendida pelas empresas para armazenar os seus projetos remotamente\ne, desta forma reduzir custos associados aos servidores locais.\nNo entanto, armazenar dados remotamente provoca o aparecimento de questões acerca\nde segurança e privacidade. De forma a garantir confiabilidade no serviço, o fornecedor\ncloud tem de garantir que os dados estejam seguros em termos de privacidade, confidencialidade\ne integridade. Ataques recentes a fornecedores de serviços cloud (Greene, 2015)\nlevantaram questões em termos de segurança da informação, provocando um decréscimo\nna confiança depositada nestes serviços.\nEsta dissertação aborda um desafio de segurança que a cloud está a enfrentar, a computação\nsobre dados. Esta computação tem de ser feita sobre dados cifrados para que o fornecedor\nde cloud não tenha acesso ao valor real da informação nem ao resultado da computação."
  },
  {
    "keywords": [
      "Wildlife conservation",
      "Endangered species",
      "Bioacoustic monitoring",
      "IoT",
      "LoRa",
      "UAV",
      "Conservação da vida selvagem",
      "Espécies em perigo",
      "Monitorização bioacústica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Wildlife conservation using UAV and LoRa network",
    "autor": "João, Adalberto Mendes",
    "data": "2024-03-11",
    "abstract": "Many animal species are on the verge of extinction due to poaching and changes in land use, such as\nthe destruction of forest areas to the detriment of agriculture, logging, and the construction of new towns\nand cities. Therefore, regular monitoring of animal populations is necessary to ensure the protection of\nwildlife, especially when pressure on animals is high, and conservation management requires reliable\nand up-to-date data on land use, the size of animal populations, and the distribution of resources in highly\nvariable ecosystems. In this context, we would like to contribute to the process of automating conventional\nmethods, taking advantage of the Internet of Things (IoT) paradigm, which has evolved rapidly in recent\nyears, and emerging digital technologies such as Low-Power Wide-Area Networks (LPWAN) and the rapid\ngrowth of Unmanned Aerial Vehicles (UAVs), also known as Drones.\nTherefore, this study proposes a wildlife monitoring system based on acoustic detection, using LoRa\nnetworks assisted by UAVs that act as mobile gateways to collect data from sensors on the ground. This\naims to contribute to the protection of fauna in Angola by helping forest rangers to obtain actionable data\nfrom their operations.\nTo check that the proposed system works correctly, tests were carried out in a simulation environment,\nverifying the correct transmission of data from the sensors to the data center. However, this technique\nprovides a more sustainable and scalable monitoring method, minimizing human effort."
  },
  {
    "keywords": [
      "Public displays",
      "Web based player",
      "Digital signage",
      "Scheduling",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Web based player for public displays",
    "autor": "Félix, André Filipe Pereira",
    "data": "2015-01-19",
    "abstract": "The most important element in a network of public displays is a piece of software, the player, it is responsible for interpreting the presentation instructions, which are sent in a specific format, and make the content visible to the users according to those instructions.\nOne of the big issues regarding this type of software is their restrict system requirements, a player is usually conceived having a specific target platform, this creates some issues when deploying new displays. With the increasing development of web technologies emerges a solution to this issue: a web based player with low system requirements and the capability to be deployed in a wider range of platforms.\n\tThe goal of this investigation is to design and implement a web based player using web technologies such as HTML5 and JavaScript, the new Chrome Packaged Apps technology is also being looked at as a way to easily distribute and deploy the software.\n\tUltimately this web based player aims to increase the reach and availability of the public displays networks by creating a platform to which the non-proprietary developer can create content to."
  },
  {
    "keywords": [
      "Deep learning",
      "Machine learning",
      "Text mining",
      "Sentiment analysis"
    ],
    "titulo": "Development of tools for sentiment analysis in the portuguese language",
    "autor": "Gonçalves, Jorge Miguel da Silva Brandão",
    "data": "2022-12-19",
    "abstract": "A Análise de Sentimentos é uma das áreas mais importantes na ciência da computação, nomeadamente no Processamento da Linguagem Natural. As suas aplicações vão desde a análise de produtos até à contenção do cyberbullying. A importância da análise dos sentimentos é inigualável, mas quando se trata de línguas menos faladas, o campo parece ficar para trás. Neste contexto, Omnium AI propôs uma dissertação onde exploramos a Análise de Sentimentos para a Língua Portuguesa, com a intenção de criar uma nova ferramenta computacional. Esta dissertação vai examinar o campo da análise de sentimentos e o desenvolvimento do package Omnia. Este package é composto por ferramentas para a leitura de dados, o seu processamento e a criação de modelos Machine Learning (ML) e Deep Learning (DL) a partir dos dados lidos. Em específico, vamos concentrarnos no desenvolvimento do package Omnia Text Mining, com objectivo de criar ferramentas de pré-processamento e modelos de ML e DL para a análise de sentimentos para a língua portuguesa.\nEsta dissertação vai criar uma abordagem para lidar com problemas de análise de sentimentos composta por um processo de recolha de dados, seguido de um passo de pré-processamento e acabando\ncom o desenvolvimento de modelos de ML e DL. Esta abordagem será aplicada ao tópico do Covid-19.\nApós serem criados os modelos para os datasets relativos ao Covid, avaliamos os resultados para as\ndiferentes combinações de métodos de pré-processamento e modelos onde apuramos que as Long Short\nTerm Memory (LSTM)s e o HFAutoModel com o embedding Bert foram os melhores modelos. No geral,\nos modelos de DL e Autogluon obtiveram melhores resultados que os modelos de ML. Nos métodos de\npré-processamento visualizamos que não existe uma Pipeline geral que possa ser utilizada para todos os\ncasos.\nNo final, iremos discutir as conclusões que podemos retirar desta dissertação juntamente com uma\nsecção de trabalho futuro, onde exploraremos os próximos passos possíveis para este projecto."
  },
  {
    "keywords": [
      "Quantum control",
      "Biproduct dagger compact closed categories",
      "Closed quantum systems",
      "Open quantum systems",
      "Controle quântico",
      "Categorias biproduct dagger compact closed",
      "Sistemas quânticos fechados",
      "Sistemas quânticos abertos",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "On conditional quantum control",
    "autor": "Carvalho, Daniel Almeida",
    "data": "2022-07-29",
    "abstract": "The purpose of this dissertation is to ascertain the feasibility of quantum control. This is a rather informal concept,\nhowever in the context of this work it simply means the control flow of a quantum program being performed without\nresorting to measurements and without being mediated by an external classical computer.\nThe approach consists in providing a definition of a conditional like statement which encapsulates the intended\nbehaviour. The definition is given as a specific morphism in a Biproduct Dagger Compact Closed Category.\nThese were introduced by Abramsky and Coecke (2004) as a semantic framework capable of expressing the\naxioms of closed system finite dimensional Quantum Mechanics. Later this framework was extended to capture\nOpen System Quantum Mechanics (Selinger, 2007). As a consequence, and as it pertains to this dissertation,\nthe construct presented in this work has an interpretation for both closed quantum systems (category of finite-dimensional Hilbert spaces and Linear maps) and open quantum systems (category of finite-dimensional Hilbert\nspaces and Completely Positive maps).\nWhat was found is that in closed quantum systems the proposed construct transpires the idea of superposition\nof programs as conceptualized in previous works on the matter (Badescu and Panangaden, 2015) (Ying et al.,\n2014), which gives validity to the notion of quantum control at least in this context. On the other hand, in open\nquantum systems the meaning of the conditional statement proposed takes the form of probabilistic branching\ndependent on the probability distribution of a bit.\nFinally a comparison is made between this work and the one carried out by Badescu and Panangaden (2015)\nin the context of the QPL programming language (Selinger, 2004), which concludes with a discussion about the\nincompatibility of quantum control in programming languages whose semantics are based on the open quantum\nsystem formalism."
  },
  {
    "keywords": [
      "Computação na nuvem",
      "Cloud",
      "Otimização",
      "Design science research",
      "Escalabilidade",
      "Elasticidade",
      "Cloud computing",
      "Optimization",
      "Service selection",
      "Scalability",
      "Elasticity",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Otimização na alocação de recursos de cloud computing num serviço de autenticação de produtos",
    "autor": "Braga, Luís Tiago Machado",
    "data": "2022-05-10",
    "abstract": "A UN1Qnx, S.A., soluções de autenticidade ciber-físicas, é uma empresa sediada em Braga,\nque desenvolve e comercializa sistemas físicos, eletrónicos e cibernéticos de validação e\nautenticação de produtos, sendo o objetivo a proteção da marca e o combate à contrafação.\nNeste momento, a empresa possui um serviço de autenticação de produtos localizado numa\nmáquina virtual na cloud, mais especificamente na Microsoft Azure. Contudo, a utilização\ndeste serviço é intermitente e passa por períodos de inatividade. Porém, quando utilizado,\ncada execução do serviço é computacionalmente custosa, o que obriga à utilização de uma\nmáquina virtual que tem em conta o caso de máxima utilização. Assim, nos intervalos entre\nutilizações os custos acumulam-se sem aproveitar os recursos alocados. Deste modo, esta\ntese passa por otimizar a utilização dos recursos na cloud, tendo em vista tirar proveito da\nescalabilidade e elasticidade das tecnologias de computação na nuvem, bem como melhorar\na latência dos pedidos.\nA otimização dos recursos passa por comparar diferentes serviços de diferentes forne cedores e selecionar o que se apresenta como a melhor opção. A fim de realizar estas\ncomparações, fez-se antes uma investigação baseada na metodologia Design Science Research.\nPrimeiramente, explorou-se o ambiente da solução (computação na nuvem) e o ambiente\ndo problema, isto é, qual a situação atual da empresa no que diz respeito ao funcionamento\ndo serviço de validação e dos recursos afetos ao mesmo.\nEm segundo lugar, fez-se uma averiguação sobre o estado da arte das tecnologias usadas,\ndas tecnologias que poderiam vir a ser usadas e de outras empresas da mesma área, sobre\nquais os seus produtos e o seu modo de funcionamento. Por último, investigaram-se métodos\nde seleção e comparação entre várias opções.\nEm terceiro lugar, realizou-se a parte mais trabalhosa e demorada: o desenvolvimento\nprático. Nesta fase realizaram-se testes de performance, a colocação do serviço num docker\ncontainer e a utilização de kubernetes. Ainda nesta última parte, houve vária experimentação\ncom diversas arquiteturas. Por fim, o sistema estabilizou numa arquitetura assíncrona, que\nfez reduzir os custos e, permitiu com que o serviço se adequasse melhor à quantidade de\ntrabalho a processar."
  },
  {
    "keywords": [
      "Engenharia Software",
      "Aprendizagem máquina",
      "Ciência dados",
      "DevOps",
      "MlOps",
      "Machine Learning",
      "Software",
      "Data Science",
      "Pipelines",
      "Automation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Automation of machine learning models benchmarking",
    "autor": "Sá, João Pedro Barros",
    "data": "2022-05-30",
    "abstract": "Na área de ciência de dados, o machine learning está-se a revelar uma ferramenta essencial para resolver problemas complexos. As empresas estão a investir em equipas de ciência de dados e Machine Learning para desenvolver modelos que apresentem valor para os clientes. No entanto, estes modelos são uma pequena percentagem de uma pipeline de projetos de Machine Learning (ML) e, para entregar um produto de ML completo, é necessário um número maior de componentes. \nDevOps é uma mentalidade de engenharia e um conjunto de práticas que visa unificar o processo de desenvolvimento e o processo de operações em um software, MLOps é um conceito similar a DevOps mas aplicado ao desenvolvimento e entrega de soluções de ML. O nível de automatização das etapas em uma pipeline de ML define a maturidade do processo de ML, que reflete a velocidade de treino de novos modelos com novos dados ou de treino de novos modelos com diferentes implementações. Um sistema de ML é um sistema de software, desenvolvimento e atualizações contínuas são necessárias para garantir um sistema que escale conforme as necessidades. \nO principal objetivo desta tese é apoiar a criação de um sistema integrado de ML com uma arquitetura que proporcione a capacidade de ser continuamente operada em um ambiente de produção. Um conceito para avaliação de desempenho de algoritmos deve ser elaborado e implementado. O principal obetivo e melhorar e ace'erar o cicio de desenvolvimento de modelos de ML na empresa. \nPara atingir este objetivo surge a necessidade de definir uma arquitetura com especificações e a implementação de processos automatizadas num pipeline de ML existente, este processo têm como objetivo alcançar uma ferramenta de benchmark de modelos, com capacidade de analisar o desempenho do modelo, um motor de inferência e um banco de dados para armazenar todas as métricas computadas. Um sistema baseado em IA em desenvolvimento fornece o caso de estudo para desenvolver e validar a arquitetura. Os avanços atuais na área da condução semiautomática introduz a necessidade de sistemas de monitoramento que podem localizar e detectar eventos especificas no veículo. Os conjuntos de sensores são instalados dentro da cabine para alimentar sistemas inteligentes que visam analisar e sinalizar certos comportamentos que podem impactar a segurança e o conforto dos passageiros.."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Common infrastructure provisioning",
    "autor": "Ribeiro, Rui Miguel Martins",
    "data": "2017",
    "abstract": "Provisioning commodity hardware used for scientific research while making it customizable\nand available for a large group of researchers is a process that requires automation.\nThis dissertation describes the infrastructure, design and implementation of MOCAS and\nBootler, an approach to management, allocation and provisioning of physical and virtual\nresources focused on enabling the users to remotely manage their nodes. MOCAS provides\nthe necessary infrastructure and tools along with an appropriate web interface so\nresearchers may lease bare metal resources and customize the full provisioning process,\nfrom installation to configuration without the need of specialized human-resources. Bootler,\non the other hand, simplifies Virtual Machine (VM) life cycle management by providing a\nstreamlined user interface and delegating VM scheduling to OpenStack. In this context,\nHigh-Assurance Software Laboratory (HASLab) researchers are now able to seemingly operate\na 104 nodes (416 cores) commodity hardware cluster by leveraging the automation\nand abstractions these platforms provide."
  },
  {
    "keywords": [
      "Freshwater mussels",
      "Transcriptomics",
      "Bioinformatics",
      "Gene expression",
      "Climate change",
      "Mexilhões de água doce",
      "Transcriptómica",
      "Bioinformática",
      "Expressão genética",
      "Alterações climática",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Identify gene expression profiles in freshwater mussels under thermal stress",
    "autor": "Silva, Beatriz Ferreira da",
    "data": "2024-06-19",
    "abstract": "Due to global climate change, the temperatures of streams and rivers are increasing, negatively \naffecting aquatic life, including bivalve species. Freshwater mussels are vital components of \nrivers, streams, and lake ecosystems, participating in essential ecological roles such as nutrient \ncycling, and increasing water quality. Furthermore, they serve as essential ecosystem engineers, \nproviding habitat to other organisms and supporting intricate food webs. Besides their biological \nimportance, freshwater mussels are poorly studied in terms of genomics. In the present work, the \nIberian dolphin freshwater mussel Unio delphinus Spengler, 1793 (Bivalvia: Unionoida) was \nused as a model species to investigate the effects of climate change in freshwater mussels. \nThe primary objective of this thesis was to determine the gene expression patterns in a model \nspecies of freshwater mussels under the effects of thermal stress exacerbated by climate change, \nwith an overall goal of understanding the potential consequences for freshwater mussel \npopulations. Two different ecological experiments were performed: chronic and acute. The chronic \nexperiments where temperatures were gradually increased to simulate a scenario of progressive \nincreasing temperatures. The acute experiments where temperatures were rapidly increased to \nreplicate the effects of a briefer extreme climatic event. To achieve this main goal, a comprehensive \nbioinformatic pipeline focused on transcriptomics analysis was developed using the R \nBioconductor package to generate the differential gene expression profiles of these individuals \nunder thermal stress. The bioinformatic methodology of this work differs from the past studies, by \ndeveloping an R code compilation of three methods, EdgeR, limma, and DESeq2 for differential \ngene expression analysis in these organisms. The output of the present work provides a \ncomprehensive overview of gene expression profile responses of U. delphinus under \nclimate change scenarios. Additionally, the results revealed a wide range of pathways \nand the corresponding genes that are impacted by thermal stress, with a particular emphasis \non the up-regulation of the genes ATP6V1A, ATP6V0A1, ATP6V0A, and ATP6V1. In the chronic \nexperiments, and high temperatures, mussels expressed these genes and, interestingly, all \nthe pathways that these genes included appeared up-regulated. The discovered genes and \npathways provide vital insights into these organisms’ adaptation tactics and identify \nprospective targets for monitoring and conservation efforts."
  },
  {
    "keywords": [
      "Plataformas móveis",
      "Sistemas de avaliação de conhecimento",
      "Mobile learning",
      "Sistemas de interação",
      "Mineração de dados",
      "Dashboards",
      "Profiling",
      "Mobile plataforms",
      "Knowledge assessment systems",
      "Interaction systems",
      "Data mining",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Plataformas móveis adaptativas para sistemas de avaliação",
    "autor": "Nogueira, Diogo Emanuel da Silva",
    "data": "2022",
    "abstract": "As Plataformas Móveis estão cada mais enraizadas no nosso quotidiano. O conforto de escolher onde\nusar e a facilidade de as utilizar em qualquer lugar criam formas de informação e comunicação para as\nmais diversas áreas, facilitando assim num grande conjunto de tarefas. Para algumas destas áreas, a\ndisponibilização de uma Plataforma Móvel pode ser a chave para o sucesso dos objetivos pretendidos, oferecendo\nao utilizador uma experiência completamente inovadora face aos métodos mais convencionais.\nA área do ensino encaixa perfeitamente nestes moldes, tendo em conta que o uso destas plataformas\npode aumentar o envolvimento dos alunos nos seus deveres, através da projeção simples e intuitiva\ndos vários exercícios e ferramentas adequadas que fomentem os seus conhecimentos e aprendizagem,\najudando-os a alcançar melhores resultados.\nTendo isso em consideração, neste trabalho de dissertação, desenvolveu-se, numa primeira etapa,\numa fundamentação teórica relativamente ao uso de Plataformas Móveis e a sua evolução ao longo dos\nanos, abordando-se com isso uma perspetiva totalmente voltada para a área do ensino. Numa segunda\nfase, foram detalhados e analisados os diferentes tipos de Plataformas Móveis que atualmente imperam\nno mercado dos smartphones, bem como alguns exemplos de plataformas especialmente criadas para\nservirem como Sistemas de Aprendizagem. Numa última etapa, com a escolha do tipo de plataforma\na desenvolver para a dissertação, idealizou-se e implementou-se um sistema de interação adaptativo\npara suporte a processos de aferição do conhecimento de estudantes ao longo do tempo, em domínios\nespecíficos."
  },
  {
    "keywords": [
      "Codes of ethics",
      "Emerging technologies",
      "Ethical challenges",
      "Information and system technologies’ ethics",
      "Códigos de ética",
      "Desafios éticos",
      "Ética das tecnologias e sistemas de informação",
      "Tecnologias emergentes",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Ethics in IST: challenges and directions for practice and research",
    "autor": "Rebelo, Carla Maria Leite",
    "data": "2023-09-07",
    "abstract": "Ethics, as a set of moral principles that guide an individual's behavior, assisting them in doing\nwhat is right, must be present in any decision taken by technology and information systems professionals, \neven if it is an unconscious process intrinsic to the individual.\nGiven its importance, ethics in technologies and information systems has been a subject under \nstudy for several decades; however, little research has been carried out focused on the ethical challenges \nof emerging technologies and, in particular, how codes of ethics are concerned with answering them. In \norder to fill this gap, a literature review was conducted to identify the ethical challenges of emerging \ntechnologies and to understand the main topics investigated in the literature on codes of ethics. The \ncurrent versions of codes of ethics were analysed, followed by a proposal for an updated Unified Structure \nof Codes of Ethics.\nOverall, this research identified forty-five ethical challenges associated with ten emerging \ntechnologies, of which eighteen are represented in the Unified Structure, indicating that, although there \nis some coverage, there is still room for improvement. This thesis ends with a set of recommendations \nfor future research, encouraging an in-depth study of ethical challenges, combined with an update of the \ncodes of ethics to reflect the results obtained and concluding with the recommendation to study the \nawareness of ethical challenges by technologies and information systems’ actors."
  },
  {
    "keywords": [
      "Arquivos históricos",
      "Gestão de depósitos",
      "DigitArq",
      "Historical archives",
      "Deposit management",
      "DigitArq",
      "681.3:930.25",
      "930.25:681.3"
    ],
    "titulo": "Gestão de depósitos em arquivos",
    "autor": "Marques, Nuno Antunes",
    "data": "2012-12-07",
    "abstract": "Os Arquivos Nacionais e Regionais têm como objectivo principal recolher e preservar a herança arquivística nacional que de alguma forma possuam interesse histórico, como por exemplo, documentos originados em cartórios, tribunais, ou em organismos religiosos. Estes documentos estão tipicamente relacionados com registos de nascimentos/óbitos, processos cíveis, entre outros. Para além de preservar esta informação, os Arquivos disponibilizam também a consulta pública destes documentos a quem assim o desejar.\nDe uma forma geral, estes documentos são organizados, de forma hierárquica num modelo que comporta três níveis, orgânico, funcional e documental, e constituídos fisicamente em depósitos, que correspondem a uma determinada área física do(s) edifício(s) que representam o Arquivo (por exemplo, salas).\nA aplicação DigitArq da KEEP Solutions é um software de gestão de Arquivos, baseada em normas internacionais, que permite a produção e exportação de registos descritivos (informação recolhida nos documentos e outros objetos presentes nos fundos), disponibilizando-os em portais agregadores de conteúdos arquivísticos. Atualmente, esta aplicação permite guardar e obter informação relativa aos documentos e à sua hierarquia lógica nos fundos em que estão inseridos, mas não permite conhecer detalhes sobre os depósitos em que estão inseridos, nem sobre os depósitos em si mesmos.\nÉ neste contexto que esta dissertação é realizada, acompanhando o processo de criação de um novo módulo experimental para o DigitArq que permita realizar a manutenção de depósitos, incluindo a gestão do seu espaço, a identificação da localização física dos seus documentos, e que possua também funcionalidades de optimização dos seus espaços em termos de armazenamento."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Aplicações Web com requisitos de armazenamento e processamento privados",
    "autor": "Couto, Diogo José Linhares",
    "data": "2018",
    "abstract": "O desenvolvimento de aplicações e serviços baseados em web está a crescer todos os dias\ncada vez mais. As facilidades que nos oferecem, entre elas a alta-disponibilidade e acessibilidade,\nlevou a que as grandes empresas de tecnologia investissem neste tipo de tecnologias,\nsurgindo assim aplicações como o Evernote, o Google Photos, o Dropbox, o Slack, entre outras.\nAssociadas à utilização constante destas aplicações e serviços pelos seus clientes estão\nas enormes quantidade de dados criados, bem como os dados gerados a partir destes. Com\na necessidade de armazenar e processar esses de forma rápida e eficiente, estes serviços\ntem vindo a optar pela utilização de serviços de computação em nuvem de terceiros.\nExistem vantagens claras associadas à migração de dados para estas plataformas, desde\na redução de custos associados armazenamento, manutenção e compra de infraestruturas,\naté às conveniências oferecidas pela disponibilização ferramentas de monitorização e\nconfiguração avançadas, entre muitas outras. Associado também à utilização desta plataformas\nde cloud computing estão também os problemas com a privacidade dos dados por elas\narmazenadas. Apesar dos esforços, por parte dos fornecedores destes serviços, em negar\no acesso a entidades não autorizadas, existem ameaças fora do seu controlo e temos visto\nmuitas vezes que o acesso a dados sensíveis por terceiros tem um risco elevado associado.\nCom vista a combater este aspeto existem hoje em dia soluções capazes de garantir a confidencialidade\ndos dados em bases de dados relacionais e não relacionais, através de técnicas\ncriptográficas. Estas soluções estão usualmente associadas a arquiteturas específicas de\nforma a precaverem sempre esta questão de segurança dos dados em todos os momentos.\nEstas arquiteturas implicam um maior esforço computacional do lado do cliente, pois é\ndesse lado que se encontra toda a lóogica da aplicacional e mecanismos de segurança.\nEsta dissertação oferece uma nova arquitetura web onde maior parte do trabalho aplicacional\né delegado para as infraestruturas de nuvem maximizando assim o desempenho\nda aplicação, tirando para isso partido da arquitetura browser servidor característica destes\nsistemas."
  },
  {
    "keywords": [
      "Interpretador",
      "Linguagem de domínio específico",
      "Notação musical",
      "Processamento de linguagens",
      "Domain specific language",
      "Interpreter",
      "Language processing",
      "Music notation",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Musikla: music and keyboard language",
    "autor": "Silva, Pedro Miguel Oliveira da",
    "data": "2020-11-13",
    "abstract": "Nesta dissertação iremos estudar uma abordagem para a análise, criação e descrição de música\natravés de uma Domain Specific Language (DSL). Trata-se de uma linguagem dinâmica, com todas as\nfuncionalidades a que estamos habituados, tais como variáveis, funções, ciclos, condicionais. Para além\ndisso, os dois fatores de diferenciação passam pela sintaxes especializadas para declaração de acom panhamentos musicais e de teclados virtuais. Esta linguagem deve depois poder ser avaliada e os seus\nresultados convertidos para diversos formatos, desde ficheiros de som, MIDI, ou reproduzir diretamente\nas notas para as colunas do computador.\nCom este intuito vamos analisar as linguagens já existentes neste espaço, bem como quais as funci onalidades que já implementam, e aquelas que consideramos estarem em falta.\nComo esses aspetos em mente, de seguida propomos uma linguagem que tente aproveitar as boas\nideias daquilo que já existe, mais as nossas soluções para os novos desafios que encontramos. Introdu zimos também vários casos de estudo para demonstrarem as vantagens que acreditamos existirem na\nnossa abordagem.\nFinalmente descrevemos também o processo de desenvolvimento da linguagem, dividido em três\nfases principais:\n1. O desenho da sintaxe, da sua gramática, e do parser.\n2. A implementação do interpretador.\n3. O desenvolvimento de uma biblioteca standard para ser incluída com a linguagem.\nA nível da sintaxe e da gramática, descrevemos sucintamente toda a linguagem. Damos particular\natenção às expressões de declarações de acompanhamentos musicais e de teclados. Em termos grama ticais, são apenas expressões, ou seja, as suas sintaxes devem integrar-se homogeneamente no resto da\nlinguagem. E como tal, podemos utilizá-las em qualquer sítio que onde podemos introduzir uma expressão,\nseja ela um número, uma string ou o que quer que for.\nEsta integração sem separação significa que todos os aspetos da linguagem têm de ser pensados de\nforma a coexistir sem problemas. Iremos por isso analisar quais os desafios encontrados pela introdução destas novas classes de expressões na gramática, e quais as soluções que foram tomadas para contornar\nessas situações.\nA nível do interpretador, discutimos várias das opções que poderiam ser escolhidas (interpretadores\ntree walk, máquinas bytecode, compilação JIT) bem como justificamos a nossa escolha de utilizar um\ninterpretador tree-walk.\nA nível da biblioteca standard, descrevemos os vários formatos suportados, quer de input, quer de\noutput, bem como os mecanismos providenciados para a utilização de teclados, como grelhas e buffers.\nNo final, descrevemos como correr scripts escritos na nossa linguagem: através de uma aplicação\nde linha de comandos desenvolvida em Python, chamada musikla, publicada no Python Package Index\n(PyPI) 1, e cujo código é disponibilizado livremente no GitHub2."
  },
  {
    "keywords": [
      "eLearning",
      "Reengenharia",
      "Módulos",
      "Serviços",
      "Reengeneering",
      "Modules",
      "Services",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Reengenharia de um sistema de eLearning",
    "autor": "Carvalho, Ricardo Vieira",
    "data": "2023-11-27",
    "abstract": "A integração de módulos de serviços é aceite como uma necessidade no desenvolvimento de sistemas \ncomplexos, uma vez que garante que todos os componentes individuais de um sistema atinjam o \npropósito para o qual o sistema, como um todo, foi desenhado. Essencialmente, construir um sistema\nnão é nada mais que integrar diversas partes num todo. A divisão de um sistema em diversas partes \ngarante uma maior produtividade e qualidade das operações de desenvolvimento e teste, permitindo \nrealizar estas tarefas de forma mais eficiente e focada em cada funcionalidade específica. Isso resulta \nnuma maior produtividade, uma vez que equipas de desenvolvimento podem trabalhar de forma \nparalela e colaborativa em diferentes módulos, agilizando o processo de construção do sistema como \num todo, bem como permite um menor custo de manutenção do próprio sistema. Esta técnica não é \napenas usada para conectar os diversos serviços de um sistema. Permite também fazer a conexão de um \nsistema com outros sistemas externos.\nNeste trabalho de dissertação identificou-se e caracterizou-se as diversas funcionalidades dos serviços \nque se pretendiam integrar num sistema de avaliação de conhecimento, de forma a implementar um \nsistema de eLearning que operasse de forma consistente. Grande parte do trabalho realizado envolveu \na análise de um conjunto de módulos de serviços implementados anteriormente num novo sistema, com \numa nova arquitetura, acolhendo os tradicionais serviços de autenticação e validação de credenciais. A \nnova versão do sistema que alcançámos, resultado de um processo de reengenharia bastante trabalhoso, \npermite definir e suportar processos de avaliação do conhecimento de estudantes em diversos domínios \nde estudo, bem como suporta todos os serviços de gestão e manutenção da informação associada como \na avaliação de estudantes, nomeadamente, os domínios e subdomínios de estudo, as estruturas de \nconhecimento dos processos de avaliação e os serviços de análise relacionados com os resultados \nobtidos pelos alunos."
  },
  {
    "keywords": [
      "Named entity recognition",
      "Archival finding aids",
      "Machine learning",
      "Deep learning",
      "BERT",
      "Data annotation",
      "Reconhecimento de entidades mencionadas",
      "Descrições arquivísticas",
      "Anotação de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Entity recognition in archival descriptions",
    "autor": "Cunha, Luís Filipe da Costa",
    "data": "2022",
    "abstract": "At the moment, there is a vast amount of archival data spread across the Portuguese\narchives, which keeps information from our ancestors’ times to the present\nday. Most of this information was already transcribed to digital format, and the\npublic can access it through archives’ online repositories. Despite that, some of these\ndocuments are structured with many plain text fields without any annotations, making\ntheir content analyses difficult. In this thesis, we implemented several Named\nEntity Recognition solutions to perform a semantic interpretation of the archival\nfinding aids by extracting named entities like Person, Place, Date, Profession, and\nOrganization. These entities translate into crucial information about the context in\nwhich they are inserted. They can be used for several purposes with high confidence\nresults, such as creating smart browsing tools by using entity linking and record\nlinking techniques.\nIn this way, the main challenge of this work was the creation of powerful NER\nmodels capable of producing high confidence results. In order to achieve high result\nscores, we annotated several corpora to train our Machine Learning algorithms in the\narchival domain. We also used different ML architectures such as MaxEnt, CNNs,\nLSTMs, and BERT models. During the model’s validation, we created different\nenvironments to test the effect of the context proximity in the training data.\nFinally, during the model’s training, we noticed a lack of available Portuguese\nannotated data, limiting the potential of several NLP tasks. In this way, we developed\nan intelligent corpus annotator that uses one of our NER models to assist and\naccelerate the annotation process."
  },
  {
    "keywords": [
      "Browser interaction recording",
      "Web elements identification",
      "Web applications testing",
      "Test automation",
      "Minium",
      "Registo de interações com o browser",
      "Identificação de elementos web",
      "Teste de aplicações web",
      "Automatização de testes",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Minium Recorder: browser interaction recording",
    "autor": "Morgado, José Miguel Morais",
    "data": "2016",
    "abstract": "Minium is a framework for automating testing of web applications. It provides an API for automating tests that combines the capabilities of the Selenium WebDriver API for automating interactions with the browser with the convenience of the jQuery API for identifying web elements.\nThe goal of this dissertation was to develop a plugin for Minium that could generate automation scripts by recording user interactions with the browser. The main requirement was that it should be capable of generating a list of expressions, ordered by ease of understandability, to identify each of the elements upon which an interaction is performed.\nPrior to the development of the solution, a research work was conducted. This research work focused on the study of the algorithms for generating expressions to identify web elements and of the techniques and tools for recording interactions with web pages.\nThe developed solution, which will be described in detail, was tested in some web applications with good results."
  },
  {
    "keywords": [
      "Ontologias",
      "Textos não estruturados",
      "Processamento de linguagem natural",
      "Text Mining",
      "Extração automática",
      "Análise de textos",
      "Ontology",
      "Unstructured texts",
      "Natural language processing",
      "Domain",
      "Automatic extraction",
      "Text analysis",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Extração automática de ontologias em textos de culinária não estruturados",
    "autor": "Silva, Bruno Vilas Boas da",
    "data": "2022-12-01",
    "abstract": "A resolução de problemas no âmbito de um domínio específico pode adotar técnicas e ideologias distintas. Para tal, é vital e imperativo elaborar uma análise contextual a todos os elementos pertencentes à teia de relações entre conceitos. Nesse sentido, o uso de uma ontologia permite construir uma rede semântica, no qual a mais importante premissa é a correta identificação dos conceitos e respetivos atributos. A automatização do processo de extração de ontologias permite construir ontologias mais escaláveis e uniformes, extraindo conhecimento assente nas mesmas premissas e padrões. No plano geral, uma extração automática facilita a análise e a leitura de informação de um problema apresentado numa linguagem própria. O trabalho desta dissertação focou-se na extração de conhecimento em textos não estruturados, mais concretamente, textos de culinária, com o intuito de disponibilizar uma ontologia que espelhasse o conhecimento interligado entre receitas. O verdadeiro desafio passa pela correta identificação de termos relevantes, com base em análise sintática, semântica, e linguística em geral, e pela formalização de relações entre os mesmos. A utilização de mecanismos de controlo e de automatização permitiu a extração do conhecimento presente nos textos não estruturados. Estes mecanismos foram aplicados conforme as características linguísticas inerentes aos documentos e restrições de domínio. A ontologia gerada pode ser consultada através de uma plataforma web, na qual o utilizador pode pesquisar os documentos importados no sistema e analisar a interligação entre receitas através da pesquisa por termos e por hiperligações que se encontram nos detalhes de cada registo de culinária."
  },
  {
    "keywords": [
      "Video retrieval",
      "Video summarization",
      "Histogram",
      "Disparity minimization",
      "Greedy algorithm",
      "Machine learning",
      "Recuperação de vídeo",
      "Sumarização de vídeo",
      "Histograma",
      "Minimização de disparidades",
      "Algorítmo ganancioso",
      "Aprendizagem automática",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Optimized video retrieval for interior vehicle monitoring",
    "autor": "Dias, Diogo Barroso",
    "data": "2023-10-09",
    "abstract": "With the rapid growth in the amount of video data, an increasing need for efficient video retrieval\nsystems has become an important problem in the multimedia management topic. Despite having a long\npast, the increase in file size of video collections, caused mostly by the increase of video resolution and\nquantity of videos, originated a big push for applying Machine Learning on the video retrieval subject. In\ntoday’s world, when dealing with Big Data, it’s unfeasible to still rely on video metadata and manually\nannotated videos to provide an accurate video retrieval engine, seeing as the sheer quantity of videos\noverwhelms an inept search and browse system, unable to provide the video the user wants. Therefore, by\nrelying on machine algorithms to accurately mass tag the video collection we achieve great improvements.\nThe process of allocating the video information to the video retrieval framework is severely less time consuming and the viewer has at his disposal more precise and semantically accurate filters. This in turn,\ndrastically reduces the quantity of redundant videos that are pulled from the user’s queries. Another way\nto also ease the time it takes to analyze an immense quantity of videos, is by summarizing the content\nthat is present on them. Condensing dozens of hours, pulled from one or more video streams, into a more\naccessible source of information that displays the most relevant data, is considerably a more efficient\nviewing experience for the user as it unburdens him of the task of surveying a grotesque amount of media\ncontent. The main focus of this thesis is to implement a video summarization method for recapping\nfootage from the interior of a vehicle, that will be integrated on a video retrieval platform that is also being\ndeveloped in parallel."
  },
  {
    "keywords": [
      "Logistics",
      "E-commerce",
      "System integration",
      "Live tracking",
      "Blockchain",
      "Logística",
      "Integração de sistemas",
      "Localização em tempo real",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Continuous monitoring of door-to-door postal service",
    "autor": "Costa, Carlos Daniel Martins da",
    "data": "2021-03-08",
    "abstract": "Logistics services, including express mail delivery areas, have been growing significantly\nby the increase in the volume of e-commerce activity worldwide. It is expected that the\nrise in the level of digital competencies of companies and citizens will not only promote\nconsiderable growth in this sector over the next few years, but also demand higher levels of\nefficiency, quality, and modernization of digital platforms for interaction with customers.\nIn terms of continuous monitoring, new technologies offer potential, namely the use of\nGPS devices to collating coordinates. With system integration, the collected coordinates can\nbe temporarily saved and then sent to a remote server.\nDoor-to-door service requires exact locations, so there are certain technologies, which\nallow us to collect that information accurately without the minimum margin of error. In the\ncontext of door-to-door distribution, most companies have simple technology that provides\na piece of insufficient information regarding the status of their order, they only present\ninformation that the postal service may be delivered, refused, or the addressee may not be\nfound.\nRegarding door-to-door distribution, technologies can be implemented to improve the\ncurrent industry solutions, providing more detailed information about the order status.\nThus, a solution was developed based on international standards, that allow live tracking\napplication ensuring also data security through blockchain technologies."
  },
  {
    "keywords": [
      "Cancer",
      "Viral infection",
      "RNAseq",
      "Cervical carcinoma",
      "Hepatocellular carcinoma",
      "Head and neck squamous cell carcinoma",
      "Cancro",
      "Infecção viral",
      "Carcinoma do colo do útero",
      "Carcinoma hepatocelular",
      "Carcinoma da cabeça e pescoço",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Genomic and transcriptomic analyses in cancers related with viral infection",
    "autor": "Ferreira, Joana Catarina da Rocha",
    "data": "2016",
    "abstract": "In the past 30 years, accumulated evidence has been supporting viral infection as one factor\nresponsible for 15-20% of human malignancies worldwide (W. S. Liang et al. 2014;\nMcLaughlin-Drubin and Munger 2008). Studies on oncogenic viruses have proved their importance\non cellular malfunction along the carcinogenic process, and showed that their association\nwith cancer can amount from 15% to 100% (McLaughlin-Drubin and Munger 2008),\ndepending on the type of tumour. With the large amount of genomic and metagenomic information\navailable on public international consortia, such as TCGA database, it is nowadays\npossible to indirectly infer viral infections from the human centred omics studies, as a portion\nof the reads will align in viruses and bacteria.\nTaking as starting point the research made by Tang et al. 2013, we focused on cervical\n(CESC), hepatocellular (LIHC) and head and neck squamous cell (HNSC) carcinomas, which\nare known to show a high proportion of viral-positive cases (Tang et al. 2013). We downloaded\nRNAseq data from 309, 424 and 566 samples, respectively, and run the unmapped reads against\na reference database of viruses (downloaded from NCBI) by using the tools Batch,\nSAMTOOLS, Bowtie and PRINTSEQ. Quantification of each virus was performed using parts\nper million reads (ppm) and only viruses with ppm above 10 were considered as positively\ninfecting the sample. We confirmed that around 94% of CESC samples were infected, mostly\nby HPV (Human papillomavirus) and specifically by the HPV16 strain. Nearly 32% of LIHC\nwere infected by HBV (hepatitis B virus). Almost 17% of HNSC samples were infected, and\nthe HPV16 was the most common present virus.\nThe evaluation of differential enrichment of metabolic pathways between infected and noninfected\ngroups, for each cancer type, was performed in GSEA. Signs of enrichment for infection\nand immune related pathways were evident in CESC infected group, while in LIHC and\nHNSC infected groups the enrichment was mostly related with DNA replication and repair.\nThis seems to indicate that infection is especially active in CESC, contradicting previous claims\nthat tumorigenesis in cervix was not directly linked with infection. For the three cancer types,\nthe viruses integrate their genome in the host genome, affecting DNA replication, maintenance\nand repair. In our investigation of integration of HPV16 genome in one HNSC tumor sample,\nwe confirmed integration in the human RAD51B gene that codes a protein involved in DNA\nrepair by homologous recombination. We thus confirmed that HPV16 can act both as indirect\nand direct carcinogen. The infection, most probably through the integration of the viral genome in the host genome,\nincreased the amount of somatic mutations in the infected group in LIHC, but not in HNSC\nwhere tobacco consumption is also an important carcinogen. The low number of non-infected\nsamples in CESC did not allow a reliable evaluation of changes in the amount of somatic mutations.\nEven so, in both LIHC and HNSC infected groups, some somatic mutations occurred\nin the context of immune-related pathways, showing that they can contribute to render these\nindividuals susceptible to infection.\nAlso, when checking expression of HPV16 genes in five samples each from CESC and\nHNSC, we confirmed that E6 and E7 genes are amongst the ones more expressed in many\nsamples, while E2 is not expressed. E6 and E7 have been said to be preferentially integrated in\nthe host genome, while E2, which controls their expression, is not integrated or it is disrupted.\nIt is believed that the overexpression of E6 and E7 initiates carcinogenesis.\nThe viral infection rates inferred here from mining the omics databases are very similar to\nthe ones evaluated by standard methods (Tang et al. 2013), showing that public international\nconsortia can indirectly provide interesting insights into the involvement of viral infection in\ntumorigenesis. The high number of samples per tumor, the wide geographic origin of the samples,\nand the high-throughput characterisation for different omics platforms allows multilayer\ncomparisons and evaluations, in a scale not affordable before."
  },
  {
    "keywords": [
      "Estimação de pose",
      "Unidades de medição inercial (IMU)",
      "Redes neuronais",
      "Pose estimation",
      "Neural networks",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desenvolvimento de um modelo de estimativa de pose usando Deep Learning",
    "autor": "Araújo, André Filipe Oliveira",
    "data": "2023-12-15",
    "abstract": "A estimação de pose procura de permitir aos computadores calcular a pose do corpo humano através\nda utilização de sensores Unidades de medição inercial (IMU). Por esta razão, apresenta diversas\nutilizações na indústria, onde pode ser usado na melhoria da colaboração entre humanos e robôs, na qual\npermite aos robôs monitorizar os movimentos humanos. A estimação de pose também tem aplicações\nno setor médico, mais precisamente na avaliação de riscos ergonómicos, reabilitação e desportos.\nEsta dissertação visa o desenvolvimento de um modelo de estimação de pose utilizando Deep Le arning, através da fusão de dados provenientes de sensores acelerómetro, giroscópio e magnetómetro,\nexistentes nos IMU.\nPara alcançar este objetivo foi implementada uma framework em Pytorch, onde estão elaborados\nos modelos para a estimativa de pose, na qual estão incluídos modelos que usam redes neuronais e\nmodelos que usam redes híbridas, que juntam redes neuronais e filtros. Nesta framework, também\nfoi implementado um modelo de calibração de dados (CalibNet), que usa redes neuronais e procura\na diminuição do erro de calibração dos dados provenientes dos sensores, para posteriormente serem\nusados na estimativa de pose.\nNa avaliação dos métodos implementados foram usados dois datasets o Ergowear e o MTw Awinda,\nonde se obtiveram como melhores resultados de 18,78º e de 7,556º, respetivamente, na estimativa de\npose."
  },
  {
    "keywords": [
      "910:681.3",
      "681.3:910",
      "614"
    ],
    "titulo": "Acesso da população aos serviços de saúde",
    "autor": "Pereira, Juliana Vanessa",
    "data": "2012",
    "abstract": "A saúde constitui um direito inegável dos cidadãos. Como tal, é necessário que\no acesso aos cuidados de saúde seja o melhor possível.\nO acesso aos cuidados de saúde pode ser definido como a possibilidade dos\nutentes receberem assistência de forma adequada às necessidades por si apresentadas,\nsejam estas de ordem geográfica, financeira ou temporal, obtendo, assim,\nmelhor qualidade de vida.\nNesta dissertação, o acesso aos cuidados de saúde é analisado na sua componente\ngeográfica. Sendo vários os factores que influenciam a acessibilidade geográfica,\na distância será aquele sobre o qual incidirá este projeto.\nCada vez mais, os sistemas de informação geográfica (SIG) têm sido utilizados\npara mapear e explorar a variação geográfica das necessidades de cuidados de saúde\ne, ainda, desenvolver indicadores inovadores, nesse sentido.\nNo presente trabalho, os SIG serão aplicados à acessibilidade aos serviços de\nsaúde, sendo o mesmo dividido em duas partes principais.\nEm primeiro lugar, pretendeu-se desenvolver uma plataforma de suporte que\npermitisse aos cidadãos decidir quais as formas mais vantajosas de acesso aos diversos\nserviços de saúde. Foi criada uma plataforma de routing, na qual o utilizador\npoderá indicar qual o equipamento de saúde a que pretende chegar e, através da\nreferência do ponto onde se encontra, ser-lhe-á apresentado o melhor trajeto.\nEm segundo lugar, criou-se um mapa de isolinhas do município de Braga, referente\nàs distâncias de acesso ao Hospital de Braga. Esse mapa torna evidente\na distância, quer em quilómetros quer em minutos, da população de Braga ao\nHospital referido.\nPara alcançar os objetivos propostos, foi necessário recolher dados relativos às\nredes de equipamentos de saúde e de corporações de bombeiros, provenientes de diversas fontes. Foi, no entanto, notada a falta de alguma informação correta e\natualizada. Recorreu-se, também, aos dados da população retirados do INE."
  },
  {
    "keywords": [
      "Biomedical text mining",
      "Invasive fungal infections;",
      "Iron overload",
      "Mineração de textos biomédicos",
      "Infeções fúngicas invasivas",
      "Excesso de ferro",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Extracting knowledge from documents related with invasive fungal infections in iron overload context",
    "autor": "Rodrigues, Andreia Dóris Pedras",
    "data": "2021",
    "abstract": "Invasive fungal infections caused by Candida are associated with high mortality and morbidity \nrates in hospitalized patients. Iron plays a major role in these infections, as they are exacerbated under\niron overload conditions. In this context, it is important to understand the association between iron \nlevels and invasive fungal infections, as it can serve as an indicator of the severity of the disease, and\neventually it can help establish measures to improve treatment efficacy.\nNowadays, manually inferring these associations from biomedical documents is a time consuming task, due to the high amount of available scientific text data. As such, these tasks naturally \nbenefit from the Biomedical Text Mining field, which includes a wide variety of methods for automatic \nextraction of high-quality information from biomedical text documents.\nIn this work, relevant documents related to iron overload and fungal infections were retrieved \nfrom PubMed to build a corpus. Then, both Named Entity Recognition and Relation Extraction\nprocesses were executed using the @Note text mining tool. Finally, relevant sentences were manually \nextracted and a curated dataset with documents containing those sentences was created.\nSince the number of publications obtained about Candida and iron overload was very low, the \nanalysis was made taking into account all fungi. A total of 15 publications were considered relevant and \n168 relevant associations were extracted.\nAlthough associations of iron levels with both severity of infection and treatment efficacy were not \nextracted, it was possible to conclude that, in many cases, iron overload is a predictor for fungal \ninfections, and patients’ iron levels highly affect treatment efficacy.\nThe Biomedical Text Mining process described in the present thesis enabled the creation of a \ndataset of relevant biomedical publications containing interesting associations between fungal \ninfections, drugs and associated diseases in a clinical context of iron overload, although in the future \nthis process could be improved, especially regarding dictionaries, in order to obtain a higher number of \nrelevant publications."
  },
  {
    "keywords": [
      "MiddleWare",
      "Micro-Services",
      "Soa architecture",
      "Events",
      "Arquitetura SOA",
      "Eventos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Monitorizações na camada Middleware e os seus desafios",
    "autor": "Ramos, José Lopes",
    "data": "2021-12-17",
    "abstract": "The MiddleWare layer is an abstraction method that acts as an intermediary in a software\ninfastructure implementing interoperability between existing applications, operating systems,\nnetworks and the hardware of a distributed system.\nIt is considered a cross-platform tool capable of providing an essential programming\nabstraction to this type of systems, allowing for easier management of the inherent heterogeneity in these. A Middleware solution application allows a responsible user to orchestrate\nmessage flows, to prepare their contents so that they always reach their destination in the\nformat they need. It also provides the users with the possibility to obtain information in\nreal time regarding the performance of the systems it encompasses, allowing evaluation and\nconsequent action to improve efficiency, in order to achieve the requirements of the systems’\noperation.\nThe project to be developed, aims to take advantage of Internet of Things and Middleware\ntechnologies and concepts, applying them to the creation of a service monitoring tool essential to a more efficient performance of a distributed system. The services and architectures\nto focus with greater attention are Micro-Services, SOA Architecture, ETL processes and\nEvents, of which one will be chosen to be the focus in the development of the project."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Towards quantum program calculation",
    "autor": "Neri, Ana Isabel Carvalho",
    "data": "2018",
    "abstract": "Based on the similarity between the categorial derivation of classical programs from their\nspecification and the category theory approach to quantum physics, this dissertation aims at\nextending the laws of classical program algebra to quantum programming.\nIn this context, the principles of the algebra of classical programs are applied to quantum\nprogramming, in order to verify the feasibility of creating correct-by-construction quantum\ncircuits that can run on quantum devices available in the IBM Q Experience.\nThe reversibility restrictions of quantum circuits are ensured by minimal complements.\nMoreover, measurements are postponed to the end of recursive computations called “quantamorphisms”\nto avoid the collapse of quantum states. Quantamorphisms are classical catamorphisms\nextended to ensure quantum reversibility.\nThe derived quantamorphisms implement quantum cycles (vulg. for-loops) and quantum\nfolds on lists. By Kleisli correspondence, quantamorphisms can be written as monadic functional\nprograms with quantum parameters. This enables the use of Haskell, a monadic functional\nprogramming language, to perform the experimental work. The examples of the calculated\nquantum programs are simulated in Haskell, Quipper and QISKit and run on the\nquantum computers of the IBM Q Experience.\nThe main conclusions of this work are that, while all the simulations produced correspond to\nthe predicted results, running these programs on real quantum devices results in a significant\namount of errors. As quantum devices are constantly evolving, it is likely that in the near\nfuture these devices will increase their reliability, allowing programs to run more accurately.\nThe extension of the quantamorphism concept to more general input structures, such as\nfinite trees, remains a challenge that is left for future work. Also relevant will be the study\nof conditional quantum control without measurements, which will extend the scope of quantamorphisms\nas quantum circuit specifications."
  },
  {
    "keywords": [
      "621.39",
      "681.324"
    ],
    "titulo": "Caracterização de tráfego de rede : Cloud Storage na Universidade do Minho",
    "autor": "Oliveira, Daniela Catarina Ferreira de",
    "data": "2013",
    "abstract": "Monitoring of Internet services reveals that there is a growing trend in the use of Cloud Services.\nConsidering the strong growth on the access to these services in a relatively short period of time,\nit is estimated that shortly they will be responsible for a significant amount of Internet flows.\nIn this context, this project aims to identify and quantify the use of Cloud Services at the\nUniversity of Minho (UM). Achieving this goal involves identifying appropriate techniques for\ntraffic classification and the definition of a model for processing the collected traces. Attending\nto the available set of Cloud Services, this study focuses on characterizing Cloud Storage\nservices, identifying the most accessed Cloud Storage Providers and the characteristics of the\ncorresponding traffic.\nCloud Storage services present several characteristics that turn the current classification methods\ninsufficient or too complex to apply, namely the use of dynamic communication ports and security\nprotocols encrypting the traffic. This motivates the use of a new classification approach based\non Tstat tool, which uses the technique of extracting signatures of servers during the handshake\nof Secure Sockets Layer (SSL) protocol.\nThe obtained results provide global statistics regarding the most used services at UM, focusing\nsubsequently on Cloud Storage services. For these, the top Cloud Storage Providers within\nUM users preferences are identified and the characteristics of the traffic associated to each one\nare discussed."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Aplicações Java em arquiteturas paralelas de acesso não uniforme à memória",
    "autor": "Sá, Carlos Diogo da Silva",
    "data": "2017",
    "abstract": "Durante várias décadas, o aumento do desempenho dos processadores era conseguido\nmaioritariamente através do aumento da frequência de relógio. Contudo, aumentar o desempenho\natravés do aumento da frequência tornou-se cada vez mais difícil conduzindo\na problemas de consumo energético e dissipação de calor. Para resolver estes problemas,\nas arquiteturas mais recentes evoluíram num sentido de aumentar o número de processadores,\ne mais tarde, o número de núcleos. As arquiteturas de memória inicialmente\nadotadas, designadas arquiteturas de acesso uniforme à memória (UMA), apresentaram\nalgumas limitações. Entre as arquiteturas UMA existentes, existe a arquitetura de multiprocessamento\nsimétrico (SMP). Esta arquitetura possui múltiplos processadores que\nacedem à memória principal utilizando um barramento partilhado pelos processadores\nque conduziu a problemas de contenção no acesso à memória principal.\nAs arquiteturas de acesso não uniforme à memória (NUMA) surgiram durante os anos\n90 com múltiplos bancos de memória. No entanto, os programadores que queiram tirar\ntotal partido das vantagens desta arquitetura, terão que lidar com novos desafios ao nível\nda programação, ao nível da afinidade dos fios de execução e das alocações de memória.\nEsta dissertação mostra que a forma como os algoritmos memory-bound acedem a\ndados de memória principal numa arquitetura NUMA, pode ter impacto no seu desempenho.\nUm conjunto de testes foi utilizado para demonstrar em que medida várias técnicas\nde afinidade podem contribuir para aumentar o desempenho de aplicações Java\ne C em arquiteturas NUMA. Os testes realizados com recurso a diferentes abordagens\ntais como ferramentas do sistema operativo, opções da JVM, técnicas de programação\ne até variáveis de afinidade para os compiladores, foram usados para aumentar a desempenho\nde dois casos de estudo em arquiteturas NUMA. A utilização destas abordagens\npermitiram aumentar o desempenho com um ganho adicional de até 1.8 vezes para as\nimplementações em Java, e de até 2.16 vezes para as versões em C das mesmas aplicações."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "An SNMP-based audio distribution service architecture",
    "autor": "Coelho, Vasco Miguel Gonçalves",
    "data": "2018",
    "abstract": "The constant growth of integration and popularity of “Internet of Things”\ndevices is affecting home automation systems, where new technologies were\nintroduced, in the recent years for this particular sector. These automation\nsystems integrate devices that can be anywhere in the house, connected\nto a home network, either through a wire or wireless connection. A home\nautomation system can be used to control air conditioning, lighting, pool\ncontrol systems, home-entertainment systems and much more.\nWithin the field of home-entertainment systems, the best known technologies\nare the Digital Living Network Alliance and the Digital Audio Access\nProtocol, which provide interoperability to allow sharing of digital media\ncontent between devices across a home network. However, these technologies\nhave the disadvantage of being proprietary, maintaining restrict documentation\naccess, complex architectures and concepts and not optimal to specific\npurposes, like audio distribution.\nThe main goal of this project was to prove that is possible to use standardized\nprotocols, such as the Simple Network Manager Protocol and open\nsource tools in order to develop a music distribution service that allows the\nimplementation of similar features than the ones already existing proprietary\ntechnologies. As such, the implementation prototype system allows a user\nto manage and play audio from a music collection that is stored in a single\nhome audio server. The system architecture enables audio streaming between\nthe server and the various devices in the same local network. Further more,\nthe music collection, can integrate virtual audio files that are available from\nexternal music sources, like iTunes, etc."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Utilização de process mining no estabelecimento de ações de web marketing",
    "autor": "Pinto, Filipe de Passos",
    "data": "2018",
    "abstract": "A mineração de processos - process mining - define-se como uma técnica de extração de informação em que, de uma forma automática, se extrai a partir de um registo de eventos informação relevante acerca do desempenho de uma dada organização ou sistema numa dada área de negócio. As técnicas de mineração de processos podem ser aplicadas em diversos contextos aplicacionais, como a informática, a medicina ou o marketing. Relativamente a esta última área, a mineração de processos pode ser utilizada para estabelecer um conjunto de ações que permitam lançar uma dada campanha na Web, tendo como base pontos de maior intensidade de ações Web. Deste modo, nesta dissertação realizou-se um estudo pormenorizado acerca de como estabelecer perfis e preferências de exploração Web a partir da informação obtida através dos processos de navegação e exploração Web dos utilizadores, e definir um conjunto de ações específicas para suporte a uma dada campanha de Web Marketing de forma (semi)automática."
  },
  {
    "keywords": [
      "681.3"
    ],
    "titulo": "Sistemas de recomendação baseados em preferências",
    "autor": "Araújo,  Luís André da Mota",
    "data": "2013",
    "abstract": "“Recommended for you”, a expressão que está a tornar-se progressivamente numa das mais\nvistas no mundo das aplicações. Não é por acaso que, grande parte das aplicações Web (mas não só)\npor onde navegamos diariamente queiram tentar adivinhar e mostrar-nos o que nos agrada e o que\nacham recomendado para nós. De facto, esta apresentação de conteúdos orientados ao utilizador, às\npreferências históricas demonstradas, tem o poder imenso de nos manter a navegar num site, de nos\nfazer comprar mais produtos, de nos fazer voltar à aplicação. Esta “técnica” de recomendação de\nconteúdos que tem como base o estudo do historial do utilizador, das suas decisões tomadas e das\nsuas demonstrações de preferências, assenta na extração de padrões que retratam o percurso\nhistórico do mesmo. Por sua vez, as decisões tomadas por este assentam num padrão preferencial\nexibido pelo utilizador que vai de encontro às características intrínsecas à sua personalidade. Ora, com\neste estudo facilmente se entende que se podem retirar dados que irão caracterizar de forma\ninequívoca um utilizador, criando, desta forma, uma assinatura que o traduz, que o define e o torna\núnico em relação aos demais. É nesta tradução de um conjunto de preferências temporais numa\nassinatura que define intemporalmente um utilizador, que assenta uma parte deste plano de trabalho.\nÉ neste contexto que surge todo este estudo, efetuado durante o presente projeto e sobre o qual\nincidirá esta dissertação. Será difícil encontrar melhores exemplos de conteúdos que caracterizem um\nindivíduo do que a música que ele ouve. “Start and Play” será o conceito inerente a uma peça de\nsoftware que se pretende futuramente construir e que procurará recomendar o que o utilizador deve\nouvir, com base nas preferências demonstradas por este, assente no seu conjunto alargado de ações\npassadas. Estas preferências definirão no seu conjunto a assinatura do utilizador, com a qual o\nsistema irá tentar definir e prever quais os conteúdos musicais que irá recomendar."
  },
  {
    "keywords": [
      "Deep Learning",
      "Machine Learning",
      "Neural Networks",
      "Optical Character Recognition",
      "CNN",
      "RNN",
      "OCR",
      "Text Detection",
      "Text Recognition",
      "Text Spotting",
      "Placard Extraction",
      "Inteligência Artificial",
      "Redes Neuronais",
      "Deteção de Texto",
      "Reconhecimento de Texto",
      "Localização de Texto",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Scene text detection and recognition",
    "autor": "Germano, André Ricardo Covelo",
    "data": "2019-12",
    "abstract": "Detecting and recognizing information contained in urban scenery such as informative boards or billboard advertising is increasingly becoming a trending topic in the machine vi sion community duo to it’s increased utility in automated applications such as, for example, assisted and autonomous software for self-driving vehicles and maintenance assessment of urban signage. Compared to text recognition in documents, which is for the most part solved with state of-art Optical Character Recognition (OCR) algorithms, text detection and recognition in urban scenes still presents several problems to the community. There is no single algorithm that can handle all the difficulties encountered in real-world scenery (scale, perspective, distortions, defocus, occlusion, etc.), making it an arduous task in real world scenes compared with text detection and recognition in documents. Regarding image pre-processing in documents, a simple image translation, such as a rotation or scaling, in most cases is sufficient to overcome some recognition issues whilst in urban scene imagery, text can appear with different alignments, languages and fonts, requiring some sort of sequential pipeline to overcome the difficulties and increase the success rate. The main goal of this dissertation is to explore and reproduce end-to-end state-of-art techniques both in urban scene text detection and recognition, further comparing the top ranked algorithms in a testing environment through several challenging benchmarks. Furthermore, we develop a pipeline combining computer vision and deep learning tech niques to assess the conditioning of informative placards in urban scenery by employing the models with the best results reported in our benchmarks. The pipeline is divided in 3 main components: Placard Extraction, Text Detection and Text Recognition. In the Placard Extraction step we crop placards of interest from the rest of the background, the Text Detection component detects text boxes in the placard and the Text Recognition component predicts character sequences in every text box detected. Additionally, we develop an intuitive front-end prototype displaying some of the results attained throughout our pipeline, showcasing the potential and usability of our research in the assessment and management of street placards."
  },
  {
    "keywords": [
      "Conteúdo multimédia",
      "Deepfake",
      "Conteúdo sintético",
      "Deep learning",
      "Detetor",
      "Multimedia content",
      "Synthetic content",
      "Detector",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Detetor de conteúdo multimédia falso gerado através de algoritmos Deep Fake",
    "autor": "Silva, Leonardo de Jesus",
    "data": "2022",
    "abstract": "sociedade encontra-se a gerar um volume de dados sem precedentes na história. Apesar disso, a\ndesinformação tem vindo a crescer, provocando preocupações no meio jornalístico e democrático. Este\nproblema se tornou ainda mais evidente com os avanços tecnológicos, como a capacidade de manipular\no significado semântico de imagens, vídeos ou áudios. Os resultados destas manipulações impõem\numa dificuldade em distinguir entre conteúdo falso e original. Desta forma, os conteúdos multimédia\nmanipulados são chamados de deepfake. Esta nomenclatura é resultado da combinação dos termos\nDeep Learning (DL) e fake.\nPara solucionar tal problemática trabalhos no estado da arte apresentam diferentes algoritmos para\nclassificação de vídeos deepfake, dos quais utilizam-se de arquiteturas neuronais baseadas em Convolutional\nNeural Network (CNN), Long Short-Term Memory (LSTM) ou redes Transformers. Há trabalhos dos\nquais apresentam resultados positivos ao classificar dados deepfake criados a partir de uma determinada\ntécnica para geração deepfake. No entanto, estes em sua maioria não apresentam resultados quando\nseus modelos são confrontados com dados gerados a partir de outra técnica deepfake distinta dos dados\nutilizados no treino do modelo. Consequentemente, há um lacuna nos trabalhos para uma classificação\nmais generalista independente do método utilizado para criação do conteúdo manipulado.\nDesta forma, as experiências desenvolvidas nesse trabalho utilizaram redes neuronais em diferentes\nestratégias. Resultando na proposta de uma solução para tal lacuna encontrada no estado da arte, a\ncapacidade de um modelo ao classificar um vídeo falso independente da técnica de criação. Após as\nexperiências escolheu-se o algoritmo de multi-classificação para a utilização no detetor. Para uma maior\nprecisão na análise dos vídeos, o detetor possui níveis de confiança (Não Confiável, Nada Confiável, Muito\nConfiável) para cada classificação realizada. Ao validar o detetor, com vídeos das quatro técnicas deepfake\ne vídeos originais, foi possível saber que o nível de confiança ”Muito Confiável”a precisão média é de 91%\nquando a label era deepfake, independente da técnica (DeepFakes, Face2Face, FaceSwap e NeuralTextures)\nutilizada para criá-lo, enquanto que ao rotular dados como reais com o mesmo nível de confiança\nobtém um precisão de 60%."
  },
  {
    "keywords": [
      "SAR",
      "Artificial Intelligence",
      "Machine learning",
      "Deep learning",
      "Neural networks",
      "Segmentation",
      "Computer vision",
      "Inteligência Artificial",
      "Redes neuronais",
      "Segmentação",
      "Visão por computador",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Deteção de padrões em imagens SAR",
    "autor": "Costa, João Carlos da Silva",
    "data": "2022-05-06",
    "abstract": "Since the last century, concerns about safety and welfare issues have increased at the same pace as technolo gical advances. Synthetic Aperture Radar (SAR) is one way of creating images using active sensors, which has\nthe advantage that it can be used in any climate and time of day. Due to its complexity, the use of these images\nis time consuming and requires an experienced user. This study aims to find a way to handle these images, in\nterms of geographic registration and image interpretation, in a more efficient way.\nBased on the literature on geographic registration techniques, image comparison and segmentation in SAR\nimages, those with the greatest potential to solve the problems identified are developed and tested. The analysis\nof the solutions demonstrates that the developed methods have the ability to evolve into complete and efficient\ntools which are capable of answering the identified problems. The results indicate that the SAR-Harris SAR image\ncomparator proves to be a stable enough algorithm to include in a semi-automatic registration tool. Concerning\nthe SAR image classification and segmentation, the U-Net neural network presented values of accuracy near\n70%, exhibiting the ability of the network to integrate a SAR image classification system. In the case of the C Means algorithm, despite not reaching the U-Net accuracy values, accuracy values in range 50-60% , it presents\nitself as an extremely versatile algorithm, also useful in the segmentation task. Still, there is a great margin of\nprogress in the automation of the processes of geographic registration and segmentation of SAR images."
  },
  {
    "keywords": [
      "Monitorização de tráfico veicular",
      "Estratégias de monitorização",
      "Cidade inteligente",
      "Técnicas de monitorização veicular",
      "Vehicle traffic monitoring",
      "Monitoring strategies",
      "Smart city",
      "Vehicle monitoring techniques",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Estratégias de monitorização de tráfego em cidades inteligentes",
    "autor": "Eurico, Isaías Chico Nambissi",
    "data": "2023-12-04",
    "abstract": "Atualmente, mais da metade da população mundial vive em centros urbanos e as estatísticas indicam que\nem 2050 essa percentagem rondará os 70%. A forte concentração da população em urbes, apresenta\ngrandes desafios, principalmente devido à densidade populacional, habitação, circulação, ou a escalabi lidade de serviços. Gerir esta realidade, garantindo as condições indispensáveis para uma alta qualidade\nde vida, é um desafio que as tecnologias inteligentes poderão ajudar a conseguir. A União Europeia de fine cidades inteligentes, ou smart cities, como um conjunto de sistemas e de pessoas que interagem de\nforma inteligente utilizando energia, materiais, serviços e recursos de forma sustentável. Assim, estima-se\nque o valor das tecnologias relacionadas com o controle e a monitorização do tráfego em smart cities é\nproporcional à redução dos acidentes de trânsito, congestionamentos urbanos, e outros impactos sociais.\nSão exemplos, a necessidade de comunicação ou controle de tráfego a partir de ferramentas inteligentes\nque na atualidade é difícil de manipular já que possuem grande impacto económico e social. Para esse\nefeito, é necessário a implementação de técnicas ou estratégias (amostragem, agregação e filtragem) que\nvão permitir monitorar fluxos de dados, a fim de garantir eficiência no tratamento de grandes volumes de\ndados nos múltiplos contextos das cidades. O objetivo desta dissertação é efetuar uma análise critica so bre estratégias de monitorização veicular, seu impacto e suas limitações frentes aos grandes volumes de\ntráfego gerados pelas smart cities. Avaliam-se ainda técnicas contextuais que serviram para a construção\nde soluções frente aos desafios da mobilidade e transportabilidade no contexto urbano."
  },
  {
    "keywords": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "titulo": "Development of an automated pipeline for meta-omics data analysis",
    "autor": "Costa, João Carlos Sequeira",
    "data": "2017",
    "abstract": "Knowing what lies around us has been a goal for many decades now, and the new advances in sequencing technologies and in meta-omics approaches have permitted to start answering some of the main questions of microbiology - what is there, and what is it doing?\nThe exponential growth of omics studies has been answered by the development of\nsome bioinformatic tools capable of handling Metagenomics (MG) analysis, with a scarce\nfew integrating such analysis with Metatranscriptomics (MT) or Metaproteomics (MP) studies.\nFurthermore, the existing tools for meta-omics analysis are usually not user friendly,\nusually limited to command-line usage.\nBecause of the variety in meta-omics approaches, a standard workflow is not possible,\nbut some routines exist, which may be implemented in a single tool, thereby facilitating\nthe work of laboratory professionals. In the framework of this master thesis, a pipeline for\nintegrative MG and MT data analysis was developed. This pipeline aims to retrieve comprehensive\ncomparative gene/transcript expression results obtained from different biological\nsamples. The user can access the data at the end of each step and summaries containing several\nparameters of evaluation of the previous step, and final graphical representations, like\nKrona plots and Differential Expression (DE) heatmaps. Several quality reports are also\ngenerated. The pipeline was constructed with tools tested and validated for meta-omics\ndata analysis. Selected tools include FastQC, Trimmomatic and SortMeRNA for preprocessing,\nMetaSPAdes and Megahit for assembly, MetaQUAST and Bowtie2 for reporting on\nthe quality of the assembly, FragGeneScan and DIAMOND for annotation and DeSEQ2 for\nDE analysis.\nFirstly, the tools were tested separately and then integrated in several python wrappers to\nconstruct the software Meta-Omics Software for Community Analysis (MOSCA). MOSCA\nperforms preprocessing of MG and MT reads, assembly of the reads, annotation of the\nassembled contigs, and a final data analysis.\nReal datasets were used to test the capabilities of the tool. Since different types of files\ncan be obtained along the workflow, it is possible to perform further analyses to obtain\nadditional information and/or additional data representations, such as metabolic pathway\nmapping."
  },
  {
    "keywords": [
      "Formal methods",
      "Behavioural specifications",
      "Automatic specification repair",
      "Alloy",
      "Métodos formais",
      "Especificações temporais",
      "Reparação automatica de especificações",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "titulo": "Automatic repair of behavioural specifications",
    "autor": "Cerqueira, Jorge Gabriel Alves",
    "data": "2022",
    "abstract": "Somewhat worryingly, software is becoming increasingly complex with the passing of time. Even\nthough society has become completely dependent on it, there’s still not enough quality teaching and tooling\nto help software engineers verify the correctness of their solutions. Furthermore, quickly put together\nsolutions are often incentivized over a more rigorous approach.\nSoftware is always bound to have bugs. However, formal specification languages allow the modeling\nof complex systems by specifying the relevant entities, how they interact, and testing the expected\nguarantees. Hence, helping developers gain valuable understanding of the systems they work with. This\napproach has the drawbacks of not only being time costly, adding another step in the development process\nthat requires deep understanding of the problem, but also being difficult to learn. The cause is due to the\nmore abstract nature of specification compared to programming, paired with the need to be comfortable\nworking with formal logic concepts.\nAlloy is a formal specification language capable of structural and behavioral analysis. It is a popular\nframework for validating and verifying requirements, in part due to its expressiveness and flexibility.\nThis makes it a prime candidate to develop and experiment new automatic repair techniques. They can\nhelp experienced developers speed up the process of writing specifications and new developers to learn\nquicker. With this in mind, some work has been done on repairing flawed structural Alloy models, but\nnone considering behavioral aspects.\nThus, this thesis presents an overview of the Alloy language, along with previously proposed automatic\nrepair techniques; it proposes the first mutation-based technique for the automatic repair of first-order\ntemporal logic specifications using Alloy6; also, it describes the integration of an automatic hint generation\nsystem for Alloy4Fun, an online platform for teaching Alloy."
  },
  {
    "keywords": [
      "Recém-nascidos de muito baixo peso",
      "CRIB",
      "SNAPPE-II",
      "Metodologia ROC",
      "Very low birth weight infants",
      "ROC methodology",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Avaliação do desempenho de indicadores com base na metodologia ROC (Receiver Operating Characteristic)",
    "autor": "Araújo, Joana Margarida Rodrigues Barros de",
    "data": "2015-03-04",
    "abstract": "O objetivo deste estudo é avaliar e comparar o desempenho de dois indicadores de previsão de risco de mortalidade\nneonatal para recém-nascidos de muito baixo peso (<1500g), o CRIB (Clinical Risk Index for Babies) e o SNAPPE\nII (Score for Neonatal Acute Physiology-Perinatal Extension II), com recurso à metodologia ROC (Receiver Operating\nCharacteristic).\nA execução prática deste estudo foi suportada com auxílio a programas estatísticos próprios para a análise da\nmetodologia ROC, como o SPSS, ROCNPA, Comp2ROC, ROCR e caTools.\nOs dados que contemplam o presente estudo foram recolhidos pelas unidades de cuidados intensivos neonatais\ndo território português entre 2010 e 2012, e enviados para o Registo Nacional de Recém-Nascidos de Muito Baixo\nPeso (RNMBP), que é a entidade responsável pelo armazenamento desta informação.\nSerá aferida também a comparação e avaliação de variáveis de elevada expressão na previsão da mortalidade,\nque compõem os indicadores de mortalidade em estudo, sendo elas, o Peso à Nascença e a Idade Gestacional.\nA amostra em estudo é composta por 789 recém-nascidos de muito baixo peso, dos quais 51,3% são do género\nmasculino. Em média os recém-nascidos em questão apresentam um peso médio ao nascimento de 1214 g ±343,1\ne 29,8 ±2,5 semanas de gestação e, dos integrantes na amostra 11,3% foram declarados óbitos hospitalares.\nA exatidão dos indicadores de mortalidade e das variáveis foi obtida através do cálculo da AUC, área abaixo da\ncurvaROC,queparaoCRIBfoide0,876±0,025,paraoSNAPPE-IIde0,867±0,026,seguindo-sedasvariáveisidade\ngestacional e o peso ao nascimento com 0,785 ±0,032 e 0,782 ±0,028, respetivamente.\nCom base nos resultados obtidos durante a elaboração do presente estudo, o CRIB provou ser melhor em pre\ndizer a mortalidade para recém-nascidos de muito baixo peso, e tem a seu favor um menor número de variáveis\ncomparativamente ao SNAPPE-II."
  },
  {
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Deteção de pontos negros em sistemas de ETL",
    "autor": "Dias, Nuno Miguel Monteiro Soares",
    "data": "2017",
    "abstract": "Os sistemas de povoamento de data warehouses, vulgarmente designados por sistema de ETL –\nExtract-Transform-Load –, constituem a base de qualquer sistema de data warehousing. No\nentanto, poucas são as vezes em que a sua implementação ocorre de uma forma linear, metódica,\nseguindo um dado modelo de trabalho devidamente comprovado. Usualmente, estes sistemas\nestabelecem uma “ponte” entre os sistemas operacionais, muitas vezes de natureza diversa, e os\nsistemas de data warehousing, de forma a que seja possível assegurar o povoamento dos seus\ndata warehouses, de uma forma regular e atual. Como tal, é muito normal terem que lidar com um\nvolume de dados considerável e envolvendo processos de tratamento bastante complexos. Esses\nprocessos, que representam trabalho extra para o ETL, só são necessários devido à da elevada\npermeabilidade dos sistemas operacionais que facilitam a ocorrência de fenómenos de\ninconsistência e de omissão de valores. Para que tal não aconteça, as atuais técnicas e modelos de\nimplementação baseados em processos típicos de “tentativa-erro” deverão ser abandonados desde\ninício, dando lugar a uma arquitetura pensada com vista num melhor desempenho evitando,\nassim, situações em que um aumento no volume de dados do processo, tende a revelar um efeito\n“bola de neve” em termos do nível de performance do sistema. Neste trabalho de dissertação\ndesenvolvemos uma técnica baseada em process mining que, recorrendo aos registos de execução\ndetalhados de um processo ETL - logs -, permite descobrir todo o processo ETL a montante. Na\nposse dos dados relativos a cada passo de execução do processo ETL (tempo médio de execução,\nfrequência absoluta, etc), podemos definir um modelo matemático que ilustra o “bem-estar”, ou\nseja, o desempenho do nosso sistema através da correlação de todas estas variáveis. Desta forma,\nao torná-lo acessível aos administradores dos sistemas, introduzimos um novo paradigma no\ndesenvolvimento e manutenção de processos ETL, mais preocupado com questões como a\nperformance ou um conhecimento mais aprofundado do impacto das decisões arquiteturais que\nsão tomadas, nomeadamente a nível da escolha de componentes para executar cada passo do\nnosso ETL."
  },
  {
    "keywords": [
      "Inteligência Artificial",
      "Reconhecimento de objetos",
      "Robô ZECA",
      "Perturbação do Espectro do Autismo (PEA)",
      "You Only Look Once (YOLO)",
      "Artificial intelligence",
      "Image recognition",
      "ZECA robot",
      "Autism Spectrum Disorder (ASD)",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Desenvolvimento de um sistema de ensino para crianças com Perturbação do Espectro do Autismo com recurso a um robô humanoide",
    "autor": "Alves, David Miguel Duarte Rodrigues",
    "data": "2023-06-14",
    "abstract": "Progressivamente, os robôs tendem a ser mais cooperativos e inteligentes, para que possam \ninteragir de forma natural com os seres humanos. Uma tipologia de robôs frequentemente \nutilizada na interação com os seres humanos são os robôs humanoides, uma vez que se \nassemelham fisicamente à aparência do corpo humano e são capazes de prestar auxílio na \nrealização de diversas tarefas do quotidiano. Ao longo da última década, investigadores têm \nestudado a viabilidade do uso de robôs humanoides no estímulo da interação social em crianças \ncom Perturbação do Espectro do Autismo (PEA). O autismo é um transtorno do \nneurodesenvolvimento que se manifesta precocemente, onde o comportamento dos indivíduos \né caracterizado por padrões repetitivos, atividades ou interesses restritos e pela dificuldade na \ncomunicação/ interação social.\nNeste contexto, a presente dissertação tem como objetivo o desenvolvimento de um sistema de \nensino interativo, capaz de promover o desenvolvimento sócio emocional em crianças com PEA\nutilizando, como mediador, um robô humanoide. O sistema desenvolvido possui uma arquitetura \ncapaz de incorporar diferentes atividades de ensino e utiliza o algoritmo You Only Look Once\n(YOLO) para deteção de objetos em imagens. Com a finalidade de testar o sistema desenvolvido, \nforam desenvolvidas duas atividades de ensino, nomeadamente, o ensino de figuras geométricas \ne cores. \nFinalmente, o sistema desenvolvido foi testado de duas formas distintas: a) sem o robô \nhumanoide, sendo este substituído pela voz do computador; b) com robô humanoide, sendo este \nresponsável pela interação robô-humano. Os resultados revelam que o sistema de ensino \ndesenvolvido é capaz de detetar, com elevada percentagem de precisão, as figuras geométricas \ne as cores pré-definidas na lista de atividades, podendo assim ser uma ferramenta promissora no \nensino de crianças com dificuldades de aprendizagem."
  },
  {
    "keywords": [
      "Visão por computador",
      "Realidade aumentada",
      "Análise de cena",
      "Contexto",
      "Objetos",
      "Reconhecimento",
      "Computer vision",
      "Augmented reality",
      "Scene understanding",
      "Context",
      "Objects",
      "Recognition",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Apreensão e discretização de ambientes tangíveis em sistemas de realidade aumentada",
    "autor": "Silva, André Filipe Proença e",
    "data": "2018",
    "abstract": "A Realidade Aumentada (RA) caracteriza-se pela mistura de elementos virtuais no mundo real de forma interativa e em tempo real. O conceito de RA levanta uma ampla variedade de questões quanto à coerência visual entre os objetos reais e virtuais num ambiente. De forma a melhorar o processo de inclusão destes elementos no meio físico foram criadas várias técnicas e algoritmos de visão por computador que através do mapeamento de espaços físicos, extração de características e marcadores fiduciais de objetos, verificação, deteção, identificação, classificação, entre outros, permitem analisar e estruturar o conteúdo de uma cena.\nO maior desafio que se coloca com a realização desta proposta de dissertação encontra-se associado à forma como é extraída e processada a informação que conseguimos obter a partir dos sensores que complementam os dispositivos de RA hoje em dia, a fim de representar e compreender, da melhor forma possível, os ambientes que nos rodeiam e preparar um espaço apto para a introdução e apresentação de conteúdo virtual com a maior harmonia.\nNeste documento é possível encontrar o estado da arte relativo aos temas previamente citados a fim de explorar, melhorar e desenvolver novas técnicas e paradigmas para, a partir da informação dos sensores mais genéricos encontrados em muitas das tecnologias móveis e óculos de realidade aumentada mais atuais, extrair várias características do cenário e objetos envolventes em tempo real. O processamento e tratamento desta informação tem como objetivo final realizar o reconhecimento e compreensão da cena e objetos que se encontram no espaço que rodeia estes sensores.\nEm paralelo à realização desta proposta de dissertação, foi desenvolvida uma framework denominada “Tangible Environments in Augmented Reality Systems (TEARS)” com o objetivo de demonstrar tudo o que é discutido neste documento não só como algo para fins de investigação científica, mas também para utilização e apoio num projeto e protótipo realizado no âmbito da unidade curricular do 5ºAno do Mestrado Integrado em Engenharia Informática (MIEI) de Projeto em Engenharia Informática (PEI) e que apresenta o título: “Assistência Remota com Realidade Mista (ARRM)”."
  },
  {
    "keywords": [
      "Aprendizagem automática",
      "Aprendizagem profunda",
      "Estimativa da pose humana",
      "Reconhecimento de expressões faciais",
      "TensorFlow",
      "OpenCV",
      "MediaPipe",
      "Machine learning",
      "Deep learning",
      "Human pose estimation",
      "Facial expression recognition",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "titulo": "Sistema de apoio a pessoas idosas baseado em aprendizagem automática",
    "autor": "Fontes, Rui Miguel Carvalho da Silva",
    "data": "2024-02-19",
    "abstract": "No presente trabalho foi desenvolvida uma solução de apoio a pessoas idosas, doentes\nou com limitações físicas, baseada em técnicas de aprendizagem automática e visão por\ncomputador. Os modelos de aprendizagem profunda utilizados resolvem problemas de\nclassificação de imagens. Para desenvolver a solução proposta foi utilizada a linguagem\nPython e as bibliotecas TensorFlow, MediaPipe e OpenCV. Antes de treinar os modelos de\naprendizagem automática, foram aplicadas técnicas de pré processamento às imagens,\npara as preparar para os modelos classificadores. A solução final desenvolvida combina\nduas tarefas de classificação diferentes: a estimação da pose humana e o reconhecimento\nde expressões faciais. Para estimar a pose humana, primeiro utilizou-se um algoritmo\nque identifica a posição das articulações do corpo, sendo estas posições posteriormente\nclassificadas com uma rede neuronal convolucional em três classes, queda, sentado e em\npé/a andar. Para efetuar o reconhecimento de expressões faciais utilizaram-se dois tipos\nde dados, os atributos de cor dos pixeis das imagens e os pontos de referência das faces\npreviamente identificadas. Estes dados foram depois classificados por uma rede neuronal\nhíbrida, que inclui uma rede completamente ligada a uma rede convolucional. A solução\nfinal proposta combina estes dois modelos, o que permite a partir de uma imagem de uma\npessoa, gerar um aviso se o modelo de estimação da pose detetar uma queda ou quando o\nmodelo de reconhecimento de expressões faciais identificar uma expressão de dor. O modelo\nde estimação da pose identificou a classe queda com uma precisão de 97%, um recall de 98%\ne uma acurácia de 97%. A expressão de dor foi identificada pelo modelo de reconhecimento\nde expressões faciais com uma precisão de 82%, um recall de 86% e uma acurácia de 92%.\nO maior desafio no reconhecimento da expressão foi a deteção da face nas imagens."
  },
  {
    "keywords": [
      "Diabetes mellitus",
      "Lentes de contacto",
      "Grafeno",
      "Transístores de efeito de campo de grafeno com gate eletrolítica",
      "Contact lenses",
      "Graphene",
      "Electrolyte-gated graphene field-effect transistors",
      "Engenharia e Tecnologia::Nanotecnologia"
    ],
    "titulo": "Functional graphene-based coatings for contact lenses and glucose sensing",
    "autor": "Lopes, Vicente Silva",
    "data": "2024-02-07",
    "abstract": "Diabetes mellitus, uma doença caracterizada por níveis elevados de açúcar no sangue, é frequentemente \nmonitorizada através de protocolos invasivos e dolorosos, o que resulta em redundância entre os doentes. \nEsta monitorização é geralmente efetuada através do rastreio dos níveis de açúcar no sangue. Devido à \nsua grande intimidade com o sangue, as lágrimas estão a ser consideradas como um outro potencial bio \nfluído de diagnóstico. As lágrimas contêm múltiplos biomarcadores relacionados com várias doenças \nsistémicas e, como tal, podem ser utilizadas para monitorizar, diagnosticar e tratar estas doenças \nutilizando protocolos não invasivos. No entanto, as concentrações dos analitos são muito mais baixas \nnas lágrimas do que no sangue. Estão a ser desenvolvidos dispositivos específicos para detetar essas \nbaixas concentrações. As lentes de contacto, em particular, são utilizadas por milhões de pessoas com \nproblemas de visão e têm potencial para serem transformadas em dispositivos portáteis funcionais. As \nlentes de contacto necessitam de materiais de elétrodos biocompatíveis e estáveis para serem integrados \nem plataformas de biossensores. O grafeno, uma camada atomicamente fina de átomos de carbono, \napresenta propriedades favoráveis, como a biocompatibilidade, a elevada condutividade elétrica, a fácil \nfuncionalização e a flexibilidade. Devido à sua espessura atómica e excelente mobilidade de portadores, \neste material pode ser utilizado para fabricar transístores de efeito de campo de grafeno para bio deteção, \ncom potencial para atingir uma sensibilidade ultraelevada.\nEste trabalho explora tecnologias baseadas em grafeno para permitir uma plataforma inovadora para a \ndeteção de glucose, potencialmente integrada em lentes de contacto. Para tal, foram testadas e \notimizadas as condições para o fabrico de uma lente de contacto à base de grafeno. Em primeiro lugar, \nfoi abordada a transferência do grafeno para a superfície altamente complexa das lentes de contacto. \nEm segundo lugar, foi estudada uma funcionalização do grafeno, concebida para induzir as reações \nquímicas envolvidas na deteção da glucose, nas várias fases intermédias, para compreender as \ninterações fundamentais que ocorrem. Por último, foram utilizados transístores de efeito de campo de \ngrafeno com gate eletrolítica, fabricados com o grafeno funcionalizado, para detetar a glucose numa \nvasta gama de concentrações."
  }
]